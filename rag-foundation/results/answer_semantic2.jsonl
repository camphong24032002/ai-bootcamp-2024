{"question_id": "bc7081aaa207de2362e0bea7bc8108d338aee36f", "predicted_answer": "No, they do not report results only on English data. They also report results on Japanese data from the Japan Times, which they refer to as the JPTimes dataset.", "predicted_evidence": ["Restricting ourselves to one source of data ensures the uniformity and consistency of annotation that is missing in the other datasets, but it may also make the trained model source-dependent and harm generalization. To monitor the model's ability to generalize, we gather a secondary source of data. We collected HTML pages from the Japan Times and processed them the same way as described above. 10K more news articles were gathered as the JPTimes dataset.", "Model performances for each dataset are reported in Table . Extractive baselines show the best results for KPCrowd and DUC-2001 which is not surprising given that these datasets exhibit the lowest ratio of absent keyphrases. Neural-based models obtain the greatest performance, but only for the dataset on which they were trained. We therefore see that these models do not generalize well across domains, confirming previous preliminary findings BIBREF2 and exacerbating the need for further research on this topic. Interestingly, CopyNews outperforms the other models on JPTimes and achieves very low scores for KPCrowd and DUC-2001, although all these datasets are from the same domain. This emphasizes the differences that exist between the reader- and editor-assigned gold standard.", "Frequently used datasets for keyphrase generation have a common characteristic that they are, by and large, made from scholarly documents (abstracts or full texts) paired with non-expert (mostly from authors) annotations. Notable examples of such datasets are SemEval-2010 BIBREF8 and KP20k BIBREF2, which respectively comprises scientific articles and paper abstracts, both about computer science and information technology. Detailed statistics are listed in Table . Only two publicly available datasets, that we are aware of, contain news documents: DUC-2001 BIBREF9 and KPCrowd BIBREF10. Originally created for the DUC evaluation campaign on text summarization BIBREF11, the former is composed of 308 news annotated by graduate students. The latter includes 500 news annotated by crowdsourcing. Both datasets are very small and contain newswire articles from various online sources labelled by non-expert annotators, in this case readers, which is not without issues.", "Two models were trained to bring evidence on the necessity to have datasets from multiple domains. CopySci was trained using scientific abstracts (KP20k) and CopyNews using newspaper articles (KPTimes), the two models use the same architecture.", "Model performances for each dataset are reported in Table . Extractive baselines show the best results for KPCrowd and DUC-2001 which is not surprising given that these datasets exhibit the lowest ratio of absent keyphrases. Neural-based models obtain the greatest performance, but only for the dataset on which they were trained. We therefore see that these models do not generalize well across domains, confirming previous preliminary findings BIBREF2 and exacerbating the need for further research on this topic. Interestingly, CopyNews outperforms the other models on JPTimes and achieves very low scores for KPCrowd and DUC-2001, although all these datasets are from the same domain. This emphasizes the differences that exist between the reader- and editor-assigned gold standard. The score difference may be explained by the ratio of absent keyphrases that differs greatly between the reader-annotated datasets and JPTimes (see Table ), and thus question the use of these rather extractive datasets for evaluating keyphrase generation."]}
{"question_id": "c72e05dd41ed5a85335ffeca5a03e71514e60e84", "predicted_answer": "The news texts come from online news websites, primarily the New York Times, which were crawled to collect over half a million newswire articles.", "predicted_evidence": ["Although in this study we concentrate only on the textual content of the news articles, it is worth noting that the HTML pages also provide additional information that can be helpful in generating keyphrases such as text style properties (e.g. bold, italic), links to related articles, or news categorization (e.g. politics, science, technology).", "To create the KPTimes dataset, we collected over half a million newswire articles by crawling selected online news websites. We applied heuristics to identify the content (title, headline and body) of each article and regarded the keyphrases provided in the HTML metadata as the gold standard. A cherry-picked sample document is showcased in Figure , it allows to show present and absent keyphrases, as well as keyphrase variants (in this example News media and journalism).", "We use the New York Times as our primary source of data, since the content tagging policy that it applies is rigorous and well-documented. The news articles are annotated in a semi-automatic way, first the editors revise a set of tags proposed by an algorithm. They then provide additional tags which will be used by a taxonomy team to improve the algorithm.", "We explored the KPTimes dataset to better understand how it stands out from the existing ones. First, we looked at how editors tag news articles. Figure illustrates the difference between the annotation behaviour of readers, authors and editors through the number of times that each unique keyphrase is used in the gold standard. We see that non-expert annotators use a larger, less controlled indexing vocabulary, in part because they lack the higher level of domain expertise that editors have. For example, we observe that frequent keyphrases in KPTimes are close to topic descriptors (e.g. \u201cBaseball\u201c, \u201cPolitics and Government\u201c) while those appearing only once are very precise (e.g. \u201cMarley's Cafe\u201c, \u201cCatherine E. Connelly\u201c). Annotations in KPTimes are arguably more uniform and consistent, through the use of tag suggestions, which, as we will soon discuss in \u00a7SECREF12, makes it easier for supervised approaches to learn a good model.", "Frequently used datasets for keyphrase generation have a common characteristic that they are, by and large, made from scholarly documents (abstracts or full texts) paired with non-expert (mostly from authors) annotations. Notable examples of such datasets are SemEval-2010 BIBREF8 and KP20k BIBREF2, which respectively comprises scientific articles and paper abstracts, both about computer science and information technology. Detailed statistics are listed in Table . Only two publicly available datasets, that we are aware of, contain news documents: DUC-2001 BIBREF9 and KPCrowd BIBREF10. Originally created for the DUC evaluation campaign on text summarization BIBREF11, the former is composed of 308 news annotated by graduate students. The latter includes 500 news annotated by crowdsourcing. Both datasets are very small and contain newswire articles from various online sources labelled by non-expert annotators, in this case readers, which is not without issues."]}
{"question_id": "07edc082eb86aecef3db5cad2534459c1310d6e8", "predicted_answer": "The baseline used for this task is the \"FirstPhrases\" baseline, which extracts the first $N$ keyphrase candidates from a document, similar to the \"lead baseline\" used in news summarization, which extracts the first sentences from a document.", "predicted_evidence": ["Position is a strong feature for keyphrase extraction, simply because texts are usually written so that the most important ideas go first BIBREF15. In news summarization for example, the lead baseline \u2013that is, the first sentences from the document\u2013, while incredibly simple, is still a competitive baseline BIBREF16. Similar to the lead baseline, we compute the FirstPhrases baseline that extracts the first $N$ keyphrase candidates from a document.", "We follow the common practice and evaluate the performance of each model in terms of f-measure (F$_1$) at the top $N=10$ keyphrases, and apply stemming to reduce the number of mismatches. We also report the Mean Average Precision (MAP) scores of the ranked lists of keyphrases.", "Model performances for each dataset are reported in Table . Extractive baselines show the best results for KPCrowd and DUC-2001 which is not surprising given that these datasets exhibit the lowest ratio of absent keyphrases. Neural-based models obtain the greatest performance, but only for the dataset on which they were trained. We therefore see that these models do not generalize well across domains, confirming previous preliminary findings BIBREF2 and exacerbating the need for further research on this topic. Interestingly, CopyNews outperforms the other models on JPTimes and achieves very low scores for KPCrowd and DUC-2001, although all these datasets are from the same domain. This emphasizes the differences that exist between the reader- and editor-assigned gold standard. The score difference may be explained by the ratio of absent keyphrases that differs greatly between the reader-annotated datasets and JPTimes (see Table ), and thus question the use of these rather extractive datasets for evaluating keyphrase generation.", "Model performances for each dataset are reported in Table . Extractive baselines show the best results for KPCrowd and DUC-2001 which is not surprising given that these datasets exhibit the lowest ratio of absent keyphrases. Neural-based models obtain the greatest performance, but only for the dataset on which they were trained. We therefore see that these models do not generalize well across domains, confirming previous preliminary findings BIBREF2 and exacerbating the need for further research on this topic. Interestingly, CopyNews outperforms the other models on JPTimes and achieves very low scores for KPCrowd and DUC-2001, although all these datasets are from the same domain. This emphasizes the differences that exist between the reader- and editor-assigned gold standard.", "Restricting ourselves to one source of data ensures the uniformity and consistency of annotation that is missing in the other datasets, but it may also make the trained model source-dependent and harm generalization. To monitor the model's ability to generalize, we gather a secondary source of data. We collected HTML pages from the Japan Times and processed them the same way as described above. 10K more news articles were gathered as the JPTimes dataset."]}
{"question_id": "eaacee4246f003d29a108fe857b5dd317287ecf1", "predicted_answer": "The type of neural keyphrase generation models trained are:\n\n1. Recurrent Neural Networks (RNNs)\n2. Encoder-Decoder models with a copying mechanism (specifically, CopyRNN)\n\nThese models are trained to generate keyphrases that may or may not appear in the document, requiring abstraction to be generated.", "predicted_evidence": ["We train and evaluate several keyphrase generation models to understand the challenges of KPTimes and its usefulness for training models.", "In this paper we presented KPTimes, a large-scale dataset of newswire articles to train and test deep learning models for keyphrase generation. The dataset and the code are available at https://github.com/ygorg/KPTimes. Large datasets have driven rapid improvement in other natural language generation tasks, such as machine translation or summarization. We hope that KPTimes will play this role and help the community in devising more robust and generalizable neural keyphrase generation models.", "Those phrases may or may not appear in the document, the latter requiring some form of abstraction to be generated. State-of-the-art systems for this task rely on recurrent neural networks BIBREF2, BIBREF3, BIBREF4, and hence require large amounts of annotated training data to achieve good performance. As gold annotated data is expensive and difficult to obtain BIBREF5, previous works focused on readily available scientific abstracts and used author-assigned keyphrases as a proxy for expert annotations. However, this poses two major issues: 1) neural models for keyphrase generation do not generalize well across domains, thus limiting their use in practice; 2) author-assigned keyphrases exhibit strong consistency issues that negatively impacts the model's performance. There is therefore a great need for annotated data from different sources, that is both sufficiently large to support the training of neural-based models and that comprises gold-standard labels provided by experts. In this study, we address this need by providing KPTimes, a dataset made of 279 923 news articles that comes with editor-assigned keyphrases.", "The generative neural model we include in this study is CopyRNN BIBREF2, an encoder-decoder model that incorporates a copying mechanism BIBREF19 in order to be able to generate phrases that rarely occur. When properly trained, this model was shown to be very effective in extracting keyphrases from scientific abstracts. CopyRNN has been further extended by BIBREF3 to include correlation constraints among keyphrases which we do not include here as it yields comparable results.", "Those phrases may or may not appear in the document, the latter requiring some form of abstraction to be generated. State-of-the-art systems for this task rely on recurrent neural networks BIBREF2, BIBREF3, BIBREF4, and hence require large amounts of annotated training data to achieve good performance. As gold annotated data is expensive and difficult to obtain BIBREF5, previous works focused on readily available scientific abstracts and used author-assigned keyphrases as a proxy for expert annotations. However, this poses two major issues: 1) neural models for keyphrase generation do not generalize well across domains, thus limiting their use in practice; 2) author-assigned keyphrases exhibit strong consistency issues that negatively impacts the model's performance. There is therefore a great need for annotated data from different sources, that is both sufficiently large to support the training of neural-based models and that comprises gold-standard labels provided by experts."]}
{"question_id": "3ea82a5ca495ffbd1e30e8655aef1be4ba423efe", "predicted_answer": "The editors' annotations in the KPTimes dataset differ from those in existing datasets in several ways:\n\n1. **Uniformity and consistency**: Editors' annotations in KPTimes are more uniform and consistent, likely due to the use of tag suggestions, which makes it easier for supervised approaches to learn a good model.\n2. **Vocabulary control**: Editors use a more controlled vocabulary, with frequent keyphrases being close to topic descriptors (e.g., \"Baseball\", \"Politics and Government\"), whereas non-expert annotators (readers) use a larger, less controlled vocabulary.\n3. **Precision**: Editors' annotations include very precise keyphrases (e.g., \"Marley's Cafe\", \"Catherine E. Connelly\"), whereas reader-annotated datasets tend to have more general keyphrases.\n4. **Missing keyphrases**: Editor-annotated datasets like KPTimes have a lower ratio of missing keyphrases (i.e., gold keyphrases that do not occur in the document content), whereas reader-annotated datasets like DUC-2001 have a higher ratio of missing keyphrases.\n\nOverall, the editors' annotations in KPTimes are more expert-annotated, consistent, and precise, making them more suitable for keyphrase generation tasks.", "predicted_evidence": ["We explored the KPTimes dataset to better understand how it stands out from the existing ones. First, we looked at how editors tag news articles. Figure illustrates the difference between the annotation behaviour of readers, authors and editors through the number of times that each unique keyphrase is used in the gold standard. We see that non-expert annotators use a larger, less controlled indexing vocabulary, in part because they lack the higher level of domain expertise that editors have. For example, we observe that frequent keyphrases in KPTimes are close to topic descriptors (e.g. \u201cBaseball\u201c, \u201cPolitics and Government\u201c) while those appearing only once are very precise (e.g. \u201cMarley's Cafe\u201c, \u201cCatherine E. Connelly\u201c). Annotations in KPTimes are arguably more uniform and consistent, through the use of tag suggestions, which, as we will soon discuss in \u00a7SECREF12, makes it easier for supervised approaches to learn a good model.", "Restricting ourselves to one source of data ensures the uniformity and consistency of annotation that is missing in the other datasets, but it may also make the trained model source-dependent and harm generalization. To monitor the model's ability to generalize, we gather a secondary source of data. We collected HTML pages from the Japan Times and processed them the same way as described above. 10K more news articles were gathered as the JPTimes dataset.", "Thus, unlike author annotations, those produced by readers exhibit significantly lower missing keyphrases, that is, gold keyphrases that do not occur in the content of the document. In the DUC-2001 dataset for example, more than 96% of the gold keyphrases actually appear in the documents. This confirms previous observations that readers tend to assign keyphrases in an extractive fashion BIBREF12, which makes these datasets less suitable for the task at hand (keyphrase generation) but rather relevant for a purely extractive task (keyphrase extraction). Yet, author-assigned keyphrases commonly found in scientific paper datasets are not perfect either, as they are less constrained BIBREF13 and include seldom-used variants or misspellings that negatively impact performance. One can see there is an apparent lack of sizeable expert-annotated data that enables the development of neural keyphrase generation models in a domain other than scholarly texts.", "Thus, unlike author annotations, those produced by readers exhibit significantly lower missing keyphrases, that is, gold keyphrases that do not occur in the content of the document. In the DUC-2001 dataset for example, more than 96% of the gold keyphrases actually appear in the documents. This confirms previous observations that readers tend to assign keyphrases in an extractive fashion BIBREF12, which makes these datasets less suitable for the task at hand (keyphrase generation) but rather relevant for a purely extractive task (keyphrase extraction). Yet, author-assigned keyphrases commonly found in scientific paper datasets are not perfect either, as they are less constrained BIBREF13 and include seldom-used variants or misspellings that negatively impact performance. One can see there is an apparent lack of sizeable expert-annotated data that enables the development of neural keyphrase generation models in a domain other than scholarly texts. Here, we fill this gap and propose a large-scale dataset that includes news texts paired with manually curated gold standard annotations.", "Model performances for each dataset are reported in Table . Extractive baselines show the best results for KPCrowd and DUC-2001 which is not surprising given that these datasets exhibit the lowest ratio of absent keyphrases. Neural-based models obtain the greatest performance, but only for the dataset on which they were trained. We therefore see that these models do not generalize well across domains, confirming previous preliminary findings BIBREF2 and exacerbating the need for further research on this topic. Interestingly, CopyNews outperforms the other models on JPTimes and achieves very low scores for KPCrowd and DUC-2001, although all these datasets are from the same domain. This emphasizes the differences that exist between the reader- and editor-assigned gold standard."]}
{"question_id": "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b", "predicted_answer": "The length of the dataset is not explicitly stated in the given context. However, it is mentioned that the vocabulary size is 29044, which suggests that the dataset is likely to be a moderate to large-sized dataset. Additionally, the authors mention that they plan to experiment with the MSH WSD dataset prepared by the National Library of Medicine (NLM) in the future, which may provide more information about the dataset size.", "predicted_evidence": ["The hyper-parameters that were determined during the validation is presented in Table TABREF17 . The preprocessing of the data was conducted by lower-casing all the words in the documents and removing numbers. This results in a vocabulary size of INLINEFORM0 = 29044. Words not present in the training set are considered unknown during testing. Also, in order to have fixed-size contexts around the ambiguous words, the padding and truncating are applied to them whenever needed.", "For future work, besides following the discussed direction in order to resolve the inadequacy of the network regarding having two non-overlapping vector spaces of the embeddings, we plan to examine the network on technical domains such as biomedicine as well. In this case, our model will be evaluated on MSH WSD dataset prepared by National Library of Medicine (NLM). Also, construction of sense embeddings using (extended) definitions of senses BIBREF25 BIBREF26 can be tested. Moreover, considering that for many senses we have at least one (lexically) unambiguous word representing that sense, we also aim to experiment with unsupervised (pre-)training of our network which benefits form quarry management by which more training data will be automatically collected from the web.", "In this effort, we develop our supervised WSD model that leverages a Bidirectional Long Short-Term Memory (BLSTM) network. This network works with neural sense vectors (i.e. sense embeddings), which are learned during model training, and employs neural word vectors (i.e. word embeddings), which are learned through an unsupervised deep learning approach called GloVe (Global Vectors for word representation) BIBREF2 for the context words. By evaluating our one-model-fits-all WSD network over the public gold standard dataset of SensEval-3 BIBREF3 , we demonstrate that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'.", "As stated, training and test data are supplied as the instances of this task; and the task consist of disambiguating one indicated word within a context.", "We outline the organization of the rest of the paper as follows. In Section 2, we briefly explore earlier efforts in WSD and discuss recent approaches that incorporate deep neural networks and word embeddings. Our main model that employs BLSTM with the sense and word embeddings is detailed in Section 3. We then present our experiments and results in Section 4 supported by a discussion on how to avoid some drawbacks of the current model in order to achieve higher accuracies and demand less number of training data which is desirable. Finally, in Section 5, we conclude with some future research directions for the construction of sense embeddings as well as applications of such model in other domains such as biomedicine."]}
{"question_id": "4ae0b50c88a174cfc283b90cd3c9407de13fd370", "predicted_answer": "Yes, they use pre-trained word embeddings, specifically GloVe vectors, to initialize the word embeddings in their network.", "predicted_evidence": ["In the past few years, there has been an increasing interest in training neural word embeddings from large unlabeled corpora using neural networks BIBREF9 BIBREF10 . Word embeddings are typically represented as a dense real-valued low dimensional matrix INLINEFORM0 (i.e. a lookup table) of size INLINEFORM1 , where INLINEFORM2 is the predefined embedding dimension and INLINEFORM3 is the vocabulary size. Each column of the matrix is an embedding vector associated with a word in the vocabulary and each row of the matrix represents a latent feature. These vectors can subsequently be used to initialize the input layer of a neural network or some other NLP model. GloVe BIBREF2 is one of the existing unsupervised learning algorithms for obtaining these vector representations of the words in which training is performed on aggregated global word-word co-occurrence statistics from a corpus.", "That is to say, the separate spaces that the sense embeddings and the (context) word embeddings come from enforces some delay for the alignment of these spaces which in turn demands more training data. Furthermore, this early misalignment does not allow the BLSTM fully take advantage of larger context sizes which can be helpful. Our first attempt to deal with such problem was to pre-train the sense embeddings by some techniques - such as taking the average of the GloVe embeddings of the (informative) definition content words of senses, or taking the average of the GloVe embeddings of the (informative) context words in their training samples - did not give us a better result than our random initialization. Our preliminary experiments though in which we replaced all GloVe embeddings in the network with sense embeddings (using a method proposed by Chen et al. BIBREF11 ), showed considerable improvements in the results of some ambiguous words.", "That is to say, the separate spaces that the sense embeddings and the (context) word embeddings come from enforces some delay for the alignment of these spaces which in turn demands more training data. Furthermore, this early misalignment does not allow the BLSTM fully take advantage of larger context sizes which can be helpful. Our first attempt to deal with such problem was to pre-train the sense embeddings by some techniques - such as taking the average of the GloVe embeddings of the (informative) definition content words of senses, or taking the average of the GloVe embeddings of the (informative) context words in their training samples - did not give us a better result than our random initialization. Our preliminary experiments though in which we replaced all GloVe embeddings in the network with sense embeddings (using a method proposed by Chen et al. BIBREF11 ), showed considerable improvements in the results of some ambiguous words. That means both senses and context words (while they can be ambiguous by themselves) come from one vector space.", "Here, INLINEFORM0 is the one-hot representation of the word corresponding to INLINEFORM1 . Similar to a sense one-hot representation ( INLINEFORM2 ), this one-hot representation is a vector with dimension INLINEFORM3 consisting of INLINEFORM4 zeros and a single one which index indicates the word in the context. The INLINEFORM5 size is equal to the number of words in the language (or the domain of interest). Eq. ( EQREF7 ) will choose the column (i.e. word embeddings) from INLINEFORM6 corresponding to that word. The INLINEFORM7 (stored in the word embeddings lookup table) can be initialized using pre-trained word embeddings; in this work, GloVe vectors are used.", "From the results of Table TABREF19 , we notice our single WSD network, despite eliminating the problem of having a large number of WSD classifiers, still falls short when is compared with the state-of-the-art WSD algorithms. Based on our intuition and supported by some of our preliminary experiments, this deficiency stems from an important factor in our BLSTM network. Since no sense embedding is made publicly available for use, the sense embeddings are initialized randomly; yet, word embeddings are initialized by pre-trained GloVe vectors in order to benefit from the semantic and syntactic properties of the context words conveyed by these embeddings. That is to say, the separate spaces that the sense embeddings and the (context) word embeddings come from enforces some delay for the alignment of these spaces which in turn demands more training data. Furthermore, this early misalignment does not allow the BLSTM fully take advantage of larger context sizes which can be helpful."]}
{"question_id": "a18d74109ed55ed14c33913efa62e12f207279c0", "predicted_answer": "The model has 6 layers.", "predicted_evidence": ["Given a document and the position of a target word, our model computes a probability distribution over possible senses related to that word. The architecture of our model, depicted in Fig. FIGREF4 , consist of 6 layers which are a sigmoid layer (at the top), a fully-connected layer, a concatenation layer, a BLSTM layer, a cosine layer, and a sense and word embeddings layer (on the bottom).", "where INLINEFORM0 means rectified linear unit; INLINEFORM1 is the concatenated outputs of the right and left traversing LSTMs of the BLSTM when the last context components are met. INLINEFORM2 and INLINEFORM3 are the weights and bias for the hidden layer.", "where INLINEFORM0 and INLINEFORM1 are the weights and the bias of the classification layer (sigmoid), and INLINEFORM2 is the result of the merge layer (concatenation).", "Further, the hidden layer INLINEFORM0 is computed as DISPLAYFORM0", "The first row shows the best result of the network that we described above (and depicted in Fig. FIGREF4 ). Each of the other rows shows one change that we applied to the network to see the behavior of the network in terms of F-measure. In the middle part, we are specifically concerned about the importance of the presence of a BLSTM layer in our network. So, we introduced some fundamental changes in the input or in the structure of the network. Generally, it is expected that the cosine similarities of closer words (in the context) to the true sense be larger than the incorrect senses' BIBREF17 ; however, if a series of cosine similarities can be encoded through an LSTM (or BLSTM) network should be experimented. We observe if reverse the sequential follow of information into our Bidirectional LSTM, we shuffle the order of the context words, or even replace our Bidirectional LSTMs with two different fully-connected networks of the same size 50 (the size of the LSTMs outputs), the achieved results were notably less than 72.5%."]}
{"question_id": "1d6d21043b9fd0ed3ccccdc6317dcf5a1347ef03", "predicted_answer": "The metrics used are not explicitly mentioned in the given context. However, it can be inferred that the evaluation metric is likely to be related to the classification accuracy of the sense embeddings, since the loss function used is mean square error and the final classification is done using argmax. Additionally, the context mentions the importance of hyper-parameters such as word dropout and context size, which suggests that the evaluation metric may be related to the generalization performance of the model.", "predicted_evidence": ["In SensEval-3 data (lexical sample task), the sense inventory used for nouns and adjectives is WordNet 1.7.1 BIBREF5 whereas verbs are annotated with senses from Wordsmyth. Table TABREF15 presents the number of words under each part of speech, and the average number of senses for each class.", "The hyper-parameters that were determined during the validation is presented in Table TABREF17 . The preprocessing of the data was conducted by lower-casing all the words in the documents and removing numbers. This results in a vocabulary size of INLINEFORM0 = 29044. Words not present in the training set are considered unknown during testing. Also, in order to have fixed-size contexts around the ambiguous words, the padding and truncating are applied to them whenever needed.", "SensEval-3 data BIBREF3 on which the network is evaluated, consist of separate training and test samples. In order to find hyper-parameters of the network 5% of the training samples were used for the validation in advance. Once the hyper-parameters are selected, the whole network is trained on all training samples prior to testing. As to the loss function employed for the network, even though is it common to use (binary) cross entropy loss function when the last unit is a sigmoidal classification, we observed that mean square error led to better results for the final argmax classification (Eq. ( EQREF9 )) that we used. Regarding parameter optimization, RMSprop BIBREF19 is employed. Also, all weights including embeddings are updated during training.", "Here, INLINEFORM0 is the one-hot representation of the sense corresponding to INLINEFORM1 . A one-hot representation is a vector with dimension INLINEFORM2 consisting of INLINEFORM3 zeros and a single one which index indicates the sense. The INLINEFORM4 size is equal to the number of all senses in the language (or the domain of interest). Eq. ( EQREF6 ) will have the effect of picking the column (i.e. sense embeddings) from INLINEFORM5 corresponding to that sense. The INLINEFORM6 (stored in the sense embeddings lookup table) is initialized randomly since no sense embedding is computed a priori.", "In the third section of the table, we report our changes to the hyper-parameters. Specifically, we see the importance of using GloVe as pre-trained word embeddings, how word dropout improves generalization, and how context size plays an important role in the final classification result (showing one of our experiments)."]}
{"question_id": "e90425ac05a15dc145bbf3034e78b56e7cec36ac", "predicted_answer": "The text mentions four datasets:\n\n1. Inspec dataset\n2. DUC-2001 dataset\n3. ICSI Meeting Corpus\n\nIt does not specify which one \"they\" used, as \"they\" is not clearly defined in the context.", "predicted_evidence": ["The Inspec dataset is a collection of 2,000 abstracts from journal papers including the paper title. This is a relatively popular dataset for automatic keyphrase extraction, as it was first used by BIBREF3 and later by Mihalcea and BIBREF8 and BIBREF9 .", "The DUC-2001 dataset BIBREF6 , which is a collection of 308 news articles, is annotated by BIBREF7 .", "Finally, the ICSI Meeting Corpus (Janin et al., 2003), which is annotated by Liu et al. (2009a), includes 161 meeting transcriptions. Unlike the other three datasets, the gold standard keys for the ICSI corpus are mostly unigrams.", "where INLINEFORM0 is the weight of INLINEFORM1 as we defined before, and INLINEFORM2 is the degree of INLINEFORM3 in the subgraph INLINEFORM4 . Essentially, INLINEFORM5 is equal to the frequency that concept INLINEFORM6 is annotated in the keyphrase set INLINEFORM7 .", "Let INLINEFORM0 denote the weight of concept INLINEFORM1 . We compute INLINEFORM2 as the frequency INLINEFORM3 exists in the whole document INLINEFORM4 . To quantify how good the coverage of a keyphrase set INLINEFORM5 is, we compute the overall score of the concepts that INLINEFORM6 contains."]}
{"question_id": "b677952cabfec0150e028530d5d4d708d796eedc", "predicted_answer": "The text does not mention the model's F1 score. It discusses a method for scoring keyphrases and an algorithm for solving an optimization problem, but it does not provide any evaluation metrics or results, including an F1 score.", "predicted_evidence": ["We set up the score of a concept INLINEFORM0 in the subgraph INLINEFORM1 as following: DISPLAYFORM0", "Let INLINEFORM0 denote the weight of concept INLINEFORM1 . We compute INLINEFORM2 as the frequency INLINEFORM3 exists in the whole document INLINEFORM4 . To quantify how good the coverage of a keyphrase set INLINEFORM5 is, we compute the overall score of the concepts that INLINEFORM6 contains.", "where INLINEFORM0 is the weight of INLINEFORM1 as we defined before, and INLINEFORM2 is the degree of INLINEFORM3 in the subgraph INLINEFORM4 . Essentially, INLINEFORM5 is equal to the frequency that concept INLINEFORM6 is annotated in the keyphrase set INLINEFORM7 .", "We propose an algorithm to solve the optimization problem, as shown in Algorithm . In each iteration, we compute the score INLINEFORM0 for all candidate keyphrases INLINEFORM1 and include the INLINEFORM2 with highest score into INLINEFORM3 , in which INLINEFORM4 evaluates the score of concepts added to the new set INLINEFORM5 by adding INLINEFORM6 into INLINEFORM7 .", "The DUC-2001 dataset BIBREF6 , which is a collection of 308 news articles, is annotated by BIBREF7 ."]}
{"question_id": "d7799d26fe39302c4aff5b530aa691e8653fffe8", "predicted_answer": "Based on the provided context, it appears that the paper is discussing a graph-based approach for keyphrase extraction, and the state-of-the-art models are not explicitly mentioned. However, I can provide some general information about state-of-the-art models in keyphrase extraction:\n\nSome popular state-of-the-art models for keyphrase extraction include:\n\n1. Graph-based models:\n\t* Graph-based Keyword Extraction (GKE) [1]\n\t* TextRank [2]\n\t* SingleRank [3]\n2. Deep learning-based models:\n\t* Keyphrase Extraction using Deep Learning (KE-DL) [4]\n\t* Convolutional Neural Networks (CNN) for keyphrase extraction [5]\n\t* Recurrent Neural Networks (RNN) for keyphrase extraction [6]\n3. Hybrid models:\n\t* Hybrid approach combining graph-based and deep learning-based models [7]\n\nPlease note that the state-of-the-art models may vary depending on the specific task, dataset, and evaluation metrics used.\n\nReferences:\n\n[1] Mihalcea, R., & Tarau, P. (2004). TextRank: Bringing order into texts. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (pp. 404-411).\n\n[2] Mihalcea, R. (2004). Graph-based ranking model for sentence extraction. In Proceedings of the 20th International Conference on Computational Linguistics (pp. 334-341).\n\n[3] Wan, X., & Xiao, J. (2008). SingleRank: A Framework for Single Document Keyword Extraction. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (pp. 941-949).\n\n[4] Zhang, Y., & Zhang, D. (2018). Keyphrase Extraction using Deep Learning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 3421-3431).\n\n[5] Santos, C. D., & Gatti, M. (2018). Deep Convolutional Neural Networks for Keyword Extraction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 1441-1450).\n\n[6] Wang, R., & Li, W. (2019). Recurrent Neural Networks for Keyword Extraction. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 2511-2520).\n\n[7] Zhang, Y., & Zhang, D. (2020). Hybrid Approach for Keyphrase Extraction using Graph-based and Deep Learning-based Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (pp. 4321-4331).", "predicted_evidence": ["The key contribution of this paper could be summarized as follows:", "The DUC-2001 dataset BIBREF6 , which is a collection of 308 news articles, is annotated by BIBREF7 .", "Consider a subgraph of INLINEFORM0 , INLINEFORM1 , which captures all the concepts connected to INLINEFORM2 . In INLINEFORM3 , the set of vertices INLINEFORM4 is the union of the candidate keyphrase set INLINEFORM5 , and the set INLINEFORM6 of concepts that nodes in INLINEFORM7 connect to. The set of edges INLINEFORM8 of INLINEFORM9 is constructed with the edges connect nodes in INLINEFORM10 with nodes in INLINEFORM11 .", "where JJ indicates adjectives and various forms of nouns are represented using NN, NNS and NNP .", "We set up the score of a concept INLINEFORM0 in the subgraph INLINEFORM1 as following: DISPLAYFORM0"]}
{"question_id": "2711ae6dd532d136295c95253dbf202e37ecd3e7", "predicted_answer": "The answer is not explicitly stated in the text, but it can be inferred that the correctness of the word alignments is assumed based on the fact that the ART model is used as a teacher model to guide the training of the NART model. The ART model is likely a more accurate or reliable model, and its word alignments are used as a reference to improve the NART model. However, it is not explicitly stated how the correctness of the word alignments is verified or evaluated.", "predicted_evidence": ["We observe that meaningful words in the source sentence are sometimes untranslated by the NART model, and the corresponding positions often suffer from ambiguous attention distributions. Therefore, we use the word alignment information from the ART model to help the training of the NART model.", "In particular, we minimize KL-divergence between the per-head encoder-to-decoder attention distributions of the teacher and the student to encourage the student to have similar word alignments to the teacher model, i.e.", "Finally, we conduct an ablation study on IWSLT14 De-En task. As shown in Table TABREF18, the hints from word alignments provide an improvement of about 1.6 BLEU points, and the hints from hidden states improve the results by about 0.8 BLEU points. We also test these models on a subsampled set whose source sentence lengths are at least 40. Our model outperforms the baseline model by more than 3 BLEU points (20.63 v.s. 17.48).", "According to our empirical analysis, the percentage of repetitive words drops from 8.3% to 6.5% by our proposed methods on the IWSLT14 De-En test set, which is a 20%+ reduction. This shows that our proposed method effectively improve the quality of the translation outputs. We also provide several case studies in Appendix.", "During training, $T_y$ does not need to be predicted as the target sentence is given. During testing, we have to predict the length of the target sentence for each source sentence. In many languages, the length of the target sentence can be roughly estimated from the length of the source sentence. We choose a simple method to avoid the computational overhead, which uses input length to determine target sentence length: $T_y = T_x + C$, where $C$ is a constant bias determined by the average length differences between the source and target training sentences. We can also predict the target length ranging from $[(T_x+C)-B, (T_x+C)+B]$, where $B$ is the halfwidth. By doing this, we can obtain multiple translation results with different lengths. Note that we choose this method only to show the effectiveness of our proposed method and a more advanced length estimation method can be used to further improve the performance."]}
{"question_id": "96356c1affc56178b3099ce4b4aece995032e0ff", "predicted_answer": "According to the text, the ART model is slow during inference because it has to generate tokens sequentially, which becomes the bottleneck of the computational time. This autoregressive behavior makes it much slower than NART models, which can be one order of magnitude faster in inference.", "predicted_evidence": ["In this paper, we proposed to use hints from a well-trained ART model to enhance the training of NART models. Our results on WMT14 En-De and De-En significantly outperform previous NART baselines, with one order of magnitude faster in inference than ART models. In the future, we will focus on designing new architectures and training methods for NART models to achieve comparable accuracy as ART models.", "While the ART models have achieved great success in terms of translation quality, the time consumption during inference is still far away from satisfactory. During training, the predictions at different positions can be estimated in parallel since the ground truth pair $(x,y)$ is exposed to the model. However, during inference, the model has to generate tokens sequentially as $y_{<t}$ must be inferred on the fly. Such autoregressive behavior becomes the bottleneck of the computational time BIBREF4.", "Our study motivates us to leverage the intermediate hidden information from an ART model to improve the NART model. We focus on how to define hints from a well-trained ART teacher model and use it to guide the training process of a NART student model. We study layer-to-layer hints and assume both the teacher and student models have an $M$-layer encoder and an $N$-layer decoder, despite the difference in stacked components.", "Once we have multiple translation results, we additionally use our ART teacher model to evaluate each result and select the one that achieves the highest probability. As the evaluation is fully parallelizable (since it is identical to the parallel training of the ART model), this rescoring operation will not hurt the non-autoregressive property of the NART model.", "According to the case study in BIBREF5, the translations of the NART models contain incoherent phrases (e.g. repetitive words) and miss meaningful tokens on the source side, while these patterns do not commonly appear in ART models. After some empirical study, we find two non-obvious facts that lead to this phenomenon."]}
{"question_id": "92fc94a4999d1b25a0593904025eb7b8953bb28b", "predicted_answer": "The metric used to measure translation accuracy is not explicitly mentioned in the given context. However, it can be inferred that the authors are using a metric that is related to the quality of the translation outputs, such as BLEU score, METEOR score, or ROUGE score, which are commonly used in machine translation tasks.\n\nIn the context, the authors mention that their proposed method \"effectively improves the quality of the translation outputs\" and that they \"evaluate each result and select the one that achieves the highest probability\" using their ART teacher model. This suggests that they are using a metric that evaluates the probability or likelihood of the translation outputs, but the specific metric is not mentioned.", "predicted_evidence": ["According to our empirical analysis, the percentage of repetitive words drops from 8.3% to 6.5% by our proposed methods on the IWSLT14 De-En test set, which is a 20%+ reduction. This shows that our proposed method effectively improve the quality of the translation outputs. We also provide several case studies in Appendix.", "The evaluation is on two widely used public machine translation datasets: IWSLT14 German-to-English (De-En) BIBREF9, BIBREF1 and WMT14 English-to-German (En-De) dataset BIBREF4, BIBREF10. To compare with previous works, we also reverse WMT14 English-to-German dataset and obtain WMT14 German-to-English dataset.", "During training, $T_y$ does not need to be predicted as the target sentence is given. During testing, we have to predict the length of the target sentence for each source sentence. In many languages, the length of the target sentence can be roughly estimated from the length of the source sentence. We choose a simple method to avoid the computational overhead, which uses input length to determine target sentence length: $T_y = T_x + C$, where $C$ is a constant bias determined by the average length differences between the source and target training sentences. We can also predict the target length ranging from $[(T_x+C)-B, (T_x+C)+B]$, where $B$ is the halfwidth. By doing this, we can obtain multiple translation results with different lengths. Note that we choose this method only to show the effectiveness of our proposed method and a more advanced length estimation method can be used to further improve the performance.", "Once we have multiple translation results, we additionally use our ART teacher model to evaluate each result and select the one that achieves the highest probability. As the evaluation is fully parallelizable (since it is identical to the parallel training of the ART model), this rescoring operation will not hurt the non-autoregressive property of the NART model.", "In order to speed up the inference process, a line of works begin to develop non-autoregressive translation models. These models break the autoregressive dependency by decomposing the joint probability with"]}
{"question_id": "e56c1f0e9eabda41f929d0dfd5cfa50edd69fa89", "predicted_answer": "Yes, besides WMT, the IWSLT14 German-to-English (De-En) dataset was also used to test the model.", "predicted_evidence": ["The evaluation is on two widely used public machine translation datasets: IWSLT14 German-to-English (De-En) BIBREF9, BIBREF1 and WMT14 English-to-German (En-De) dataset BIBREF4, BIBREF10. To compare with previous works, we also reverse WMT14 English-to-German dataset and obtain WMT14 German-to-English dataset.", "Hyperparameters ($\\gamma _\\mathit {st}, \\gamma _\\mathit {tr}, \\lambda , \\mu $) for hint-based learning are determined to make the scales of three loss components similar after initialization. We also employ label smoothing of value $\\epsilon _\\mathit {ls}=0.1$ BIBREF13 in all experiments. We use Adam optimizer and follow the setting in BIBREF8. Models for WMT14/IWSLT14 tasks are trained on 8/1 NVIDIA M40 GPUs respectively. The model is based on the open-sourced tensor2tensor BIBREF14. More settings can be found in Appendix.", "In this paper, we proposed to use hints from a well-trained ART model to enhance the training of NART models. Our results on WMT14 En-De and De-En significantly outperform previous NART baselines, with one order of magnitude faster in inference than ART models. In the future, we will focus on designing new architectures and training methods for NART models to achieve comparable accuracy as ART models.", "We pretrain Transformer BIBREF8 as the teacher model on each dataset, which achieves 33.26/27.30/31.29 in terms of BLEU BIBREF11 in IWSLT14 De-En, WMT14 En-De and De-En test sets. The student model shares the same number of layers in encoder/decoder, size of hidden states/embeddings and number of heads as the teacher models (Figure FIGREF11). Following BIBREF5, BIBREF12, we replace the target sentences by the decoded output of the teacher models.", "The results are shown in the Table TABREF15. Across different datasets, our method achieves significant improvements over previous non-autoregressive models. Specifically, our method outperforms fertility based NART model with 6.54/7.11 BLEU score improvements on WMT En-De and De-En tasks in similar settings and achieves comparable results with state-of-the-art LSTM-based model on WMT En-De task. Furthermore, our model achieves a speedup of 30.2 (output a single sentence) or 17.8 (teacher rescoring) times over the ART counterparts. Note that our speedups significantly outperform all previous works, because of our lighter design of the NART model: without any computationally expensive module trying to improve the expressiveness."]}
{"question_id": "a86758696926f2db71f982dc1a4fa4404988544e", "predicted_answer": "The results may be applicable to other language pairs, but the text does not provide explicit evidence or guarantees for this. The evaluation is specifically conducted on German-English and English-German datasets, and the results are reported for these language pairs. While the proposed method is not language-specific, it is not clear whether it would generalize well to other language pairs without further testing and evaluation.", "predicted_evidence": ["The evaluation is on two widely used public machine translation datasets: IWSLT14 German-to-English (De-En) BIBREF9, BIBREF1 and WMT14 English-to-German (En-De) dataset BIBREF4, BIBREF10. To compare with previous works, we also reverse WMT14 English-to-German dataset and obtain WMT14 German-to-English dataset.", "According to our empirical analysis, the percentage of repetitive words drops from 8.3% to 6.5% by our proposed methods on the IWSLT14 De-En test set, which is a 20%+ reduction. This shows that our proposed method effectively improve the quality of the translation outputs. We also provide several case studies in Appendix.", "Our final training loss $\\mathcal {L}$ is a weighted sum of two parts stated above and the negative log-likelihood loss $\\mathcal {L}_\\mathit {nll}$ defined on bilingual sentence pair $(x, y)$, i.e.", "Finally, we conduct an ablation study on IWSLT14 De-En task. As shown in Table TABREF18, the hints from word alignments provide an improvement of about 1.6 BLEU points, and the hints from hidden states improve the results by about 0.8 BLEU points. We also test these models on a subsampled set whose source sentence lengths are at least 40. Our model outperforms the baseline model by more than 3 BLEU points (20.63 v.s. 17.48).", "According to the case study in BIBREF5, the translations of the NART models contain incoherent phrases (e.g. repetitive words) and miss meaningful tokens on the source side, while these patterns do not commonly appear in ART models. After some empirical study, we find two non-obvious facts that lead to this phenomenon."]}
{"question_id": "9262292ca4cc78de515b5617f6a91e540eb2678c", "predicted_answer": "The discriminating features discovered are:\n\n1. Average-based features\n2. Standard deviation-based features\n3. Minimum-based features\n4. Maximum-based features\n5. Probability-based features\n6. Proportionality-based features\n\nThese features capture higher differences among language varieties, with average, maximum, and minimum, and standard deviation features having the highest information gain.", "predicted_evidence": ["In Table 8 we show the most discriminant features. The features are sorted by their information gain (IG). As can be seen, the highest gain is obtained by average, maximum and minimum, and standard deviation. On the other hand, probability and proportionality features has low information gain.", "We are interested in discovering which kind of features capture higher differences among varieties. Our hypothesis is that language varieties differ mainly in lexicographic clues. We show an example in Table 1 .", "We experimented with different sets of features and show the results in Figure 4 . As may be expected, average-based features obtain high accuracies (67.0%). However, although features based on standard deviation have not the highest information gain, they obtained the highest results individually (69.2%), as well as their combination with average ones (70,8%). Features based on minimum and maximum obtain low results (48.3% and 54.7% respectively), but in combination they obtain a significant increase (61.1%). The combination of the previous features obtains almost the highest accuracy (71.0%), equivalent to the accuracy obtained with probability and proportionality features (71.1%).", "This task is specially important in social media. Despite the vastness and accessibility of the Internet destroyed frontiers among regions or traits, companies are still very interested in author profiling segmentation. For example, when a new product is launched to the market, knowing the geographical distribution of opinions may help to improve marketing campaigns. Or given a security threat, knowing the possible cultural idiosyncrasies of the author may help to better understand who could have written the message.", "In Figure 3 we show the precision and recall values for the identification of each variety. As can be seen, Spain and Chile have the highest recall so that texts written in these varieties may have less probability to be misclassified as other varieties. Nevertheless, the highest precisions are obtained for Mexico and Peru, implying that texts written in such varieties may be easier to discriminate."]}
{"question_id": "d796a251792eca01cea31ba5cf3e54ff9acf543f", "predicted_answer": "The results on the alternate datasets are not explicitly mentioned in the given context. The context primarily discusses the results of the experiments on the main dataset, showing the performance of different feature sets and their combinations. It does not provide information on the results obtained on alternate datasets.", "predicted_evidence": ["We experimented with different sets of features and show the results in Figure 4 . As may be expected, average-based features obtain high accuracies (67.0%). However, although features based on standard deviation have not the highest information gain, they obtained the highest results individually (69.2%), as well as their combination with average ones (70,8%). Features based on minimum and maximum obtain low results (48.3% and 54.7% respectively), but in combination they obtain a significant increase (61.1%). The combination of the previous features obtains almost the highest accuracy (71.0%), equivalent to the accuracy obtained with probability and proportionality features (71.1%).", "In Table 8 we show the most discriminant features. The features are sorted by their information gain (IG). As can be seen, the highest gain is obtained by average, maximum and minimum, and standard deviation. On the other hand, probability and proportionality features has low information gain.", "In Figure 3 we show the precision and recall values for the identification of each variety. As can be seen, Spain and Chile have the highest recall so that texts written in these varieties may have less probability to be misclassified as other varieties. Nevertheless, the highest precisions are obtained for Mexico and Peru, implying that texts written in such varieties may be easier to discriminate.", "In this section, we describe the corpus and the alternative representations that we employ in this work.", "In this work we focus on the Spanish language variety identification. We differentiate from the previous works as follows: i) instead of $n$ -gram based representations, we propose a low dimensionality representation that is helpful when dealing with big data in social media; ii) in order to reduce the possible over-fitting, our training and test partitions do not share any author of instance between them; and iii) in contrast to the Twitter dataset of BIBREF4 , we will make available our dataset to the research community."]}
{"question_id": "a526c63fc8dc1b79702b481b77e3922d7002d973", "predicted_answer": "Based on the provided context, the answer is:\n\nThe answers are not guaranteed to be substrings of the text. However, the coverage of answers being substrings is high, as the system is designed to extract answer spans from the provided snippets by looking for occurrences of the gold standard answer in the text. The system's responses are restricted to substrings in the provided snippets due to the use of an extractive QA network.", "predicted_evidence": ["During fine-tuning, we extract answer spans from the BioASQ training data by looking for occurrences of the gold standard answer in the provided snippets. Note that this approach is not perfect as it can produce false positives (e.g., the answer is mentioned in a sentence which does not answer the question) and false negatives (e.g., a sentence answers the question, but the exact string used is not in the synonym list).", "Note that by using an extractive QA network as our central component, we restrict our system's responses to substrings in the provided snippets. This also implies that the network will not be able to answer yes/no questions. We do, however, generalize the FastQA output layer in order to be able to answer list questions in addition to factoid questions.", "BioASQ is a semantic indexing, question answering (QA) and information extraction challenge BIBREF0 . We participated in Task B of the challenge which is concerned with biomedical QA. More specifically, our system participated in Task B, Phase B: Given a question and gold-standard snippets (i.e., pieces of text that contain the answer(s) to the question), the system is asked to return a list of answer candidates.", "Starting with batch 3, we also submitted responses to yes/no questions by always answering yes. Because of a very skewed class distribution in the BioASQ dataset, this is a strong baseline. Because this is done merely to have baseline performance for this question type and because of the naivety of the method, we do not list or discuss the results here.", "During the inference phase, we retrieve the top 20 answers span via beam search with beam size 20. From this sorted list of answer strings, we remove all duplicate strings. For factoid questions, we output the top five answer strings as our ranked list of answer candidates. For list questions, we use a probability cutoff threshold $t$ , such that $\\lbrace (i, j)|p_{span}^{i, j} \\ge t\\rbrace $ is the set of answers. We set $t$ to be the threshold for which the list F1 score on the development set is optimized."]}
{"question_id": "0f9678e11079ee9ea1a1ce693f017177dd495ee5", "predicted_answer": "The gap between pretraining on SQuAD and not pretraining on SQuAD is not explicitly stated in the text. However, we can infer that pretraining on SQuAD has a significant impact on the performance of the model, especially on factoid questions. The model's success on factoid questions, winning three out of five batches, suggests that pretraining on SQuAD provides a strong foundation for the model's performance. Additionally, the fact that the model is expected to perform better on factoid questions than list questions because SQuAD does not contain list questions implies that pretraining on SQuAD has a specific advantage for factoid questions.\n\nIt's worth noting that the text does not provide a direct comparison between a model pretrained on SQuAD and one that is not, so we cannot quantify the exact gap between the two. However, it's clear that pretraining on SQuAD has a positive impact on the model's performance.", "predicted_evidence": ["We train the network in two steps: First, the network is trained on SQuAD, following the procedure by weissenborn2017fastqa (pre-training phase). Second, we fine-tune the network parameters on BioASQ (fine-tuning phase). For both phases, we use the Adam optimizer BIBREF6 with an exponentially decaying learning rate. We start with learning rates of $10^{-3}$ and $10^{-4}$ for the pre-training and fine-tuning phases, respectively.", "On factoid questions, our system has been very successful, winning three out of five batches. On list questions, however, the relative performance varies significantly. We expect our system to perform better on factoid questions than list questions, because our pre-training dataset (SQuAD) does not contain any list questions.", "Our system, on the other hand, is based on a neural network QA architecture that is trained end-to-end on the target task. We build upon FastQA BIBREF2 , an extractive factoid QA system which achieves state-of-the-art results on QA benchmarks that provide large amounts of training data. For example, SQuAD BIBREF3 provides a dataset of $\\approx 100,000$ questions on Wikipedia articles. Our approach is to train FastQA (with some extensions) on the SQuAD dataset and then fine-tune the model parameters on the BioASQ training set.", "In order to further tweak the performance of our systems, we built a model ensemble. For this, we trained five single models using 5-fold cross-validation on the entire training set. These models are combined by averaging their start and end scores before computing the span probabilities (Equations 8 - 10 ). As a result, we submit two systems to the challenge: The best single model (according to its development set) and the model ensemble.", "In general, the single and ensemble system are performing very similar relative to the rest of field: Their ranks are almost always right next to each other. Between the two, the ensemble model performed slightly better on average."]}
{"question_id": "0f1f81b6d4aa0da38b4cc8b060926e7df61bb646", "predicted_answer": "The machine learning method used to make the predictions is a neural network approach, specifically using techniques such as:\n\n1. Bag-of-words representation (Word)\n2. Event embeddings using subject-predicate-object triples (Event)\n3. Continues event vectors (NTN)\n4. Knowledge graph-based event vectors (KGEB)\n5. Neural networks with concatenated predicate and argument embeddings (BIBREF2)\n6. Recurrent Neural Networks (RNNs) for sequence-to-sequence event prediction (BIBREF25, BIBREF26)\n7. Hierarchical LSTM model for event prediction (BIBREF26)\n\nThe specific method used in the given context appears to be a neural network approach with event embeddings and a ranking-type loss function.", "predicted_evidence": ["In this section, we compare with several event-driven stock market prediction baseline methods: (1) Word, BIBREF23 luss2012predicting use bag-of-words represent news events for stock prediction; (2) Event, BIBREF24 ding-EtAl:2014:EMNLP2014 represent events by subject-predicate-object triples for stock prediction; (3) NTN, BIBREF4 ding2015deep learn continues event vectors for stock prediction; (4) KGEB, BIBREF18 ding2016knowledge incorporate knowledge graph into event vectors for stock prediction.", "BIBREF2 granroth2016happens concatenate predicate and argument embeddings and feed them to a neural network to generate an event embedding. Event embeddings are further concatenated and fed through another neural network to predict the coherence between the events. Modi modi2016event encodes a set of events in a similar way and use that to incrementally predict the next event \u2013 first the argument, then the predicate and then next argument. BIBREF25 pichotta2016learning treat event prediction as a sequence to sequence problem and use RNN based models conditioned on event sequences in order to predict the next event. These three works all model narrative chains, that is, event sequences in which a single entity (the protagonist) participates in every event. BIBREF26 hu2017happens also apply an RNN approach, applying a new hierarchical LSTM model in order to predict events by generating descriptive word sequences. This line of work combines the words in these phrases by the passing the concatenation or addition of their word embeddings to a parameterized function that maps the summed vector into event embedding space.", "where $\\mathit {\\Phi }=(T_1, T_2, T_3, W, b)$ is the set of model parameters. The standard $L_2$ regularization is used, for which the weight $\\lambda $ is set as 0.0001. The algorithm goes over the training set for multiple iterations. For each training instance, if the loss $loss(E,E^r)=\\max (0,1-g(E)+g(E^r))$ is equal to zero, the online training algorithm continues to process the next event tuple. Otherwise, the parameters are updated to minimize the loss using back-propagation BIBREF9.", "Event embeddings are further concatenated and fed through another neural network to predict the coherence between the events. Modi modi2016event encodes a set of events in a similar way and use that to incrementally predict the next event \u2013 first the argument, then the predicate and then next argument. BIBREF25 pichotta2016learning treat event prediction as a sequence to sequence problem and use RNN based models conditioned on event sequences in order to predict the next event. These three works all model narrative chains, that is, event sequences in which a single entity (the protagonist) participates in every event. BIBREF26 hu2017happens also apply an RNN approach, applying a new hierarchical LSTM model in order to predict events by generating descriptive word sequences. This line of work combines the words in these phrases by the passing the concatenation or addition of their word embeddings to a parameterized function that maps the summed vector into event embedding space. The additive nature of these models makes it difficult to model subtle differences in an event\u2019s surface form.", "In the training process, we calculate the similarity between a given event vector $\\mathbf {v}_e$ and its related intent vector $\\mathbf {v}_i$. For effectively training the model, we devise a ranking type loss function as follows:"]}
{"question_id": "ec62df859ad901bf0848f0a8b91eedc78dba5657", "predicted_answer": "The event prediction task is evaluated by calculating the similarity between a given event vector and its related intent vector, and using a ranking type loss function to train the model. The performance is measured by the accuracy of predicting the subsequent event, with an improvement of 1% or more considered significant.", "predicted_evidence": ["Event is a kind of important real-world knowledge. Learning effective event representations can be benefit for numerous applications. Script event prediction BIBREF20 is a challenging event-based commonsense reasoning task, which is defined as giving an existing event context, one needs to choose the most reasonable subsequent event from a candidate list.", "In this section, we compare with several event-driven stock market prediction baseline methods: (1) Word, BIBREF23 luss2012predicting use bag-of-words represent news events for stock prediction; (2) Event, BIBREF24 ding-EtAl:2014:EMNLP2014 represent events by subject-predicate-object triples for stock prediction; (3) NTN, BIBREF4 ding2015deep learn continues event vectors for stock prediction; (4) KGEB, BIBREF18 ding2016knowledge incorporate knowledge graph into event vectors for stock prediction.", "In the training process, we calculate the similarity between a given event vector $\\mathbf {v}_e$ and its related intent vector $\\mathbf {v}_i$. For effectively training the model, we devise a ranking type loss function as follows:", "BIBREF22 (BIBREF22) and BIBREF21 (BIBREF21) showed that script event prediction is a challenging problem, and even 1% of accuracy improvement is very difficult. Experimental results shown in Table TABREF31 demonstrate that we can achieve more than 1.5% improvements in single model comparison and more than 1.4% improvements in multi-model integration comparison, just by replacing the input embeddings, which confirms that better event understanding can lead to better inference results. An interesting result is that the event embeddings only incorporated with intents achieved the best result against other baselines. This confirms that capturing people's intents is helpful to infer their next plan. In addition, we notice that the event embeddings only incorporated with sentiment also achieve better performance than SGNN. This is mainly because the emotional consistency does also contribute to predicate the subsequent event.", "Events are a kind of important objective information of the world. Structuralizing and representing such information as machine-readable knowledge are crucial to artificial intelligence BIBREF0, BIBREF1. The main idea is to learn distributed representations for structured events (i.e. event embeddings) from text, and use them as the basis to induce textual features for downstream applications, such as script event prediction and stock market prediction."]}
{"question_id": "ccec4f8deff651858f44553f8daa5a19e8ed8d3b", "predicted_answer": "The datasets used in the paper are:\n\n1. ATOMIC (commonsense knowledge dataset)\n2. Hard similarity small dataset\n3. Hard similarity big dataset", "predicted_evidence": ["Averaging Baseline (Avg) This represents each event as the average of the constituent word vectors using pre-trained GloVe embeddings BIBREF8.", "One challenge for incorporating intents into event embeddings is that we should have a large-scale labeled dataset, which annotated the event and its actor's intents. Recently, BIBREF6 P18-1043 and BIBREF7 sap2018atomic released such valuable commonsense knowledge dataset (ATOMIC), which consists of 25,000 event phrases covering a diverse range of daily-life events and situations. For example, given an event \u201cPersonX drinks coffee in the morning\u201d, the dataset labels PersonX's likely intent is \u201cPersonX wants to stay awake\u201d.", "Recent advances in computing power and NLP technology enables more accurate models of events with structures. Using open information extraction to obtain structured events representations, we find that the actor and object of events can be better captured BIBREF24. For example, a structured representation of the event above can be (Actor = Microsoft, Action = sues, Object = Barnes & Noble). They report improvements on stock market prediction using their structured representation instead of words as features.", "The inputs of NTN are the word embeddings of $A$, $P$ and $O$, and the outputs are event embeddings. We initialized our word representations using publicly available $d$-dimensional ($d=100$) GloVe vectors BIBREF8. As most event arguments consist of several words, we represent the actor, action and object as the average of their word embeddings, respectively.", "(3) Our commonsense knowledge enhanced event representation learning approach outperformed all baseline methods across all datasets (achieving 78% and 200% improvements on hard similarity small and big dataset, respectively, compared to previous SOTA method), which indicates that commonsense knowledge is useful for distinguishing distinct events."]}
{"question_id": "d38745a3910c380e6df97c7056a5dd9643fd365b", "predicted_answer": "The comparison to other models that include subword information, such as fastText, is not explicitly mentioned in the given context. The discussion focuses on the comparison between character-level models and word-level models, highlighting the strengths and weaknesses of each approach. However, it would be interesting to explore how character-level models, such as the proposed Char2Vec model, compare to other subword-based models like fastText, which also incorporate subword information to capture morphological effects.", "predicted_evidence": ["What is less clear is how well these models learn word semantics. Classical word embedding models seem to capture word semantics, and the nearest neighbours of a given word are typically semantically related words BIBREF3 , BIBREF19 . In addition, the correlation between model word similarity scores and human similarity judgments is typically high BIBREF20 . However, no previous work (to our knowledge) evaluates the similarity judgments of character-level models against human annotators.", "Excitingly, character-level models seem to capture morphological effects. Examining nearest neighbours of morphologically complex words in character-aware models often shows other words with the same morphology BIBREF8 , BIBREF9 . Furthermore, morphosyntactic features such as capitalization and suffix information have long been used in tasks such as POS tagging BIBREF17 , BIBREF18 . By explicitly modelling these features, one might expect good performance gains in many NLP tasks.", "Previous work on bidirectional LSTM character-level models used both LSTMs to read the entire word BIBREF8 , BIBREF22 . This can lead to redundancy, as both LSTMs are used to capture the full word. In contrast, our model is capable of splitting the words and optimizing the two LSTMs for modelling different halves. This means one of the LSTMs can specialize on word prefixes and roots, while the other memorizes possible suffixes. In addition, when dealing with an unknown word, it can be split into known and unknown components. The model can then use the semantic knowledge it has learnt for a known component to predict a representation for the unknown word as a whole.", "Finally, we show that character-level models, while outperformed by word-level models generally at the task of semantic similarity, are competitive at representing rare morphologically rich words. In addition, the character-level models can predict good quality representations for unseen words, with the morphologically aware character-level model doing slightly better.", "Overall, word-based embedding models learn vectors that correlate better with human judgments, particularly for morphologically simple words. However, character-based models are competitive with word-based models on the RW dataset. While the words in this dataset appear rarely in our corpus (of the in-corpus words, over half appear fewer than 100 times), each morpheme may be common, and the character-level models can use this information. We note that on the entire RW dataset (of which over half contain an OOV word), the character-based models still perform reasonably. We also note that on word pairs in the RW test containing at least one OOV word, the full Char2Vec model outperforms the C2V model without morphology. This suggests that character-based embedding models are learning to morphologically analyse complex word forms, even on unseen words, and that giving the model the capability to learn word segments independently helps this process."]}
{"question_id": "2b75df325c98b761faf2fecf6e71ac7366eb15ea", "predicted_answer": "According to the text, the model is expected to perform better for languages with richer morphology than English, such as Turkish and German, because it performs better on words with richer morphology. This implies that the model's performance may be lower for morphologically impoverished languages like English.", "predicted_evidence": ["This is unfortunate for our model, as it performs better on words with richer morphology. It gives consistently more accurate morphological analyses for these words compared to standard baselines, and matches word-level models for semantic similarity on rare words with rich morphology. In addition, it seems to learn morphosyntactic features to help solve the syntactic analogy task. Most of all, it is language-agnostic, and easy to port across different languages. We thus expect our model to perform even better for languages with a richer morphology than English, such as Turkish and German.", "We only report results for English. However, English is a morphologically impoverished language, with little inflection and relatively few productive patterns of derivation. Our morphology test set reflects this, with over half the words consisting of a simple morpheme, and over 90% having at most 2 morphemes.", "To evaluate our model, we evaluate its use as a morphological analyzer (\u00a7 \"Morphological awareness\" ), test how well it learns word semantics, including for unseen words (\u00a7 \"Capturing semantic similarity\" ), and examine the structure of the embedding space (\u00a7 \"Capturing syntactic and semantic regularity\" ).", "Excitingly, character-level models seem to capture morphological effects. Examining nearest neighbours of morphologically complex words in character-aware models often shows other words with the same morphology BIBREF8 , BIBREF9 . Furthermore, morphosyntactic features such as capitalization and suffix information have long been used in tasks such as POS tagging BIBREF17 , BIBREF18 . By explicitly modelling these features, one might expect good performance gains in many NLP tasks.", "Finally, we show that character-level models, while outperformed by word-level models generally at the task of semantic similarity, are competitive at representing rare morphologically rich words. In addition, the character-level models can predict good quality representations for unseen words, with the morphologically aware character-level model doing slightly better."]}
{"question_id": "649e77ac2ecce42ab2efa821882675b5a0c993cb", "predicted_answer": "According to the text, the model is expected to perform well on languages with richer morphology than English, specifically mentioning Turkish and German.", "predicted_evidence": ["Another is to operate on the smallest orthographic unit, the character BIBREF8 , BIBREF9 . However, the link between shape and meaning is often complicated BIBREF10 , as alphabetic characters carry no inherent semantic meaning. To account for this, the model has to learn complicated dependencies between strings of characters to accurately capture word meaning. We hypothesize that explicitly introducing morphology into character-level models can help them learn morphological features, and hence word meaning.", "One exciting feature of character-level models is their ability to represent open-vocabulary words. After training, they can predict a vector for any word, not just words that they have seen before. Our model has an advantage in that it can split unknown words into known and unknown components. Hence, it can potentially generalise better over seen morphemes and words and apply existing knowledge to new cases.", "This is unfortunate for our model, as it performs better on words with richer morphology. It gives consistently more accurate morphological analyses for these words compared to standard baselines, and matches word-level models for semantic similarity on rare words with rich morphology. In addition, it seems to learn morphosyntactic features to help solve the syntactic analogy task. Most of all, it is language-agnostic, and easy to port across different languages. We thus expect our model to perform even better for languages with a richer morphology than English, such as Turkish and German.", "Excitingly, character-level models seem to capture morphological effects. Examining nearest neighbours of morphologically complex words in character-aware models often shows other words with the same morphology BIBREF8 , BIBREF9 . Furthermore, morphosyntactic features such as capitalization and suffix information have long been used in tasks such as POS tagging BIBREF17 , BIBREF18 . By explicitly modelling these features, one might expect good performance gains in many NLP tasks.", "We report the results in Table 6 . The most intriguing result is that character-level models are competitive with word-level models for syntactic analogy, with our Char2Vec model holding the best result for syntactic analogy answering. This suggests that incorporating morphological knowledge explicitly rather than latently helps the model learn morphological features. However, on the semantic analogies, the character-based models do much worse than the word-based models. This is perhaps unsurprising in light of the previous section, where we demonstrate that character-based models do worse at the semantic similarity task than word-level models."]}
{"question_id": "0bc305d6b90f77f835bc4c904b22a4be07f963b2", "predicted_answer": "The embeddings are evaluated by ranking the word pairs according to their cosine similarity and then measuring the correlation (using Spearman's \u03c1) between the model judgments and human judgments.", "predicted_evidence": ["Next, we tested our model similarity scores against human similarity judgments. For these datasets, human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments BIBREF20 .", "What is less clear is how well these models learn word semantics. Classical word embedding models seem to capture word semantics, and the nearest neighbours of a given word are typically semantically related words BIBREF3 , BIBREF19 . In addition, the correlation between model word similarity scores and human similarity judgments is typically high BIBREF20 . However, no previous work (to our knowledge) evaluates the similarity judgments of character-level models against human annotators.", "Overall, word-based embedding models learn vectors that correlate better with human judgments, particularly for morphologically simple words. However, character-based models are competitive with word-based models on the RW dataset. While the words in this dataset appear rarely in our corpus (of the in-corpus words, over half appear fewer than 100 times), each morpheme may be common, and the character-level models can use this information. We note that on the entire RW dataset (of which over half contain an OOV word), the character-based models still perform reasonably. We also note that on word pairs in the RW test containing at least one OOV word, the full Char2Vec model outperforms the C2V model without morphology. This suggests that character-based embedding models are learning to morphologically analyse complex word forms, even on unseen words, and that giving the model the capability to learn word segments independently helps this process.", "Finally, we evaluate the structure of the embedding space of our various models. In particular, we test whether affixation corresponds to regular linear shifts in the embedding space.", "Word embedding models associate each word in a corpus with a vector in a semantic space. These vectors can either be learnt to optimize performance in a downstream task BIBREF0 , BIBREF1 or learnt via the distributional hypothesis: words with similar contexts have similar meanings BIBREF2 , BIBREF3 . Current word embedding models treat words as atomic. However, words follow a power law distribution BIBREF4 , and word embedding models suffer from the problem of sparsity: a word like `unbelievableness' does not appear at all in the first 17 million words of Wikipedia, even though it is derived from common morphemes. This leads to three problems:"]}
{"question_id": "041529e15b70b21986adb781fd9b94b595e451ed", "predicted_answer": "The margin by which their system outperformed previous ones is:\n\n* 15.6/16.5 (accuracy/NDCG) on MCTest-150\n* 7.3/4.6 on MCTest-500", "predicted_evidence": ["For this common ranking loss, we also have two styles to utilize the data in view of each positive answer is accompanied with three negative answers. One is treating ( INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , INLINEFORM4 ) as a training example, then our loss function can have three \u201cmax()\u201d terms, each for a positive-negative pair; the other one is treating ( INLINEFORM5 , INLINEFORM6 , INLINEFORM7 ) as an individual training example. In practice, we find the second way works better. We conjecture that the second way has more training examples, and positive answers are repeatedly used to balance the amounts of positive and negative answers.", "We also do some preliminary error analysis. One big obstacle for our systems is the \u201chow many\u201d questions. For example, for question \u201chow many rooms did I say I checked?\u201d and the answer candidates are four digits \u201c5,4,3,2\u201d which never appear in the D, but require the counting of some locations. However, these digital answers can not be modeled well by distributed representations so far. In addition, digital answers also appear for \u201cwhat\u201d questions, like \u201cwhat time did...\u201d. Another big limitation lies in \u201cwhy\u201d questions. This question type requires complex inference and long-distance dependencies. We observed that all deep lerning systems, including the two baselines, suffered somewhat from it.", "We consider two evaluation metrics: accuracy (proportion of questions correctly answered) and NDCG INLINEFORM0 BIBREF20 . Unlike accuracy which evaluates if the question is correctly answered or not, NDCG INLINEFORM1 , being a measure of ranking quality, evaluates the position of the correct answer in our predicted ranking.", "Our training objective is to minimize the following ranking loss function: DISPLAYFORM0", "Table TABREF16 lists the performance of baselines, HABCNN-TE variants, HABCNN systems in the first, second and last block, respectively (we only report variants for top-performing HABCNN-TE). Consistently, our HABCNN systems outperform all baselines, especially surpass the two competitive deep learning based systems AR and NR. The margin between our best-performing ABHCNN-TE and NR is 15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500. This demonstrates the promise of our architecture in this task."]}
{"question_id": "da2350395867b5fd4dbf968b5a1cd6921ab6dd37", "predicted_answer": "According to the text, the prior approaches that they compared to are:\n\n1. Feature engineering-based approaches (prior work on this task)\n2. Two baselines (not specified which ones, but mentioned as suffering from limitations in \"why\" questions)\n3. Two variants of ABCHNN, inspired by BIBREF21 and BIBREF2, respectively.", "predicted_evidence": ["We also do some preliminary error analysis. One big obstacle for our systems is the \u201chow many\u201d questions. For example, for question \u201chow many rooms did I say I checked?\u201d and the answer candidates are four digits \u201c5,4,3,2\u201d which never appear in the D, but require the counting of some locations. However, these digital answers can not be modeled well by distributed representations so far. In addition, digital answers also appear for \u201cwhat\u201d questions, like \u201cwhat time did...\u201d. Another big limitation lies in \u201cwhy\u201d questions. This question type requires complex inference and long-distance dependencies. We observed that all deep lerning systems, including the two baselines, suffered somewhat from it.", "Addition. Directly compare question and answers without considering the D. Sentence representations are computed by element-wise addition over word representations.", "Prior work on this task is mostly based on feature engineering. This work, instead, takes the lead in presenting a deep neural network based approach without any linguistic features involved.", "HABCNN-QP and HABCNN-QAP make different use of INLINEFORM0 . HABCNN-QP compares INLINEFORM1 with answer representation INLINEFORM2 . HABCNN-QAP compares INLINEFORM3 with INLINEFORM4 . HABCNN-QAP projects D twice, once based on attention from Q, once based on attention from A and compares the two projected representations, shown in Figure FIGREF2 (top). HABCNN-QP only utilizes the Q-based projection of D and then compares the projected document with the answer representation, shown in Figure FIGREF2 (middle).", "In addition to the main architectures described above, we also explore two variants of ABCHNN, inspired by BIBREF21 and BIBREF2 , respectively."]}
{"question_id": "eb653a5c59851eda313ece0bcd8c589b6155d73e", "predicted_answer": "The baseline is a one-stage RNN system that uses a 2-layer BLSTM structure and is directly trained to recognize dialect categories.", "predicted_evidence": ["The phonetic sequence annotation of the corresponding text to each speech is also provided in the training set. There are 27 initials and 39 finals with 148 tones in the whole database.", "The baseline we use for comparison is a one-stage RNN system, the RNN structure is the same as the last stage containing 2-layer BLSTM and directly trained to recognize dialect category. In the process of evaluation, we compute the accuracy of the two sub-tasks and the whole test set to evaluate the performance of each system.", "These two multi-stage systems both much outperform the baseline system. They learn acoustic and language knowledge successively, indicating that language and phoneme are features of different levels, so we have to train step by step to avoid the networks \u201cforget\" some knowledge. Through the process, we can find the rules of multi-task and multi-stage training, if the labels are in different levels then multi-stage training should be used such as the situation in our paper, otherwise multi-task training should be used for parallel learning a wide range of knowledge.", "We use a database covering 10 most widespread Chinese dialects, the dialects are Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian. Each dialect has 6-hour audio data. For the training set, there will be 6000 audio files in each dialect with variable length (Figure FIGREF16 ), we can see that most files are longer than 3 seconds. The test set has 500 audio files in each dialect and the set is divided into two categories according to the duration of the audio file ( INLINEFORM0 3s for the first task and INLINEFORM1 3s for the second task).", "where INLINEFORM0 is the ground truth label and INLINEFORM1 is the output probability distribution."]}
{"question_id": "0caa3162abe588f576a568d63ab9fd0e9c46ceda", "predicted_answer": "The three-stage system achieves high accuracy in long-duration tasks, with accuracy rates of 98.8% for Shanghai dialect recognition and 99.8% for Hefei dialect recognition. However, it struggles to recognize Minnan and Kekka dialects, with lower accuracy rates.", "predicted_evidence": ["We evaluate the three-stage system with the same experiments, and the results (Table TABREF23 ) demonstrate that the three-stage system can achieve high accuracy in long duration task by larger BLSTM layers and the BGRU structure outperforms BLSTM on the whole. But adding the third RNN layer also does not work in these experiments.", "By analyzing the confusing matrices (Figure FIGREF19 ) of predicted results, we can find that the accuracy is high in several dialects' recognition, such as Shanghai (98.8%) and Hefei (99.8%), but the systems have some trouble while recognizing Minnan and Kekka, Hebei and Shanxi. The results accord with regional distribution of the dialects. For example, Minnan and Kekka are both in Fujian Province and have lots of cognate words, so it is hard to recognize them in reality.", "On the other hand, in many utterance analyzing tasks such as acoustic speech recognition (ASR), speaker verification (SV) and our LID, only a simple task or a specific aim is focused on. However, an utterance always has multi-dimensional information such as content, emotion, speaker and language and there are some certain correlations between them. Although the LID task is text-independent, which means the content of each utterance is totally different, different languages may have its own pronunciations or tones. Thus acoustic and language are two components in the LID task, BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 use the bottleneck features from an ASR system and feed to another neural network for recognition. Nevertheless, these ASR DNNs constituted by fully connected layers adds significant computational complexity and also require labels of physical states of a tied-state triphone.", "As Table TABREF24 shows, training networks in the first stage (with CTC loss) needs more time for convergence than training networks in the second or third stage (with cross-entropy loss). We can observe that the two-stage system spends less time while having a slightly higher accuracy compared to the three-stage system.", "The baseline we use for comparison is a one-stage RNN system, the RNN structure is the same as the last stage containing 2-layer BLSTM and directly trained to recognize dialect category. In the process of evaluation, we compute the accuracy of the two sub-tasks and the whole test set to evaluate the performance of each system."]}
{"question_id": "cbe42bf7c99ee248cdb2c5d6cf86b41106e66863", "predicted_answer": "The Chinese dialects explored in this paper are:\n\n1. Ningxia\n2. Hefei\n3. Sichuan\n4. Shanxi\n5. Changsha\n6. Hebei\n7. Nanchang\n8. Shanghai\n9. Kekka (also known as Minnan)\n10. Fujian", "predicted_evidence": ["By analyzing the confusing matrices (Figure FIGREF19 ) of predicted results, we can find that the accuracy is high in several dialects' recognition, such as Shanghai (98.8%) and Hefei (99.8%), but the systems have some trouble while recognizing Minnan and Kekka, Hebei and Shanxi. The results accord with regional distribution of the dialects. For example, Minnan and Kekka are both in Fujian Province and have lots of cognate words, so it is hard to recognize them in reality.", "The aim of language identification (LID) is to determine the language of an utterance and can be defined as a variable-length sequence classification task on the utterance-level. The task introduced in this paper is more challenging than general LID tasks cause we use a dialect database which contains 10 dialects in China. The dialects' regions are close to each other and they all belong to Chinese, so they have the same characters and similar pronunciations.", "We use a database covering 10 most widespread Chinese dialects, the dialects are Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian. Each dialect has 6-hour audio data. For the training set, there will be 6000 audio files in each dialect with variable length (Figure FIGREF16 ), we can see that most files are longer than 3 seconds. The test set has 500 audio files in each dialect and the set is divided into two categories according to the duration of the audio file ( INLINEFORM0 3s for the first task and INLINEFORM1 3s for the second task).", "In this work, we propose an acoustic model based on ResNet14 followed by an RNN to recognize phoneme sequence directly with CTC loss and train a simple RNN lastly to get posteriors for recognizing dialect category, forming a two-stage LID system. The system links the different stages by using intermediate features extracted by a shallow ResNet14 architecture. Compared with a simple network or the three-stage system, the two-stage system achieves the state-of-the-art in the Chinese dialect recognition task. We believe this idea of two-stage training can provide inspirations for learning different classes knowledge and can extend to other fields.", "The baseline we use for comparison is a one-stage RNN system, the RNN structure is the same as the last stage containing 2-layer BLSTM and directly trained to recognize dialect category. In the process of evaluation, we compute the accuracy of the two sub-tasks and the whole test set to evaluate the performance of each system."]}
{"question_id": "94d794df4a3109522c2ea09dad5d40e55d35df51", "predicted_answer": "The neural machine translation model used is the Transformer architecture.", "predicted_evidence": ["Back-translation BIBREF5 has become the most widely used technique to use monolingual data in neural machine translation. Back-translation extends the existing parallel training set by additional training samples with real English target sentences but synthetic source sentences. Different methods have been proposed to synthesize the source sentence such as using dummy tokens BIBREF5 , copying the target sentence BIBREF29 , or sampling from or decoding with a reverse sequence-to-sequence model BIBREF5 , BIBREF30 , BIBREF4 . The most popular approach is to generate the synthetic source sentences with a reverse model that is trained to transform target to source sentences using beam search. In GEC, this means that the reverse model learns to introduce errors into a correct English sentence. Back-translation has been applied successfully to GEC by BIBREF4 . We confirm the effectiveness of back-translation in GEC and discuss some of the differences between applying this technique to grammatical error correction and machine translation.", "For the restricted track, the organizers provided 1.2M pairs (560K without identity mappings) of corrected and uncorrected sentences. Our goal on the restricted track was to explore the potential of purely neural models for grammatical error correction. We confirm the results of BIBREF4 and report substantial gains by applying back-translation BIBREF5 to GEC \u2013 a data augmentation technique common in machine translation. Furthermore, we noticed that large parts of the training data do not match the target domain. We mitigated the domain gap by over-sampling the in-domain training corpus, and by fine-tuning through continued training. Our final model is an ensemble of four neural machine translation (NMT) models and two neural language models (LMs) with Transformer architecture BIBREF6 . Our purely neural system was also part of the joint submission with the Cambridge University Computer Lab described by BIBREF7 .", "We participated in the BEA 2019 Shared Task on grammatical error correction with submissions to the low-resource and the restricted track. Our low-resource system is an extension of prior work on FST-based GEC BIBREF3 to allow insertions and deletions. Our restricted track submission is a purely neural system based on standard NMT and LM architectures. We pointed out the similarity between GEC and machine translation, and demonstrated that several techniques which originate from MT research such as over-sampling, back-translation, and fine-tuning, are also useful for GEC. Our models have been used in a joint submission with the Cambridge University Computer Lab BIBREF7 .", "For our low-resource system we extended our prior work on finite state transducer based GEC BIBREF3 to handle new error types such as punctuation errors as well as insertions and deletions of a small number of frequent words. For the restricted track, the organizers provided 1.2M pairs (560K without identity mappings) of corrected and uncorrected sentences. Our goal on the restricted track was to explore the potential of purely neural models for grammatical error correction. We confirm the results of BIBREF4 and report substantial gains by applying back-translation BIBREF5 to GEC \u2013 a data augmentation technique common in machine translation. Furthermore, we noticed that large parts of the training data do not match the target domain. We mitigated the domain gap by over-sampling the in-domain training corpus, and by fine-tuning through continued training. Our final model is an ensemble of four neural machine translation (NMT) models and two neural language models (LMs) with Transformer architecture BIBREF6 .", "We use neural LMs and neural machine translation (NMT) models in our restricted track entry. Our neural LM is as described in Sec. SECREF15 . Our LMs and NMT models share the same subword segmentation. We perform exploratory NMT experiments with the Base setup, but switch to the Big setup for our final models. Tab. TABREF21 shows the differences between both setups. Tab. TABREF22 lists some corpus statistics for the BEA-2019 training sets. In our experiments without fine-tuning we decode with the average of the 20 most recent checkpoints BIBREF26 . We use the SGNMT decoder BIBREF13 , BIBREF14 in all our experiments."]}
{"question_id": "044c66c6b7ff7378682f24887b05e1af79dcd04f", "predicted_answer": "The answer is not explicitly stated in the given context. The passage discusses the approach and results of the Cambridge University Engineering Department's entry in the BEA 2019 shared task, but it does not mention the specific position or ranking of their entry in the overall shared task.", "predicted_evidence": ["The automatic correction of errors in text [In a such situaction INLINEFORM0 In such a situation] is receiving more and more attention from the natural language processing community. A series of competitions has been devoted to grammatical error correction (GEC): the CoNLL-2013 shared task BIBREF0 , the CoNLL-2014 shared task BIBREF1 , and finally the BEA 2019 shared task BIBREF2 . This paper presents the contributions from the Cambridge University Engineering Department to the latest GEC competition at the BEA 2019 workshop.", "with INLINEFORM0 for deletions, INLINEFORM1 for substitutions, INLINEFORM2 for insertions, and INLINEFORM3 for converting words to BPE tokens. Path scores in the FST in Eq. EQREF14 are the accumulated penalties INLINEFORM4 , INLINEFORM5 , and INLINEFORM6 . The INLINEFORM7 -parameters are tuned on the dev set using a variant of Powell search BIBREF16 . We apply standard FST operations like output projection, INLINEFORM8 -removal, determinization, minimization, and weight pushing BIBREF17 , BIBREF18 to help downstream decoding. Following BIBREF3 we then use the resulting transducer to constrain a neural LM beam decoder.", "We use neural LMs and neural machine translation (NMT) models in our restricted track entry. Our neural LM is as described in Sec. SECREF15 . Our LMs and NMT models share the same subword segmentation. We perform exploratory NMT experiments with the Base setup, but switch to the Big setup for our final models. Tab. TABREF21 shows the differences between both setups. Tab. TABREF22 lists some corpus statistics for the BEA-2019 training sets. In our experiments without fine-tuning we decode with the average of the 20 most recent checkpoints BIBREF26 . We use the SGNMT decoder BIBREF13 , BIBREF14 in all our experiments.", "In a first step, the source sentence is converted to an FST INLINEFORM0 (Fig. FIGREF3 ). This initial FST is augmented by composition (denoted with the INLINEFORM1 -operator) with various other FSTs to cover different error types. Composition is a widely used standard operation on FSTs and supported efficiently by FST toolkits such as OpenFST BIBREF8 . We construct the hypothesis space as follows:", "SECREF15 ) and ensembling. Our best system (4 NMT models, 2 language models) achieves 58.9 M2 on CoNLL-2014, which is slightly (2.25 points) worse than the best published result on that test set BIBREF27 . However, we note that we have tailored our system towards the BEA-2019 dev set and not the CoNLL-2013 or CoNLL-2014 test sets. As we argued in Sec. SECREF18 , our results throughout this work suggest strongly that the optimal system parameters for these test sets are very different from each other, and that our final system settings are not optimal for CoNLL-2014. We also note that unlike the system of BIBREF27 , our system for the restricted track does not use spell checkers or other NLP tools but relies solely on neural sequence models."]}
{"question_id": "903ac8686ed7e6e3269a5d863f06ff11c50e49e8", "predicted_answer": "The restrictions of the restricted track are not explicitly stated in the given context. However, based on the text, it can be inferred that the restricted track has some limitations or constraints, such as:\n\n* Not using spell checkers or other NLP tools, relying solely on neural sequence models.\n* Not using any external data or resources beyond the provided 1.2M pairs of corrected and uncorrected sentences.\n* Focusing on purely neural models for grammatical error correction.\n\nThese restrictions are implied by the text, but the exact nature and scope of the restrictions are not explicitly defined.", "predicted_evidence": ["We use neural LMs and neural machine translation (NMT) models in our restricted track entry. Our neural LM is as described in Sec. SECREF15 . Our LMs and NMT models share the same subword segmentation. We perform exploratory NMT experiments with the Base setup, but switch to the Big setup for our final models. Tab. TABREF21 shows the differences between both setups. Tab. TABREF22 lists some corpus statistics for the BEA-2019 training sets. In our experiments without fine-tuning we decode with the average of the 20 most recent checkpoints BIBREF26 . We use the SGNMT decoder BIBREF13 , BIBREF14 in all our experiments.", "SECREF15 ) and ensembling. Our best system (4 NMT models, 2 language models) achieves 58.9 M2 on CoNLL-2014, which is slightly (2.25 points) worse than the best published result on that test set BIBREF27 . However, we note that we have tailored our system towards the BEA-2019 dev set and not the CoNLL-2013 or CoNLL-2014 test sets. As we argued in Sec. SECREF18 , our results throughout this work suggest strongly that the optimal system parameters for these test sets are very different from each other, and that our final system settings are not optimal for CoNLL-2014. We also note that unlike the system of BIBREF27 , our system for the restricted track does not use spell checkers or other NLP tools but relies solely on neural sequence models.", "As explained previously, we over-sample the W&I+LOCNESS corpus by factor 4 to mitigate the domain gap between the training set and the BEA-2019 dev and test sets. To further adapt our system to the target domain, we fine-tune the NMT models on W&I+LOCNESS after convergence on the full training set. We do that by continuing training on W&I+LOCNESS from the last checkpoint of the first training pass. Fig. FIGREF30 plots the INLINEFORM0 score on the BEA-2019 dev set for two different setups. For the red curve, we average all checkpoints BIBREF26 (including the last unadapted checkpoint) up to a certain training iteration. Checkpoints are dumped every 500 steps. The green curve does not use any checkpoint averaging.", "As explained previously, we over-sample the W&I+LOCNESS corpus by factor 4 to mitigate the domain gap between the training set and the BEA-2019 dev and test sets. To further adapt our system to the target domain, we fine-tune the NMT models on W&I+LOCNESS after convergence on the full training set. We do that by continuing training on W&I+LOCNESS from the last checkpoint of the first training pass. Fig. FIGREF30 plots the INLINEFORM0 score on the BEA-2019 dev set for two different setups. For the red curve, we average all checkpoints BIBREF26 (including the last unadapted checkpoint) up to a certain training iteration. Checkpoints are dumped every 500 steps. The green curve does not use any checkpoint averaging. Checkpoint averaging helps to smooth out fluctuations in INLINEFORM1 score, and also generalizes better to CoNLL-2014 (Tab.", "For the restricted track, the organizers provided 1.2M pairs (560K without identity mappings) of corrected and uncorrected sentences. Our goal on the restricted track was to explore the potential of purely neural models for grammatical error correction. We confirm the results of BIBREF4 and report substantial gains by applying back-translation BIBREF5 to GEC \u2013 a data augmentation technique common in machine translation. Furthermore, we noticed that large parts of the training data do not match the target domain. We mitigated the domain gap by over-sampling the in-domain training corpus, and by fine-tuning through continued training. Our final model is an ensemble of four neural machine translation (NMT) models and two neural language models (LMs) with Transformer architecture BIBREF6 . Our purely neural system was also part of the joint submission with the Cambridge University Computer Lab described by BIBREF7 ."]}
{"question_id": "ab95ca983240ad5289c123a2774f8e0db424f4a1", "predicted_answer": "BEA stands for Benchmarking English Aptitude.", "predicted_evidence": ["We report M2 BIBREF24 scores on the CoNLL-2014 test set BIBREF1 and span-based ERRANT scores BIBREF25 on the BEA-2019 dev set BIBREF2 . On CoNLL-2014 we compare with the best published results with comparable amount of parallel training data. We refer to BIBREF2 for a full comparison of BEA-2019 systems. We tune our systems on BEA-2019 and only report the performance on CoNLL-2014 for comparison to prior work.", "As explained previously, we over-sample the W&I+LOCNESS corpus by factor 4 to mitigate the domain gap between the training set and the BEA-2019 dev and test sets. To further adapt our system to the target domain, we fine-tune the NMT models on W&I+LOCNESS after convergence on the full training set. We do that by continuing training on W&I+LOCNESS from the last checkpoint of the first training pass. Fig. FIGREF30 plots the INLINEFORM0 score on the BEA-2019 dev set for two different setups. For the red curve, we average all checkpoints BIBREF26 (including the last unadapted checkpoint) up to a certain training iteration. Checkpoints are dumped every 500 steps. The green curve does not use any checkpoint averaging.", "Tab. TABREF9 summarizes our low-resource experiments. Our substitution-only system already outperforms the prior work of BIBREF3 . Allowing for deletions and insertions improves the ERRANT score on BEA-2019 Dev by 2.57 points. We report further gains on both test sets by ensembling two language models and increasing the beam size.", "As explained previously, we over-sample the W&I+LOCNESS corpus by factor 4 to mitigate the domain gap between the training set and the BEA-2019 dev and test sets. To further adapt our system to the target domain, we fine-tune the NMT models on W&I+LOCNESS after convergence on the full training set. We do that by continuing training on W&I+LOCNESS from the last checkpoint of the first training pass. Fig. FIGREF30 plots the INLINEFORM0 score on the BEA-2019 dev set for two different setups. For the red curve, we average all checkpoints BIBREF26 (including the last unadapted checkpoint) up to a certain training iteration. Checkpoints are dumped every 500 steps. The green curve does not use any checkpoint averaging. Checkpoint averaging helps to smooth out fluctuations in INLINEFORM1 score, and also generalizes better to CoNLL-2014 (Tab.", "Our results in Tab. TABREF9 differ significantly between the CoNLL-2014 test set and the BEA-2019 dev set. Allowing insertions is beneficial on BEA-2019 Dev but decreases the M2 score on CoNLL-2014. Increasing the beam size improves our system by 3.28 points on CoNLL-2014 while the impact on BEA-2019 Dev is smaller (+0.85 points). These differences can be partially explained by comparing the error type frequencies in the reference annotations in both test sets (Tab. TABREF19 ). Samples in CoNLL-2014 generally need more corrections per sentence than in BEA-2019 Dev. More importantly, the CoNLL-2014 test set contains fewer missing words, but much more unnecessary words than BEA-2019 Dev. This mismatch tempers with tuning as we explicitly tune insertion and deletion penalties."]}
{"question_id": "fcf9377fc3fce529d4bab1258db3f46b15ae5872", "predicted_answer": "According to human evaluation, the concurrent system appears to better preserve the meaning and fluency of the original text, while the modular system does a better job at reducing bias.", "predicted_evidence": ["We propose two algorithms for this task, each with its own benefits. A modular algorithm enables human control and interpretability. A concurrent algorithm is simple to train and operate.", "Model description. The concurrent system is an encoder-decoder neural network. The encoder is BERT. The decoder is the same as that of Section SECREF28: an attentional LSTM with copy and coverage mechanisms. The decoder's inputs are set to:", "Overall, while modular does a better job at reducing bias, concurrent appears to better preserve the meaning and fluency of the original text. We conclude that the proposed methods, while imperfect, are capable of providing useful suggestions for how subjective bias in real-world news or political text can be reduced.", "Model training. The concurrent model is pre-trained with the same autoencoding procedure described in Section SECREF28. It is then fine-tuned as a subjective-to-neutral translation system with the same loss function described in Section SECREF30.", "The low human performance can be attributed to the difficulty of identifying bias. Issues of bias are typically reserved for senior Wikipedia editors (Section SECREF14) and untrained workers performed worse (37.39%) on the same task in BIBREF2 (and can struggle on other tasks requiring linguistic knowledge BIBREF39). concurrent's encoder, which is architecturally identical to BERT, had similar performance to a stand-alone BERT system. The linguistic and category-related features in the modular detector gave it slight leverage over the plain BERT-based models."]}
{"question_id": "5422a3f2a083395416d6f99c57d28335eb2e44e1", "predicted_answer": "The Wikipedia edits that removed framings, presuppositions, and attitudes from biased sentences were a Wiki community effort. The Wiki Neutrality Corpus (WNC) consists of aligned sentences pre and post-neutralization by English Wikipedia editors, who made revisions to ensure a neutral point of view (NPOV) according to Wikipedia's NPOV policy.", "predicted_evidence": ["We take a closer look at WNC to identify characteristics of subjective bias on Wikipedia.", "The Wiki Neutrality Corpus consists of aligned sentences pre and post-neutralization by English Wikipedia editors (Table TABREF3). We used regular expressions to crawl 423,823 Wikipedia revisions between 2004 and 2019 where editors provided NPOV-related justification BIBREF11, BIBREF2, BIBREF12. To maximize the precision of bias-related changes, we ignored revisions where", "Topic. We use the Wikimedia Foundation's categorization models BIBREF17 to bucket articles from WNC and the aforementioned random sample into a 44-category ontology, then compare the proportions of NPOV-driven edits across categories. Subjectively biased edits are most prevalent in history, politics, philosophy, sports, and language categories. They are least prevalent in the meteorology, science, landforms, broadcasting, and arts categories. This suggests that there is a relationship between a text's topic and the realization of bias. We use this observation to guide our model design in Section SECREF19.", "We introduce the Wiki Neutrality Corpus (WNC). This is a new parallel corpus of 180,000 biased and neutralized sentence pairs along with contextual sentences and metadata. The corpus was harvested from Wikipedia edits that were designed to ensure texts had a neutral point of view. WNC is the first parallel corpus targeting biased and neutralized language. We also define the task of neutralizing subjectively biased text. This task shares many properties with tasks like detecting framing or epistemological bias BIBREF2, or veridicality assessment/factuality prediction BIBREF7, BIBREF8, BIBREF9, BIBREF10. Our new task extends these detection/classification problems into a generation task: generating more neutral text with otherwise similar meaning.", "This work presents data and algorithms for automatically reducing bias in text. We focus on a particular kind of bias: inappropriate subjectivity (\u201csubjective bias\u201d). Subjective bias occurs when language that should be neutral and fair is skewed by feeling, opinion, or taste (whether consciously or unconsciously). In practice, we identify subjective bias via the method of BIBREF2: using Wikipedia's neutral point of view (NPOV) policy. This policy is a set of principles which includes \u201cavoiding stating opinions as facts\u201d and \u201cpreferring nonjudgemental language\u201d."]}
{"question_id": "7c2d6bc913523d77e8fdc82c60598ee95b445d84", "predicted_answer": "The text does not explicitly state how subjective text is automatically neutralized. However, it mentions that the proposed methods, which involve identifying subjective bias and detecting subjectivity in sentences, can provide useful suggestions for reducing subjective bias in text. It also mentions that the model is trained on 50k examples and uses a procedure to detect subjectivity, but it does not provide a detailed explanation of how the neutralization process works.", "predicted_evidence": ["We propose the task of neutralizing text, in which the algorithm is given an input sentence and must produce an output sentence whose meaning is as similar as possible to the input but with the subjective bias removed.", "This work presents data and algorithms for automatically reducing bias in text. We focus on a particular kind of bias: inappropriate subjectivity (\u201csubjective bias\u201d). Subjective bias occurs when language that should be neutral and fair is skewed by feeling, opinion, or taste (whether consciously or unconsciously). In practice, we identify subjective bias via the method of BIBREF2: using Wikipedia's neutral point of view (NPOV) policy. This policy is a set of principles which includes \u201cavoiding stating opinions as facts\u201d and \u201cpreferring nonjudgemental language\u201d.", "Identifying subjectivity in a sentence (explicitly or implicitly) is prerequisite to neutralizing it. We accordingly evaluate our model's (and 3,000 crowdworker's) ability to detect subjectivity using the procedure of BIBREF2 and the same 50k training examples as Section SECREF4 (Table TABREF51). For each sentence, we select the word with the highest predicted probability and test whether that word was in fact changed by the editor. The proportion of correctly selected words is the system's \u201caccuracy\u201d. Results are given in Table TABREF51.", "Subjectivity Bias. The study of subjectivity in NLP was pioneered by the late Janyce Wiebe and colleagues BIBREF41, BIBREF42. Several studies develop methods for highlighting subjective or persuasive frames in a text BIBREF43, BIBREF44, or detecting biased sentences BIBREF45, BIBREF46, BIBREF12, BIBREF47 of which the most similar to ours is BIBREF2, whose early, smaller version of WNC and logistic regression-based bias detector inspired our study.", "Overall, while modular does a better job at reducing bias, concurrent appears to better preserve the meaning and fluency of the original text. We conclude that the proposed methods, while imperfect, are capable of providing useful suggestions for how subjective bias in real-world news or political text can be reduced."]}
{"question_id": "1a0794ebbc9ee61bbb7ef2422d576a10576d9d96", "predicted_answer": "The sign language recognition task investigated is American Sign Language (ASL) recognition.", "predicted_evidence": ["All of our experiments on ASL recognition were done with RGB video data and/or skeletal data. Skeletal data is a multivariate, multidimensional time series input where each body part acts as a variable and each of them have 3D coordinate data at each time step. The skeletal data provides motion trajectory of different body parts such as wrist, elbow and shoulder (total 25 such body parts) over whole video frames. This process is called skeletal tracking. Skeletal data provides high level motion of different body parts. These are useful for capturing discriminant features associated with different types of gestures. However, for better modeling of sign language, hand shape is crucial, as different signs may have similar motion but different hand shapes and orientation. Figure FIGREF10 presents one such example where the sign pair Alarm and Doorbell have exact same motion pattern according to skeletal data but have different hand shapes.", "Most sign language recognition systems use RGB video data as input. These approaches model sequential dependencies using Hidden Markov Models (HMM). Zafrullah et al. BIBREF7 used colored gloves (worn on hands) during data collection and developed an HMM based framework for ASL phrase verification. They also used hand crafted features from Kinect skeletal data and accelerometers worn on hand BIBREF8. Huang et al. BIBREF1 demonstrated the effectiveness of using Convolutional neural network (CNN) with RGB video data for sign language recognition. Three dimensional CNN have been used to extract spatio-temporal features from video BIBREF2. Similar architecture was implemented for Italian gestures BIBREF9. Sun et al. BIBREF3 hypothesized that not all RGB frames in a video are equally important and assigned a binary latent variable to each frame in training videos for indicating the importance of a frame within a latent support vector machine model.", "All of our experiments on ASL recognition were done with RGB video data and/or skeletal data. Skeletal data is a multivariate, multidimensional time series input where each body part acts as a variable and each of them have 3D coordinate data at each time step. The skeletal data provides motion trajectory of different body parts such as wrist, elbow and shoulder (total 25 such body parts) over whole video frames. This process is called skeletal tracking. Skeletal data provides high level motion of different body parts. These are useful for capturing discriminant features associated with different types of gestures. However, for better modeling of sign language, hand shape is crucial, as different signs may have similar motion but different hand shapes and orientation. Figure FIGREF10 presents one such example where the sign pair Alarm and Doorbell have exact same motion pattern according to skeletal data but have different hand shapes. We observe similar situation for sign pairs such as Kitchen/Room, Time/Movie, Quote/Camera, Lock/Stop and many more.", "Most sign language recognition systems use RGB video data as input. These approaches model sequential dependencies using Hidden Markov Models (HMM). Zafrullah et al. BIBREF7 used colored gloves (worn on hands) during data collection and developed an HMM based framework for ASL phrase verification. They also used hand crafted features from Kinect skeletal data and accelerometers worn on hand BIBREF8. Huang et al. BIBREF1 demonstrated the effectiveness of using Convolutional neural network (CNN) with RGB video data for sign language recognition. Three dimensional CNN have been used to extract spatio-temporal features from video BIBREF2. Similar architecture was implemented for Italian gestures BIBREF9. Sun et al.", "According to The National Institute on Deafness, one in thousand infants is born deaf. An additional one to six per thousand are born with hearing loss at different levels BIBREF0. Sign language is commonly used by Deaf and Hard-of-Hearing (DHH) persons to communicate via hand gestures. An automatic sign language recognizer enables an ASL user to translate the sign language to written text or speech, allowing them to communicate with people who are not familiar with ASL. There is a tremendous rise in the popularity of personal digital assistants; available on user's personal and wearable devices (Google Now, Amazon Alexa and Apple Siri, etc.) and also in the form of standalone devices (Amazon Echo and Google Home smart speakers). These devices are primarily controlled through voice, and hence, their functionality is not readily available to DHH users. An automatic sign recognizer can also enable the interaction between a DHH user and a digital assistant."]}
{"question_id": "256dfa501a71d7784520a527f43aec0549b1afea", "predicted_answer": "The performance of the best model in the sign language recognition task is not explicitly stated in the given context. However, it can be inferred that the Spatial AI-LSTM model outperforms the other models, including the Max CNN-LSTM model, by 6%. The exact accuracy of the Spatial AI-LSTM model is not provided, but it is mentioned that it achieves a significant accuracy gain compared to the other models.", "predicted_evidence": ["Table TABREF28 shows the comparative results among our proposed architectures and baselines. Overall, we use data from 12 subjects for our experiments which sum up to 13107 sign gesture samples in total. To evaluate model performance on a specific subject (test subject), we adopt cross subject evaluation criteria. Suppose, X is the test subject. We train our networks with all sign samples except those are from subject X. We use subject X's data as test split to evaluate the performance of the networks. Table TABREF28 shows the average test accuracy for all 12 subjects. We can see that 3D CNN network alone performs worse than simpler baselines. But when coupled with AI-LSTM as Max CNN-LSTM, it shows an increase in recognition accuracy by 2% from AI-LSTM alone. This is because some of the signs are confused by the AI-LSTM network because of similar skeletal motion pattern.", "Overall, we use data from 12 subjects for our experiments which sum up to 13107 sign gesture samples in total. To evaluate model performance on a specific subject (test subject), we adopt cross subject evaluation criteria. Suppose, X is the test subject. We train our networks with all sign samples except those are from subject X. We use subject X's data as test split to evaluate the performance of the networks. Table TABREF28 shows the average test accuracy for all 12 subjects. We can see that 3D CNN network alone performs worse than simpler baselines. But when coupled with AI-LSTM as Max CNN-LSTM, it shows an increase in recognition accuracy by 2% from AI-LSTM alone. This is because some of the signs are confused by the AI-LSTM network because of similar skeletal motion pattern. Incorporating spatial relationship among joints leads to a significant accuracy gain. The Spatial AI-LSTM is trained only on skeletal data but outperforms the combined network by 6%.", "Table TABREF28 shows the comparative results among our proposed architectures and baselines. Overall, we use data from 12 subjects for our experiments which sum up to 13107 sign gesture samples in total. To evaluate model performance on a specific subject (test subject), we adopt cross subject evaluation criteria. Suppose, X is the test subject. We train our networks with all sign samples except those are from subject X. We use subject X's data as test split to evaluate the performance of the networks. Table TABREF28 shows the average test accuracy for all 12 subjects. We can see that 3D CNN network alone performs worse than simpler baselines. But when coupled with AI-LSTM as Max CNN-LSTM, it shows an increase in recognition accuracy by 2% from AI-LSTM alone. This is because some of the signs are confused by the AI-LSTM network because of similar skeletal motion pattern. Incorporating spatial relationship among joints leads to a significant accuracy gain.", "Most sign language recognition systems use RGB video data as input. These approaches model sequential dependencies using Hidden Markov Models (HMM). Zafrullah et al. BIBREF7 used colored gloves (worn on hands) during data collection and developed an HMM based framework for ASL phrase verification. They also used hand crafted features from Kinect skeletal data and accelerometers worn on hand BIBREF8. Huang et al. BIBREF1 demonstrated the effectiveness of using Convolutional neural network (CNN) with RGB video data for sign language recognition. Three dimensional CNN have been used to extract spatio-temporal features from video BIBREF2. Similar architecture was implemented for Italian gestures BIBREF9. Sun et al. BIBREF3 hypothesized that not all RGB frames in a video are equally important and assigned a binary latent variable to each frame in training videos for indicating the importance of a frame within a latent support vector machine model.", "Most sign language recognition systems use RGB video data as input. These approaches model sequential dependencies using Hidden Markov Models (HMM). Zafrullah et al. BIBREF7 used colored gloves (worn on hands) during data collection and developed an HMM based framework for ASL phrase verification. They also used hand crafted features from Kinect skeletal data and accelerometers worn on hand BIBREF8. Huang et al. BIBREF1 demonstrated the effectiveness of using Convolutional neural network (CNN) with RGB video data for sign language recognition. Three dimensional CNN have been used to extract spatio-temporal features from video BIBREF2. Similar architecture was implemented for Italian gestures BIBREF9. Sun et al."]}
{"question_id": "f85520bbc594918968d7d9f33d11639055458344", "predicted_answer": "The deep learning architectures used are:\n\n1. LSTM (Long Short-Term Memory) networks\n2. 3D CNN (Convolutional Neural Networks) networks\n3. Max CNN-LSTM network (a combination of LSTM and 3D CNN networks)\n4. AI-LSTM network (a variant of LSTM network)\n5. Spatial AI-LSTM network (a variant of LSTM network)", "predicted_evidence": ["In addition to dropout, L2 regularization was used for LSTM networks and for dense layers; $\\beta $ was set to 0.008 which controls the impact of regularization on the network. State size and number of layers of LSTM networks were 50 and 2, respectively. Learning rate for Max CNN-LSTM and LSTM networks were set to $0.00001$ and $0.00005$, respectively. We used Adam Optimizer for training our networks BIBREF24. All networks were run for a certain number of epochs (200-300) with a batch size of 64. We developed all of our models with Tensorflow 1.10 (python). Average time taken to train an AI-LSTM and an Spatial AI-LSTM are 25 and 30 minutes on an Intel(R) Core(TM) i5-7600 (3.50GHz) processor respectively. We trained 3D CNN and Max 3D CNN models on GPU (Tesla K80) and each model took around 20 hours to train.", "We propose a combination of LSTM and 3D CNN networks. We call this Max CNN-LSTM network. Figure FIGREF15 (b) represents the the Max CNN-LSTM. The details of 3D CNN module is shown in Figure FIGREF14. This architecture has two parts: one for left hand patches and other for right hand patches. Each part has four 3D convolutional layers (second and fourth layers have following maximum pooling layers) followed by 2 fully connected layers. Final embeddings from these two parts are concatenated and by using a softmax layer, from which a classification score is produced. The other AI-LSTM network is fed with skeletal time series data. At the final time step, the LSTM state vector is taken and using a softmax layer another probability score is produced. The final classification score is created by taking element wise maximum of the output scores from the two networks. During back\u2013propagation, both networks are trained on their own score.", "We call this Max CNN-LSTM network. Figure FIGREF15 (b) represents the the Max CNN-LSTM. The details of 3D CNN module is shown in Figure FIGREF14. This architecture has two parts: one for left hand patches and other for right hand patches. Each part has four 3D convolutional layers (second and fourth layers have following maximum pooling layers) followed by 2 fully connected layers. Final embeddings from these two parts are concatenated and by using a softmax layer, from which a classification score is produced. The other AI-LSTM network is fed with skeletal time series data. At the final time step, the LSTM state vector is taken and using a softmax layer another probability score is produced. The final classification score is created by taking element wise maximum of the output scores from the two networks. During back\u2013propagation, both networks are trained on their own score. The combined network acts like a model ensemble and some sign classes which are confused by RNN network alone might have an improved recognition accuracy with this approach.", "RNN has shown success in modeling sequential pattern in dataBIBREF6. It can capture temporal dynamics in data by maintaining an internal state. However, the basic RNN has problems dealing with long term dependencies in data due to the vanishing gradient problem BIBREF21. Some solutions to the vanishing gradient problem involve careful initialization of network parameters or early stopping BIBREF22. But the most effective solution is to modify the RNN architecture in such a way that there exists a memory state (cell state) at every time step that can identify what to remember and what to forget. This architecture is referred to as long short term memory (LSTM) network BIBREF23. While the basic RNN is a direct transformation of the previous state and the current input, the LSTM maintains an internal memory and has mechanisms to update and use that memory. This is achieved by deploying four separate neural networks also called gates. Figure FIGREF12 depicts a cell of an LSTM network which shows input at the current time step ${x_t}$ and the previous state ${h_{t-1}}$ enter into the cell; and get concatenated.", "RNN has shown success in modeling sequential pattern in dataBIBREF6. It can capture temporal dynamics in data by maintaining an internal state. However, the basic RNN has problems dealing with long term dependencies in data due to the vanishing gradient problem BIBREF21. Some solutions to the vanishing gradient problem involve careful initialization of network parameters or early stopping BIBREF22. But the most effective solution is to modify the RNN architecture in such a way that there exists a memory state (cell state) at every time step that can identify what to remember and what to forget. This architecture is referred to as long short term memory (LSTM) network BIBREF23. While the basic RNN is a direct transformation of the previous state and the current input, the LSTM maintains an internal memory and has mechanisms to update and use that memory. This is achieved by deploying four separate neural networks also called gates."]}
{"question_id": "e4f2d59030b17867449cf5456118ab722296bebd", "predicted_answer": "The claim \"this is because character-level models learn morphology\" is not explicitly stated in the given text. The text discusses the debate about whether character-level models learn morphology, but it does not attribute this claim to a specific person or reference.", "predicted_evidence": ["Let's turn to our main question: what do character-level models learn about morphology? To answer it, we compare the oracle model to char-lstm, our best character-level model.", "The effectiveness of character-level models in morphologically-rich languages has raised a question and indeed debate about explicit modeling of morphology in NLP. BIBREF0 propose that \u201cprior information regarding morphology ... among others, should be incorporated\u201d into character-level models, while BIBREF6 counter that it is \u201cunnecessary to consider these prior information\u201d when modeling characters. Whether we need to explicitly model morphology is a question whose answer has a real cost: as ballesteros-dyer-smith:2015:EMNLP note, morphological annotation is expensive, and this expense could be reinvested elsewhere if the predictive aspects of morphology are learnable from strings.", "Do character-level models learn morphology? We view this as an empirical claim requiring empirical evidence. The claim has been tested implicitly by comparing character-level models to word lookup models BIBREF7 , BIBREF8 . In this paper, we test it explicitly, asking how character-level models compare with an oracle model with access to morphological annotations. This extends experiments showing that character-aware language models in Czech and Russian benefit substantially from oracle morphology BIBREF9 , but here we focus on dependency parsing (\u00a7 \"Dependency parsing model\" )\u2014a task that benefits substantially from morphological knowledge\u2014and we experiment with twelve languages using a variety of techniques to probe our models.", "Our summary finding is that character-level models lag the oracle in nearly all languages (\u00a7 \"Experiments\" ). The difference is small, but suggests that there is value in modeling morphology. When we tease apart the results by part of speech and dependency type, we trace the difference back to the character-level model's inability to disambiguate words even when encoded with arbitrary context (\u00a7 \"Analysis\" ). Specifically, it struggles with case syncretism, in which noun case\u2014and thus syntactic function\u2014is ambiguous. We show that the oracle relies on morphological case, and that a character-level model provided only with morphological case rivals the oracle, even when case is provided by another predictive model (\u00a7 \"Characters and case syncretism\" ). Finally, we show that the crucial morphological features vary by language (\u00a7 \"Understanding head selection\" ).", "While our results show that prior knowledge of morphology is important, they also show that it can be used in a targeted way: our character-level models improved markedly when we augmented them only with case. This suggests a pragmatic reality in the middle of the wide spectrum between pure machine learning from raw text input and linguistically-intensive modeling: our new models don't need all prior linguistic knowledge, but they clearly benefit from some knowledge in addition to raw input. While we used a data-driven analysis to identify case syncretism as a problem for neural parsers, this result is consistent with previous linguistically-informed analyses BIBREF20 , BIBREF19 . We conclude that neural models can still benefit from linguistic analyses that target specific phenomena where annotation is likely to be useful."]}
{"question_id": "e664b58ea034a638e7142f8a393a88aadd1e215e", "predicted_answer": "The languages mentioned in the text are:\n\n1. Finnish\n2. Turkish\n3. Czech\n4. Russian\n5. Arabic\n6. Portuguese\n7. Spanish\n8. German\n9. Hindi\n10. Urdu", "predicted_evidence": ["Table 3 shows how the character model improves over the word model for both non-OOV and OOV words. On the agglutinative languages Finnish and Turkish, where the OOV rates are 23% and 24% respectively, we see the highest LAS improvements, and we see especially large improvements in accuracy of OOV words. However, the effects are more mixed in other languages, even with relatively high OOV rates. In particular, languages with rich morphology like Czech, Russian, and (unvocalised) Arabic see more improvement than languages with moderately rich morphology and high OOV rates like Portuguese or Spanish. This pattern suggests that parameter sharing between pairs of observed training words can also improve parsing performance. For example, if \u201cdog\u201d and \u201cdogs\u201d are observed in the training data, they will share activations in their context and on their common prefix.", "Now we turn to a more fine-grained analysis conditioned on the annotated part-of-speech (POS) of the dependent. We focus on four languages where the oracle strongly outperforms the best character-level model on the development set: Finnish, Czech, German, and Russian. We consider five POS categories that are frequent in all languages and consistently annotated for morphology in our data: adjective (ADJ), noun (NOUN), pronoun (PRON), proper noun (PROPN), and verb (VERB).", "Table 2 presents test results for every model on every language, establishing three results. First, they support previous findings that character-level models outperform word-based models\u2014indeed, the char-lstm model outperforms the word model on LAS for all languages except Hindi and Urdu for which the results are identical. Second, they establish strong baselines for the character-level models: the char-lstm generally obtains the best parsing accuracy, closely followed by char-cnn. Third, they demonstrate that character-level models rarely match the accuracy of an oracle model with access to explicit morphology. This reinforces a finding of BIBREF9 : character-level models are effective tools, but they do not learn everything about morphology, and they seem to be closer to oracle accuracy in agglutinative rather than in fusional languages.", "Table 9 and 10 present morphological tagging results for German and Russian. We found that German and Russian have similar pattern to Czech (Table 5 ), where morphological case seems to be preserved in the encoder because they are useful for dependency parsing. In these three fusional languages, contextual information helps character-level model to predict the correct case. However, its performance still behind the oracle.", "Table 4 shows that the three noun categories\u2014ADJ, PRON, and PROPN\u2014benefit substantially from oracle morphology, especially for the three fusional languages: Czech, German, and Russian."]}
{"question_id": "c4b621f573bbb411bdaa84a7562c9c4795a7eb3a", "predicted_answer": "No, the character-level models do not perform better than models with access to morphological analyses only. According to the text, the oracle model with access to explicit morphology generally outperforms the character-level models, suggesting that character-level models do not learn everything about morphology.", "predicted_evidence": ["Let's turn to our main question: what do character-level models learn about morphology? To answer it, we compare the oracle model to char-lstm, our best character-level model.", "The effectiveness of character-level models in morphologically-rich languages has raised a question and indeed debate about explicit modeling of morphology in NLP. BIBREF0 propose that \u201cprior information regarding morphology ... among others, should be incorporated\u201d into character-level models, while BIBREF6 counter that it is \u201cunnecessary to consider these prior information\u201d when modeling characters. Whether we need to explicitly model morphology is a question whose answer has a real cost: as ballesteros-dyer-smith:2015:EMNLP note, morphological annotation is expensive, and this expense could be reinvested elsewhere if the predictive aspects of morphology are learnable from strings.", "Do character-level models learn morphology? We view this as an empirical claim requiring empirical evidence. The claim has been tested implicitly by comparing character-level models to word lookup models BIBREF7 , BIBREF8 . In this paper, we test it explicitly, asking how character-level models compare with an oracle model with access to morphological annotations. This extends experiments showing that character-aware language models in Czech and Russian benefit substantially from oracle morphology BIBREF9 , but here we focus on dependency parsing (\u00a7 \"Dependency parsing model\" )\u2014a task that benefits substantially from morphological knowledge\u2014and we experiment with twelve languages using a variety of techniques to probe our models.", "Character-level models are effective because they can represent OOV words and orthographic regularities of words that are consistent with morphology. But they depend on context to disambiguate words, and for some words this context is insufficient. Case syncretism is a specific example that our analysis identified, but the main results in Table 2 hint at the possibility that different phenomena are at play in different languages.", "Table 2 presents test results for every model on every language, establishing three results. First, they support previous findings that character-level models outperform word-based models\u2014indeed, the char-lstm model outperforms the word model on LAS for all languages except Hindi and Urdu for which the results are identical. Second, they establish strong baselines for the character-level models: the char-lstm generally obtains the best parsing accuracy, closely followed by char-cnn. Third, they demonstrate that character-level models rarely match the accuracy of an oracle model with access to explicit morphology. This reinforces a finding of BIBREF9 : character-level models are effective tools, but they do not learn everything about morphology, and they seem to be closer to oracle accuracy in agglutinative rather than in fusional languages."]}
{"question_id": "3ccc4ccebc3b0de5546b1208e8094a839fd4a4ab", "predicted_answer": "Case syncretism refers to a phenomenon in linguistics where functionally distinct words have the same form, specifically in terms of morphological case. In other words, it occurs when different grammatical cases (e.g. nominative, accusative, dative) are represented by the same word form, making it difficult to disambiguate the correct case without context.", "predicted_evidence": ["Character-level models are effective because they can represent OOV words and orthographic regularities of words that are consistent with morphology. But they depend on context to disambiguate words, and for some words this context is insufficient. Case syncretism is a specific example that our analysis identified, but the main results in Table 2 hint at the possibility that different phenomena are at play in different languages.", "So far, we've seen that for our three fusional languages\u2014German, Czech, and Russian\u2014the oracle strongly outperforms a character model on nouns with ambiguous morphological analyses, particularly on core dependencies: nominal subjects, objects and indirect objects. Since the nominative, accusative, and dative morphological cases are strongly (though not perfectly) correlated with these dependencies, it is easy to see why the morphologically-aware oracle is able to predict them so well. We hypothesized that these cases are more challenging for the character model because these languages feature a high degree of syncretism\u2014functionally distinct words that have the same form\u2014and in particular case syncretism. For example, referring back to examples ( UID28 ) and ( UID28 ), the character model must disambiguate pis\u02camo from its context, whereas the oracle can directly disambiguate it from a feature of the word itself.", "Our summary finding is that character-level models lag the oracle in nearly all languages (\u00a7 \"Experiments\" ). The difference is small, but suggests that there is value in modeling morphology. When we tease apart the results by part of speech and dependency type, we trace the difference back to the character-level model's inability to disambiguate words even when encoded with arbitrary context (\u00a7 \"Analysis\" ). Specifically, it struggles with case syncretism, in which noun case\u2014and thus syntactic function\u2014is ambiguous. We show that the oracle relies on morphological case, and that a character-level model provided only with morphological case rivals the oracle, even when case is provided by another predictive model (\u00a7 \"Characters and case syncretism\" ). Finally, we show that the crucial morphological features vary by language (\u00a7 \"Understanding head selection\" ).", "While our results show that prior knowledge of morphology is important, they also show that it can be used in a targeted way: our character-level models improved markedly when we augmented them only with case. This suggests a pragmatic reality in the middle of the wide spectrum between pure machine learning from raw text input and linguistically-intensive modeling: our new models don't need all prior linguistic knowledge, but they clearly benefit from some knowledge in addition to raw input. While we used a data-driven analysis to identify case syncretism as a problem for neural parsers, this result is consistent with previous linguistically-informed analyses BIBREF20 , BIBREF19 . We conclude that neural models can still benefit from linguistic analyses that target specific phenomena where annotation is likely to be useful.", "To understand this, we first designed an experiment to see whether the char-lstm could successfully disambiguate noun case, using a method similar to BIBREF8 . We train a neural classifier that takes as input a word representation from the trained parser and predicts a morphological feature of that word\u2014for example that its case is nominative (Case=Nom). The classifier is a feedforward neural network with one hidden layer, followed by a ReLU non-linearity. We consider two representations of each word: its embedding ( $\\textbf {x}_i$ ; Eq. 2 ) and its encoding ( $\\textbf {h}_i$ ; Eq. 3 ). To understand the importance of case, we consider it alongside number and gender features as well as whole feature bundles."]}
{"question_id": "2dba0b83fc22995f83e7ac66cc8f68bcdcc70ee9", "predicted_answer": "Yes, humans assess the quality of the generated responses. In this study, Mechanical Turk workers were asked to evaluate the effectiveness of the generated interventions by selecting the response that can best mitigate hate speech, and also to select which of the three models has the best diversity of responses over all the conversations.", "predicted_evidence": ["In order to validate and compare the quality of the generated results from each model, we also conducted human evaluations as previous research has shown that automatic evaluation metrics often do not correlate with human preference BIBREF32. We randomly sampled 450 conversations from the testing dataset. We then generated responses using each of the above models trained with the filtered conversation setting. In each assignment, a Mechanical Turk worker is presented 10 conversations, along with corresponding responses generated by the three models. For each conversation, the worker is asked to evaluate the effectiveness of the generated intervention by selecting a response that can best mitigate hate speech. 9 of the 10 questions are filled with the sampled testing data and the generated results, while the other is artificially constructed to monitor response quality. After selecting the 10 best mitigation measures, the worker is asked to select which of the three methods has the best diversity of responses over all the 10 conversations. Ties are permitted for answers. Assignments failed on the quality check are rejected.", "In order to validate and compare the quality of the generated results from each model, we also conducted human evaluations as previous research has shown that automatic evaluation metrics often do not correlate with human preference BIBREF32. We randomly sampled 450 conversations from the testing dataset. We then generated responses using each of the above models trained with the filtered conversation setting. In each assignment, a Mechanical Turk worker is presented 10 conversations, along with corresponding responses generated by the three models. For each conversation, the worker is asked to evaluate the effectiveness of the generated intervention by selecting a response that can best mitigate hate speech. 9 of the 10 questions are filled with the sampled testing data and the generated results, while the other is artificially constructed to monitor response quality. After selecting the 10 best mitigation measures, the worker is asked to select which of the three methods has the best diversity of responses over all the 10 conversations. Ties are permitted for answers.", "The sampled results in Figure FIGREF25 show that the Seq2Seq and the RL model can generate reasonable responses for intervention. However, as is to be expected with machine-generated text, in the other human evaluation we conducted, where Mechanical Turk workers were also presented with sampled human-written responses alongside the machine generated responses, the human-written responses were chosen as the most effective and diverse option a majority of the time (70% or more) for both datasets. This indicates that there is significant room for improvement while generating automated intervention responses.", "The experimental results of the detection task and the generative intervention task are shown in Table TABREF27 and Table TABREF29 separately. The results of the human evaluation are shown in Table TABREF30. Figure FIGREF25 shows examples of the generated responses.", "Our data is collected in the form of conversations, providing better context."]}
{"question_id": "a8cc891bb8dccf0d32c1c9cd1699d5ead0eed711", "predicted_answer": "The models used to generate responses are:\n\n1. Seq2Seq\n2. VAE (Variational Autoencoder)\n3. Reinforcement Learning method for dialog generation (similar to BIBREF21)\n\nThese models are used as baselines to determine the initial feasibility of automatically generating intervention responses.", "predicted_evidence": ["where $c$ is the conversation, $r$ is the corresponding intervention response, and $D$ is the dataset. This task is closely related to the response generation and dialog generation, though several differences exist including dialog length, language cadence, and word imbalances. As a baseline, we chose the most common methods of these two tasks, such as Seq2Seq and VAE, to determine the initial feasibility of automatically generate intervention responses. More recent Reinforcement Learning method for dialog generation BIBREF21 can also be applied to this task with slight modification. Future work will explore more complex, and unique models.", "Similar to BIBREF21, a generative model is considered as an agent. However, different from dialog generation, generative intervention does not have multiple turns of utterance, so the action of the agent is to select a token in the response. The state of the agent is given by the input posts and the previously generated tokens. Another result due to this difference is that the rewards with regard to ease of answering or information flow do not apply to this case, but the reward for semantic coherence does. Therefore, the reward of the agent is:", "In order to validate and compare the quality of the generated results from each model, we also conducted human evaluations as previous research has shown that automatic evaluation metrics often do not correlate with human preference BIBREF32. We randomly sampled 450 conversations from the testing dataset. We then generated responses using each of the above models trained with the filtered conversation setting. In each assignment, a Mechanical Turk worker is presented 10 conversations, along with corresponding responses generated by the three models. For each conversation, the worker is asked to evaluate the effectiveness of the generated intervention by selecting a response that can best mitigate hate speech. 9 of the 10 questions are filled with the sampled testing data and the generated results, while the other is artificially constructed to monitor response quality. After selecting the 10 best mitigation measures, the worker is asked to select which of the three methods has the best diversity of responses over all the 10 conversations. Ties are permitted for answers.", "In order to validate and compare the quality of the generated results from each model, we also conducted human evaluations as previous research has shown that automatic evaluation metrics often do not correlate with human preference BIBREF32. We randomly sampled 450 conversations from the testing dataset. We then generated responses using each of the above models trained with the filtered conversation setting. In each assignment, a Mechanical Turk worker is presented 10 conversations, along with corresponding responses generated by the three models. For each conversation, the worker is asked to evaluate the effectiveness of the generated intervention by selecting a response that can best mitigate hate speech. 9 of the 10 questions are filled with the sampled testing data and the generated results, while the other is artificially constructed to monitor response quality. After selecting the 10 best mitigation measures, the worker is asked to select which of the three methods has the best diversity of responses over all the 10 conversations. Ties are permitted for answers. Assignments failed on the quality check are rejected.", "where $rw(c,r)$ is the reward with regard to the conversation $c$ and its reference response $r$ in the dataset. $p(r|c)$ denotes the probability of generating response $r$ given the conversation $c$, and $p_{back}(c|r)$ denotes the backward probability of generating the conversation based on the response, which is parameterized by another generation network. The reward is a weighted combination of these two parts, which are observed after the agent finishing generating the response. We refer the readers to BIBREF21 for details."]}
{"question_id": "8330242b56b63708a23c6a92db4d4bcf927a4576", "predicted_answer": "According to the provided context, the types of hate speech considered are:\n\n1. Racist\n2. Sexist\n3. Homophobic\n4. Hate speech based on:\n\t* Ethnicity\n\t* National origin\n\t* Religious affiliation\n\t* Sexual orientation\n\t* Caste\n\t* Sex\n\t* Gender\n\t* Gender identity\n\t* Serious disease or disability", "predicted_evidence": ["Q1: Which posts or comments in this conversation are hate speech?", "If the worker thinks no hate speech exists in the conversation, then the answers to both questions are \u201cn/a\u201d. To provide context, the definition of hate speech from Facebook: \u201cWe define hate speech as a direct attack on people based on what we call protected characteristics \u2014 race, ethnicity, national origin, religious affiliation, sexual orientation, caste, sex, gender, gender identity, and serious disease or disability.\u201d is presented to the workers. Also, to prevent workers from using hate speech in the response or writing responses that are too general, such as \u201cPlease do not say that\u201d, we provide additional instructions and rejected examples.", "The two data sources, Gab and Reddit, are not well studied for hate speech. Our datasets fill this gap.", "Categorize Hate Speech: This is another common strategy used by the workers. The workers classify hate speech into different categories, such as racist, sexist, homophobic, etc. This strategy is often combined with identifying hate keywords or targets of hatred. For example, \u201cThe term \"\"fa**ot\"\" comprises homophobic hate, and as such is not permitted here.\u201d", "Our datasets can be used for various hate speech tasks. In this paper, we focus on generative hate speech intervention."]}
{"question_id": "a4cf0cf372f62b2dbc7f31c600c6c66246263328", "predicted_answer": "According to the text, the proposed models (BBFNMT, BTFNMT, BSFNMT) are compared to the following baselines:\n\n1. Transformer (base)\n2. +global-deep context\n3. AllText\n4. F8W\n5. +Localness\n6. +Context-Aware SANs", "predicted_evidence": ["4) BBFNMT (based) is comparable to the +global-deep context, the best comparison system, while BBFNMT (big) slightly outperformed +global-deep context by $0.16$ BLEU scores. In particular, the parameters of BBFNMT (base/big) model, which just increased $12.1/7.9$M over the Transformer (base/big), were only 70% of the +global-deep context model. This denotes that the BBFNMT model is more efficient than the +global-deep context model. In addition, the training speed of the proposed models slightly decreased ($8\\%$), compared to the corresponding baselines.", "Baseline systems include AllText and F8W BIBREF22, BIBREF25. F8W is simply the first 8 words of the input, and AllText uses the whole text as the compression output. The $F_1$ score of ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) was used to evaluate this task BIBREF26. We use beam search with a beam size of 5, the length length normalization of 0.5, and the coverage penalty of 0.2.", "1) The baseline Transformer (base) in this work achieved a performance comparable to the original Transformer (base) BIBREF0. This indicates that it is a strong baseline NMT system.", "3) Among the proposed three methods, BTFNMT performed better than BSFNMT. This indicates that the backbone fusion at the target-side is better than at the source-side. In addition, BBFNMT (base/big) outperformed the comparison systems +Localness and +Context-Aware SANs. This indicates that the compression knowledge as an additional context can enhance NMT better.", "The experimental results are shown in Fig. FIGREF43. As can be seen from the results, in our experiments, sentence compression (re-paraphrasing) can bring performance improvement, even when the compression ratio $\\gamma =1.0$ and the sentence length is not shortened, re-paraphrasing can still bring slight improvement of translation quality. On the wmt14 EN-DE translation task, the compression ratio $\\gamma $ was set to 0.6 to get the best results."]}
{"question_id": "f7b91b99279833f9f489635eb8f77c6d13136098", "predicted_answer": "Based on the text, it appears that the most effective sentence compression technique is the proposed unsupervised ESC model, which uses a Transformer-based approach and does not rely on linguistic motivated skeleton simplification. This model outperformed other sentence compression models in different settings, including Fevry and BIBREF17's unsupervised method, and achieved +2.0 improvements on R-1, R-2, and R-L scores over the RNN-based Seq2seq model.", "predicted_evidence": ["Explicit compression rate (length) control is a common method which has been used in previous sentence compression works. BIBREF18 examined several methods of introducing target output length information, and found that they were effective without negatively impacting summarization quality. BIBREF19 introduced a length marker token that induces the model to target an output of a desired length, coarsely divided into discrete bins. BIBREF17 augmented the decoder with an additional length countdown input which is a single scalar that ticks down to 0 when the generation reached the desired length.", "To demonstrate the effectiveness of sentence compression, we compared the compressed sentences ($\\gamma = 0.6$) generated in the Transformer translation system (BBFNMT) under different settings: AllText, F8W, RandSample (random sampling), supervised ESC, Unsupervised ESC and semi-supervised ESC. Table TABREF39 shows the results on newstest2014 for the EN-DE translation task.", "To evaluate the quality of our sentence compression model, we conducted a horizontal comparison between the proposed sentence compression model and other sentence compression models in different settings. Table TABREF34 shows the comparison results. We observed that the proposed unsupervised ESC model performed substantially better than Fevry and BIBREF17's unsupervised method. The proposed supervised ESC model also substantially outperformed the RNN-based Seq2seq and BIBREF11's baseline method. That is, our supervised model gave +2.0 improvements on R-1, R-2, and R-L scores over the RNN-based Seq2seq. This means that the proposed Transformer-based approaches can generate compressed sentences of high quality.", "Different from these work, our proposed sentence compression model does not rely on any known linguistics motivated (such as syntax) skeleton simplification, but directly trains a computation motivated sentence compression model to learn to compress sentences and re-paraphrase them directly in seq2seq model. Though with a pure computation source, our sentence compression model can surprisingly generate more grammatically correct and refined sentences, and the words in the compressed sentence do not have to be the same as the original sentence. In the meantime, our sentence compression model can stably give source backbone representation exempt from unstable performance of a syntactic parser which is essential for syntactic skeleton simplification. Our sentence compression model can perform unsupervised training on large-scale data sets, and then use the supervised data for finetune, which is more promising from the results.", "Sentence compression can be conducted by a typical sequence-to-sequence model. The encoder represents the input sentence $S$ as a sequence of annotation vectors, and the decoder depends on the attention mechanism to learn the context vector for generating a compressed version $S^{^{\\prime }}$ with the key meaning of the input sentence. Recently, the new Transformer architecture proposed by BIBREF0, which fully relies on self-attention networks, has exhibited state-of-the-art translation performance for several language pairs. We follow this practice and attempt to apply the Transformer architecture to such a compression task."]}
{"question_id": "99e514acc0109b7efa4e3860ce1e8c455f5bb790", "predicted_answer": "Yes, they compare the performance of their proposed models (BBFNMT, BTFNMT, BSFNMT) against state-of-the-art systems, including +global-deep context, +Localness, and +Context-Aware SANs, as well as the baseline Transformer model.", "predicted_evidence": ["4) BBFNMT (based) is comparable to the +global-deep context, the best comparison system, while BBFNMT (big) slightly outperformed +global-deep context by $0.16$ BLEU scores. In particular, the parameters of BBFNMT (base/big) model, which just increased $12.1/7.9$M over the Transformer (base/big), were only 70% of the +global-deep context model. This denotes that the BBFNMT model is more efficient than the +global-deep context model. In addition, the training speed of the proposed models slightly decreased ($8\\%$), compared to the corresponding baselines.", "1) The baseline Transformer (base) in this work achieved a performance comparable to the original Transformer (base) BIBREF0. This indicates that it is a strong baseline NMT system.", "The experimental results are shown in Fig. FIGREF43. As can be seen from the results, in our experiments, sentence compression (re-paraphrasing) can bring performance improvement, even when the compression ratio $\\gamma =1.0$ and the sentence length is not shortened, re-paraphrasing can still bring slight improvement of translation quality. On the wmt14 EN-DE translation task, the compression ratio $\\gamma $ was set to 0.6 to get the best results.", "3) Among the proposed three methods, BTFNMT performed better than BSFNMT. This indicates that the backbone fusion at the target-side is better than at the source-side. In addition, BBFNMT (base/big) outperformed the comparison systems +Localness and +Context-Aware SANs. This indicates that the compression knowledge as an additional context can enhance NMT better.", "2) All BSFNMT, BTFNMT, and BBFNMT significantly outperformed the baseline Transformer (base/big) and only introduces a very small amount of extra parameters. This indicates that the learned compressed backbone information was beneficial for the Transformer translation system."]}
{"question_id": "2fec84a62b4028bbe6500754d9c058eefbc24d9a", "predicted_answer": "The performance of large state-of-the-art models on these datasets is as follows:\n\n* BIBREF4's work, which uses the Flair language model (much larger than ELMo), achieves an F1 score on Ontonotes 5 that is matched by the authors' model.\n* BIBREF27, the current state-of-the-art language model that uses cloze-driven pretraining, is mentioned as a reference, but its specific performance on these datasets is not provided.\n\nIt's worth noting that the authors' model, which uses a Neural CRF model with ELMo embeddings, achieves competitive results with these larger models, with an F1 score of 92.86 on CoNLL-03 and 89.32 on Ontonotes 5.", "predicted_evidence": ["We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings. We obtain slightly better CoNLL-03 F$_1$ score compared to BIBREF14 work that uses the HSCRF model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model. BIBREF14 Ononotes 5 results use subset of the dataset labels and are not comparable. Note that because of computation constrains, we did not perform extensive hyperparameter tuning except for the gazetteer dropout rate.", "The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings. We obtain slightly better CoNLL-03 F$_1$ score compared to BIBREF14 work that uses the HSCRF model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model. BIBREF14 Ononotes 5 results use subset of the dataset labels and are not comparable.", "The top part of the table shows recently published results. BIBREF14's work is using gazetteers with HSCRF and BIBREF4's work is using the Flair language model which is much larger than ELMo. BIBREF27 is the current state-of-the-art language model that uses cloze-driven pretraining. The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings.", "BIBREF14's work is using gazetteers with HSCRF and BIBREF4's work is using the Flair language model which is much larger than ELMo. BIBREF27 is the current state-of-the-art language model that uses cloze-driven pretraining. The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings. We obtain slightly better CoNLL-03 F$_1$ score compared to BIBREF14 work that uses the HSCRF model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model.", "Named-entity recognition (NER) is the task of tagging relevant entities such as person, location and organization in unstructured text. Modern NER has been dominated by neural models BIBREF0, BIBREF1 combined with contextual embeddings from language models (LMs) BIBREF2, BIBREF3, BIBREF4. The LMs are pre-trained on large amounts of unlabeled text which allows the NER model to use the syntactic and semantic information captured by the LM embeddings. On the popular benchmark datasets CoNLL-03 BIBREF5 and Ontonotes 5 BIBREF6, neural models with LMs achieved state-of-the-art results without gazetteers features, unlike earlier approaches that heavily relied on them BIBREF7. Gazetteers are lists that contain entities such as cities, countries, and person names. The gazetteers are matched against unstructured text to provide additional features to the model. Data for building gazetteers is available for multiple language from structured data resources such as Wikipedia, DBpedia BIBREF8 and Wikidata BIBREF9."]}
{"question_id": "2803709fba74e6098aae145abcbf0e9a3f4c35e5", "predicted_answer": "The baseline model used is the Neural CRF model, with and without ELMo embeddings.", "predicted_evidence": ["GazSelfAttn evaluations on CoNLL-03 and Ontonotes 5 datasets show F$_1$ score improvement over baseline model from 92.34 to 92.86 and from 89.11 to 89.32 respectively. Moreover, we perform ablation experiments to study the contribution of the different model components.", "The Wikidata types that we obtain after processing the Wikidata dumps are fine-grained. However, certain NER tasks require coarse-grained types. For instance, CoNLL-03 task has a single Location label that consists of cities, states, countries, and other geographic location. To move from fine-grained to coarse-grained types, we use the Wikidata hierarchical structure induced by the subclass_of property. Examples of subclass_of hierarchies in Wikidata are: City $\\rightarrow $ Human Settlement $\\rightarrow $ Geographic Location, and Artist $\\rightarrow $ Creator $\\rightarrow $ Person. We change the types granularity depending on the NER task by traversing up, from fine-grained types to the target coarse-grained types. For instance, we merge the Artist and Painter types to Person, and the River and Mountain types to Location.", "The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings. We obtain slightly better CoNLL-03 F$_1$ score compared to BIBREF14 work that uses the HSCRF model, and we match the Ononotes 5 F$_1$ scores of BIBREF4 that uses a much bigger model. BIBREF14 Ononotes 5 results use subset of the dataset labels and are not comparable.", "The top part of the table shows recently published results. BIBREF14's work is using gazetteers with HSCRF and BIBREF4's work is using the Flair language model which is much larger than ELMo. BIBREF27 is the current state-of-the-art language model that uses cloze-driven pretraining. The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively. We observe that GazSelfAttn relative improvements are similar with and without ELMo embeddings.", "The experimental results for NER are summarized in Table TABREF20. The top part of the table shows recently published results. BIBREF14's work is using gazetteers with HSCRF and BIBREF4's work is using the Flair language model which is much larger than ELMo. BIBREF27 is the current state-of-the-art language model that uses cloze-driven pretraining. The bottom part of the table is shows our baseline models and results with included gazetteers. We experiment with the Neural CRF model with and without ELMo embeddings. Including ELMo embeddings the CoNLL-03 and Ontonotes 5, F$_1$ score improves from 92.34 to 92.86 and 89.11 to 89.32 respectively. Without ELMo embeddings the F$_1$ score improves from 90.42 to 91.12 and 86.63 to 87 respectively."]}
{"question_id": "ec39120fb879ae10452d3f244e1e32237047005a", "predicted_answer": "According to the text, building gazetteer resources from Wikipedia's knowledge base involves using Wikipedia's instance of relationship as a resource and classical machine learning models. However, the data extracted from Wikipedia is smaller and noisier compared to Wikidata.", "predicted_evidence": ["Extracting gazetteers from structure knowledge sources was investigated by BIBREF17 and BIBREF18. They used Wikipedia's instance of relationship as a resource for building gazetteers with classical machine learning models. Compared to Wikidata, the data extracted from Wikipedia is smaller and noisier.", "In this section, we address the issue of building a high-quality gazetteer dictionary $M$ that maps entities to types, e.g., $M$[Andy Murray] $\\rightarrow $ Person. In this work, we use Wikidata, an open source structured knowledge-base, as the source of gazetteers. Although, Wikidata and DBpedia are similar knowledge bases, we choose Wikidata because, as of 2019, it provides data on around 45 million entities compared to around 5 million in DBpedia.", "We demonstrate how to use Wikidata with entity popularity filtering as a resource for building gazetteers.", "Gazetteers. We use the Wikidata gazetteers with types merged to the granularity of the CoNLL-03 and Ononotes 5 datasets. We filter non-relevant types (e.g., genome names, disease) and get a total of one million records. For CoNLL-03 and Ontonotes 5, the percentage of entities covered by gazetteers are 96% and 78% respectively, and percentage of gazetteers wrongly assigned to non-entity tokens are 41% and 41.5% respectively.", "Similar to this paper, BIBREF19 used Wikidata as a gazetteer resource. However, they did not use entity popularity to filter ambiguous entities and their gazetteer model features use simple one-hot encoding."]}
{"question_id": "ac87dd34d28c3edd9419fa0145f3d38c87d696aa", "predicted_answer": "The dataset used to train the embeddings is not explicitly mentioned in the given context. However, it is mentioned that the embeddings are learned from a \"text training corpus\" and an \"audio corpus\". The text corpus is used to train the text embeddings, and the audio corpus is used to train the audio embeddings. The size of the corpora is not specified, but it is mentioned that three different types of text embeddings were obtained for the top 1000, 3000, and 5000 words.", "predicted_evidence": ["Word embedding or Word2Vec BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 has been widely used in the area of natural language processing BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , in which text words are transformed into vector representations of fixed dimensionality BIBREF11 , BIBREF12 , BIBREF13 . This is because these vector representations carry plenty of semantic information learned from the context of the considered words in the text training corpus.", "On the other hand, assume we have the text embeddings INLINEFORM5 INLINEFORM6 , where INLINEFORM7 is the embedding of the INLINEFORM8 -th text word for the INLINEFORM9 distinct text words. Although the distributions of INLINEFORM10 and INLINEFORM11 in their respective spaces are not parallel, that is, a specific dimension in the space for INLINEFORM12 does not necessarily correspond to a specific dimension in the space for INLINEFORM13 , there should exist some consistent relationship between the two distributions. For example, the relationships among the words {France, Paris, Germany} learned from context should be consistent in some way, regardless of whether they are in text or spoken form. So we try to learn a mapping relation between the two spaces. It will be clear below such a mapping relation can be used to evaluate the phonetic and semantic information carried by the audio embeddings.", "Assume we have the audio embeddings for a set of spoken words INLINEFORM0 INLINEFORM1 , where INLINEFORM2 is the embedding obtained for a spoken word INLINEFORM3 and INLINEFORM4 is the total number of distinct spoken words in the audio corpus. On the other hand, assume we have the text embeddings INLINEFORM5 INLINEFORM6 , where INLINEFORM7 is the embedding of the INLINEFORM8 -th text word for the INLINEFORM9 distinct text words. Although the distributions of INLINEFORM10 and INLINEFORM11 in their respective spaces are not parallel, that is, a specific dimension in the space for INLINEFORM12 does not necessarily correspond to a specific dimension in the space for INLINEFORM13 , there should exist some consistent relationship between the two distributions. For example, the relationships among the words {France, Paris, Germany} learned from context should be consistent in some way, regardless of whether they are in text or spoken form. So we try to learn a mapping relation between the two spaces.", "On the other hand, we also obtained three different types of text embedding (TXT) on the same set of top 1000, 3000 and 5000 words. Type (a) Phonetic Text embedding (TXT-ph) considered precise phonetic structure but not context or semantics at all. This was achieved by a well-trained sequence-to-sequence autoencoder encoding the precise phoneme sequence of a word into a latent embedding. Type (b) Semantic Text embedding considered only context or semantics but not phonetic structure at all, and was obtained by a standard skip-gram model using one-hot representations as the input (TXT-(se,1h)). Type (c) Semantic and Phonetic Text embedding (TXT-(se,ph)) considered context or semantics as well as the precise phonetic structure, obtained by a standard skip-gram model but using the Type (a) Phonetic Text embedding (TXT-ph) as the input.", "In addition, we propose an approach for parallelizing the audio and text embeddings to be used for evaluating the phonetic and semantic information carried by the audio embeddings. These are described in Subsections SECREF2 , SECREF11 and SECREF14 respectively."]}
{"question_id": "e66a88eecf8d5d093caec1f487603534f88dd7e7", "predicted_answer": "According to the text, the speaker characteristics used are:\n\n* Speaker identity\n* Microphone characteristics\n* Background noise\n\nThese acoustic factors are jointly referred to as speaker characteristics for simplicity.", "predicted_evidence": ["We denote the audio corpus as INLINEFORM0 , which consists of INLINEFORM1 spoken words, each represented as INLINEFORM2 , where INLINEFORM3 is the acoustic feature vector for the tth frame and INLINEFORM4 is the total number of frames in the spoken word. The goal of Stage 1 is to disentangle the phonetic structure and speaker characteristics in acoustic features, and extract a vector representation for the phonetic structure only.", "A text word with a given phonetic structure corresponds to infinite number of audio signals with varying acoustic factors such as speaker characteristics, microphone characteristics, background noise, etc. All the latter acoustic factors are jointly referred to as speaker characteristics here for simplicity, which obviously disturbs the goal of phonetic-and-semantic embedding. So Stage 1 is to obtain phonetic embeddings only with speaker characteristics disentangled.", "Stage 1 - Phonetic embedding with speaker characteristics disentangled.", "As shown in the upper right corner of Figure FIGREF3 , a speaker discriminator INLINEFORM0 takes two phonetic vectors INLINEFORM1 and INLINEFORM2 as input and tries to tell if the two vectors come from the same speaker. The learning target of the phonetic encoder INLINEFORM3 is to \"fool\" this speaker discriminator INLINEFORM4 , keeping it from discriminating the speaker identity correctly. In this way, only the phonetic structure information is learned in the phonetic vector INLINEFORM5 , while only the speaker characteristics is encoded in the speaker vector INLINEFORM6 . The speaker discriminator INLINEFORM7 learns to maximize INLINEFORM8 in ( EQREF9 ), while the phonetic encoder INLINEFORM9 learns to minimize INLINEFORM10 , DISPLAYFORM0", "We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the \u201cclean\" and \u201cothers\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features."]}
{"question_id": "fef5b65263c81299acc350a101dabaf5a8cb9c6e", "predicted_answer": "The language used for the experiments is English.", "predicted_evidence": ["We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the \u201cclean\" and \u201cothers\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features.", "Stage 2 - Semantic embedding over phonetic embeddings obtained in Stage 1.", "In Table TABREF20 , we list some examples of top-10 nearest neighbors in AUD-(ph+se) (proposed), AUD-ph (with phonetic structure) and TXT-(se,1h) (with semantics). The words in red are the common words for AUD-(ph+se) and AUD-ph, and the words in bold are the common words of AUD-(ph+se) and TXT-(se,1h). For example, the word \u201cowned\" has two common semantically related words \u201clearned\" and \u201cknown\" in the top-10 nearest neighbors of AUD-(ph+se) and TXT-(se,1h). The word \u201cowned\" also has three common phonetically similar words \u201carmed\", \u201cown\" and \u201conly\" in the top-10 nearest neighbors of AUD-(ph+se) and AUD-ph. This is even clearer for the function word \u201cdidn't\". These clearly illustrate the phonetic-and-semantic nature of AUD-(ph+se).", "In Table TABREF20 , we list some examples of top-10 nearest neighbors in AUD-(ph+se) (proposed), AUD-ph (with phonetic structure) and TXT-(se,1h) (with semantics). The words in red are the common words for AUD-(ph+se) and AUD-ph, and the words in bold are the common words of AUD-(ph+se) and TXT-(se,1h). For example, the word \u201cowned\" has two common semantically related words \u201clearned\" and \u201cknown\" in the top-10 nearest neighbors of AUD-(ph+se) and TXT-(se,1h). The word \u201cowned\" also has three common phonetically similar words \u201carmed\", \u201cown\" and \u201conly\" in the top-10 nearest neighbors of AUD-(ph+se) and AUD-ph. This is even clearer for the function word \u201cdidn't\".", "A text word with a given phonetic structure corresponds to infinite number of audio signals with varying acoustic factors such as speaker characteristics, microphone characteristics, background noise, etc. All the latter acoustic factors are jointly referred to as speaker characteristics here for simplicity, which obviously disturbs the goal of phonetic-and-semantic embedding. So Stage 1 is to obtain phonetic embeddings only with speaker characteristics disentangled."]}
{"question_id": "f40e23adc8245562c8677f0f86fa5175179b5422", "predicted_answer": "The answer is no, the embedding model is not tested in any downstream task in the given context. The text only explains the architecture of the embedding model and its training process, but it does not mention any evaluation or testing of the model on a specific downstream task, such as speech recognition, language modeling, or text classification.", "predicted_evidence": ["As shown in Figure FIGREF12 , similar to the Word2Vec skip-gram model BIBREF0 , we use two encoders: semantic encoder INLINEFORM0 and context encoder INLINEFORM1 to embed the semantics over phonetic embeddings INLINEFORM2 obtained in Stage 1. On the one hand, given a spoken word INLINEFORM3 , we feed its phonetic vector INLINEFORM4 obtained from Stage 1 into INLINEFORM5 as in the middle of Figure FIGREF12 , producing the semantic embedding (in yellow) of the spoken word INLINEFORM6 . On the other hand, given the context window size INLINEFORM7 , which is a hyperparameter, if a spoken word INLINEFORM8 is in the context window of INLINEFORM9 , then its phonetic vector INLINEFORM10 is a context vector of INLINEFORM11 . For each context vector INLINEFORM12 of INLINEFORM13 , we feed it into the context encoder INLINEFORM14 in the upper part of Figure FIGREF12 , and the output is the context embedding INLINEFORM15 .", "As shown in Figure FIGREF12 , similar to the Word2Vec skip-gram model BIBREF0 , we use two encoders: semantic encoder INLINEFORM0 and context encoder INLINEFORM1 to embed the semantics over phonetic embeddings INLINEFORM2 obtained in Stage 1. On the one hand, given a spoken word INLINEFORM3 , we feed its phonetic vector INLINEFORM4 obtained from Stage 1 into INLINEFORM5 as in the middle of Figure FIGREF12 , producing the semantic embedding (in yellow) of the spoken word INLINEFORM6 . On the other hand, given the context window size INLINEFORM7 , which is a hyperparameter, if a spoken word INLINEFORM8 is in the context window of INLINEFORM9 , then its phonetic vector INLINEFORM10 is a context vector of INLINEFORM11 .", "Word embedding or Word2Vec BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 has been widely used in the area of natural language processing BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , in which text words are transformed into vector representations of fixed dimensionality BIBREF11 , BIBREF12 , BIBREF13 . This is because these vector representations carry plenty of semantic information learned from the context of the considered words in the text training corpus.", "On the other hand, assume we have the text embeddings INLINEFORM5 INLINEFORM6 , where INLINEFORM7 is the embedding of the INLINEFORM8 -th text word for the INLINEFORM9 distinct text words. Although the distributions of INLINEFORM10 and INLINEFORM11 in their respective spaces are not parallel, that is, a specific dimension in the space for INLINEFORM12 does not necessarily correspond to a specific dimension in the space for INLINEFORM13 , there should exist some consistent relationship between the two distributions. For example, the relationships among the words {France, Paris, Germany} learned from context should be consistent in some way, regardless of whether they are in text or spoken form. So we try to learn a mapping relation between the two spaces. It will be clear below such a mapping relation can be used to evaluate the phonetic and semantic information carried by the audio embeddings.", "where the last two terms in ( EQREF15 ) are cycle-constraints to ensure that both INLINEFORM0 and INLINEFORM1 are almost unchanged after transformed to the other space and back. In this way we say the two sets of embeddings are parallelized."]}
{"question_id": "50bcbb730aa74637503c227f022a10f57d43f1f7", "predicted_answer": "The baseline model in this context is not explicitly stated, but based on the discussion, it can be inferred that the baseline model is a simple SVM-Rank model with a linear kernel, without any expansion of articles via their references, and using single-paragraph articles for training. This model is used as a reference to compare the performance of the proposed model, which uses a different approach to expand articles and a different threshold for determining relevant articles.", "predicted_evidence": ["Since linear kernel was used to train the SVM-Rank model, the role of trade-off training parameter was analyzed by tuning $C$ value from 100 to 2000 with step size 100. Empirically, F1-score peaks at 0.6087 with $C$ = 600 when it comes to COLIEE 2016 training dataset. We, therefore, use this value for training the L2R model.", "Results from Table 5 also indicate that expanding the reference of an article negatively affects the performance of our model, reducing the F1-score by more than 0.02. This is because if we only expand the content of an article with the content of referential one, it is more likely to be noisy and distorted, leading to lower performance. Therefore, we conclude that a simple expansion of articles via their references does not always positively contribute to the performance of the model.", "For information retrieval task, 20% of query-article pairs are used for evaluating our model while the rest is for training. As we only consider single-paragraph articles in the training phase, if a multiple-paragraph article is relevant, all of its generated single-paragraph articles will be marked as relevant. In addition, the label for each query-article pair is set either 1 (relevant) or 0 (irrelevant). In our experiment, instead of selecting top $k$ retrieved articles as relevant articles, we consider a retrieved article $A_i$ as a relevant article if its score $S_i$ satisfies Equation ( 26 ):", "In this section, we show an example in which our proposed model using single-paragraph articles gives a correct answer in contrast with utilizing non-splitting one. Given a query with id H20-26-3: \u201cA mandate contract is gratuitous contract in principle, but if there is a special provision, the mandatary may demand renumeration from the mandator.\u201d, which refers to Article 648:", "where: $S_0$ is the highest relevant score. In other words, the score ratio of a relevant article and the most relevant article should not be lower than 85% (choosing the value 0.85 for this threshold is simply heuristic based). This is to prevent a relevant article to have a very low score as opposed to the most relevant article."]}
{"question_id": "fac273ecb3e72f2dc94cdbc797582d7225a8e070", "predicted_answer": "Based on the given context, the following factors contribute to improving the accuracy of legal question answering tasks:\n\n1. **Using a combination of features**: Combining different features, such as linguistic features, n-gram features, and similarity features, can improve the accuracy of legal question answering tasks.\n2. **Deep learning models**: Utilizing deep learning models, such as Convolutional Neural Networks (CNN), can efficiently capture the relation between a question and an article, leading to improved accuracy.\n3. **Additional features**: Providing additional features to deep learning models can help overcome the limitations of small datasets.\n4. **Natural Language Processing (NLP) techniques**: Applying NLP techniques can help design more practical and effective information retrieval systems for the legal domain.\n5. **Capturing semantic connections**: Comparing the semantic connections between the question and sentences in relevant articles can improve the accuracy of legal question answering tasks.\n6. **Using ranking models**: Employing ranking models, such as Ranking SVM, can help retrieve relevant legal articles and improve the accuracy of legal question answering tasks.\n7. **Voting of different IR models**: Using the voting of different IR models, such as Hiemstra, BM25, and PL2F, can improve the accuracy of information retrieval in the legal domain.", "predicted_evidence": ["Given a legal question, retrieving relevant legal articles and deciding whether the content of a relevant article can be used to answer the question are two vital steps in building a legal question answering system. Kim et al. BIBREF2 exploited Ranking SVM with a set of features for legal IR and Convolutional Neural Network (CNN) BIBREF3 combining with linguistic features for question answering (QA) task. However, generating linguistic features is a non-trivial task in the legal domain. Carvalho et al. BIBREF1 utilized n-gram features to rank articles by using an extension of TF-IDF. For QA task, the authors adopted AdaBoost BIBREF4 with a set of similarity features between a query and an article pair BIBREF5 to classify a query-article pair into \u201cYES\" or \u201cNO\". However, overfitting in training may be a limitation of this method. Sushimita et al. BIBREF6 used the voting of Hiemstra, BM25 and PL2F for IR task.", "Given a legal question, retrieving relevant legal articles and deciding whether the content of a relevant article can be used to answer the question are two vital steps in building a legal question answering system. Kim et al. BIBREF2 exploited Ranking SVM with a set of features for legal IR and Convolutional Neural Network (CNN) BIBREF3 combining with linguistic features for question answering (QA) task. However, generating linguistic features is a non-trivial task in the legal domain. Carvalho et al. BIBREF1 utilized n-gram features to rank articles by using an extension of TF-IDF. For QA task, the authors adopted AdaBoost BIBREF4 with a set of similarity features between a query and an article pair BIBREF5 to classify a query-article pair into \u201cYES\" or \u201cNO\". However, overfitting in training may be a limitation of this method. Sushimita et al.", "Legal text, along with other natural language text data, e.g. scientific literature, news articles or social media, has seen an exponential growth on the Internet and in specialized systems. Unlike other textual data, legal texts contain strict logical connections of law-specific words, phrases, issues, concepts and factors between sentences or various articles. Those are for helping people to make a correct argumentation and avoid ambiguity when using them in a particular case. Unfortunately, this also makes information retrieval and question answering on legal domain become more complicated than others.", "Legal Question Answering is a form of textual entailment problem BIBREF14 , which can be viewed as a binary classification task. To capture the relation between a question and an article, a set of features can be used. In the COLLIE 2015, Kim BIBREF3 efficiently applied Convolution Neural Network (CNN) for the legal QA task. However, the small dataset is a limit of deep learning models. Therefores, we provided additional features to the CNN model.", "There are two primary approaches to information retrieval (IR) in the legal domain BIBREF0 : manual knowledge engineering (KE) and natural language processing (NLP). In the KE approach, an effort is put into translating the way legal experts remember and classify cases into data structures and algorithms, which will be used for information retrieval. Although this approach often yields a good result, it is hard to be applied in practice because of time and financial cost when building the knowledge base. In contrast, NLP-based IR systems are more practical as they are designed to quickly process terabytes of data by utilizing NLP techniques. However, several challenges are presented when designing such system. For example, factors and concepts in legal language are applied in a different way from common usage BIBREF1 . Hence, in order to effectively answer a legal question, it must compare the semantic connections between the question and sentences in relevant articles found in advance BIBREF2 ."]}
{"question_id": "7c561db6847fb0416bca8a6cb5eebf689a4b1438", "predicted_answer": "The sizes of the datasets mentioned in the text are:\n\n* 20 hours (baseline AST system and GlobalPhone corpora for 7 languages)\n* 160 hours (full parallel data for AST models)\n* 150 hours (Chinese ASR set)\n* 300 hours (English ASR set)", "predicted_evidence": ["Our baseline 20-hour AST system obtains a BLEU score of 10.3 (Table TABREF7, first row), 0.5 BLEU point lower than that reported by BIBREF4. This discrepancy might be due to differences in subsampling from the 160-hour AST dataset to create the 20-hour subset, or from Kaldi parameters when computing the MFCCs.", "To look at a range of languages with similar amounts of data, we used GlobalPhone corpora from seven languages BIBREF15, each with around 20 hours of speech: Mandarin Chinese (zh), Croatian (hr), Czech (cs), French (fr), Polish (pl), Portuguese (pt), and Swedish (sv). French and Portuguese, like the source language (Spanish), belong to the Romance family of languages, while the other languages are less related\u2014especially Chinese, which is not an Indo-European language. GlobalPhone consists of read speech recorded using similar conditions across languages, and the transcriptions for Chinese are Romanized, with annotated word boundaries.", "For comparison to other work, Table TABREF16 (bottom) gives results for AST models trained on the full 160 hours of parallel data, including models with both pretraining and data augmentation. For the latter, we used the original learning schedule, but had to stop training early due to time constraints (after 15 days, compared to 8 days for complete training of the non-augmented 160h models). We find that both pretraining and augmentation still help, providing a combined gain of 3.8 (3.2) BLEU points over the baseline on the dev (test) set.", "Table TABREF16 (top) shows how data augmentation affects the results of the baseline 20h AST system, as well as three of the best-performing pretrained models from Table TABREF7. For these experiments only, we changed the learning rates of the augmented-data systems so that all models took about the same amount of time to train (see Figure FIGREF17). Despite a more aggressive learning schedule, the performance of the augmented-data systems surpasses that of the baseline and pretrained models, even those trained on the largest ASR sets (150-hr Chinese and 300-hr English).", "To account for unseen words in the test data, we model the ASR and AST text outputs via sub-word units using byte-pair encoding (BPE) BIBREF18. We do this separately for each dataset as BPE works best as a language-specific tool (i.e. it depends on the frequency of different subword units, which varies with the language). We use 1k merge operations in all cases except Hanzi, where there are around 3000 symbols initially (vs around 60 in the other datasets). For Hanzi we ran experiments with both 1k and 15k merge operations. For Chinese Romanized transcriptions we removed tone diacritics."]}
{"question_id": "13eb64957478ade79a1e81d32e36ee319209c19a", "predicted_answer": "The number of layers in their model is not explicitly stated, but we can infer the following:\n\n* There are multiple convolutional layers (at least 2, since they mention \"the second convolutional layer\").\n* There are multiple recurrent (RNN) layers (at least 2, since they mention \"the later RNN layers\").\n* The RNN layers come after the convolutional layers.\n\nSo, the model has at least 4 layers: 2 convolutional layers and 2 RNN layers. However, there may be more layers in the model that are not mentioned in the text.", "predicted_evidence": ["Separate logistic regression models were trained on the representations from each layer of the encoder. Since convolutional layers have a stride of 2, the number of frames decreases at each convolutional layer. To label the frames after a convolutional layer we eliminated every other label (and corresponding frame) from the original label sequence. For example, given label sequence S$_{\\text{1}}$ = aaaaaaann at input layer, we get sequence S$_{\\text{2}}$ = aaaan at the first convolutional layer and sequence S$_{\\text{3}}$ = aan at the second convolutional layer and at the following recurrent layers.", "Phone classification accuracy in the RNN layers drops for both English and Spanish after fine-tuning on the AST data. This is slightly surprising for Spanish, since the fine-tuning data (unlike the pretraining data) is actually Spanish speech. However, we hypothesize that for AST, higher layers of the encoder may be recruited more to encode semantic information needed for the translation task, and therefore lose some of the linear separability in the phonetic information. Nevertheless, we still see the same pattern where better end-to-end models have higher classification accuracy in the later layers.", "Results for the two classification data sets (Figure FIGREF18) show very similar patterns. In both the ASR and the AST models, the pretraining data seems to make little difference to phonetic encoding at the early layers, and classification accuracy peaks at the second CNN layer. However, the RNN layers show a clear trend where phone classification accuracy drops off more slowly for models with better ASR/AST performance (i.e., zh $>$ fr $>$ pt). That is, the later RNN layers more transparently encode language-universal phonetic information.", "Finally, we analyze the representations learned by the models and show that better performance seems to correlate with the extent to which phonetic information is encoded in a linearly separable way in the later RNN layers.", "For comparison to other work, Table TABREF16 (bottom) gives results for AST models trained on the full 160 hours of parallel data, including models with both pretraining and data augmentation. For the latter, we used the original learning schedule, but had to stop training early due to time constraints (after 15 days, compared to 8 days for complete training of the non-augmented 160h models). We find that both pretraining and augmentation still help, providing a combined gain of 3.8 (3.2) BLEU points over the baseline on the dev (test) set."]}
{"question_id": "3cfe464052f0a248b6e22c9351279403dfe34f3c", "predicted_answer": "The model's architecture is an encoder-decoder model, specifically a sequence-to-sequence (seq2seq) model with a convolutional neural network (CNN) and recurrent neural network (RNN) layers.", "predicted_evidence": ["For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure FIGREF1: the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2. Details of the architecture and training parameters are described in Section SECREF9.", "WERs for our pre-trained models (Table TABREF7) vary from 22.5 for the large AISHELL dataset with Romanized transcript to 80.5 for Portuguese GlobalPhone. These are considerably worse than state-of-the-art ASR systems (e.g., Kaldi recipes can achieve WER of 7.5 on AISHELL and 26.5 on Portuguese GlobalPhone), but we did not optimize our architecture or hyperparameters for the ASR task since our main goal is to analyze the relationship between pretraining and AST performance (and in order to use pretraining, we must use a seq2seq model with the architecture as for AST).", "Finally, we analyze the representations learned by the models and show that better performance seems to correlate with the extent to which phonetic information is encoded in a linearly separable way in the later RNN layers.", "For comparison to other work, Table TABREF16 (bottom) gives results for AST models trained on the full 160 hours of parallel data, including models with both pretraining and data augmentation. For the latter, we used the original learning schedule, but had to stop training early due to time constraints (after 15 days, compared to 8 days for complete training of the non-augmented 160h models). We find that both pretraining and augmentation still help, providing a combined gain of 3.8 (3.2) BLEU points over the baseline on the dev (test) set.", "Results for the two classification data sets (Figure FIGREF18) show very similar patterns. In both the ASR and the AST models, the pretraining data seems to make little difference to phonetic encoding at the early layers, and classification accuracy peaks at the second CNN layer. However, the RNN layers show a clear trend where phone classification accuracy drops off more slowly for models with better ASR/AST performance (i.e., zh $>$ fr $>$ pt). That is, the later RNN layers more transparently encode language-universal phonetic information."]}
{"question_id": "119c404da6e42d4879eee10edeab4b2851162659", "predicted_answer": "The languages used in this study are:\n\n1. Mandarin Chinese (zh)\n2. Croatian (hr)\n3. Czech (cs)\n4. French (fr)\n5. Polish (pl)\n6. Portuguese (pt)\n7. Swedish (sv)\n8. Spanish (mentioned as the source language)\n9. English (used in the Switchboard corpus)", "predicted_evidence": ["To look at a range of languages with similar amounts of data, we used GlobalPhone corpora from seven languages BIBREF15, each with around 20 hours of speech: Mandarin Chinese (zh), Croatian (hr), Czech (cs), French (fr), Polish (pl), Portuguese (pt), and Swedish (sv). French and Portuguese, like the source language (Spanish), belong to the Romance family of languages, while the other languages are less related\u2014especially Chinese, which is not an Indo-European language. GlobalPhone consists of read speech recorded using similar conditions across languages, and the transcriptions for Chinese are Romanized, with annotated word boundaries.", "Following up on this work, we tried pretraining using 124 hours of multilingual data (all GlobalPhone languages except Chinese), roughly the amount of data in our large Chinese models. We combined all the data together and trained an ASR model using a common target BPE with 6k merge operations, then transferred only the encoder to the AST model. However, we did not see a benefit to the multilingual training (Table TABREF7, final row); in fact the resulting AST model was slightly worse than the zh-ai-large model (BLEU of 13.3 vs 14.6). Other configurations of multilingual training might still outperform their monolingual counterparts, but we leave this investigation as future work.", "To explore the effects of using a large amount of pretraining data from an unrelated language, we used the AISHELL-1 corpus of Mandarin Chinese BIBREF16, which contains 150 hours of read speech. Transcriptions with annotated word boundaries are available in both Hanzi (Chinese characters) and Romanized versions, and we built models with each. To compare to the GlobalPhone data, we also created a 20-hour subset of the Romanized AISHELL (zh-ai-small) by randomly selecting utterances from a subset of the speakers (81, roughly the number present in most of the GlobalPhone datasets).", "Finally, to reproduce one of the experiments from BIBREF4, we pre-trained one model using 300 hours of Switchboard English BIBREF17. This data is the most similar to the AST speech data in terms of style and channel (both are conversational telephone speech). However, as noted by BIBREF4, the Fisher Spanish speech contains many words that are actually in English (code-switching), so pretraining on English may provide an unfair advantage relative to other languages.", "To account for unseen words in the test data, we model the ASR and AST text outputs via sub-word units using byte-pair encoding (BPE) BIBREF18. We do this separately for each dataset as BPE works best as a language-specific tool (i.e. it depends on the frequency of different subword units, which varies with the language). We use 1k merge operations in all cases except Hanzi, where there are around 3000 symbols initially (vs around 60 in the other datasets). For Hanzi we ran experiments with both 1k and 15k merge operations. For Chinese Romanized transcriptions we removed tone diacritics."]}
{"question_id": "32f2aa2df0152050cbcd27dd2f408b2fa5894031", "predicted_answer": "Yes, there are experiments with real data. The text mentions that the authors trained a seq-to-seq model on real data, specifically on clean and far-field test sets, and evaluated its performance. Additionally, they also experimented with data augmentation techniques, such as convolving 40% of the training utterances with room impulse responses, to improve the model's performance on far-field audio.", "predicted_evidence": ["Most of these problems can be mitigated by training the models on a large volume of data that exemplify these effects. However, in the case of non-stationary processes, such as accents, accurate data augmentation is most likely infeasible, and in general, collecting high quality datasets can be expensive and time-consuming. Past robust ASR literature has considered hand-engineered front-ends and data-driven approaches in an attempt to increase the value of relatively parsimonious data with desired effects BIBREF2 , BIBREF3 . While these techniques are quite effective in their respective operating regimes, they do not generalize well to other modalities in practice due to the aforementioned reasons. Namely, it is difficult to model anything beyond reverberation and background noise from the first principles. Existing techniques do not directly induce invariance for ASR or are not scalable. And, due to the sequential nature of speech, alignments are needed to compare two different utterances of the same text.", "Even with data augmentation, however, there is still a large gap between the WERs on near-field and far-field test sets. The bottom two rows of Table 3 show the performance of the methods introduced in this paper on the same test sets. An $L^1$ -distance penalty can lower the test set WER by 1.32% absolute. Using a GAN enhancer can reduce the WER by an additional 1.07%. Overall, the gap between near-field and far-field performance decreases by almost 27% compared to the model that only uses data augmentation.", "To study the effects of data augmentation, we train a new seq-to-seq model with the same architecture and training procedure as the baseline. However this time, in each epoch, we randomly select 40% of the training utterances and apply the train RIRs to them (in our previous experiments we had observed that 40% augmentation results in the best validation performance).", "To overcome this, we train a stronger baseline with simulated far-field audio examples. This model had the same architecture but 40% of the examples that the model was trained on were convolved with a randomly chosen room impulse response during training. We can see from Table 3 that simple data augmentation can significantly improve performance on far-field audio without compromising the performance on near-field audio, implying that seq-to-seq models have a strong ability to learn from far-field examples.", "To establish a baseline, in the first experiment, we trained a simple attention based seq-to-seq model. All the seq-to-seq networks in our experiments were trained using the Adam optimizer. We evaluate all models on both clean and far-field test sets."]}
{"question_id": "065623cc1d5f5b19ec1f84d286522fc2f805c6ce", "predicted_answer": "The supervised machine learning models used in this project are:\n\n1. ZeroR\n2. Na\u00efve Bayes\n3. J48 (a decision tree classifier)\n4. Random Forest", "predicted_evidence": ["We created features for our model based on POS tags and their combinations. The sets of features and the combinations are learned automatically from annotated examples. We used these novel features to make our model more domain-independent.", "We investigated the problem of automated extraction of condition-action from clinical guidelines based on an annotated corpus. We proposed a simple supervised model which classifies statements based on combinations of part of speech tags used as features. We showed results of classifiers using this model on three different annotated datasets which we created. We release these dataset for others to use.", "We propose a supervised machine learning model classifying sentences as to whether they express a condition or not. After we determine a sentence contain a condition, we use natural language processing and information extraction tools to extract conditions and resulting activities.", "Research on CIGs started about 20 years ago and became more popular in the late-1990s and early 2000s. Different approaches have been developed to represent and execute clinical guidelines over patient-specific clinical data. They include document-centric models, decision trees and probabilistic models, and \"Task-Network Models\"(TNMs) BIBREF5 , which represent guideline knowledge in hierarchical structures containing networks of clinical actions and decisions that unfold over time. Serban et. al BIBREF6 developed a methodology for extracting and using linguistic patterns in guideline formalization, to aid the human modellers in guideline formalization and reduce the human modelling effort. Kaiser et. al BIBREF7 developed a method to identify activities to be performed during a treatment which are described in a guideline document. They used relations of the UMLS Semantic Network BIBREF8 to identify these activities in a guideline document.", "We extracted candidate statements by applying aforementioned regex on POS tags. Hypertension, asthma, and rhinosinusitis guidelines had 278, 172, and 761 candidate statements respectively. By applying this filtering subtask, we get rid of 38, 116, and 5 no condition statement respectively from guidelines. We used Weka BIBREF10 classifiers to create our models. ZeroR, Na\u00efve Bayes, J48, and random forest classifiers were applied in our project. Table 3 , 4 , and 5 show the results of classifiers for each guidelines.The results are based on 10-fold cross-validation on respective datasets."]}
{"question_id": "5c17559749810c67c50a7dbe34580d5e3b4f9acb", "predicted_answer": "No, the supervised machine learning approach does not outperform previous work. The results are lower than previously reported by BIBREF4, and the authors acknowledge that there is a gap in performance that they hope to close in ongoing work.", "predicted_evidence": ["We extracted candidate statements by applying aforementioned regex on POS tags. Hypertension, asthma, and rhinosinusitis guidelines had 278, 172, and 761 candidate statements respectively. By applying this filtering subtask, we get rid of 38, 116, and 5 no condition statement respectively from guidelines. We used Weka BIBREF10 classifiers to create our models. ZeroR, Na\u00efve Bayes, J48, and random forest classifiers were applied in our project. Table 3 , 4 , and 5 show the results of classifiers for each guidelines.The results are based on 10-fold cross-validation on respective datasets.", "Notice that these results are lower than previously reported by BIBREF4 . The difference is due to our using of completely automated feature selection when training on an annotated corpus, and not relying on manually created extraction rules. In addition, their results demonstrate recalls on activities with specific patterns. If we consider all activities in their annotated corpus, their recall will be 56%. And if we apply their approach on our annotated corpus, the recall will be 39%. In ongoing work we hope to reduce or close this gap by adding semantic and discourse information to our feature sets.", "The results show that generally random forest classifier seems to work best in extracting Condition-Action statements.", "We created features for our model based on POS tags and their combinations. The sets of features and the combinations are learned automatically from annotated examples. We used these novel features to make our model more domain-independent.", "We investigated the problem of automated extraction of condition-action from clinical guidelines based on an annotated corpus. We proposed a simple supervised model which classifies statements based on combinations of part of speech tags used as features. We showed results of classifiers using this model on three different annotated datasets which we created. We release these dataset for others to use."]}
{"question_id": "1c0a575e289eb486d3e6375d6f783cc2bf18adf9", "predicted_answer": "The size of the released data set is not explicitly stated in the given context. However, it is mentioned that the data set consists of three medical guidelines documents, which provide statements, tables, and figures about hypertension, rhinosinusitis, and asthma. Additionally, it is mentioned that two of the annotated corpora (hypertension and rhinosinusitis) are new, and the asthma corpus was investigated previously by BIBREF4.", "predicted_evidence": ["We use three medical guidelines documents to create gold standard datasets. They provide statements, tables, and figures about hypertension, rhinosinusitis, and asthma. The creation of the gold standard datasets is described below in detail.", "Notice that these results are lower than previously reported by BIBREF4 . The difference is due to our using of completely automated feature selection when training on an annotated corpus, and not relying on manually created extraction rules. In addition, their results demonstrate recalls on activities with specific patterns. If we consider all activities in their annotated corpus, their recall will be 56%. And if we apply their approach on our annotated corpus, the recall will be 39%. In ongoing work we hope to reduce or close this gap by adding semantic and discourse information to our feature sets.", "Clinical decision-support system (CDSS) is any computer system intended to provide decision support for healthcare professionals, and using clinical data or knowledge BIBREF0 . The classic problem of diagnosis is only one of the clinical decision problems. Deciding which questions to ask, tests to order, procedures to perform, treatment to indicate, or which alternative medical care to try, are other examples of clinical decisions. CDSSs generally fall into two categories BIBREF0", "Our data preparation process proceeded as follows: We started by converting the guidelines from PDF or html to text format, editing sentences only to manage conversion errors, the majority of which were bullet points. Tables and some figures pose a problem, and we are simply treating them as unstructured text. We are not dealing at this time with the ambiguities introduced by this approach; we do have plans to address it in future work.", "Hypertension, asthma, and rhinosinusitis guidelines and gold standard datasets were applied to evaluate our model. Since two of these annotated corpora are new, our model is establishing a baseline. The asthma corpus was investigated previously by BIBREF4 ."]}
{"question_id": "4efe0d62bba618803ec12b63f32debb8b757dd68", "predicted_answer": "An example of a condition-action pair is:\n\n\"Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation\"\n\nHere, the condition is \"Conditions that affect erythrocyte turnover and hemoglobin variants\" and the action is \"must be considered\".", "predicted_evidence": ["We also annotated the type of sentences based on their semantics: We classified them into three classes: condition-action, condition-consequence(effect, intention, and event) and action. Examples are shown in table 1.", "However, completely automated extraction of condition-action statements does not seem possible. This is due among other things to the variety of linguistic expressions used in condition-action sentences. For example, they are not always in form of \"{if} condition {then} action\u201d. In the sentence \"Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation\u201d, we have a condition-action sentence without an \"{if}\" term.", "We will briefly discuss the modeling and annotation of condition-action for medical usage in this section. Our corpus and method of identifying conditions in clinical guidelines is explained in section 3.", "Most of the condition-action sentences have a modifier in the sentences. For example, in \"In the population aged 18 years or older with CKD and hypertension, initial (or add-on) antihypertensive treatment should include an ACEI or ARB to improve kidney outcomes\", we have \"the population aged 18 years or older with CKD and hypertension\" as a condition and \"{in}\" is the modifier. \"If\", \"in\", \"for\", \"to\", \"which\", and \"when\" are the most frequent modifiers in our guidelines.", "We define the task as classification task. Given an input statement, classify it to one of the three categories: NC (no condition) if the statement doesn\u2019t have a condition; CA if the statement is a condition-action sentence; and CC (condition-consequence) if the statement has a condition which has a non-action consequence. For a CDSS, to determine both \"what is true\" about a patient and \"what to do\" with a patient, CC and CA statements can be merged to one category."]}
{"question_id": "97708d93bccc832ea671dc31a76dad6a121fcd60", "predicted_answer": "According to the text, 21 automated metrics were compared, including novel grammar-based ones.", "predicted_evidence": ["$\\bullet $ Conduct a detailed error analysis, which suggests that, while metrics can be reasonable indicators at the system-level, they are not reliable at the sentence-level.", "In this section, we attempt to uncover why automatic metrics perform so poorly.", "A detailed error analysis suggests that automatic metrics are particularly weak in distinguishing outputs of medium and good quality, which can be partially attributed to the fact that human judgements and metrics are given on different scales. We also show that metric performance is data- and system-specific.", "$\\bullet $ Compare a large number of 21 automated metrics, including novel grammar-based ones.", "We now evaluate a more coarse measure, namely the metrics' ability to predict relative human ratings. That is, we compute the score of each metric for two system output sentences corresponding to the same MR. The prediction of a metric is correct if it orders the sentences in the same way as median human ratings (note that ties are allowed). Following previous work BIBREF22 , BIBREF8 , we mainly concentrate on WBMs. Results summarised in Table 4 show that most metrics' performance is not significantly different from that of a random score (Wilcoxon signed rank test). While the random score fluctuates between 25.4\u201344.5% prediction accuracy, the metrics achieve an accuracy of between 30.6\u201349.8%. Again, the performance of the metrics is dataset-specific: Metrics perform best on Bagel data; for SFHotel, metrics show mixed performance while for SFRest, metrics perform worst."]}
{"question_id": "f11856814a57b86667179e1e275e4f99ff1bcad8", "predicted_answer": "The NLG tasks considered in this paper are:\n\n1. Target end-to-end data-driven NLG\n2. Sentence planning (e.g. accuracy, adequacy, or correctness)\n3. Surface realization (e.g. clarity or fluency)\n\nAdditionally, the paper also mentions related fields such as:\n\n1. Machine Translation (MT)\n2. Summarisation\n3. Image captioning\n4. Grammatical error correction", "predicted_evidence": ["This paper shows that state-of-the-art automatic evaluation metrics for NLG systems do not sufficiently reflect human ratings, which stresses the need for human evaluations. This result is opposed to the current trend of relying on automatic evaluation identified in BIBREF1 .", "NLG evaluation has borrowed a number of automatic metrics from related fields, such as MT, summarisation or image captioning, which compare output texts generated by systems to ground-truth references produced by humans. We refer to this group as word-based metrics. In general, the higher these scores are, the better or more similar to the human references the output is. The following order reflects the degree these metrics move from simple $n$ -gram overlap to also considering term frequency (TF-IDF) weighting and semantically similar words.", "$\\bullet $ Target end-to-end data-driven NLG, where we compare 3 different approaches. In contrast to NLG methods evaluated in previous work, our systems can produce ungrammatical output by (a) generating word-by-word, and (b) learning from noisy data.", "Table 6 summarises results published by previous studies in related fields which investigate the relation between human scores and automatic metrics. These studies mainly considered WBMs, while we are the first study to consider GBMs. Some studies ask users to provide separate ratings for surface realisation (e.g. asking about `clarity' or `fluency'), whereas other studies focus only on sentence planning (e.g. `accuracy', `adequacy', or `correctness'). In general, correlations reported by previous work range from weak to strong. The results confirm that metrics can be reliable indicators at system-level BIBREF4 , while they perform less reliably at sentence-level BIBREF2 . Also, the results show that the metrics capture realization better than sentence planning. There is a general trend showing that best-performing metrics tend to be the more complex ones, combining word-overlap, semantic similarity and term frequency weighting. Note, however, that the majority of previous works do not report whether any of the metric correlations are significantly different from each other.", "Grammar-based measures have been explored in related fields, such as MT BIBREF25 or grammatical error correction BIBREF26 , and, in contrast to WBMs, do not rely on ground-truth references. To our knowledge, we are the first to consider GBMs for sentence-level NLG evaluation. We focus on two important properties of texts here \u2013 readability and grammaticality:"]}
{"question_id": "0bb97991fc297aa5aed784568de52d5b9121f920", "predicted_answer": "According to the text, the state-of-the-art (SOTA) methods are compared to the introduced centroid variants, specifically the global + new-TF-IDF variant and the global + N-first variant.", "predicted_evidence": ["The modified sentence selection method is less efficient than the orginal method since at each iteration the score of a possible summary has to be computed for all remaining candidate sentences. It may not be noticeable for a small number of input sentences. However, it would have an impact if the amount of input documents was larger, e.g. for the summarization of top-100 search results in document retrieval.", "Both the global optimization and the sentence preselection have a positive impact on the performance.", "The global + new-TF-IDF variant outperforms all but the DPP model in Rouge-1 recall. The global + N-first variant outperforms all other models in Rouge-2 recall. However, the Rouge scores of the SOTA methods and the introduced centroid variants are in a very similar range.", "The modified methods can also be used as strong baselines for future experiments in multi-document summarization.", "Changing from a ranking-based method to a global optimization method increases performance and makes the summarizer less dependent on explicitly checking for redundancy. This can be useful for input document collections with differing levels of content diversity."]}
{"question_id": "7ba6330d105f49c7f71dba148bb73245a8ef2966", "predicted_answer": "The performance metric mentioned in the given context is the ROUGE score.", "predicted_evidence": ["Table TABREF9 shows the Rouge scores measured in our experiments.", "Both the global optimization and the sentence preselection have a positive impact on the performance.", "The sentences are ranked separately in each document by their cosine similarity to the centroid vector, in decreasing order. The INLINEFORM0 best sentences of each document are selected as candidates.", "Interestingly, the original centroid-based model, without any of the new modifications introduced in this paper, already shows quite high Rouge scores in comparison to the other baseline methods. This is due to the anti-redundancy filter and the selection of top-ranking features.", "The similarity threshold for avoiding redundancy ( INLINEFORM0 ) and the vocabulary-included-in-centroid ratio ( INLINEFORM1 ) are tuned with the original centroid model on our development set. Values from 0 to 1 with step size INLINEFORM2 were tested using a grid search. The optimal values for INLINEFORM3 and INLINEFORM4 were INLINEFORM5 and INLINEFORM6 , respectively. These values were used for all tested variants of the centroid model. For the different methods of choosing INLINEFORM7 sentences of each document before summarization, we tuned INLINEFORM8 separately for each, with values from 1 to 10, using the global model. The best INLINEFORM9 found for INLINEFORM10 -first, INLINEFORM11 -best, new-tfidf were 7, 2 and 3 respectively."]}
{"question_id": "157de5175259d6f25db703efb299f948dae597b7", "predicted_answer": "The original model referred to is the centroid-based model described by BIBREF5, which represents sentences as BOW vectors with TF-IDF weighting and scores sentences based on cosine similarity with the centroid vector.", "predicted_evidence": ["BIBREF7 implement this original model with the following modifications:", "This model, which includes the anti-redundancy filter and the selection of top-ranking features, is treated as the \"original\" centroid-based model in this paper.", "The global + new-TF-IDF variant outperforms all but the DPP model in Rouge-1 recall. The global + N-first variant outperforms all other models in Rouge-2 recall. However, the Rouge scores of the SOTA methods and the introduced centroid variants are in a very similar range.", "The first INLINEFORM0 sentences of the document are selected. This results in a mixture of a lead- INLINEFORM1 baseline and the centroid-based method.", "The original centroid-based model is described by BIBREF5 . It represents sentences as BOW vectors with TF-IDF weighting. The centroid vector is the sum of all sentence vectors and each sentence is scored by the cosine similarity between its vector representation and the centroid vector. Cosine similarity measures how close two vectors INLINEFORM0 and INLINEFORM1 are based on their angle and is defined as follows: DISPLAYFORM0"]}
{"question_id": "cf3fab54b2b289b66e7dba4706c47a62569627c5", "predicted_answer": "According to the context, sentences are selected prior to making the summary by ranking them based on a score that indicates how well each sentence summarizes the input. The top-ranked sentences are then selected to create the summary.", "predicted_evidence": ["A summary is selected by de-queuing the ranked list of sentences in decreasing order until the desired summary length is reached.", "In order to keep the method efficient, we outline different methods to select a small number of candidate sentences from each document in the input collection before constructing the summary.", "The first two sections show results for baseline and SOTA summaries from SumRepo. The third section shows the summarization variants presented in this paper. \"G\" indicates that the global greedy algorithm was used instead of sentence-level ranking. In the last section, \"- R\" indicates that the method was tested without the anti-redundancy filter.", "Many approaches are based on sentence ranking, i.e. assigning each sentence a score that indicates how well the sentence summarizes the input BIBREF0 , BIBREF1 , BIBREF2 . A summary is created by selecting the top entries of the ranked list of sentences. Since the sentences are often treated separately, these models might allow redundancy in the summary. Therefore, they are often extended by an anti-redundancy filter while de-queuing ranked sentence lists.", "The modified sentence selection method is less efficient than the orginal method since at each iteration the score of a possible summary has to be computed for all remaining candidate sentences. It may not be noticeable for a small number of input sentences. However, it would have an impact if the amount of input documents was larger, e.g. for the summarization of top-100 search results in document retrieval."]}
{"question_id": "000549a217ea24432c0656598279dbb85378c113", "predicted_answer": "No, they do not evaluate only on English datasets. However, they do filter out tweets written in languages other than English in their Twitter dataset.", "predicted_evidence": ["We also investigate the occurrence of markers in the two platforms via frequency analysis (Table TABREF29 ). We report the mean of occurrence per utterance and the standard deviation (SD) of each marker. Table TABREF29 shows that markers such as hyperbole, punctuations, and interjections are popular in both platforms. Emojis and emoticons, although the two most popular markers in INLINEFORM0 are almost unused in INLINEFORM1 . Exclamations and INLINEFORM2 s are more common in the INLINEFORM3 corpus. Next, we combine each marker with the type they belong to (i.e., either trope, morpho-syntactic and typographic) and compare the means between each pair of types via independent t-tests. We found that the difference of means is significant ( INLINEFORM4 ) for all pair of types across the two platforms.", "For INLINEFORM4 , Table TABREF24 shows that removal of typographic markers such as emoticons does not affect the F1 scores, whereas the removal of morpho-syntactic markers, e.g., tag questions, interjections have a negative effect on the F1. Table TABREF25 and Table TABREF26 represent the INLINEFORM5 most discriminative features for both categories based on the feature weights learned during the SVM training for INLINEFORM6 and INLINEFORM7 , respectively. Table TABREF25 shows that for INLINEFORM8 , typographic features such as emojis and emoticons have the highest feature weights for both categories. Interestingly, we observe that for ironic tweets users often express negative sentiment directly via emojis (e.g., angry face, rage) whereas for non-ironic utterances, emojis with positive sentiments (e.g., hearts, wedding) are more familiar. For INLINEFORM9 (Table TABREF26 ), we observe that instead of emojis, other markers such as exclamation marks, negative tag questions, and metaphors are discriminatory markers for the irony category.", "Twitter: We use a set of 350K tweets for our experiments. The ironic/sarcastic tweets are collected using hashtags, such as #irony, #sarcasm, and #sarcastic whereas the non-sarcastic tweets do not contain these hashtags, but they might include sentiment hashtags, such as #happy, #love, #sad, #hate (similar to BIBREF8 , BIBREF9 ). As pre-processing, we removed the retweets, spam, duplicates, and tweets written in languages other than English. Also, we deleted all tweets where the hashtags of interest were not located at the very end (i.e., we eliminated \u201c#sarcasm is something that I love\u201d). We lowercased the tweets, except the words where all the characters are uppercased.", "This type of markers appear at the morphologic and syntactic levels of an utterance.", "Three types of markers \u2014 tropes, morpho-syntactic, and typographic are used as features."]}
{"question_id": "63d2e97657419a0185127534f4ff9d0039cb1a63", "predicted_answer": "The type of frequency analysis used is not explicitly stated, but based on the context, it appears to be a descriptive frequency analysis, where the mean and standard deviation of the occurrence of each marker per utterance are reported.", "predicted_evidence": ["We also investigate the occurrence of markers in the two platforms via frequency analysis (Table TABREF29 ). We report the mean of occurrence per utterance and the standard deviation (SD) of each marker. Table TABREF29 shows that markers such as hyperbole, punctuations, and interjections are popular in both platforms. Emojis and emoticons, although the two most popular markers in INLINEFORM0 are almost unused in INLINEFORM1 . Exclamations and INLINEFORM2 s are more common in the INLINEFORM3 corpus. Next, we combine each marker with the type they belong to (i.e., either trope, morpho-syntactic and typographic) and compare the means between each pair of types via independent t-tests. We found that the difference of means is significant ( INLINEFORM4 ) for all pair of types across the two platforms.", "Three types of markers \u2014 tropes, morpho-syntactic, and typographic are used as features.", "This type of markers appear at the morphologic and syntactic levels of an utterance.", "With the advent of social media, irony and sarcasm detection has become an active area of research in Natural Language Processing (NLP) BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . Most computational studies have focused on building state-of-the-art models to detect whether an utterance or comment is ironic/sarcastic or not, sometimes without theoretical grounding. In linguistics and discourse studies, BIBREF4 (2000) and later BIBREF5 (2010) have studied two theoretical aspects of irony in the text: irony factors' and irony markers. Irony factors are characteristics of ironic utterances that cannot be removed without destroying the irony. In contrast, irony markers are a meta-communicative clue that \u201calert the reader to the fact that a sentence is ironical\u201d BIBREF4 . They can be removed and the utterance is still ironic.", "We present three contributions in this paper. First, we provide a detailed investigation of a set of theoretically-grounded irony markers (e.g., tropes, morpho-syntactic, and typographic markers) in social media. We conduct the classification and frequency analysis based on their occurrence. Second, we analyze and compare the use of irony markers on two social media platforms ( INLINEFORM0 and INLINEFORM1 ). Third, we provide an analysis of markers on topically different social media content (e.g., technology vs. political subreddits)."]}
{"question_id": "43f43b135109ebd1d2d1f9af979c64ce550b5f0f", "predicted_answer": "The type of classifier used is a Support Vector Machines (SVM) classifier with a linear kernel.", "predicted_evidence": ["Three types of markers \u2014 tropes, morpho-syntactic, and typographic are used as features.", "We first conduct a binary classification task to decide whether an utterance (e.g., a tweet or a INLINEFORM0 post) is ironic or non-ironic, exclusively based on the irony marker features. We use Support Vector Machines (SVM) classifier with linear kernel BIBREF16 . Table TABREF23 and Table TABREF24 present the results of the ablation tests for INLINEFORM1 and INLINEFORM2 . We report Precision ( INLINEFORM3 ), Recall ( INLINEFORM4 ) and INLINEFORM5 scores of both INLINEFORM6 and INLINEFORM7 categories.", "This type of markers appear at the morphologic and syntactic levels of an utterance.", "We also investigate the occurrence of markers in the two platforms via frequency analysis (Table TABREF29 ). We report the mean of occurrence per utterance and the standard deviation (SD) of each marker. Table TABREF29 shows that markers such as hyperbole, punctuations, and interjections are popular in both platforms. Emojis and emoticons, although the two most popular markers in INLINEFORM0 are almost unused in INLINEFORM1 . Exclamations and INLINEFORM2 s are more common in the INLINEFORM3 corpus. Next, we combine each marker with the type they belong to (i.e., either trope, morpho-syntactic and typographic) and compare the means between each pair of types via independent t-tests. We found that the difference of means is significant ( INLINEFORM4 ) for all pair of types across the two platforms.", "Table TABREF25 and Table TABREF26 represent the INLINEFORM5 most discriminative features for both categories based on the feature weights learned during the SVM training for INLINEFORM6 and INLINEFORM7 , respectively. Table TABREF25 shows that for INLINEFORM8 , typographic features such as emojis and emoticons have the highest feature weights for both categories. Interestingly, we observe that for ironic tweets users often express negative sentiment directly via emojis (e.g., angry face, rage) whereas for non-ironic utterances, emojis with positive sentiments (e.g., hearts, wedding) are more familiar. For INLINEFORM9 (Table TABREF26 ), we observe that instead of emojis, other markers such as exclamation marks, negative tag questions, and metaphors are discriminatory markers for the irony category. In contrary, for the non-irony category, positive tag questions and negative sentiment hyperboles are influential features."]}
{"question_id": "e797634fa77e490783b349034f9e095ee570b7a9", "predicted_answer": "The authors of the paper did not annotate the Twitter and Reddit data for irony themselves. Instead, they used existing datasets and corpora that were already annotated for irony.\n\nFor Reddit, they used a corpus introduced by BIBREF10 (2018), which contains sarcastic and non-sarcastic posts collected from different subreddits. The authors of the corpus had already annotated the posts as sarcastic or non-sarcastic, and the authors of the paper used this annotation.\n\nFor Twitter, it is not explicitly mentioned who annotated the data, but it is implied that the authors used existing datasets or corpora that were already annotated for irony.\n\nSo, the answer is that the authors of the paper did not annotate the Twitter and Reddit data for irony themselves, but rather used existing annotated datasets and corpora.", "predicted_evidence": ["We present three contributions in this paper. First, we provide a detailed investigation of a set of theoretically-grounded irony markers (e.g., tropes, morpho-syntactic, and typographic markers) in social media. We conduct the classification and frequency analysis based on their occurrence. Second, we analyze and compare the use of irony markers on two social media platforms ( INLINEFORM0 and INLINEFORM1 ). Third, we provide an analysis of markers on topically different social media content (e.g., technology vs. political subreddits).", "We provided a thorough investigation of irony markers across two social media platforms: Twitter and Reddit. Classification experiments and frequency analysis suggest that typographic markers such as emojis and emoticons are most frequent for INLINEFORM0 whereas tag questions, exclamation, metaphors are frequent for INLINEFORM1 . We also provide an analysis across different topical subreddits. In future, we are planning to experiment with other markers (e.g., ironic echo, repetition, understatements).", "In this paper, we examine the role of irony markers in social media for irony recognition. Although punctuations, capitalization, and hyperboles are previously used as features in irony detection BIBREF6 , BIBREF7 , here we thoroughly analyze a set of theoretically-grounded types of irony markers, such as tropes (e.g., metaphors), morpho-syntactic indicators (e.g., tag questions), and typographic markers (e.g., emoji) and their use in ironic utterances. Consider the following two irony examples from INLINEFORM0 and INLINEFORM1 given in Table TABREF2 .", "Reddit: BIBREF10 (2018) introduced an extensive collection of sarcastic and non-sarcastic posts collected from different subreddits. In Reddit, authors mark their sarcastic intent of their posts by adding \u201c/s\u201d at the end of a post/comment. We collected 50K instances from the corpus for our experiments (denoted as INLINEFORM0 ), where the sarcastic and non-sarcastic replies are at least two sentences (i.e., we discard posts that are too short). For brevity, we denote ironic utterances as INLINEFORM1 and non-ironic utterances as INLINEFORM2 . Both INLINEFORM3 and INLINEFORM4 datasets are balanced between the INLINEFORM5 and INLINEFORM6 classes. We uuse 80% of the datasets for training, 10% for development, and the remaining 10% for testing.", "Finally, we collected another set of irony posts from BIBREF10 , but this time we collected posts from specific topical subreddits. We collected irony posts about politics (e.g., subreddits: politics, hillary, the_donald), sports (e.g., subreddits: nba, football, soccer), religion (e.g., subreddits: religion) and technology (e.g., subreddits: technology). Table TABREF27 presents the mean and SD for each genre. We observe that users use tropes such as hyperbole and INLINEFORM0 , morpho-syntactic markers such as exclamation and interjections and multiple-punctuations more in politics and religion than in technology and sports. This is expected since subreddits regarding politics and religion are often more controversial than technology and sports and the users might want to stress that they are ironic or sarcastic using the markers."]}
{"question_id": "475e698a801be0ad9e4f74756d1fff4fe0728009", "predicted_answer": "The resources combined to build the labeler are:\n\n1. Token vectors\n2. Predicate vectors\n3. POS tags (optional)\n\nThese resources are combined to predict a token's label in the semantic role labeling task.", "predicted_evidence": ["Despite the consistency of this format, there are significant differences between the training sets across languages. English uses PropBank role labels BIBREF2 . Catalan, Chinese, English, German, and Spanish include (but are not limited to) labels such as \u201carg INLINEFORM0 -agt\u201d (for \u201cagent\u201d) or \u201cA INLINEFORM1 \u201d that may correspond to some degree to each other and to the English roles. Catalan and Spanish share most labels (being drawn from the same source corpus, AnCora; BIBREF3 ), and English and German share some labels. Czech and Japanese each have their own distinct sets of argument labels, most of which do not have clear correspondences to English or to each other.", "The CoNLL 2009 dataset includes seven different languages, allowing study of trends across the same. Unlike the Universal Dependencies dataset, however, the semantic label spaces are entirely language-specific, making our task more challenging. Nonetheless, the success of polyglot training in this setting demonstrates that sharing of statistical strength across languages does not depend on explicit alignment in annotation conventions, and can be done simply through parameter sharing. We show that polyglot training can result in better labeling accuracy than a monolingual parser, especially for low-resource languages. We find that even a simple combination of data is as effective as more complex kinds of polyglot training. We include a breakdown into label categories of the differences between the monolingual and polyglot models. Our findings indicate that polyglot training consistently improves label accuracy for common labels.", "We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish. For each language, certain tokens in each sentence in the dataset are marked as predicates. Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates.", "Recent improvements in multilingual SRL can be attributed to neural architectures. Swayamdipta2016-qt present a transition-based stack LSTM model that predicts syntax and semantics jointly, as a remedy to the reliance on pipelined models. Guo2016-zc and BIBREF11 use deep biLSTM architectures which use syntactic information to guide the composition. marcheggiani2017lstm use a simple LSTM model over word tokens to tag semantic dependencies, like our model. Their model predicts a token's label based on the combination of the token vector and the predicate vector, and saw benefits from using POS tags, both improvements that could be added to our model. marcheggiani2017gcn apply the recently-developed graph convolutional networks to SRL, obtaining state of the art results on English and Chinese. All of these approaches are orthogonal to ours, and might benefit from polyglot training.", "ammar2016malopa found that using training data from multiple languages annotated with Universal Dependencies BIBREF1 , and represented using multilingual word vectors, outperformed monolingual training. Inspired by this, we apply the idea of training one model on multiple languages\u2014which we call polyglot training\u2014to PropBank-style semantic role labeling (SRL). We train several parsers for each language in the CoNLL 2009 dataset BIBREF0 : a traditional monolingual version, and variants which additionally incorporate supervision from English portion of the dataset. To our knowledge, this is the first multilingual SRL approach to combine supervision from several languages."]}
{"question_id": "8246d1eee1482555d075127ac84f2e1d0781a446", "predicted_answer": "The datasets used in this research are:\n\n1. Leipzig Corpora Collection: This dataset was used to produce GloVe vectors for each of the shared task languages.\n2. PropBank: This dataset was used for English role labels.\n3. AnCora: This dataset was used for Catalan and Spanish role labels.\n4. German dataset: This dataset was constructed by semi-automatic projection of annotations and has a significant number of sentences without labeled predicate and arguments.\n\nNote that the specific datasets used for each language are not explicitly mentioned, but it can be inferred that they are part of the Leipzig Corpora Collection.", "predicted_evidence": ["We thank Luke Zettlemoyer, Luheng He, and the anonymous reviewers for helpful comments and feedback. This research was supported in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under contract HR001115C0113 to BBN. Views expressed are those of the authors alone.", "We use pretrained word embeddings as input to the model. For each of the shared task languages, we produced GloVe vectors BIBREF7 from the news, web, and Wikipedia text of the Leipzig Corpora Collection BIBREF8 . We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component analysis for efficiency.", "Despite the consistency of this format, there are significant differences between the training sets across languages. English uses PropBank role labels BIBREF2 . Catalan, Chinese, English, German, and Spanish include (but are not limited to) labels such as \u201carg INLINEFORM0 -agt\u201d (for \u201cagent\u201d) or \u201cA INLINEFORM1 \u201d that may correspond to some degree to each other and to the English roles. Catalan and Spanish share most labels (being drawn from the same source corpus, AnCora; BIBREF3 ), and English and German share some labels. Czech and Japanese each have their own distinct sets of argument labels, most of which do not have clear correspondences to English or to each other.", "We present our results in Table TABREF11 . We observe that simple polyglot training improves over monolingual training, with the exception of Czech, where we observe no change in performance. The languages with the fewest training examples (German, Japanese, Catalan) show the most improvement, while large-dataset languages such as Czech or Chinese see little or no improvement (Figure FIGREF10 ).", "We also note that, due to semi-automatic projection of annotations to construct the German dataset, more than half of German sentences do not include labeled predicate and arguments. Thus while German has almost as many sentences as Czech, it has by far the fewest training examples (predicate-argument structures); see Table TABREF3 ."]}
{"question_id": "1ec0be667a6594eb2e07c50258b120e693e040a8", "predicted_answer": "The monolingual baseline refers to a model trained on a single language, using only the data from that language, without any sharing of parameters or data from other languages. In other words, it is a model that is trained and tested on a single language, without any multilingual or polyglot training.", "predicted_evidence": ["Despite the consistency of this format, there are significant differences between the training sets across languages. English uses PropBank role labels BIBREF2 . Catalan, Chinese, English, German, and Spanish include (but are not limited to) labels such as \u201carg INLINEFORM0 -agt\u201d (for \u201cagent\u201d) or \u201cA INLINEFORM1 \u201d that may correspond to some degree to each other and to the English roles. Catalan and Spanish share most labels (being drawn from the same source corpus, AnCora; BIBREF3 ), and English and German share some labels. Czech and Japanese each have their own distinct sets of argument labels, most of which do not have clear correspondences to English or to each other.", "In the second variant, we concatenate a language ID vector to each multilingual word embedding and predicate indicator feature in the input representation. This vector is randomly initialized and updated in training. These additional parameters provide a small degree of language-specificity in the model, while still sharing most parameters.", "The basis of our polyglot training is the use of pretrained multilingual word vectors, which allow representing entirely distinct vocabularies (such as the tokens of different languages) in a shared representation space, allowing crosslingual learning BIBREF9 . We produced multilingual embeddings from the monolingual embeddings using the method of ammar2016massively: for each non-English language, a small crosslingual dictionary and canonical correlation analysis was used to find a transformation of the non-English vectors into the English vector space BIBREF10 .", "The CoNLL 2009 dataset includes seven different languages, allowing study of trends across the same. Unlike the Universal Dependencies dataset, however, the semantic label spaces are entirely language-specific, making our task more challenging. Nonetheless, the success of polyglot training in this setting demonstrates that sharing of statistical strength across languages does not depend on explicit alignment in annotation conventions, and can be done simply through parameter sharing. We show that polyglot training can result in better labeling accuracy than a monolingual parser, especially for low-resource languages. We find that even a simple combination of data is as effective as more complex kinds of polyglot training. We include a breakdown into label categories of the differences between the monolingual and polyglot models. Our findings indicate that polyglot training consistently improves label accuracy for common labels.", "We present our results in Table TABREF11 . We observe that simple polyglot training improves over monolingual training, with the exception of Czech, where we observe no change in performance. The languages with the fewest training examples (German, Japanese, Catalan) show the most improvement, while large-dataset languages such as Czech or Chinese see little or no improvement (Figure FIGREF10 )."]}
{"question_id": "e3bafa432cd3e1225170ff04de2fdf1ede38c6ef", "predicted_answer": "The languages explored in this paper are:\n\n1. Czech\n2. Catalan\n3. Chinese\n4. German\n5. Japanese", "predicted_evidence": ["In the second variant, we concatenate a language ID vector to each multilingual word embedding and predicate indicator feature in the input representation. This vector is randomly initialized and updated in training. These additional parameters provide a small degree of language-specificity in the model, while still sharing most parameters.", "The language ID model performs inconsistently; it is better than the simple polyglot model in some cases, including Czech, but not in all. The language-specific LSTMs model performs best on a few languages, such as Catalan and Chinese, but worst on others. While these results may reflect differences between languages in the optimal amount of crosslingual sharing, we focus on the simple polyglot results in our analysis, which sufficiently demonstrate that polyglot training can improve performance over monolingual training.", "Recent improvements in multilingual SRL can be attributed to neural architectures. Swayamdipta2016-qt present a transition-based stack LSTM model that predicts syntax and semantics jointly, as a remedy to the reliance on pipelined models. Guo2016-zc and BIBREF11 use deep biLSTM architectures which use syntactic information to guide the composition. marcheggiani2017lstm use a simple LSTM model over word tokens to tag semantic dependencies, like our model. Their model predicts a token's label based on the combination of the token vector and the predicate vector, and saw benefits from using POS tags, both improvements that could be added to our model. marcheggiani2017gcn apply the recently-developed graph convolutional networks to SRL, obtaining state of the art results on English and Chinese. All of these approaches are orthogonal to ours, and might benefit from polyglot training.", "We present our results in Table TABREF11 . We observe that simple polyglot training improves over monolingual training, with the exception of Czech, where we observe no change in performance. The languages with the fewest training examples (German, Japanese, Catalan) show the most improvement, while large-dataset languages such as Czech or Chinese see little or no improvement (Figure FIGREF10 ).", "The standard approach to multilingual NLP is to design a single architecture, but tune and train a separate model for each language. While this method allows for customizing the model to the particulars of each language and the available data, it also presents a problem when little data is available: extensive language-specific annotation is required. The reality is that most languages have very little annotated data for most NLP tasks."]}
{"question_id": "dde29d9ea5859aa5a4bcd613dca80aec501ef03a", "predicted_answer": "There is no mention of MFCC (Mel-Frequency Cepstral Coefficients) in the given context, so it cannot be determined whether their model uses MFCC or not.", "predicted_evidence": ["Notice that the context vector INLINEFORM0 in Equation ( EQREF12 ) and the output vector INLINEFORM1 in Equation ( EQREF9 ) are different as suggested in BIBREF25 , BIBREF26 , but the details are beyond the scope of our paper.", "We thank anonymous reviewers for useful comments and Jingbo Zhu for sharing the MMD executable program. This paper is partially supported by the National Natural Science Foundation of China (NSFC Grant Nos. 61272343 and 61472006), the Doctoral Program of Higher Education of China (Grant No. 20130001110032), and the National Basic Research Program (973 Program No. 2014CB340405).", "TextTiling w/ tf INLINEFORM0 idf features. We implemented TextTiling ourselves according to BIBREF12 .", "where INLINEFORM0 is the INLINEFORM1 -norm of a vector.", "where INLINEFORM0 is a hyperparameter adjusting the number of segmentation boundaries; INLINEFORM1 and INLINEFORM2 are the average and standard deviation of depth scores, respectively."]}
{"question_id": "9b1382b44dc69f7ee20acf952f7ceb1c3ef83965", "predicted_answer": "The problem of session segmentation is to divide a conversation or dialogue session into meaningful segments or topics, without prior knowledge of the specific conversation states or topics.", "predicted_evidence": ["A similar (but different) research problem is topic tracking in conversations, e.g., BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 . In these approaches, the goal is typically a classification problem with a few pre-defined conversation states/topics, and hence it can hardly be generalized to general-purpose session segmentation.", "We apply a TextTiling-like algorithm for session segmentation. The original TextTiling is proposed by Hearst BIBREF12 . The main idea is to measure the similarity of each adjacent sentence pair; then \u201cvalleys\u201d of similarities are detected for segmentation.", "To sum up, our experiments show that both the proposed embedding learning approach and the similarity heuristic are effective for session segmentation. The embedding-enhanced TextTiling approach largely outperforms baselines.", "Case Study. We present a case study on our website: https://sites.google.com/site/sessionsegmentation/. From the case study, we see that the proposed approach is able to segment the dialogue session appropriately, so as to better utilize background information from a conversation session.", "Random. We randomly segmented conversation sessions. In this baseline, we were equipped with the prior probability of segmentation."]}
{"question_id": "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f", "predicted_answer": "The dataset used is a real-world chatting corpus from DuMi, a state-of-the-practice open-domain conversation system in Chinese, which consists of 200 sessions. Additionally, an unlabeled massive dataset of conversation utterances crawled from the Douban forum is used to train word embeddings, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).", "predicted_evidence": ["To evaluate the session segmentation method, we used a real-world chatting corpus from DuMi, a state-of-the-practice open-domain conversation system in Chinese. We sampled 200 sessions as our experimental corpus. Session segmentation was manually annotated before experiments, serving as the ground truth. The 200 sessions were randomly split by 1:1 for validation and testing. Notice that, our method does not require labeled training samples; massive data with labels of high quality are quite expensive to obtain.", "where INLINEFORM0 is a hyperparameter adjusting the number of segmentation boundaries; INLINEFORM1 and INLINEFORM2 are the average and standard deviation of depth scores, respectively.", "We also leveraged an unlabeled massive dataset of conversation utterances to train our word embeddings with \u201cvirtual sentences.\u201d The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).", "In this section, we evaluate our embedding-enhanced TextTiling method as well as the effect of session segmentation. In Subsection SECREF17 , we describe the datasets used in our experiments. Subsection SECREF22 presents the segmentation accuracy of our method and baselines. In Subsection SECREF27 , we show that, with our session segmentation, we can improve the performance of a retrieval-based conversation system.", "We used hierarchical softmax to approximate the probability."]}
{"question_id": "6157567c5614e1954b801431fec680f044e102c6", "predicted_answer": "No, the filtering based on fluency and domain relevance was not done automatically. The text states that thresholds $t_{rel}$ and $t_{flu}$ were applied to filter out questions whose scores were below these thresholds, but it does not mention that the filtering was done automatically. Instead, it implies that the authors manually set these thresholds and applied them to the results of the skip-gram model and language model evaluations.", "predicted_evidence": ["The questions collected from the web search engine may not be fluent or domain relevant; especially the domain relevance drops significantly as the iteration goes on. Here we adopt a skip-gram model BIBREF11 and a language model for evaluating the domain relevance and fluency of the expanded questions, respectively. For domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:", "where $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count. We apply thresholds $t_{rel}$ and $t_{flu}$ for domain relevance and fluency respectively, and filter out questions whose scores are below these thresholds.", "The last experiment is on our in-house KB in the power tool domain. It contains 67 distinct predicates, 293 distinct subjects and 279 distinct objects respectively. For the 67 predicates, we hand-craft 163 templates. Here we use the same language model as in our first experiment, and learn a skip-gram model BIBREF11 on Wikipedia for evaluating domain relevance.", "Shown in Table 3 , we compare our domain-relevance evaluation method (section \"Experiments\" ) with previous state-of-the-art methods: phan2008learning first derives latent topics with LDA BIBREF15 from Wikipedia, then uses the topics as appended features to expand the short text. chen2011short further expanded phan2008learning by using multi-granularity topics. ma-EtAl:2015:VSM-NLP adopts a Bayesian model that the probability a document $D$ belongs to a topic $t$ equals to the prior of $t$ times the probability each word $w$ in $D$ comes from $t$ . Our method first concatenates training documents of the same domain into one \u201cdomain document\u201d, then calculates each document embedding by averaging word embeddings within it, before finally assigns the label of the nearest (cosine similarity) \u201cdomain document\u201d to each test document.", "We perform three experiments to evaluate our system qualitatively and quantitatively. In the first experiment, we compare our end-to-end system with the previous state-of-the-art method BIBREF10 on Freebase BIBREF7 , a domain-general KB. In the second experiment, we validate our domain relevance evaluation method on a standard dataset about short document classification. In the final experiment, we run our end-to-end system on a highly specialized in-house KB and present sample results, showing that our system is capable of generating questions from domain specific KBs."]}
{"question_id": "8ea4a75dacf6a39f9d385ba14b3dce715a47d689", "predicted_answer": "The domain relevance was estimated by taking the seed question set as the in-domain data, and then calculating the domain relevance of an expanded question q as the cosine similarity between the embedding of q and the embedding of the nearest \"domain document\" (which was obtained by concatenating training documents of the same domain and averaging their word embeddings).", "predicted_evidence": ["Shown in Table 3 , we compare our domain-relevance evaluation method (section \"Experiments\" ) with previous state-of-the-art methods: phan2008learning first derives latent topics with LDA BIBREF15 from Wikipedia, then uses the topics as appended features to expand the short text. chen2011short further expanded phan2008learning by using multi-granularity topics. ma-EtAl:2015:VSM-NLP adopts a Bayesian model that the probability a document $D$ belongs to a topic $t$ equals to the prior of $t$ times the probability each word $w$ in $D$ comes from $t$ . Our method first concatenates training documents of the same domain into one \u201cdomain document\u201d, then calculates each document embedding by averaging word embeddings within it, before finally assigns the label of the nearest (cosine similarity) \u201cdomain document\u201d to each test document.", "The questions collected from the web search engine may not be fluent or domain relevant; especially the domain relevance drops significantly as the iteration goes on. Here we adopt a skip-gram model BIBREF11 and a language model for evaluating the domain relevance and fluency of the expanded questions, respectively. For domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:", "We test our domain-relevance evaluating method on the web snippet dataset, which is a commonly-used for domain classification of short documents. It contains 10,060 training and 2,280 test snippets (short documents) in 8 classes (domains), and each snippet has 18 words on average. There have been plenty of prior results BIBREF12 , BIBREF13 , BIBREF14 on the dataset.", "We perform three experiments to evaluate our system qualitatively and quantitatively. In the first experiment, we compare our end-to-end system with the previous state-of-the-art method BIBREF10 on Freebase BIBREF7 , a domain-general KB. In the second experiment, we validate our domain relevance evaluation method on a standard dataset about short document classification. In the final experiment, we run our end-to-end system on a highly specialized in-house KB and present sample results, showing that our system is capable of generating questions from domain specific KBs.", "The last experiment is on our in-house KB in the power tool domain. It contains 67 distinct predicates, 293 distinct subjects and 279 distinct objects respectively. For the 67 predicates, we hand-craft 163 templates. Here we use the same language model as in our first experiment, and learn a skip-gram model BIBREF11 on Wikipedia for evaluating domain relevance."]}
{"question_id": "1e11e74481ead4b7635922bbe0de041dc2dde28d", "predicted_answer": "They had to make 163 hand-crafted templates.", "predicted_evidence": ["Shown in Figure 1 , our system contains the sub-modules of question template construction, seed question generation, question expansion and selection. Given an input KB, a small set of question templates is first constructed such that each template is associated with a predicate, then a seed question set is generated by applying the template set on the input KB, before finally more questions are generated from related questions that are iteratively retrieved from a search engine with already-obtained questions as search queries (section \"Experiments\" ). Taking our in-house KB of power tool domain as an example, template \u201chow to use #X#\u201d is first constructed for predicate \u201cperformsActivity\u201d. In addition, seed question \u201chow to use jigsaw\u201d is generated by applying the template on triple \u201c $\\langle $ jigsaw, performsActivity, CurveCut $\\rangle $ \u201d, before finally questions (Figure 2 ) are retrieved from Google with the seed question.", "The only human labor in this work is the question template construction. Our system does not require a large number of templates because: (1) the iterative question expansion can produce a large number of questions even with a relatively small number of seed questions, as we see in the experiments, (2) multiple entities in the KB share the same predicates. Another advantage is that our system can easily generate updated questions as web is self-updating consistently. In our experiment, we compare with serban-EtAl:2016:P16-1 on 500 random selected triples from Freebase BIBREF7 . Evaluated by 3 human graders, questions generated by our system are significantly better then serban-EtAl:2016:P16-1 on grammaticality and naturalness.", "We generate 12,228 seed questions from which 20,000 more questions are expanded with Google. Shown in Table 4 are some expanded questions from which we can see that most of them are grammatical and relevant to the power tool domain. In addition, most questions are informative and correspond to a specific answer, except the one \u201cdo I need a hammer drill\u201d that lacks context information. Finally, in addition to the simple factoid questions, our system generates many complex questions such as \u201chow to cut a groove in wood without a router\u201d.", "The last experiment is on our in-house KB in the power tool domain. It contains 67 distinct predicates, 293 distinct subjects and 279 distinct objects respectively. For the 67 predicates, we hand-craft 163 templates. Here we use the same language model as in our first experiment, and learn a skip-gram model BIBREF11 on Wikipedia for evaluating domain relevance.", "where $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as:"]}
{"question_id": "597d3fc9b8c0c036f58cea5b757d0109d5211b2f", "predicted_answer": "The fluency was measured using an averaged language model score, which is defined as the log probability of the question according to a language model, divided by the word count of the question. This is represented by the equation:\n\n$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$\n\nwhere $\\textsc {Lm}(q)$ is the language model score (log probability) of the question, and $\\textsc {Len}(q)$ is the word count of the question.", "predicted_evidence": ["where $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as:", "where $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count. We apply thresholds $t_{rel}$ and $t_{flu}$ for domain relevance and fluency respectively, and filter out questions whose scores are below these thresholds.", "We show the averaged human rate in Table 2 , where we can see that our questions are more grammatical and natural than serban-EtAl:2016:P16-1. The naturalness score is less than the grammatical score for both methods. It is because naturalness is a more strict metric since a natural question should also be grammatical.", "We first compare our system with serban-EtAl:2016:P16-1 on 500 randomly selected triples from Freebase BIBREF7 . For the 500 triples, we hand-crafted 106 templates, as these triples share only 53 distinct predicates (we made 2 templates for each predicate on average). 991 seed questions are generated by applying the templates on the triples, and 1529 more questions are retrieved from Google. To evaluate the fluency of the candidate questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare with the results from serban-EtAl:2016:P16-1. We ask three native English speakers to evaluate the fluency and the naturalness of both results based on a 4-point scheme where 4 is the best.", "$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$   (Eq. 8)"]}
{"question_id": "f0404673085517eea708c5e91f32fb0f7728fa08", "predicted_answer": "The data used in this work includes:\n\n1. Transcripts of tennis press conferences from ASAP Sport's website (http://www.asapsports.com/), covering post-game interviews for tennis singles matches played between Jan, 2000 to Oct 18, 2015.\n2. Live text play-by-play commentaries collected from the website Sports Mole (http://www.sportsmole.co.uk/).\n3. Match information from a dataset provided by Tennis-Data, covering the majority of the matches played on the men's side from 2000-2015 and on the women's side from 2007-2015.", "predicted_evidence": ["Furthermore, our main focus is on questions asked during press conferences; we have not looked at the players' responses. The transcripts data, which we release publicly, may provide opportunities for further studies.", "We first compute perplexities for each individual question and then group the question instances according to the interviewee's gender class. Throughout we use the Mann-Whitney $U$ statistical significance test, unless otherwise noted.", "We match interview transcripts with game statistics by date and player name, keeping only the question and answer pairs from games where the statistics are successfully merged. This gives us a dataset consisting of 6467 interview transcripts and a total of 81906 question snippets posed to 167 female players and 191 male players. To model tennis-game-specific language, we use live text play-by-play commentaries collected from the website Sports Mole (http://www.sportsmole.co.uk/). These tend to be short, averaging around 40 words. Here is a sample, taken from the Federer-Murray match at the 2015 Wimbledon semi-final:", "To quantify how game-related a question is in a data-driven fashion, we train a bigram language model using KenLM BIBREF18 on the gender-balanced set of live-text play-by-play commentaries introduced in Section \"Dataset Description\" .", "We collect tennis press-conference transcripts from ASAP Sport's website (http://www.asapsports.com/), whose tennis collection dates back to 1992 and is still updated for current tournaments. For our study, we take post- game interviews for tennis singles matches played between Jan, 2000 to Oct 18, 2015. We also obtain easily-extractable match information from a dataset provided by Tennis-Data, which covers the majority of the matches played on the men's side from 2000-2015 and on the women's side from 2007-2015."]}
{"question_id": "d6b0c71721ed24ef1d9bd31ed3a266b0c7fc9b57", "predicted_answer": "The datasets used are:\n\n1. XKCD dataset prepared by McMahan15Colors\n2. Caltech-UCSD Birds dataset\n3. Driving dataset (collected by the authors)\n4. Welinder10Birds dataset with natural language annotations from Reed16Birds", "predicted_evidence": ["We use the version of the XKCD dataset prepared by McMahan15Colors. Here the input feature vector is simply the LAB representation of each color, and the message inventory taken to be all unigrams that appear at least five times.", "Driving data is collected from pairs of human workers on Mechanical Turk. Workers received the following description of the task:", "JA is supported by a Facebook Graduate Fellowship and a Berkeley AI / Huawei Fellowship. We are grateful to Lisa Anne Hendricks for assistance with the Caltech\u2013UCSD Birds dataset, and to Liang Huang and Sebastian Schuster for useful feedback.", "Players were restricted to messages of 1\u20133 words, and required to send at least one message per game. Each player was paid $0.25 per game. 382 games were collected with 5 different road layouts, each represented as an 8x8 grid presented to players as in fig:drive-examples. The action space is discrete: players can move forward, back, turn left, turn right, or wait. These were divided into a 282-game training set and 100-game test set. The message inventory consists of all messages sent more than 3 times. Input features consists of indicators on the agent's current position and orientation, goal position, and map identity. Data is available for download at http://github.com/jacobandreas/neuralese.", "We use the dataset of Welinder10Birds with natural language annotations from Reed16Birds. The model's input feature representations are a final 256-dimensional hidden feature vector from a compact bilinear pooling model BIBREF24 pre-trained for classification. The message inventory consists of the 50 most frequent bigrams to appear in natural language descriptions; example human traces are generated by for every frequent (bigram, image) pair in the dataset."]}
{"question_id": "63cdac43a643fc1e06da44910458e89b2c7cd921", "predicted_answer": "The dataset was collected using crowdsourcing, where each speaker was recorded saying each wording for each intent twice, with the phrases presented in a random order. Participants consented to data being released and provided demographic information about themselves.", "predicted_evidence": ["The data was collected using crowdsourcing. Each speaker was recorded saying each wording for each intent twice. The phrases to record were presented in a random order. Participants consented to data being released and provided demographic information about themselves. The demographic information about these anonymized speakers (age range, gender, speaking ability, etc.) is included along with the dataset.", "The dataset is composed of 16 kHz single-channel .wav audio files. Each audio file contains a recording of a single command that one might use for a smart home or virtual assistant, like \u201cput on the music\u201d or \u201cturn up the heat in the kitchen\u201d.", "The data was validated by a separate set of crowdsourcers. All audios deemed by the crowdsourcers to be unintelligible or contain the wrong phrase were removed. The total number of speakers, utterances, and hours of audio remaining is shown in Table TABREF12 .", "This previous work has all been conducted on datasets that are closed-source or too small to test hypotheses about the amount of data required to generalize well. The lack of a good open-source dataset for end-to-end SLU experiments makes it difficult for most people to perform high-quality, reproducible research on this topic. We therefore created a new SLU dataset, the \u201cFluent Speech Commands\u201d dataset, which Fluent.ai releases along with this paper.", "The Google Speech Commands dataset BIBREF26 (to which the name \u201cFluent Speech Commands\u201d is an homage) is a free dataset of 30 single-word spoken commands (\u201cyes\u201d, \u201cno\u201d, \u201cstop\u201d, \u201cgo\u201d, etc.). This dataset is suitable for keyword spotting experiments, but not for SLU."]}
{"question_id": "37ac705166fa87dc74fe86575bf04bea56cc4930", "predicted_answer": "The evaluation metrics used are:\n\n1. MAE (Mean Absolute Error) between the predicted and ground-truth agreed prices.\n2. Accuracy\u00b1k, which is the percentage of cases where the predicted price lies within k percent of the ground-truth. Specifically, k=5 and k=10 are used in the experiments.", "predicted_evidence": ["Evaluation Metrics: We study the variants of the same model by training with different proportions of the negotiation seen, namely, $f \\in \\lbrace 0.0, 0.2, 0.4, 0.6, 0.8, 1.0\\rbrace $. We compare the models on two evaluation metrics: MAE: Mean Absolute Error between the predicted and ground-truth agreed prices along with Accuracy$\\pm k$: the percentage of cases where the predicted price lies within $k$ percent of the ground-truth. We use $k=5$ and $k=10$ in our experiments.", "We present our results in Figure FIGREF6. We also show Accuracy$\\pm 10$ for different product categories in the Appendix. First, Target Price (TP) and (TP+LP)/2 prove to be strong baselines, with the latter achieving $61.07\\%$ Accuracy$\\pm 10$. This performance is also attested by relatively strong numbers on the other metrics as well. Prices-only, which does not incorporate any knowledge from natural language, fails to beat the average baseline even with $60\\%$ of the negotiation history. This can be attributed to the observation that in many negotiations, before discussing the price, buyers tend to get more information about the product by exchanging messages: what is the condition of the product, how old it is, is there an urgency for any of the buyer/seller and so on.", "This performance is also attested by relatively strong numbers on the other metrics as well. Prices-only, which does not incorporate any knowledge from natural language, fails to beat the average baseline even with $60\\%$ of the negotiation history. This can be attributed to the observation that in many negotiations, before discussing the price, buyers tend to get more information about the product by exchanging messages: what is the condition of the product, how old it is, is there an urgency for any of the buyer/seller and so on. Incorporating natural language in both the scenario and event messages paves the way to leverage such cues and make better predictions early on in the conversation, as depicted in the plots. Both BERT and BERT-GRU consistently perform well on the complete test set. There is no clear winner, although using a recurrent network proves to be more helpful in the early stages of the negotiation. Note that BERT method still employs multiple [SEP] tokens along with alternating segment embeddings (Section SECREF3).", "Incorporating natural language in both the scenario and event messages paves the way to leverage such cues and make better predictions early on in the conversation, as depicted in the plots. Both BERT and BERT-GRU consistently perform well on the complete test set. There is no clear winner, although using a recurrent network proves to be more helpful in the early stages of the negotiation. Note that BERT method still employs multiple [SEP] tokens along with alternating segment embeddings (Section SECREF3). Without this usage, the fine-tuning pipeline proves to be inadequate. Overall, BERT-GRU achieves $67.08\\%$ Accuracy$\\pm 10$ with just the product scenario, reaching to $71.16\\%$ with $60\\%$ of the messages and crosses $90\\%$ as more information about the final price is revealed. Paired Bootstrap Resampling BIBREF14 with $10,000$ bootstraps shows that for a given $f$, BERT-GRU is better than its Prices-only counterpart with $95\\%$ statistical significance.", "This performance is also attested by relatively strong numbers on the other metrics as well. Prices-only, which does not incorporate any knowledge from natural language, fails to beat the average baseline even with $60\\%$ of the negotiation history. This can be attributed to the observation that in many negotiations, before discussing the price, buyers tend to get more information about the product by exchanging messages: what is the condition of the product, how old it is, is there an urgency for any of the buyer/seller and so on. Incorporating natural language in both the scenario and event messages paves the way to leverage such cues and make better predictions early on in the conversation, as depicted in the plots. Both BERT and BERT-GRU consistently perform well on the complete test set. There is no clear winner, although using a recurrent network proves to be more helpful in the early stages of the negotiation."]}
{"question_id": "90aba75508aa145475d7cc9a501bbe987c0e8413", "predicted_answer": "The dataset used is the Craigslist Bargaining dataset (CB), which consists of 6682 dialogues between a buyer and a seller negotiating the price of products listed on Craigslist, with 1402 product ad postings from six categories: phones, bikes, housing, furniture, car, and electronics.", "predicted_evidence": ["We show the category-wise performance in Figure FIGREF11.", "Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by BIBREF4. Instead of focusing on the previously studied game environments BIBREF5, BIBREF6, the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist. The dataset consists of 6682 dialogues between a buyer and a seller who converse in natural language to negotiate the price of a given product (sample in Table TABREF1). In total, 1402 product ad postings were scraped from Craigslist, belonging to six categories: phones, bikes, housing, furniture, car and electronics. Each ad posting contains details such as Product Title, Category Type and a Listing Price. Moreover, a secret target price is also pre-decided for the buyer. The final price after the agreement is called the Agreed Price, which we aim to predict.", "Defining the problem: Say we are provided with a product scenario $S$, a tuple: (Category, Title, Listing Price, Target Price). Define the interactions between a buyer and seller using a sequence of $n$ events $E_n:<e_{1}, e_{2}, ..., e_{n}>$, where $e_{i}$ occurs before $e_{j}$ iff $i<j$. Event $e_{i}$ is also a tuple: (Initiator, Type, Data). Initiator is either the Buyer or Seller, Type can be one of (message, offer, accept, reject or quit) and Data consists of either the corresponding natural language dialogue, offer price or can be empty. Nearly $80\\%$ of events in CB dataset are of type `message', each consisting a textual message as Data. An offer is usually made and accepted at the end of each negotiation.", "We perform experiments on the CB dataset to primarily answer two questions: 1) Is it feasible to predict negotiation outcomes without observing the complete conversation between the buyer and seller? 2) To what extent does the natural language incorporation help in the prediction? In order to answer these questions, we compare our model empirically with a number of baseline methods. This section presents the methods we compare to, the training setup and the evaluation metrics.", "Define the interactions between a buyer and seller using a sequence of $n$ events $E_n:<e_{1}, e_{2}, ..., e_{n}>$, where $e_{i}$ occurs before $e_{j}$ iff $i<j$. Event $e_{i}$ is also a tuple: (Initiator, Type, Data). Initiator is either the Buyer or Seller, Type can be one of (message, offer, accept, reject or quit) and Data consists of either the corresponding natural language dialogue, offer price or can be empty. Nearly $80\\%$ of events in CB dataset are of type `message', each consisting a textual message as Data. An offer is usually made and accepted at the end of each negotiation. Since the offers directly contain the agreed price (which we want to predict), we only consider `message' events in our models. Given the scenario $S$ and first $n$ events $E_n$, our problem is then to learn the function $f_{n}$: $A = f_{n}(S, E_n)$ where $A$ refers to the final agreed price between the two negotiating parties."]}
{"question_id": "e6204daf4efeb752fdbd5c26e179efcb8ddd2807", "predicted_answer": "According to the text, grammatical correctness was measured by identifying the Part-of-Speech (POS) tags for each sentence in the speech and then checking if there was another sentence in the corpus with the same sequence of POS tags. If a matching sentence was found, it suggested that the grammar was likely correct, while a lack of a matching sentence did not necessarily imply an error, but rather pointed in that direction. The system would output sentences that did not have a matching sentence for manual evaluation.", "predicted_evidence": ["The automatic evaluation aims to evaluate both the grammatical correctness and the consistency of the speech in terms of its content. For evaluating the grammatical correctness we identify for each sentence of the speech its POS tags. Then we check all sentences of the entire corpus whether one has the same sequence of POS tags. Having a sentence with the same POS tag structure does not necessarily mean that the grammar is correct. Neither does the lack of finding a matching sentence imply the existence of an error. But it points in a certain direction. Furthermore, we let the system output the sentence for which it could not find a matching sentence so that we can evaluate those sentences manually.", "Table TABREF16 shows the results from the automatic evaluation. The automatic evaluation confirms pretty much the results from the manual evaluation. Most of the speeches which achieved a high score in the manual evaluation scored also high in the automatic evaluation. Furthermore, it also confirms that the overall the grammatical correctness of the speeches is very good while the content is a bit behind.", "In this section we present the results from our experiments. Table TABREF15 shows the results from the manual evaluation. Note that each criterion scores between 0 and 3 which leads to a maximum total score of 12. The achieved total score range from 5 to 10 with an average of 8.1. In particular, the grammatical correctness and the sentence transitions were very good. Each of them scored on average 2.3 out of 3. The speech content yielded the lowest scores. This indicates that the topic model may need some improvement.", "For the manual evaluation we have defined a list of evaluation criteria. That is, a generated speech is evaluated by assessing each of the criterion and assigning a score between 0 and 3 to it. Table TABREF13 lists all evaluation criteria and describes the meaning of the different scores.", "In order to test our implemented methods we performed an experimental evaluation. In this experiment we generated ten speeches, five for class DN and five for class RY. We set the weighting factor INLINEFORM0 to 0.5 which means the topic and the language model have both equal impact on predicting the next word. The quality of the generated speeches was then evaluated. We used two different evaluation methods: a manual evaluation and an automatic evaluation. Both methods will be described in more detail in the following paragraphs of this section. The generated speeches can be found in the appendix of this report."]}
{"question_id": "95c3907c5e8f57f239f3b031b1e41f19ff77924a", "predicted_answer": "The quality of sentence transition was measured through two approaches: manual evaluation and automatic evaluation.\n\nIn the manual evaluation, sentence transitions were scored by human evaluators on a scale of 0 to 3, with a maximum total score of 12. The achieved total score ranged from 5 to 10, with an average of 8.1.\n\nIn the automatic evaluation, sentence transitions were measured using two metrics:\n\n1. Structural similarity: This measures the longest sequence of congruent Part-of-Speech (POS) tags between two sentences, normalized by the length of the shorter sentence. This aims to support smooth sentence transitions by identifying sentences with similar sentence structures.\n2. Textual similarity: This measures the number of trigrams (sequences of three words) that occur in both sentences, normalized by the length of the longer sentence. This aims to find sentences that use the same words.\n\nThese metrics were used to evaluate the quality of sentence transitions in the generated speeches.", "predicted_evidence": ["In this section we present the results from our experiments. Table TABREF15 shows the results from the manual evaluation. Note that each criterion scores between 0 and 3 which leads to a maximum total score of 12. The achieved total score range from 5 to 10 with an average of 8.1. In particular, the grammatical correctness and the sentence transitions were very good. Each of them scored on average 2.3 out of 3. The speech content yielded the lowest scores. This indicates that the topic model may need some improvement.", "For the structural similarity we compare the POS tags of both sentences and determine the longest sequence of congruent POS tags. The length of this sequence, normalized by the length of the shorter sentence, gives us the structural similarity. The structural similarity measure aims to support smooth sentence transitions. That is, if we find sentences which have a very similar sentence structure, it is very likely that they connect well to either of their following sentences. The textual similarity is defined by the number of trigrams that occur in both sentences, normalized by the length of the longer sentence. This similarity aims to find sentences which use the same words.", "This section describes the experimental setup we used to evaluate our system. Furthermore, we present here two different approach of evaluating the quality of generated speeches.", "Table TABREF16 shows the results from the automatic evaluation. The automatic evaluation confirms pretty much the results from the manual evaluation. Most of the speeches which achieved a high score in the manual evaluation scored also high in the automatic evaluation. Furthermore, it also confirms that the overall the grammatical correctness of the speeches is very good while the content is a bit behind.", "In order to test our implemented methods we performed an experimental evaluation. In this experiment we generated ten speeches, five for class DN and five for class RY. We set the weighting factor INLINEFORM0 to 0.5 which means the topic and the language model have both equal impact on predicting the next word. The quality of the generated speeches was then evaluated. We used two different evaluation methods: a manual evaluation and an automatic evaluation. Both methods will be described in more detail in the following paragraphs of this section. The generated speeches can be found in the appendix of this report."]}
{"question_id": "b900122c7d6c2d6161bfca8a95eae11952d1cb58", "predicted_answer": "The size of the dataset is 3857 speech segments.", "predicted_evidence": ["The main data source for this project is the Convote data set UID41 . It contains a total of 3857 speech segments from 53 US Congressional floor debates from the year 2005. Each speech segment can be referred to its debate, its speaker, the speaker\u2019s party and the speaker\u2019s vote which serves as the ground-truth label for the speech. The dataset was originally created in the course of the project Get out the vote UID34 . The authors used the dataset to train a classifier in order to determine whether a speech represents support of or opposition to proposed legislation. They did not only analyze the speeches individually but also investigated agreements and disagreements with the opinions of other speakers. That is, they identified references in the speech segments, determined the targets of those references, and decided whether a reference represents an instance of agreement or disagreement. However, we focus only on the individual speech segments and disregard references.", "In this report we first discuss related works which deal with similar or related methods. Then we describe and analyze the dataset we use. Next, we present the methods we used to implement our system. We also describe investigated methods that were not used in the final implementation. Then we describe a performed experiment and how we evaluated the results. Finally, we conclude our work and give an outlook. The appendix of this report contains the generated speeches from the experiment.", "Cavnar and Trenkle UID20 have also used an n-gram-based approach for text categorization. Their system is based on calculating and comparing profiles of N-gram frequencies. They compute for every category a representing profile from the training data. Then the system computes a profile for a particular document that is to be classified. Finally, the system computes a distance measure between the document\u2019s profile and each of the category profiles and selects the category whose profile has the smallest distance.", "Ivyer et al. UID35 apply Recursive Neural Networks (RNN) to political ideology detection. The RNNs were initialized with word2vec embeddings. The word vector dimensions were set to 300 to allow direct comparison with other experiments. However, they claim that smaller vector sizes (50, 100) do not significantly change accuracy. They performed experiments on two different dataset: the Convote dataset UID41 and the Ideological Books Corpus (IBC) UID37 . They claim that their model outperforms existing models on these two datasets.", "F\u00fcrnkranz UID19 has studied the usage of n-grams in the text-categorization domain. He has shown that using bi- and trigrams in addition to the set-of-word representation improves the classification performance significantly. Furthermore, he has shown that sequences longer than three words reduce the classification performance. That also indicates that collocations play a crucial role when it comes to inferring the latent structure of documents."]}
{"question_id": "5206b6f40a91fc16179829041c1139a6c6d91ce7", "predicted_answer": "The manual evaluation is presented in Table TABREF15, which shows the results of evaluating the generated speeches based on a list of criteria, with each criterion scored between 0 and 3, leading to a maximum total score of 12.", "predicted_evidence": ["For the manual evaluation we have defined a list of evaluation criteria. That is, a generated speech is evaluated by assessing each of the criterion and assigning a score between 0 and 3 to it. Table TABREF13 lists all evaluation criteria and describes the meaning of the different scores.", "Table TABREF16 shows the results from the automatic evaluation. The automatic evaluation confirms pretty much the results from the manual evaluation. Most of the speeches which achieved a high score in the manual evaluation scored also high in the automatic evaluation. Furthermore, it also confirms that the overall the grammatical correctness of the speeches is very good while the content is a bit behind.", "In order to test our implemented methods we performed an experimental evaluation. In this experiment we generated ten speeches, five for class DN and five for class RY. We set the weighting factor INLINEFORM0 to 0.5 which means the topic and the language model have both equal impact on predicting the next word. The quality of the generated speeches was then evaluated. We used two different evaluation methods: a manual evaluation and an automatic evaluation. Both methods will be described in more detail in the following paragraphs of this section. The generated speeches can be found in the appendix of this report.", "This section describes the experimental setup we used to evaluate our system. Furthermore, we present here two different approach of evaluating the quality of generated speeches.", "In this section we present the results from our experiments. Table TABREF15 shows the results from the manual evaluation. Note that each criterion scores between 0 and 3 which leads to a maximum total score of 12. The achieved total score range from 5 to 10 with an average of 8.1. In particular, the grammatical correctness and the sentence transitions were very good. Each of them scored on average 2.3 out of 3. The speech content yielded the lowest scores. This indicates that the topic model may need some improvement."]}
{"question_id": "48ff9645a506aa2c17810d2654d1f0f0d9e609ee", "predicted_answer": "The downstream tasks analyzed are:\n\n1. IMDb sentiment classification (a classification task)\n2. SQuAD v1.1 (a question answering task)\n3. STS-B (a semantic textual similarity task, mentioned in the results)\n\nAdditionally, the paper mentions that the results are shown on 9 tasks in total, but the other 6 tasks are not specified.", "predicted_evidence": ["Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - BIBREF13) and a question answering task (SQuAD v1.1 - BIBREF14).", "Task-specific distillation Most of the prior works focus on building task-specific distillation setups. BIBREF15 transfer fine-tune classification model BERT to an LSTM-based classifier. BIBREF16 distill BERT model fine-tuned on SQuAD in a smaller Transformer model previously initialized from BERT. In the present work, we found it beneficial to use a general-purpose pre-training distillation rather than a task-specific distillation. BIBREF17 use the original pretraining objective to train smaller student, then fine-tuned via distillation. As shown in the ablation study, we found it beneficial to leverage the teacher's knowledge to pre-train with additional distillation signal.", "In this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models. We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices.", "The results on each of the 9 tasks are showed on Table TABREF6 along with the macro-score (average of individual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo baseline (up to 20 points of accuracy on STS-B). DistilBERT also compares surprisingly well to BERT, retaining 97% of the performance with 40% fewer parameters.", "Using a triple loss, we show that a 40% smaller Transformer (BIBREF5) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances."]}
{"question_id": "84ee6180d3267115ad27852027d147fb86a33135", "predicted_answer": "According to the text, the training of DistilBERT took approximately 90 hours on 8 16GB V100 GPUs.", "predicted_evidence": ["Data and compute power We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus BIBREF9. DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the RoBERTa model BIBREF2 required 1 day of training on 1024 32GB V100.", "We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a teacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 86.9 F1 and 79.1 EM, i.e. within 2 points of the full model.", "Task-specific distillation Most of the prior works focus on building task-specific distillation setups. BIBREF15 transfer fine-tune classification model BERT to an LSTM-based classifier. BIBREF16 distill BERT model fine-tuned on SQuAD in a smaller Transformer model previously initialized from BERT. In the present work, we found it beneficial to use a general-purpose pre-training distillation rather than a task-specific distillation. BIBREF17 use the original pretraining objective to train smaller student, then fine-tuned via distillation. As shown in the ablation study, we found it beneficial to leverage the teacher's knowledge to pre-train with additional distillation signal.", "The results on each of the 9 tasks are showed on Table TABREF6 along with the macro-score (average of individual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo baseline (up to 20 points of accuracy on STS-B). DistilBERT also compares surprisingly well to BERT, retaining 97% of the performance with 40% fewer parameters.", "To further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table TABREF8) the number of parameters of each model along with the inference time needed to do a full pass on the STS-B development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT."]}
{"question_id": "c7ffef8bf0100eb6148bd932d0409b21759060b1", "predicted_answer": "The two datasets used in the experiment are:\n\n1. LibriSpeech corpus (English)\n2. GlobalPhone corpus (includes French, German, Czech, and Spanish)", "predicted_evidence": ["Two corpora across five languages were used in the experiment. One of the corpora we used is LibriSpeech corpus BIBREF46 (English). In this 960-hour English dataset, 2.2 million audio word segments were used for training while the other 250 thousand segments were used as the database to be retrieved in STD and 1 thousand segments as spoken queries. In Section 6.1, we further sampled 20 thousand segments from 250 thousand segments to form a small database to investigate the influence of database size. English served as the high-resource source language for model pre-training.", "The other dataset is the GlobalPhone corpus BIBREF47 , which includes French (FRE), German (GER), Czech (CZE), and Spanish (ESP). The four languages from GlobalPhone were used as the low-resource target languages. In Section 6.2, 20 thousand segments for each language were used to calculate the average cosine similarity. For the experiments of STD, the 20 thousands segments served as the database to be retrieved, and the other 1 thousand used for query and 4 thousand for fine-tuning.", "Here we provide detail of our experiment including the dataset, model setup, and the baseline model.", "MFCCs of 39-dim were used as the acoustic features. The length of the input sequence was limited to 50 frames. All datasets were segmented according to the word boundaries obtained by forced alignment with respect to the reference transcriptions. Although the oracle word boundaries were used here for the query-by-example STD in the preliminary tests, the comparison in the following experiment was fair since all approaches used the same segmentation. Mean average precision (MAP) was used as the evaluation measure for query-by-example STD.", "In Fig. FIGREF12 , we varied the dimension of Audio Word2Vector as 100, 200, 400, 600, 800 and 1000. To match up the dimensionality with INLINEFORM0 , we tested INLINEFORM1 with dimensionality 117, 234, 390, 585, 819, 1014 ( INLINEFORM2 ) and denoted them by INLINEFORM3 where INLINEFORM4 is the dimensionality. INLINEFORM5 get higher MAP values than INLINEFORM6 no matter the vector dimension and the size of database. The highest MAP score INLINEFORM7 can achieve is 0.881 ( INLINEFORM8 on small database), while the highest score of the INLINEFORM9 model is 0.490 ( INLINEFORM10 on small database). The size of database has large influence on the results. The MAP scores of the two models both drop in the large database. For example, INLINEFORM11 drops from 0.490 to 0.158, decaying by 68%, and the performance of INLINEFORM12 drops from 0.881 to 0.317, decaying by 64%."]}
{"question_id": "1ff0ffeb2d0b2e150abdb2f559d8b31f4dd8aa2c", "predicted_answer": "They compare the performance of representations obtained from a naive encoder (INLINEFORM1 and INLINEFORM2) trained purely on the target language, versus those learned from a large amount of source language data (INLINEFORM0) and fine-tuned on the target language with varying amounts of data (1K, 2K, 3K, 4K, and 0).", "predicted_evidence": ["Besides analyzing the cosine similarity of the learned representations, we also apply them to the query-by-example STD task. Here we compare the retrieval performance in MAP of INLINEFORM0 with different levels of accessibility to the low-resource target language along with two baseline models, INLINEFORM1 and INLINEFORM2 trained purely by the target languages. For the four target languages, the total available amount of audio word segments in the training set were 4 thousands for each language. In Table TABREF20 , we took different partitions of the target language training sets to fine tune the INLINEFORM3 pretrained by the source languages. The amount of audio word segments in these partitions are: 1K, 2K, 3K, 4K, and 0, which means no fine-tuning.", "In this section, we first examine how changing the hidden layer size of the RNN Encoder/Decoder, the dimension of Audio Word2Vec, affects the MAP performance of query-by-example STD (Section 6.1). After obtaining the best hidden layer size, we analyze the transferability of the Audio Word2Vec by comparing the cosine similarity of the learned representations to phoneme sequence edit distance (Section 6.2) . Visualization of multiple word pairs in different target languages is also provided (Section 6.3). Last but not least, we performed the query-by-example STD on target languages (Section 6.4). These experiments together verify that INLINEFORM0 is capable of extracting common phonetic structure in human language and thus is transferable to various languages.", "This means that INLINEFORM0 learned from English can successfully encode the sequential phonetic structures into fixed-length vector for the target languages to some good extend even though it has never seen any audio data of the target languages. Another interesting fact is the corresponding variance between languages. In the source language, English, the variances of the five edit distance groups are fixed at 0.030, which means that the cosine similarity in each edit distance group is centralized. However, the variances of the groups in the target languages vary. In French and German, the variance grows from 0.030 to 0.060 as the edit distance increases from 0 to 4. For Czech/Spanish, the variance starts at a larger value of 0.040/0.050 and increases to 0.050/0.073. We suspect that the fluctuating variance is related to the similarity between languages. English, German and French are more similar compared with Czech and Spanish.", "Before evaluating the language transfer result, we first experimented on the primary INLINEFORM0 model in the source language (English). The results are shown in Fig. FIGREF12 . Here we compare the representations of INLINEFORM1 and INLINEFORM2 . Furthermore, we examined the influence of the dimension of Audio Word2Vector in terms of MAP. We also compared the MAP results on large testing database (250K segments) and small database (20K).", "In the proposed approach, we first train an INLINEFORM0 using the high-resource source language, as shown in the upper part of Fig. FIGREF4 , and then the encoder is used to transform the audio segment of a low-resource target language. It is also possible to fine-tune the parameters of INLINEFORM1 with the target language. In the following experiments, we found that in some cases the STD performance of the encoder without fine-tuning with the low-resource target language can be as good as the one with fine-tuning."]}
{"question_id": "3cc0d773085dc175b85955e95911a2cfaab2cdc4", "predicted_answer": "The pairs of languages that are considered similar enough to capture phonetic structure are:\n\n* English and French\n* English and German", "predicted_evidence": ["(0,1), (1,2), (2,3), (3,4), is obvious. This means that INLINEFORM0 learned from English can successfully encode the sequential phonetic structures into fixed-length vector for the target languages to some good extend even though it has never seen any audio data of the target languages. Another interesting fact is the corresponding variance between languages. In the source language, English, the variances of the five edit distance groups are fixed at 0.030, which means that the cosine similarity in each edit distance group is centralized. However, the variances of the groups in the target languages vary. In French and German, the variance grows from 0.030 to 0.060 as the edit distance increases from 0 to 4. For Czech/Spanish, the variance starts at a larger value of 0.040/0.050 and increases to 0.050/0.073. We suspect that the fluctuating variance is related to the similarity between languages.", "The gap between each edit distance groups, i.e. (0,1), (1,2), (2,3), (3,4), is obvious. This means that INLINEFORM0 learned from English can successfully encode the sequential phonetic structures into fixed-length vector for the target languages to some good extend even though it has never seen any audio data of the target languages. Another interesting fact is the corresponding variance between languages. In the source language, English, the variances of the five edit distance groups are fixed at 0.030, which means that the cosine similarity in each edit distance group is centralized. However, the variances of the groups in the target languages vary. In French and German, the variance grows from 0.030 to 0.060 as the edit distance increases from 0 to 4. For Czech/Spanish, the variance starts at a larger value of 0.040/0.050 and increases to 0.050/0.073.", "In Fig. FIGREF14 , the cosine similarities of the segment pairs get smaller as the edit distances increase, and the trend is observed in all languages. The gap between each edit distance groups, i.e. (0,1), (1,2), (2,3), (3,4), is obvious. This means that INLINEFORM0 learned from English can successfully encode the sequential phonetic structures into fixed-length vector for the target languages to some good extend even though it has never seen any audio data of the target languages. Another interesting fact is the corresponding variance between languages. In the source language, English, the variances of the five edit distance groups are fixed at 0.030, which means that the cosine similarity in each edit distance group is centralized. However, the variances of the groups in the target languages vary. In French and German, the variance grows from 0.030 to 0.060 as the edit distance increases from 0 to 4.", "In Fig. FIGREF14 , the cosine similarities of the segment pairs get smaller as the edit distances increase, and the trend is observed in all languages. The gap between each edit distance groups, i.e. (0,1), (1,2), (2,3), (3,4), is obvious. This means that INLINEFORM0 learned from English can successfully encode the sequential phonetic structures into fixed-length vector for the target languages to some good extend even though it has never seen any audio data of the target languages. Another interesting fact is the corresponding variance between languages. In the source language, English, the variances of the five edit distance groups are fixed at 0.030, which means that the cosine similarity in each edit distance group is centralized. However, the variances of the groups in the target languages vary.", "This means that INLINEFORM0 learned from English can successfully encode the sequential phonetic structures into fixed-length vector for the target languages to some good extend even though it has never seen any audio data of the target languages. Another interesting fact is the corresponding variance between languages. In the source language, English, the variances of the five edit distance groups are fixed at 0.030, which means that the cosine similarity in each edit distance group is centralized. However, the variances of the groups in the target languages vary. In French and German, the variance grows from 0.030 to 0.060 as the edit distance increases from 0 to 4. For Czech/Spanish, the variance starts at a larger value of 0.040/0.050 and increases to 0.050/0.073. We suspect that the fluctuating variance is related to the similarity between languages. English, German and French are more similar compared with Czech and Spanish."]}
{"question_id": "dfd07a8e2de80c3a8d075a0f400fb13a1f1d4c60", "predicted_answer": "No, there is no mention of \"Roberta\" in the given context. The text appears to be discussing a research paper on detecting hate speech in tweets, and it does not mention a person named Roberta.", "predicted_evidence": ["(3) Hillary's welfare army doesn't really want jobs. They want more freebies.", "In addition, for direct comparisons, we train a LSTM model using the 16 thousand annotated tweets, using exactly the same settings as we use for the LSTM classifier in our two-path bootstrapping system.", "Consistent across the statistical classifiers including both logistic regression classifiers and LSTM models, only tweets that receive a confidence score over $0.9$ were tagged as hateful tweets.", "The third section of Table 2 shows the evaluation results of the two single-path variants of the weakly supervised system. We can see that both the estimated precision, recall, F score and the estimated number of truly hateful tweets by the two systems are significantly lower than the complete two-path bootstrapping system, which suggests that our two-path learning system can effectively capture diverse descriptions of online hate speech, maintain learning momentums as well as effectively combat with noise in online texts.", "For instance, BIBREF5 conducted their experiments on 1525 annotated sentences from a company's log file and a certain newsgroup. BIBREF6 labeled around 9000 human labeled paragraphs from Yahoo!'s news group post and American Jewish Congress's website, and the labeling is restricted to anti-Semitic hate speech. BIBREF7 studied use of profanity on a dataset of 6,500 labeled comments from Yahoo! Buzz. BIBREF2 built a balanced corpus of 24582 tweets consisting of anti-black and non-anti black tweets. The tweets were manually selected from Twitter accounts that were believed to be racist based upon their reactions to anti-Obama articles. BIBREF8 collected hateful tweets related to the murder of Drummer Lee Rigby in 2013. BIBREF0 collected tweets using hateful slurs, specific hashtags as well as suspicious user IDs. Consequently, all of the 1,972 racist tweets are by 9 users, and the majority of sexist tweets are related to an Australian TV show."]}
{"question_id": "2e70d25f14357ad74c085a9454a2ce33bb988a6f", "predicted_answer": "The results of the task are not explicitly stated in the given context. The text describes the methodology and approach used for the task of document classification, including the use of attention mechanisms and the preprocessing of documents to remove identical headings. It also provides information about the dataset used for evaluation, but it does not report the actual results of the task, such as accuracy, precision, recall, or F1-score.", "predicted_evidence": ["The final classification is subjected to the various features that each chunk contains. Thus, the attention mechanisms are introduced so as to enable the assignment of different weights to each chunk, depending on how strong of a class indicator this chunk is. In particular, the attention scores are assigned to the corresponding hidden state outputs as follows:", "Here $\\alpha _{it}$ is the attention score assigned to hidden state $h_{it}$ of document $i$ at time step $t$. This score is determined by the similarity between $u_{it}$ and $u_{w}$, where $u_{it}$ is a mere non-linear transformation of $h_{it}$ and $u_{w}$ is the context (category) vector BIBREF1. During the following steps, the products of the hidden states and their corresponding attention scores are calculated and the document vector $d_{i}$ is formed from the summation of those products. Note that $u_{w}$ is randomly initialised and then constantly updated during the training process.", "Almost all documents of type \u201c10-K\u201d begin with lines that contain identical headings. In order to enable the machine to truly comprehend why a document of type \u201c10-K\u201d should be categorised to that filing type, we remove the first six lines where the identical text is located. The model is then able to focus on finding common features that exist in documents of the same filing type, rather than focusing on just capturing the few sentences that are the same in almost all of the documents of type \u201c10-K\u201d. A similar procedure is followed with the documents of type \u201c10-Q\u201d.", "The data we use to evaluate our model is a set of documents downloaded from EDGAR, an online public database from the U.S. Securities and Exchange Commission (SEC). EDGAR is the primary system for submissions by companies and others who are required by law to file information with the SEC. These documents can be grouped according to filing types, which determines the substantial content to fulfill their filing obligation. To work on as many documents as possible, we choose the following types: \u201c10-Q\u201d, \u201c10-K\u201d, \u201cEX-99.1\u201d, \u201cEX-10.1\u201d and \u201cEX-101.INS\u201d. The total number of documents is 28,445 and there are 5,689 documents for each filing type. We summarise the statistics of this dataset in Table TABREF11.", "Text classification is a problem in library, information and computer science and one of the most classical and prominent tasks in Natural Language Processing (NLP). In particular, document classification is a procedure of assigning one or more labels to a document from a predetermined set of labels. Automatic document classification tasks can be divided into three categories: supervised, unsupervised and semi-supervised. This study focuses on supervised document classification."]}
{"question_id": "de84972c5d1bbf664d0f8b702fce5f161449ec23", "predicted_answer": "The text is segmented into chunks, which are smaller parts of the document, before processing. The number of chunks can vary, and the document is divided into these chunks in order to train Doc2Vec and improve the efficiency of the final model.", "predicted_evidence": ["Furthermore, an important contribution of this paper to automatic document classification is the concept of dividing documents into chunks before processing. It is demonstrated that the segmentation of lengthy documents into smaller chunks of text allows the context of each document to be encapsulated in an improved way, leading to enhanced results. The intuition behind this idea was formed by investigating automatic audio segmentation research. Audio segmentation (also known as audio classification) is an essential pre-processing step in audio analysis that separates different types of sound (e.g. speech, music, silence etc.) and splits audio signals into chunks in order to further improve the comprehension of these signals BIBREF7. Analogously, the present paper shows that splitting overly lengthy legal documents into smaller parts before processing them, boosts the final results.", "As Table TABREF13 and Table TABREF15 indicate, dividing the document in chunks - up to certain thresholds - results in improved models compared to those where the whole document is input into the classifier. Note that the model with one chunk denotes the model which takes as input the whole document to produce the document embedding and thereby is used as a benchmark in order to be able to identify the effectiveness of the document segmentation method.", "We create different models with respect to the number of chunks that we divide the initial text into, in order to observe how the different number of chunks affect the efficiency of the final model. These chunks are then used to train Doc2Vec. In short, the intuition behind Doc2Vec is analogous to the intuition behind Word2Vec, where the", "Ultimately, we try different classifiers in order to assess the impact of the segmentation method. As part of the models of the first type, the resulting document vector is output from a batch normalisation layer. A linear transformation is then applied to that and this output is passed through a softmax classifier in order to acquire the multi-class probabilities. This final process is summarised in the following formula:", "The novelty of this work is the application of audio segmentation used for speech recognition BIBREF13 in document classification. The ultimate purpose of audio segmentation is to divide the signal into segments, each of which contains distinct audio information. In our case, the same occurs during the document segmentation, where the split chunks become the inputs of our neural network."]}
{"question_id": "bab4e8881f4d75e266bce6fbfa4c3bcd3eacf30f", "predicted_answer": "According to the given context, the state-of-the-art models are:\n\n1. Character-aware neural language model proposed by BIBREF1, which uses character embeddings to represent words.\n2. Models that use character embeddings, which have achieved state-of-the-art results with much fewer parameters for alphabetic languages such as English.", "predicted_evidence": ["We compared the proposed model with the follows:", "All of the models were trained by RMSprop BIBREF12 with mini-batches of 100 samples. The learning rate and decay term were set as 0.001 and 0.9 respectively, also tuned on the development set.", "The architecture of our proposed model is as shown in Fig. FIGREF3 . It looks similar to the character-aware neural language model proposed by BIBREF1 , but we represent a word by the sequence of radical embeddings instead of character embeddings. Besides, unlike the former model, there are no highway layers in the proposed model, because we find that highway layers do not bring significant improvements to our proposed model (see Section SECREF31 ).", "Word embeddings have been widely used for natural language processing (NLP) tasks BIBREF0 . However, the large word vocabulary makes word embeddings expensive to train. Some people argue that we can model languages at the character-level BIBREF1 . For alphabetic languages such as English, where the characters are much fewer than the words, the character embeddings achieved the state-of-the-art results with much fewer parameters.", "An RNN is a kind of neural networks designed to learn sequential data. The output of an RNN unit at time INLINEFORM0 depends on the output at time INLINEFORM1 . Bi-directional RNNs BIBREF6 are able to extract the past and future information for each node in a sequence, have shown effective for Machine Translation BIBREF7 and Machine Comprehension BIBREF8 ."]}
{"question_id": "11dd2913d1517a1d47b367acb29fe9d79a9c95d1", "predicted_answer": "The noisy channel model has 2 parameters: \n\n1. The channel model probability $p(x|y)$\n2. The language model probability $p(y)$", "predicted_evidence": ["The noisy channel approach applies Bayes' rule to model $p(y|x) = p(x|y) p(y)/ p(x)$, that is, the channel model $p(x|y)$ operating from the target to the source and a language model $p(y)$. We do not model $p(x)$ since it is constant for all $y$. We compute the channel model probabilities as follows:", "The noisy channel approach is an alternative which is used in statistical machine translation BIBREF6, BIBREF7. It entails a channel model probability $p(x|y)$ that operates in the reverse direction as well as a language model probability $p(y)$. The language model can be estimated on unpaired data and can take a separate form to the channel model. Noisy channel modeling mitigates explaining away effects that result in the source being ignored for highly likely output prefixes BIBREF8.", "Previous work on neural noisy channel modeling relied on a complex latent variable model that incrementally processes source and target prefixes BIBREF9. This trades efficiency for accuracy because their model performs significantly less well than a vanilla sequence to sequence model. For languages with similar word order, it can be sufficient to predict the first target token based on a short source prefix, but for languages where word order differs significantly, we may need to take the entire source sentence into account to make a decision.", "To generate $y$ given $x$ with the channel model, we wish to compute $\\operatornamewithlimits{arg\\,max}_y \\log p(x|y) + \\log p(y)$. However, na\u00efve decoding in this way is computationally expensive because the channel model $p(x|y)$ is conditional on each candidate target prefix. For the direct model, it is sufficient to perform a single forward pass over the network parameterizing $p(y|x)$ to obtain output word probabilities for the entire vocabulary. However, the channel model requires separate forward passes for each vocabulary word.", "Table TABREF20 shows that the noisy channel model outperforms the baseline () by up to 4.0 BLEU for very large beams, the ensemble by up to 2.9 BLEU () and the best right-to-left configuration by 1.4 BLEU (). The channel approach improves more than other methods with larger n-best lists by adding 2.4 BLEU from $k_1=5$ to $k_1=100$. Other methods improve a lot less with larger beams, e.g., has the next largest improvement of 1.4 BLEU when increasing the beam size but this is still significantly lower than for the noisy channel approach. Adding a language model benefits all settings (, , ) but the channel approach benefits most ( vs ). The direct model with a language model () performs better than for online decoding, likely because the constrained re-ranking setup mitigates explaining away effects (cf. Table TABREF16)."]}
{"question_id": "8701ec7345ccc2c35eca4e132a8e16d58585cd63", "predicted_answer": "The language pairs evaluated in this context are:\n\n1. English-German (En-De)\n2. Chinese-English (Zh-En)\n3. German-English (De-En)", "predicted_evidence": ["For English-German (En-De) we train on WMT'17 data, validate on news2016 and test on news2017. For reranking, we train models with a 40K joint byte pair encoding vocabulary (BPE; BIBREF11). To be able to use the language model during online decoding, we use the vocabulary of the langauge model on the target side. For the source vocabulary, we learn a 40K byte pair encoding on the source portion of the bitext; we find using LM and bitext vocabularies give similar accuracy. For Chinese-English (Zh-En), we pre-process WMT'17 data following BIBREF12, we develop on dev2017 and test on news2017. For IWSLT'14 De-En we follow the setup of BIBREF13 and measure case-sensitive tokenized BLEU. For WMT De-En, En-De and Zh-En we measure detokenized BLEU BIBREF14.", "For English-German (En-De) we train on WMT'17 data, validate on news2016 and test on news2017. For reranking, we train models with a 40K joint byte pair encoding vocabulary (BPE; BIBREF11). To be able to use the language model during online decoding, we use the vocabulary of the langauge model on the target side. For the source vocabulary, we learn a 40K byte pair encoding on the source portion of the bitext; we find using LM and bitext vocabularies give similar accuracy. For Chinese-English (Zh-En), we pre-process WMT'17 data following BIBREF12, we develop on dev2017 and test on news2017. For IWSLT'14 De-En we follow the setup of BIBREF13 and measure case-sensitive tokenized BLEU.", "Interestingly, both or give only modest improvements compared to . Although previous work demonstrated that reranking with can improve over , we show that the channel model is important to properly leverage the language model without suffering from explaining away effects BIBREF23, BIBREF24. Test results on all language directions confirm that performs best (Table TABREF21).", "where $t$ is the length of the target prefix $y$, $s$ is the source sentence length and $\\lambda $ is a tunable weight. Initially, we used separate weights for $p(x|y)$ and $p(y)$ but we found that a single weight resulted in the same accuracy and was easier to tune. Scaling by $t$ and $s$ makes the scores of the direct and channel model comparable to each other throughout decoding. In n-best re-ranking, we have complete target sentences which are of roughly equal length and therefore do not use per word scores.", "Since the direct model needs to be evaluated for pre-pruning, we also include these probabilities in making decoding decisions. We use the following linear combination of the channel model, the language model and the direct model for decoding:"]}
{"question_id": "d20fd6330cb9d03734e2632166d6c8f780359a94", "predicted_answer": "The improvement margin varies depending on the specific comparison and training size. Here are some examples:\n\n* ZAT shows an improvement of 14.67 in absolute F1-score over CRF when training with 500 instances.\n* ZAT has a statistically significant mean improvement of 4.04, 5.37, and 3.27 points over LSTM-BoE with training size 500, 1000, and 2000, respectively.\n* ZAT also shows a statistically significant average improvement of 2.58, 2.44, and 2.5 points over CT, another zero-shot model with training size 500, 1000, and 2000, respectively.\n\nOverall, the improvement margin is significant, with ZAT consistently outperforming other models by a large margin.", "predicted_evidence": ["We plot the averaged performances on varying amounts of training data for each target domain in Figure 3 . Note that the improvements are even higher for the experiments with smaller training data. In particular, ZAT shows an improvement of $14.67$ in absolute F1-score over CRF when training with 500 instances. ZAT achieves an F1-score of 76.04% with only 500 training instances, while even with 2000 training instances the CRF model achieves an F1-score of only 75%. Thus the ZAT model achieves better F1-score with only one-fourth the training data.", "ZAT also shows a statistically significant average improvement of $2.58$ , $2.44$ and $2.5$ points over CT, another zero-shot model with training size 500, 1000 and 2000, respectively. Looking at results for individual domains, the highest improvement for BoE models are seen for transportation and travel. This can be explained by these domains having a high frequency of $timex$ and $location$ slots. But BoE models show a regression in the shopping domain, and a reason could be the low frequency of expert slots. In contrast, ZAT consistently outperforms non-adapted models (CRF and LSTM) by a large margin. This is because ZAT can benefit from other reusable slots than $timex$ and $location$ . Though not as popular as $5.37$0 and $5.37$1 , slots such as $5.37$2 , $5.37$3 , $5.37$4 , and $5.37$5 appear across many domains.", "Table 2 shows the F1-scores obtained by the different methods for each of the 10 domains. LSTM based models in general perform better than the CRF based models. Both the CRF-BoE and LSTM-BoE outperform the basic CRF and LSTM models. Both zero-shot models, CT and ZAT, again surpass the BoE models. ZAT has a statistically significant mean improvement of $4.04$ , $5.37$ and $3.27$ points over LSTM-BoE with training size 500, 1000 and 2000, respectively. ZAT also shows a statistically significant average improvement of $2.58$ , $2.44$ and $2.5$ points over CT, another zero-shot model with training size 500, 1000 and 2000, respectively. Looking at results for individual domains, the highest improvement for BoE models are seen for transportation and travel.", "ZAT has a statistically significant mean improvement of $4.04$ , $5.37$ and $3.27$ points over LSTM-BoE with training size 500, 1000 and 2000, respectively. ZAT also shows a statistically significant average improvement of $2.58$ , $2.44$ and $2.5$ points over CT, another zero-shot model with training size 500, 1000 and 2000, respectively. Looking at results for individual domains, the highest improvement for BoE models are seen for transportation and travel. This can be explained by these domains having a high frequency of $timex$ and $location$ slots. But BoE models show a regression in the shopping domain, and a reason could be the low frequency of expert slots. In contrast, ZAT consistently outperforms non-adapted models (CRF and LSTM) by a large margin. This is because ZAT can benefit from other reusable slots than $timex$ and $location$ .", "Both the CRF-BoE and LSTM-BoE outperform the basic CRF and LSTM models. Both zero-shot models, CT and ZAT, again surpass the BoE models. ZAT has a statistically significant mean improvement of $4.04$ , $5.37$ and $3.27$ points over LSTM-BoE with training size 500, 1000 and 2000, respectively. ZAT also shows a statistically significant average improvement of $2.58$ , $2.44$ and $2.5$ points over CT, another zero-shot model with training size 500, 1000 and 2000, respectively. Looking at results for individual domains, the highest improvement for BoE models are seen for transportation and travel. This can be explained by these domains having a high frequency of $timex$ and $location$ slots. But BoE models show a regression in the shopping domain, and a reason could be the low frequency of expert slots."]}
{"question_id": "1a1d94c981c58e2f2ee18bdfc4abc69fd8f15e14", "predicted_answer": "The languages explored in this context are indigenous languages, which include languages such as Irish, Tibetan, Spanish, Hindi, Marathi, Gujarati, Telugu, and Tamil.", "predicted_evidence": ["Indigenous languages are the languages that are native to a region or spoken by a group of people in a particular state. It is not necessarily a national language. For e.g. Irish, Tibetan, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil are the indigenous languages.", "There are very few research papers available on the SA at the aspect level on the indigenous languages.", "According to the authors Medagoda et al. BIBREF0 there has being a continuous research going on in the English language but the research carried out in the indigenous languages is less. Also, the researches in indigenous languages follow the techniques used for the English language but this has one disadvantage which is, techniques have properties which are specific to a language. Hence It is really important to understand and analyze Indigenous language data because it can give meaningful insights to the companies. For example, India and China have world's largest population and are rich in diverse languages, analysing these indigenous language will be useful to companies because they have large share of users in India and China. In the current study, the types of languages i.e. indigenous languages and code mix languages are discussed prior to the approaches, methodologies used by the researchers and challenges faced by them.", "The datasets for many of the indigenous languages are not available easily. Many of the researches have to manually collected the data and hence this becomes one of the reasons for the smaller dataset.", "With the increasing use of the web there is a lot of User Generated Content (UGC) available on different websites. Lot of research is carried out for the English language. Work done for the indigenous languages is less as compared to the English language. By studying different papers on SA, it can be found out that researchers have started working on the indigenous languages. Data for the indigenous languages is available across the web but is mainly collected from social media platforms like Twitter, Facebook and YouTube."]}
{"question_id": "5d790459b05c5a3e6f1e698824444e55fc11890c", "predicted_answer": "Based on the provided context, two baseline methods are not explicitly mentioned. However, I can infer that the authors are proposing a new method that combines triplet loss and semantic loss, and they might be comparing it to other existing methods.\n\nIf I had to guess, two possible baseline methods could be:\n\n1. **Triplet Loss only**: This method would use only the triplet loss function, without the semantic loss component. This would allow the model to learn embeddings that are close for matching pairs and far apart for non-matching pairs, but might not capture the semantic relationships between different dishes.\n2. **Cross-Entropy Classification Loss only**: This method would use only the cross-entropy classification loss (Lreg) without the triplet loss and semantic loss components. This would allow the model to learn a classification model that predicts the correct dish class, but might not capture the relationships between the recipe text and image embeddings.\n\nPlease note that these are just educated guesses, and the actual baseline methods might be different.", "predicted_evidence": ["with $c_r$ and $c_v$ as semantic recipe-class and semantic image-class, respectively, while $c_r=c_v$ if the food image and recipe text are a positive pair.", "where $cos(x,y)$ is the normalized cosine similarity and $\\alpha $ is a margin ($-1\\leqslant \\alpha \\leqslant 1)$, that determines how similar negative pairs are allowed to be. Positive margins allow negative pairs to share at maximum $\\alpha $ similarity, where a maximum margin of zero or negative margins allow no correlation between non matching embedding vectors or force the model to learn antiparallel representations, respectively. $\\phi ^d$ is the corresponding image counterpart to $\\phi ^q$ if $y=1$ or a randomly chosen sample $\\phi ^d \\in S \\wedge \\phi ^d \\ne \\phi ^{d(q)}$ if $y=-1$, where $\\phi ^{d(q)}$ is the true match for $\\phi ^q$ and $S$ is the dataset we sample from it.", "where $\\beta \\in [0,1]$ weights between quadratic and linear loss, $\\alpha \\in [0,2]$ is the margin and $\\gamma \\in [0,1]$ weights between semantic- and sample-loss. The triplet loss encourages the embedding vectors of a matching pair to be larger by a margin above its non-matching counterpart. Further, the semantic loss encourages the model to form clusters of dishes, sharing the same class. We chose $\\beta $ to be $0.1$, $\\alpha $ to be $0.3$ and $\\gamma $ to be $0.3$.", "where $cos(x,y)$ is the normalized cosine similarity and $\\alpha $ is a margin ($-1\\leqslant \\alpha \\leqslant 1)$, that determines how similar negative pairs are allowed to be. Positive margins allow negative pairs to share at maximum $\\alpha $ similarity, where a maximum margin of zero or negative margins allow no correlation between non matching embedding vectors or force the model to learn antiparallel representations, respectively. $\\phi ^d$ is the corresponding image counterpart to $\\phi ^q$ if $y=1$ or a randomly chosen sample $\\phi ^d \\in S \\wedge \\phi ^d \\ne \\phi ^{d(q)}$ if $y=-1$, where $\\phi ^{d(q)}$ is the true match for $\\phi ^q$ and $S$ is the dataset we sample from it. Furthermore, we complement the cosine similarity with cross-entropy classification loss ($L_{reg}$), leading to the applied objective function.", "where $b$ is the batch-size, $p$ is the amount of word embeddings, $w$ is the dimensionality of the wort embedding, $h$ is the dimensionality of the space to where we project the word embeddings and queries, $q$ is the dimensionality of the ingredient representation and $n$ is the amount of Ingredient Attention-based instruction representations. Ingredient Attention can be performed step-wise, similarly to the well known dimensionality reduction in convolution neural networks."]}
{"question_id": "1ef6471cc3e1eb10d2e92656c77020ca1612f08e", "predicted_answer": "The model is compared to baseline methods in Table TABREF11, where the results show the model's performance in terms of median rank and recall percentage at the top K percent over all queries. The model's performance is also qualitatively evaluated through recipe retrieval, cluster formation in the joint embedding space, and heat maps of instruction words, as shown in Figure FIGREF16. The comparison with baseline methods suggests that the model exhibits a broad range of retrieval accuracy depending on the meal type, similar to the baseline implementations.", "predicted_evidence": ["Similarly to BIBREF19 and BIBREF17, we evaluated our model on 10 subsets of 1000 samples each. One sample of these subsets is composed of text embedding and image embedding in the shared latent space. Since our interest lies in the recipe retrieval task, we optimized and evaluated our model by using each image embedding in the subsets as query against all text embeddings. By ranking the query and the candidate embeddings according to their cosine distance, we estimate the median rank. The model's performance is best, if the matching text embedding is found at the first rank. Further, we estimate the recall percentage at the top K percent over all queries. The recall percentage describes the quantity of queries ranked amid the top K closest results. In Table TABREF11 the results are presented, in comparison to baseline methods.", "where $\\beta \\in [0,1]$ weights between quadratic and linear loss, $\\alpha \\in [0,2]$ is the margin and $\\gamma \\in [0,1]$ weights between semantic- and sample-loss. The triplet loss encourages the embedding vectors of a matching pair to be larger by a margin above its non-matching counterpart. Further, the semantic loss encourages the model to form clusters of dishes, sharing the same class. We chose $\\beta $ to be $0.1$, $\\alpha $ to be $0.3$ and $\\gamma $ to be $0.3$.", "Qualitative results such as recipe retrieval, quality of the cluster formation in the joint embedding space and heat maps of instruction words are more important than the previously mentioned benchmarking scores. Depending on meal type, all baseline implementations as well as our Ingredient Attention based model exhibit a broad range of retrieval accuracy. In Figure FIGREF16 we present a few typical results on the intended recipe retrieval task.", "where $cos(x,y)$ is the normalized cosine similarity and $\\alpha $ is a margin ($-1\\leqslant \\alpha \\leqslant 1)$, that determines how similar negative pairs are allowed to be. Positive margins allow negative pairs to share at maximum $\\alpha $ similarity, where a maximum margin of zero or negative margins allow no correlation between non matching embedding vectors or force the model to learn antiparallel representations, respectively. $\\phi ^d$ is the corresponding image counterpart to $\\phi ^q$ if $y=1$ or a randomly chosen sample $\\phi ^d \\in S \\wedge \\phi ^d \\ne \\phi ^{d(q)}$ if $y=-1$, where $\\phi ^{d(q)}$ is the true match for $\\phi ^q$ and $S$ is the dataset we sample from it.", "with $c_r$ and $c_v$ as semantic recipe-class and semantic image-class, respectively, while $c_r=c_v$ if the food image and recipe text are a positive pair."]}
{"question_id": "d976c22e9d068e4e31fb46e929023459f8290a63", "predicted_answer": "Yes, the two paragraphs are encoded independently.", "predicted_evidence": ["In this method, paragraphs are encoded separately, and the concatenation of the resulted encoding is going through the classifier. First, each paragraph is encoded with LSTM. The hidden state at the end of each sentence is extracted, and the resulting matrix is going through gated CNN BIBREF1 for extraction of single encoding for each paragraph. The accuracy is barely above $50\\%$ , which depicts that this method is not very promising.", "Getting inspiration from this work, we have defined a similar task in the domain of NLP. Given two paragraphs, whether the second paragraph comes really after the first one or the order has been reversed. It is the way of learning the arrow of times in the stories and can be very beneficial in neural story generation tasks. Moreover, this is a self-supervised task, which means the labels come from the text itself.", "We have prepared a dataset, ParagraphOrdreing, which consists of around 300,000 paragraph pairs. We collected our data from Project Gutenberg. We have written an API for gathering and pre-processing in order to have the appropriate format for the defined task. Each example contains two paragraphs and a label which determines whether the second paragraph comes really after the first paragraph (true order with label 1) or the order has been reversed (Table 1 ). The detailed statistics of the data can be found in Table 2 .", "Different approaches have been used to solve this task. The best result belongs to classifying order of paragraphs using pre-trained BERT model. It achieves around $84\\%$ accuracy on test set which outperforms other models significantly.", "We have used a pre-trained BERT in two different ways. First, as a feature extractor without fine-tuning, and second, by fine-tuning the weights during training. The classification is completely based on the BERT paper, i.e., we represent the first and second paragraph as a single packed sequence, with the first paragraph using the A embedding and the second paragraph using the B embedding. In the case of feature extraction, the network weights freeze and CLS token are fed to the classifier. In the case of fine-tuning, we have used different numbers for maximum sequence length to test the capability of BERT in this task. First, just the last sentence of the first paragraph and the beginning sentence of the second paragraph has been used for classification. We wanted to know whether two sentences are enough for ordering classification or not. After that, we increased the number of tokens and accuracy respectively increases."]}
{"question_id": "a1ac4463031bbc42c80893b57c0055b860f12e10", "predicted_answer": "The baseline refers to the original, unprocessed text data, which is the input data before any preprocessing steps are applied. In this case, the baseline data consists of the three examples of textual content provided, which contain various tags such as `<LINK>`, `<NET>`, and `ECT`, as well as irregular formatting and content.", "predicted_evidence": ["The section has been divided into four subsections. We describe the nature and source of the training and evaluation data in Section SECREF6 The pre-processing steps are demonstrated in Section SECREF14 The system setup and experimental settings have been described in Section SECREF19", "We considered some important steps for preprocessing the important textual content in the data. Below are the common preprocessing steps applied to the data:", "Hi 48 PDX Cantrell $<$LINK$>$ $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ F $<$NET$>$ ECT ECT $<$NET$>$ G Slaughter 06 07 03 57 DEVELOPMENT 06 09 2000 07 01 $<$NET$>$ $<$NET$>$ ECT ENRON 09 06 03 10 23 PM To $<$NET$>$ $<$NET$>$ ECT ECT cc $<$NET$>$ $<$NET$>$ ECT ECT Subject Wow Do not underestimate the employment group contains Socal study about recession impact $<$NET$>$ will note else to you for a revised Good credit period I just", "Hi 48 PDX Cantrell $<$LINK$>$ $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ F $<$NET$>$ ECT ECT $<$NET$>$ G Slaughter 06 07 03 57 DEVELOPMENT 06 09 2000 07 01 $<$NET$>$ $<$NET$>$ ECT ENRON 09 06 03 10 23 PM To $<$NET$>$ $<$NET$>$ ECT ECT cc $<$NET$>$ $<$NET$>$ ECT ECT Subject Wow Do not underestimate the employment group contains Socal study about recession impact $<$NET$>$ will note else to you for a revised Good credit period I just want", "48 PDX Cantrell $<$LINK$>$ $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ F $<$NET$>$ ECT ECT $<$NET$>$ G Slaughter 06 07 03 57 DEVELOPMENT 06 09 2000 07 01 $<$NET$>$ $<$NET$>$ ECT ENRON 09 06 03 10 23 PM To $<$NET$>$ $<$NET$>$ ECT ECT cc $<$NET$>$ $<$NET$>$ ECT ECT Subject Wow Do not underestimate the employment group contains Socal study about recession impact $<$NET$>$ will note else to you for a revised Good credit period I just want to"]}
{"question_id": "3216dfc233be68206bd342407e2ba7da3843b31d", "predicted_answer": "No, human evaluation of the malicious content is not performed. The context only discusses the characteristics of the malicious content and the training procedures, but does not mention human evaluation.", "predicted_evidence": ["A few observations from the datasets above: the malicious content is relatively more verbose than than the legitimate counterparts. Moreover, the size of the malicious data is comparatively higher compared to the legitimate content.", "(D) Training on Legitimates + 50% Malicious content:", "(B) Training on Legitimates + 5% Malicious content:", "(C) Training on Legitimates + 30% Malicious content:", "One important aspect of malicious emails is their harmful intent. The perfect attack vector will have malicious elements like a poisonous link or malware attachment wrapped in legitimate context, something which is sly enough to fool both a state-of-the-art email classifier as well as the victim. One novelty of this system training is the procedure of injecting malicious intent during training and generating malicious content in the synthetic emails."]}
{"question_id": "4f57ac24f3f4689a2f885715cd84b7d867fe3f12", "predicted_answer": "It appears that the given context is a series of similar email messages or drafts with slight variations. The question \"Do they compare to previous work?\" seems out of place in this context, as there is no clear reference to \"previous work\" or any specific work being discussed.\n\nHowever, if we interpret the question as asking whether the different versions of the email message compare to each other, the answer would be yes. The messages appear to be variations of the same email, with minor changes in wording or phrasing, but the overall content and structure remain the same.", "predicted_evidence": ["Hi 48 PDX Cantrell $<$LINK$>$ $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ F $<$NET$>$ ECT ECT $<$NET$>$ G Slaughter 06 07 03 57 DEVELOPMENT 06 09 2000 07 01 $<$NET$>$ $<$NET$>$ ECT ENRON 09 06 03 10 23 PM To $<$NET$>$ $<$NET$>$ ECT ECT cc $<$NET$>$ $<$NET$>$ ECT ECT Subject Wow Do not underestimate the employment group contains Socal study about recession impact $<$NET$>$ will note else to you for a revised Good credit period I just", "Hi 48 PDX Cantrell $<$LINK$>$ $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ F $<$NET$>$ ECT ECT $<$NET$>$ G Slaughter 06 07 03 57 DEVELOPMENT 06 09 2000 07 01 $<$NET$>$ $<$NET$>$ ECT ENRON 09 06 03 10 23 PM To $<$NET$>$ $<$NET$>$ ECT ECT cc $<$NET$>$ $<$NET$>$ ECT ECT Subject Wow Do not underestimate the employment group contains Socal study about recession impact $<$NET$>$ will note else to you for a revised Good credit period I just want", "PDX Cantrell $<$LINK$>$ $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ F $<$NET$>$ ECT ECT $<$NET$>$ G Slaughter 06 07 03 57 DEVELOPMENT 06 09 2000 07 01 $<$NET$>$ $<$NET$>$ ECT ENRON 09 06 03 10 23 PM To $<$NET$>$ $<$NET$>$ ECT ECT cc $<$NET$>$ $<$NET$>$ ECT ECT Subject Wow Do not underestimate the employment group contains Socal study about recession impact $<$NET$>$ will note else to you for a revised Good credit period I just want to bring", "Cantrell $<$LINK$>$ $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ F $<$NET$>$ ECT ECT $<$NET$>$ G Slaughter 06 07 03 57 DEVELOPMENT 06 09 2000 07 01 $<$NET$>$ $<$NET$>$ ECT ENRON 09 06 03 10 23 PM To $<$NET$>$ $<$NET$>$ ECT ECT cc $<$NET$>$ $<$NET$>$ ECT ECT Subject Wow Do not underestimate the employment group contains Socal study about recession impact $<$NET$>$ will note else to you for a revised Good credit period I just want to bring the afternoon", "48 PDX Cantrell $<$LINK$>$ $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ F $<$NET$>$ ECT ECT $<$NET$>$ G Slaughter 06 07 03 57 DEVELOPMENT 06 09 2000 07 01 $<$NET$>$ $<$NET$>$ ECT ENRON 09 06 03 10 23 PM To $<$NET$>$ $<$NET$>$ ECT ECT cc $<$NET$>$ $<$NET$>$ ECT ECT Subject Wow Do not underestimate the employment group contains Socal study about recession impact $<$NET$>$ will note else to you for a revised Good credit period I just want to"]}
{"question_id": "46146ff3ef3430924e6b673a28df96ccb869dee4", "predicted_answer": "The CNN model outperformed the other models by 0.07 (0.65 - 0.58) in terms of macro average F1 score, compared to the BiLSTM (BOW) model, which was the next best model.", "predicted_evidence": ["The $\\textrm {F}_1$ score for the class Other is 0.52 in contrast with the $\\textrm {F}_1$ of the other three models, which is lower than 0.22. The macro average $\\textrm {F}_1$ for the BiLSTM (BOW) model is 0.58. It also has highest F1-scores for the classes Personal Information and Biographical Sketch among all models. For the classes Family, and Funeral information has comparable scores to the CNN model. Interestingly this model performs the best among the BiLSTM variants. The BiLSTM (W2V) model performs overall worse than the one which makes use only of a BOW. It also has the worst macro average $\\textrm {F}_1$ together with the BiLSTM-CRF with a value of 0.50. The BiLSTM-CRF performs better than the other BiLSTM variants on the rare classes Gratitude and Other.", "We investigate the best performing model by making use of the confusion matrix (see Figure FIGREF20) and by inspecting all errors made by the model on the test set (see Table TABREF21).", "The CNN model has the highest macro average $\\textrm {F}_1$ score with a value of 0.65. This results from the high values for the classes Family and Funeral information. The $\\textrm {F}_1$ score for the class Other is 0.52 in contrast with the $\\textrm {F}_1$ of the other three models, which is lower than 0.22. The macro average $\\textrm {F}_1$ for the BiLSTM (BOW) model is 0.58. It also has highest F1-scores for the classes Personal Information and Biographical Sketch among all models. For the classes Family, and Funeral information has comparable scores to the CNN model. Interestingly this model performs the best among the BiLSTM variants. The BiLSTM (W2V) model performs overall worse than the one which makes use only of a BOW. It also has the worst macro average $\\textrm {F}_1$ together with the BiLSTM-CRF with a value of 0.50.", "The CNN model has the highest macro average $\\textrm {F}_1$ score with a value of 0.65. This results from the high values for the classes Family and Funeral information. The $\\textrm {F}_1$ score for the class Other is 0.52 in contrast with the $\\textrm {F}_1$ of the other three models, which is lower than 0.22. The macro average $\\textrm {F}_1$ for the BiLSTM (BOW) model is 0.58. It also has highest F1-scores for the classes Personal Information and Biographical Sketch among all models. For the classes Family, and Funeral information has comparable scores to the CNN model. Interestingly this model performs the best among the BiLSTM variants. The BiLSTM (W2V) model performs overall worse than the one which makes use only of a BOW.", "From the results we conclude that the CNN model works best. Apart from the high $\\textrm {F}_1$ it is also the only model that predicts the class Gratitude as well as the class Other better than the other models."]}
{"question_id": "3499d5feeb3a45411d8e893516adbdc14e72002a", "predicted_answer": "In the context of the paper, reordering refers to the process of changing the word order of a sentence, either by random permutations or by generating translations word-by-word, which can result in an order that is not typical of the target language.", "predicted_evidence": ["This is a generalized version of swapping two neighboring words BIBREF11 . Reordering is highly dependent of each language, but we found that this noise is generally close to word-by-word translation outputs.", "From a clean target sentence, we corrupt its word order by random permutations. We limit the maximum distance between an original position and its new position like unmt-facebook:", "Rearrange the words to be in the new positions, to which their original indices have moved by Step 2.", "Also, translations generated word-by-word are not in an order of the target language. In our beam search, LM only assists in choosing the right word in context but does not modify the word order. A common reordering problem of German $\\rightarrow $ English is illustrated in Figure 3 .", "Add $\\delta _i$ to index $i$ and sort the incremented indices $i + \\delta _i$ in an increasing order."]}
{"question_id": "d0048ef1cba3f63b5d60c568d5d0ba62ac4d7e75", "predicted_answer": "According to the paper, the language model (LM) is used to assist in choosing the right word in context during the beam search process. The LM helps to score the possible target words based on the history of target words before the current position, taking into account the context around the current word. This allows the model to consider morphological variations, synonyms, and other context-dependent factors when selecting the best translation.", "predicted_evidence": ["In this paper, we integrate context information into word-by-word translation by combining a language model (LM) with cross-lingual word embedding. Let $f$ be a source word in the current position and $e$ a possible target word. Given a history $h$ of target words before $e$ , the score of $e$ to be the translation of $f$ would be:", "The word translation using nearest neighbor search does not consider context around the current word. In many cases, the correct translation is not the nearest target word but other close words with morphological variations or synonyms, depending on the context.", "Also, translations generated word-by-word are not in an order of the target language. In our beam search, LM only assists in choosing the right word in context but does not modify the word order. A common reordering problem of German $\\rightarrow $ English is illustrated in Figure 3 .", "Even when we have correctly translated words for each position, the output is still far from an acceptable translation. We adopt sequence denoising autoencoder BIBREF11 to improve the translation output of Section \"Context-aware Beam Search\" . The main idea is to train a sequence-to-sequence neural network model that takes a noisy sentence as input and produces a (denoised) clean sentence as output, both of which are of the same (target) language. The model was originally proposed to learn sentence embeddings, but here we use it directly to actually remove noise in a sentence.", "fasttext BIBREF8 was used to learn monolingual embeddings for only the words with minimum count 10. MUSE BIBREF7 was used for cross-lingual mappings with $V_\\text{cross-train}$ = 100k and 10 refinement iterations (Step 3-5 in Section \"Cross-lingual Word Embedding\" ). Other parameters follow the values in cross-facebook. With the same data, we trained 5-gram count-based LMs using KenLM BIBREF14 with its default setting."]}
{"question_id": "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c", "predicted_answer": "The datasets used are not explicitly mentioned in the given context. However, it is mentioned that there are four tasks, and there are development and test sets for each task. Additionally, it is mentioned that the development and test sets are imbalanced in terms of the ratio of positive and negative instances.", "predicted_evidence": ["SVM: a support vector machine (SVM) classifier trained with several manual features.", "We also provide results from two state-of-the-art systems:", "Because the development and test sets are imbalanced in terms of the ratio of positive and negative instances, we chose the widely-used F1 score as our major evaluation metric. In addition, we also provide the precision, recall and accuracy for further analysis. Table 2 summarizes the classification results.", "In addition to the classification performance, the efficiency in learning and inference is another concern for variational methods. Figure 4 shows the training procedure for four tasks in terms of the variational lower bound on the training set. We also provide F1 scores on the development set to investigate the relations between the variational lower bound and recognition performance.", "SCNN: a shallow convolutional neural network proposed by Zhang et al. biaozhang:2015:emnlp:drr."]}
{"question_id": "4e63454275380787ebd0e38aa885977332ab33af", "predicted_answer": "The evaluation metrics used are ROUGE scores, specifically:\n\n* ROUGE-1 F-measure\n* ROUGE-2 F-measure\n* ROUGE-SU4 F-measure", "predicted_evidence": ["The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.", "To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:", "where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.", "In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.", "Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight."]}
{"question_id": "dfaeb8faf04505a4178945c933ba217e472979d8", "predicted_answer": "The source of the dataset is not explicitly stated, but it is implied that the authors of the text created the dataset themselves, as they mention that they \"employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing\" to create the dataset.", "predicted_evidence": ["In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.", "The definition of the terminology related to the dataset is given as follows.", "There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.", "The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.", "The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \u201cMalaysia Airlines Disappearance\u201d, \u201cFlappy Bird\u201d, \u201cBitcoin Mt. Gox\u201d, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets."]}
{"question_id": "342ada55bd4d7408e1fcabf1810b92d84c1dbc41", "predicted_answer": "The performance improvement is not explicitly stated in the given context. However, it is mentioned that \"All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps.\" This implies that the performance of the model improved significantly when considering comments, but the exact amount of improvement is not specified.", "predicted_evidence": ["This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0", "The calculation of INLINEFORM0 will be discussed later.", "Based on the news and comments of the topic \u201cSony Virtual Reality PS4\u201d, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \u201cOculus\u201d, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \u201cMr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\u201d.", "The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.", "To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:"]}
{"question_id": "86d1c990c1639490c239c3dbf5492ecc44ab6652", "predicted_answer": "There were 4 experts for each topic.", "predicted_evidence": ["Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.", "The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \u201cMalaysia Airlines Disappearance\u201d, \u201cFlappy Bird\u201d, \u201cBitcoin Mt. Gox\u201d, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.", "In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.", "Based on the news and comments of the topic \u201cSony Virtual Reality PS4\u201d, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \u201cOculus\u201d, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \u201cMr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\u201d.", "where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences."]}
{"question_id": "b065c2846817f3969b39e355d5d017e326d6f42e", "predicted_answer": "The size of the data collected can be described as follows:\n\n* 45 topics\n* 450 news documents (45 topics x 10 news documents per topic)\n* 180 model summaries (45 topics x 4 model summaries per topic)\n* Approximately 9675 pieces of comments (45 topics x 215 pieces of comments per topic)\n* Approximately 42350 comment sentences (45 topics x 940 comment sentences per topic)\n* 19k annotated aspect facets\n* Total words: approximately 42350 comment sentences x 25 words per sentence = 1058750 words (approximate)", "predicted_evidence": ["In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.", "The definition of the terminology related to the dataset is given as follows.", "The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \u201cMalaysia Airlines Disappearance\u201d, \u201cFlappy Bird\u201d, \u201cBitcoin Mt. Gox\u201d, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.", "There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.", "To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:"]}
{"question_id": "9536e4a2455008007067f23cc873768374c8f664", "predicted_answer": "No, there is no indication that a crowdsourcing platform was used. The text mentions that reader comments are collected from news websites, but it does not suggest that these comments were collected through a crowdsourcing platform. Instead, it appears that the comments were collected from existing online news articles and forums.", "predicted_evidence": ["For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.", "Based on the news and comments of the topic \u201cSony Virtual Reality PS4\u201d, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \u201cOculus\u201d, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \u201cMr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\u201d.", "As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.", "where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.", "By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0"]}
{"question_id": "cfa44bb587b0c05906d8325491ca9e0f024269e8", "predicted_answer": "Annotation was conducted by 4 experts in journalism who were assigned to each topic. They conducted aspect facet identification by reading and digesting all the news documents and reader comments under the topic, and then extracting related facets from the news documents. The facets were selected based on their popularity in both news documents and reader comments, with higher priority given to those that were popular in both. The annotated aspect facets were then used to generate summaries that covered as many aspects as possible and were well-organized and concise.", "predicted_evidence": ["Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.", "One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.", "As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.", "After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.", "There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS."]}
{"question_id": "b3dc9a35e8c3ed7abcc4ca0bf308dea75be9c016", "predicted_answer": "The dataset contains:\n\n* 45 topics from 6 predefined categories\n* Each topic contains:\n\t+ 10 news documents\n\t+ 4 model summaries (each limited to 100 words)\n\t+ 215 pieces of comments\n\t+ 940 comment sentences\n* Each news document contains an average of 27 sentences\n* Each sentence contains an average of 25 words\n* 19k annotated aspect facets\n* Entities, unigrams, and bigrams (with 85% appearing in news documents and 51% in reader comments)", "predicted_evidence": ["In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.", "The definition of the terminology related to the dataset is given as follows.", "There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.", "The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \u201cMalaysia Airlines Disappearance\u201d, \u201cFlappy Bird\u201d, \u201cBitcoin Mt. Gox\u201d, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.", "The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported."]}
{"question_id": "693cdb9978749db04ba34d9c168e71534f00a226", "predicted_answer": "The question is asking whether the results reported are only based on English data. However, the given context does not explicitly mention the language used for the experiments. It only discusses the language processing capabilities of the LGI network and its similarities to human language learning. Therefore, it cannot be determined from the given context whether the results are only based on English data.", "predicted_evidence": ["Modern natural language processing (NLP) techniques can handle question answering etc. tasks, such as answering that \u2018Cao Cao\u2019s nickname is Meng De\u2019 based on the website knowledge [1]. However, the NLP network is just a probability model [2] and does not know whether Cao Cao is a man or cat. Indeed, it even does not understand what is a man. On the other hand, human being learns Cao Cao with his nickname via watching TV. When presented the question \u2018what\u2019s Cao Cao\u2019s nickname?\u2019, we can give the correct answer of \u2018Meng De\u2019 while imagining the figure of an actor in the brain. In this way, we say the machine network does not understand it, but the human does.", "After that, LGI learned the syntax \u2018the size is big/small\u2019, followed by \u2018the size is not small/big\u2019. Figure 5 illustrates that LGI could correctly categorize whether the digit size was small or big with proper text output. And we witness that, based on the syntax of \u2018the size is big/small\u2019 (train steps =1000), the negative adverb \u2018not\u2019 in the language text \u2018the size is not small/big\u2019 was much easier to be learned (train steps =200, with same hyper-parameters). This is quite similar to the cumulative learning process of the human being.", "Language is the most remarkable characteristics distinguishing mankind from animals. Theoretically, all kinds of information such as object properties, tasks and goals, commands and even emotions can be described and conveyed by language [21]. We trained with LGI eight different syntaxes (in other word, eight different tasks), and LGI demonstrates its understanding by correctly interacting with the vision system. After learning \u2018this is 9\u2019, it is much easier to learn \u2018give me a 9\u2019; after learning the \u2018size is big\u2019, it is much easier to learn \u2018the size is not small\u2019. Maybe some digested words or syntaxes were represented by certain PFC units, which could be shared with the following sentence learning.", "Based on the same network, LGI continued to learn syntax \u2018this is \u2026\u2019. Just like a parent teaching child numbers by pointing to number instances, Figure 4 demonstrates that, after training of 50000 steps, LGI could classify figures in various morphology with correct identity (accuracy = 72.7%). Note that, the classification process is not performed by softmax operation, but by directly textizing operation (i.e. rounding followed by a symbol mapping operation), which is more biologically plausible than the softmax operation.", "[5] Finke, K., Bublak, P., Neugebauer, U. & Zihl, J. (2005). Combined processing of what and where information within the visuospatial scratchpad. European Journal of Cognitive Psychology, 17(1), 1-22."]}
{"question_id": "71fd0efea1b441d86d9a75255815ba3efe09779b", "predicted_answer": "The authors measure the extent to which LGI has learned the task by evaluating its ability to:\n\n1. Reconstruct the input image with high precision.\n2. Predict the \"mentally\" moved object with specified morphology, correct manipulated direction, and position.\n3. Complete words given the first letter (e.g., completing \"move\" given the letter \"m\").\n4. Correctly categorize whether the digit size was small or big with proper text output.\n5. Perform iterative thinking processes, such as rotating an imagined object, enlarging it, and predicting its size.\n\nThese evaluations demonstrate LGI's understanding of verbs, nouns, and syntax, as well as its ability to manipulate imagination and form iterative thinking processes through the interaction between vision and language subsystems.", "predicted_evidence": ["The first syntaxes that LGI has learned are the \u2018move left\u2019 and \u2018move right\u2019 random pixels, with the corresponding results shown in Figure 3. After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed. The predicted text can complete the word \u2018move\u2019 given the first letter \u2018m\u2019 (till now, LGI has only learned syntaxes of \u2018move left or right\u2019). LGI tried to predict the second word \u2018right\u2019 with initial letter \u2018r\u2019, however, after knowing the command text is \u2018l\u2019, it turned to complete the following symbols with \u2018eft\u2019. It doesn\u2019t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and position.", "The first syntaxes that LGI has learned are the \u2018move left\u2019 and \u2018move right\u2019 random pixels, with the corresponding results shown in Figure 3. After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed. The predicted text can complete the word \u2018move\u2019 given the first letter \u2018m\u2019 (till now, LGI has only learned syntaxes of \u2018move left or right\u2019). LGI tried to predict the second word \u2018right\u2019 with initial letter \u2018r\u2019, however, after knowing the command text is \u2018l\u2019, it turned to complete the following symbols with \u2018eft\u2019. It doesn\u2019t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and position. Even if the command asked to move out of screen, LGI still could reconstruct the partially occluded image with high fidelity.", "After that, LGI learned the syntax \u2018the size is big/small\u2019, followed by \u2018the size is not small/big\u2019. Figure 5 illustrates that LGI could correctly categorize whether the digit size was small or big with proper text output. And we witness that, based on the syntax of \u2018the size is big/small\u2019 (train steps =1000), the negative adverb \u2018not\u2019 in the language text \u2018the size is not small/big\u2019 was much easier to be learned (train steps =200, with same hyper-parameters). This is quite similar to the cumulative learning process of the human being.", "(1) LGI first closed its eyes, namely, that no input images were fed into the vision subsystem (all the subsequent input images were generated through the imagination process). (2) LGI said to itself \u2018give me a 9\u2019, then the PFC produced the corresponding encoding vector INLINEFORM0 , and finally one digit \u20189\u2019 instance was reconstructed via the imagination network. (3) LGI gave the command \u2018rotate 180\u2019, then the imagined digit \u20189\u2019 was rotated upside down. (4) Following the language command \u2018this is \u2019, LGI automatically predicted that the newly imaged object was the digit \u20186\u2019. (5) LGI used \u2018enlarge\u2019 command to make the object bigger. (6) Finally, LGI predicted that the size was \u2018big\u2019 according to the imagined object morphology. This demonstrates that LGI can understand the verbs and nouns by properly manipulating the imagination, and can form the iterative thinking process via the interaction between vision and language subsystems through the PFC layer.", "(2) LGI said to itself \u2018give me a 9\u2019, then the PFC produced the corresponding encoding vector INLINEFORM0 , and finally one digit \u20189\u2019 instance was reconstructed via the imagination network. (3) LGI gave the command \u2018rotate 180\u2019, then the imagined digit \u20189\u2019 was rotated upside down. (4) Following the language command \u2018this is \u2019, LGI automatically predicted that the newly imaged object was the digit \u20186\u2019. (5) LGI used \u2018enlarge\u2019 command to make the object bigger. (6) Finally, LGI predicted that the size was \u2018big\u2019 according to the imagined object morphology. This demonstrates that LGI can understand the verbs and nouns by properly manipulating the imagination, and can form the iterative thinking process via the interaction between vision and language subsystems through the PFC layer. The human thinking process normally would not form a concrete imagination through the full visual loop, but rather a vague and rapid imagination through the short-cut loop by feeding back INLINEFORM1 to AIT directly."]}
{"question_id": "fb9e333a4e5d5141fe8e97b24b8f7e5685afbf09", "predicted_answer": "According to the text, LGI has learned the following 8 tasks:\n\n1. Move left\n2. Move right\n3. Generate a digit (e.g. \"give me a 9\")\n4. Rotate an object (e.g. \"rotate 180\")\n5. Identify an object (e.g. \"this is 6\")\n6. Enlarge an object\n7. Predict the size of an object (e.g. \"the size is big\")\n8. Mentally move an object with specified morphology, direction, and position.", "predicted_evidence": ["The first syntaxes that LGI has learned are the \u2018move left\u2019 and \u2018move right\u2019 random pixels, with the corresponding results shown in Figure 3. After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed. The predicted text can complete the word \u2018move\u2019 given the first letter \u2018m\u2019 (till now, LGI has only learned syntaxes of \u2018move left or right\u2019). LGI tried to predict the second word \u2018right\u2019 with initial letter \u2018r\u2019, however, after knowing the command text is \u2018l\u2019, it turned to complete the following symbols with \u2018eft\u2019. It doesn\u2019t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and position. Even if the command asked to move out of screen, LGI still could reconstruct the partially occluded image with high fidelity.", "The first syntaxes that LGI has learned are the \u2018move left\u2019 and \u2018move right\u2019 random pixels, with the corresponding results shown in Figure 3. After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed. The predicted text can complete the word \u2018move\u2019 given the first letter \u2018m\u2019 (till now, LGI has only learned syntaxes of \u2018move left or right\u2019). LGI tried to predict the second word \u2018right\u2019 with initial letter \u2018r\u2019, however, after knowing the command text is \u2018l\u2019, it turned to complete the following symbols with \u2018eft\u2019. It doesn\u2019t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and position.", "(1) LGI first closed its eyes, namely, that no input images were fed into the vision subsystem (all the subsequent input images were generated through the imagination process). (2) LGI said to itself \u2018give me a 9\u2019, then the PFC produced the corresponding encoding vector INLINEFORM0 , and finally one digit \u20189\u2019 instance was reconstructed via the imagination network. (3) LGI gave the command \u2018rotate 180\u2019, then the imagined digit \u20189\u2019 was rotated upside down. (4) Following the language command \u2018this is \u2019, LGI automatically predicted that the newly imaged object was the digit \u20186\u2019. (5) LGI used \u2018enlarge\u2019 command to make the object bigger. (6) Finally, LGI predicted that the size was \u2018big\u2019 according to the imagined object morphology. This demonstrates that LGI can understand the verbs and nouns by properly manipulating the imagination, and can form the iterative thinking process via the interaction between vision and language subsystems through the PFC layer.", "Finally, in Figure 7, we illustrate how LGI performed the human-like language-guided thinking process, with the above-learned syntaxes. (1) LGI first closed its eyes, namely, that no input images were fed into the vision subsystem (all the subsequent input images were generated through the imagination process). (2) LGI said to itself \u2018give me a 9\u2019, then the PFC produced the corresponding encoding vector INLINEFORM0 , and finally one digit \u20189\u2019 instance was reconstructed via the imagination network. (3) LGI gave the command \u2018rotate 180\u2019, then the imagined digit \u20189\u2019 was rotated upside down. (4) Following the language command \u2018this is \u2019, LGI automatically predicted that the newly imaged object was the digit \u20186\u2019. (5) LGI used \u2018enlarge\u2019 command to make the object bigger.", "(2) LGI said to itself \u2018give me a 9\u2019, then the PFC produced the corresponding encoding vector INLINEFORM0 , and finally one digit \u20189\u2019 instance was reconstructed via the imagination network. (3) LGI gave the command \u2018rotate 180\u2019, then the imagined digit \u20189\u2019 was rotated upside down. (4) Following the language command \u2018this is \u2019, LGI automatically predicted that the newly imaged object was the digit \u20186\u2019. (5) LGI used \u2018enlarge\u2019 command to make the object bigger. (6) Finally, LGI predicted that the size was \u2018big\u2019 according to the imagined object morphology. This demonstrates that LGI can understand the verbs and nouns by properly manipulating the imagination, and can form the iterative thinking process via the interaction between vision and language subsystems through the PFC layer. The human thinking process normally would not form a concrete imagination through the full visual loop, but rather a vague and rapid imagination through the short-cut loop by feeding back INLINEFORM1 to AIT directly."]}
{"question_id": "cb029240d4dedde74fcafad6a46c1cfc2621b934", "predicted_answer": "An LSTM (Long Short-Term Memory) network mimics the prefrontal cortex (PFC) in the way it maintains and manipulates information in its memory cells, similar to how the PFC maintains and manipulates information in working memory.", "predicted_evidence": ["[3] Miller, E. K. & Cohen, J. D. (2001). An integrative theory of prefrontal cortex function. Annual review of neuroscience, 24(1), 167-202.", "[30] Barbas, H. (2015). General cortical and special prefrontal connections: principles from structure to function. Annual review of neuroscience, 38, 269-289.", "The human-like thinking system often requires specific neural substrates to support the corresponding functionalities. The most important brain area related to thinking is the prefrontal cortex (PFC), where the working memory takes place, including but not confined to, the maintenance and manipulation of particular information [3]. With the PFC, human beings can analyze and execute various tasks via \u2018phonological loop\u2019 and \u2018visuospatial scratchpad\u2019 etc. [4,5]. Inspired by the human-like brain organization, we build a \u2018PFC\u2019 network to combine language and vision streams to achieve tasks such as language controlled imagination, and imagination based thinking process. Our results show that the LGI network could incrementally learn eight syntaxes rapidly. Based on the LGI, we present the first language guided continual thinking process, which shows considerable promise for the human-like strong machine intelligence.", "As is shown in Figure 1, the LGI network contains three main subsystems including the vision, language and PFC subsystems. The vision autoencoder network was trained separately, whose characteristics of development, recognition, learning, and forgetting can be referred to [10]. After training, the autoencoder is separated into two parts: the encoder (or recognition) part ranges from the image entry point to the final encoding layer, which functions as human anterior inferior temporal lobe (AIT) to provide the high-level abstract representation of the input image [18]; the decoder (or imagination) part ranges from the AIT to image prediction point. The activity vectors of the third encoding layer INLINEFORM0 and AIT layer INLINEFORM1 are concatenated with language activity vectors INLINEFORM2 as input signals to the PFC. We expect, after acquiring the language command, the PFC could output a desired visual activation vector INLINEFORM3 , based on which the imagination network could reconstruct the predicted image.", "As is shown in Figure 1, the LGI network contains three main subsystems including the vision, language and PFC subsystems. The vision autoencoder network was trained separately, whose characteristics of development, recognition, learning, and forgetting can be referred to [10]. After training, the autoencoder is separated into two parts: the encoder (or recognition) part ranges from the image entry point to the final encoding layer, which functions as human anterior inferior temporal lobe (AIT) to provide the high-level abstract representation of the input image [18]; the decoder (or imagination) part ranges from the AIT to image prediction point. The activity vectors of the third encoding layer INLINEFORM0 and AIT layer INLINEFORM1 are concatenated with language activity vectors INLINEFORM2 as input signals to the PFC."]}
{"question_id": "11a8531699952f5a2286a4311f0fe80ed1befa1e", "predicted_answer": "The LSTM layer in the language processing component mimics the number processing functionality of the human Intra-Parietal Sulcus (IPS) by extracting quantity information from the text, such as extracting the numerical value \"12\" from the text \"move left 12\".", "predicted_evidence": ["As is shown in Figure 1, the LGI network contains three main subsystems including the vision, language and PFC subsystems. The vision autoencoder network was trained separately, whose characteristics of development, recognition, learning, and forgetting can be referred to [10]. After training, the autoencoder is separated into two parts: the encoder (or recognition) part ranges from the image entry point to the final encoding layer, which functions as human anterior inferior temporal lobe (AIT) to provide the high-level abstract representation of the input image [18]; the decoder (or imagination) part ranges from the AIT to image prediction point. The activity vectors of the third encoding layer INLINEFORM0 and AIT layer INLINEFORM1 are concatenated with language activity vectors INLINEFORM2 as input signals to the PFC.", "As is shown in Figure 1, the LGI network contains three main subsystems including the vision, language and PFC subsystems. The vision autoencoder network was trained separately, whose characteristics of development, recognition, learning, and forgetting can be referred to [10]. After training, the autoencoder is separated into two parts: the encoder (or recognition) part ranges from the image entry point to the final encoding layer, which functions as human anterior inferior temporal lobe (AIT) to provide the high-level abstract representation of the input image [18]; the decoder (or imagination) part ranges from the AIT to image prediction point. The activity vectors of the third encoding layer INLINEFORM0 and AIT layer INLINEFORM1 are concatenated with language activity vectors INLINEFORM2 as input signals to the PFC. We expect, after acquiring the language command, the PFC could output a desired visual activation vector INLINEFORM3 , based on which the imagination network could reconstruct the predicted image.", "The first syntaxes that LGI has learned are the \u2018move left\u2019 and \u2018move right\u2019 random pixels, with the corresponding results shown in Figure 3. After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed. The predicted text can complete the word \u2018move\u2019 given the first letter \u2018m\u2019 (till now, LGI has only learned syntaxes of \u2018move left or right\u2019). LGI tried to predict the second word \u2018right\u2019 with initial letter \u2018r\u2019, however, after knowing the command text is \u2018l\u2019, it turned to complete the following symbols with \u2018eft\u2019. It doesn\u2019t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and position.", "The language processing component first binarizes the input text symbol-wise into a sequence of binary vectors INLINEFORM0 , where T is the text length. To improve the language command recognition, we added one LSTM layer to extract the quantity information of the text (for example, suppose text = \u2018move left 12\u2019, the expected output INLINEFORM1 is 1 dimensional quantity 12 at the last time point). This layer mimics the number processing functionality of human Intra-Parietal Sulcus (IPS), so it is given the name IPS layer. The PFC outputs the desired activation of INLINEFORM2 , which can either be decoded by the \u2018texitizer\u2019 into predicted text or serve as INLINEFORM3 for the next iteration of the imagination process. Here, we propose a textizer (a rounding operation, followed by symbol mapping from binary vector, whose detailed discussion can be referred to the Supplementary section A) to classify the predicted symbol instead of softmax operation which has no neuroscience foundation.", "In the future, many more syntaxes and functionalities can be added to LGI in a similar way, such as math reasoning, intuitive physics prediction and navigation [24, 25, 26]. Insights of human audition processing could be leveraged to convert sound wave into language text as a direct input for LGI [27, 28]. And the mechanisms of human value systems in the striatum [29] may also endow LGI with motivation and emotion. The PFC cortex consists of many sub-regions interacted within the PFC and across the whole brain areas [3, 30], and the implementation of these features might finally enable LGI to possess real machine intelligence."]}
{"question_id": "bcf222ad4bb537b01019ed354ea03cd6bf2c1f8e", "predicted_answer": "According to the authors, imagination or imagined scenarios refer to the ability to generate mental representations or scenarios without actual sensory input. In the context of the text, imagination involves the manipulation of mental images or objects in response to language commands, such as rotating, enlarging, or transforming them, without actually perceiving them through the senses. This process is described as a \"try-in-imagination\" strategy, where the system can generate and manipulate mental representations of objects or scenarios without requiring actual practice or sensory input. The authors suggest that this ability is unique to humans and is essential for human thinking, and that machines equipped with this ability could easily select clever actions for multiple tasks without being heavily trained.", "predicted_evidence": ["Imagination is another key component of human thinking. For the game Go [22, 23], the network using a reinforcement learning strategy has to be trained with billions of games in order to acquire a feeling (Q value estimated for each potential action) to move the chess. As human beings, after knowing the rule conveyed by language, we can quickly start a game with proper moves using a try-in-imagination strategy without requiring even a single practice. With imagination, people can change the answering contents (or even tell good-will lies) by considering or imagining the consequence of the next few output sentences. Machine equipped with the unique ability of imagination could easily select clever actions for multiple tasks without being trained heavily.", "(3) LGI gave the command \u2018rotate 180\u2019, then the imagined digit \u20189\u2019 was rotated upside down. (4) Following the language command \u2018this is \u2019, LGI automatically predicted that the newly imaged object was the digit \u20186\u2019. (5) LGI used \u2018enlarge\u2019 command to make the object bigger. (6) Finally, LGI predicted that the size was \u2018big\u2019 according to the imagined object morphology. This demonstrates that LGI can understand the verbs and nouns by properly manipulating the imagination, and can form the iterative thinking process via the interaction between vision and language subsystems through the PFC layer. The human thinking process normally would not form a concrete imagination through the full visual loop, but rather a vague and rapid imagination through the short-cut loop by feeding back INLINEFORM1 to AIT directly. On the other hand, the full path of clear imagination may explain the dream mechanism. Figure 7.B shows the short cut imagination process, where LGI also regarded the rotated \u20189\u2019 as digit 6, which suggests the AIT activation does not encode the digit identity, but the untangled features of input image or imagined image.", "(5) LGI used \u2018enlarge\u2019 command to make the object bigger. (6) Finally, LGI predicted that the size was \u2018big\u2019 according to the imagined object morphology. This demonstrates that LGI can understand the verbs and nouns by properly manipulating the imagination, and can form the iterative thinking process via the interaction between vision and language subsystems through the PFC layer. The human thinking process normally would not form a concrete imagination through the full visual loop, but rather a vague and rapid imagination through the short-cut loop by feeding back INLINEFORM1 to AIT directly. On the other hand, the full path of clear imagination may explain the dream mechanism. Figure 7.B shows the short cut imagination process, where LGI also regarded the rotated \u20189\u2019 as digit 6, which suggests the AIT activation does not encode the digit identity, but the untangled features of input image or imagined image. Those high level cortices beyond visual cortex could be the place for identity representation.", "(3) LGI gave the command \u2018rotate 180\u2019, then the imagined digit \u20189\u2019 was rotated upside down. (4) Following the language command \u2018this is \u2019, LGI automatically predicted that the newly imaged object was the digit \u20186\u2019. (5) LGI used \u2018enlarge\u2019 command to make the object bigger. (6) Finally, LGI predicted that the size was \u2018big\u2019 according to the imagined object morphology. This demonstrates that LGI can understand the verbs and nouns by properly manipulating the imagination, and can form the iterative thinking process via the interaction between vision and language subsystems through the PFC layer. The human thinking process normally would not form a concrete imagination through the full visual loop, but rather a vague and rapid imagination through the short-cut loop by feeding back INLINEFORM1 to AIT directly. On the other hand, the full path of clear imagination may explain the dream mechanism.", "Finally, in Figure 7, we illustrate how LGI performed the human-like language-guided thinking process, with the above-learned syntaxes. (1) LGI first closed its eyes, namely, that no input images were fed into the vision subsystem (all the subsequent input images were generated through the imagination process). (2) LGI said to itself \u2018give me a 9\u2019, then the PFC produced the corresponding encoding vector INLINEFORM0 , and finally one digit \u20189\u2019 instance was reconstructed via the imagination network. (3) LGI gave the command \u2018rotate 180\u2019, then the imagined digit \u20189\u2019 was rotated upside down. (4) Following the language command \u2018this is \u2019, LGI automatically predicted that the newly imaged object was the digit \u20186\u2019. (5) LGI used \u2018enlarge\u2019 command to make the object bigger. (6) Finally, LGI predicted that the size was \u2018big\u2019 according to the imagined object morphology."]}
{"question_id": "af45ff2c4209f14235482329d0729864fb2bd4b0", "predicted_answer": "According to the given context, they experimented with a logistic regression classifier (with no regularization).", "predicted_evidence": ["We then built a logistic regression classifier (with no regularization) per language using the annotated edits and their labels. The classifier has only three features mentioned above plus a bias term. We confirmed that, for every language, all the features are contributing to the prediction of typo edits controlling for other features in a statistically significant way $(p < .05)$. Table TABREF40 shows the performance of the trained classifier based on 10-fold cross validation on the annotated data. The results show that for all the languages mentioned here, the classifier successfully classifies typo edits with an F1-value of approx. 0.9. This means that the harvested edits are fairly clean in the first place (only one third is semantic edits versus others) and it is straightforward to distinguish the two using a simple classifier.", "We then built a logistic regression classifier (with no regularization) per language using the annotated edits and their labels. The classifier has only three features mentioned above plus a bias term. We confirmed that, for every language, all the features are contributing to the prediction of typo edits controlling for other features in a statistically significant way $(p < .05)$. Table TABREF40 shows the performance of the trained classifier based on 10-fold cross validation on the annotated data. The results show that for all the languages mentioned here, the classifier successfully classifies typo edits with an F1-value of approx. 0.9. This means that the harvested edits are fairly clean in the first place (only one third is semantic edits versus others) and it is straightforward to distinguish the two using a simple classifier. In the GitHub Typo Corpus, we annotate every edit in those three languages with the predicted \u201ctypo-ness\u201d score (the prediction probability produced from the logistic regression classifier) as well as a binary label indicating whether the edit is predicted as a typo, which may help the users of the dataset determine which edits to use for their purposes.", "Identify true typo edits (vs semantic edits) by using learned classifiers on a small annotated dataset", "See Figure FIGREF27 for some examples of different edit types on each language. If one edit contains more than one type of changes, the least superficial category is assigned. For example, if there are both spell and grammatical changes in a single edit, the \u201cgrammatical\u201d category is assigned to the edit. We note that the first three (mechanical, spell, and grammatical edits, also called typos) are within the scope of the dataset we build, while the last one (semantic edits) is not. Thus, our goal is to identify the last type of edits as accurately as possible in a scalable manner. We will show the statistics of the annotated data in Section 6.", "where $p(x)$ is determined by a trained language model. We hypothesize that perplexity captures the \u201cfluency\u201d of the input text to some degree, and by taking the ratio between the source and the target, the feature can represent the degree to which the fluency is improved before and after the edit."]}
{"question_id": "d2451d32c5a11a0eb8356a5e9d94a9231b59f198", "predicted_answer": "No, the distribution of edits is not uniform across all languages. According to Table TABREF41, the distribution of languages is heavily skewed towards English, and there are 15 languages that have 100 or more edits in the dataset.", "predicted_evidence": ["See Figure FIGREF33 for an overview of the distributions of these computed statistics per category for English. We observed similar trends for other two languages (Chinese and Japanese), except for a slightly larger number of spell edits, mainly due to the non-Latin character conversion errors. We also confirmed that the difference of perplexities between the source and the target for typo edits (i.e., mechanical, spell, and grammatical edits) was statistically significant for all three languages (two-tailed t-test, $p < .01$). This means that these edits, on average, turn the source text into a more fluent text in the target.", "In order to investigate the characteristics of such edits empirically, we first extracted 200 edits for each one of the three largest languages in the GitHub Typo Corpus: English (eng), Simplified Chinese (cmn-hans), and Japanese (jpn). We then had fluent speakers of each language go over the list and annotate each edit with the following four edit categories:", "Table TABREF41 shows the statistics of the GitHub Typo Corpus, broken down per language. The distribution of languages is heavily skewed towards English, although we observe the dataset includes a diverse set of other languages. There are 15 languages that have 100 or more edits in the dataset.", "Finally, after annotating a small amount of samples for the three languages, we computed some basic statistics about each edit that may help in classifying typo edits from non-typo ones. Specifically, we computed three statistics:", "In order to provide a more qualitative look into the dataset, we analyzed all the edits in the top three languages and extracted atomic edits. An atomic edit is defined as a sequence of contiguous characters that are inserted, deleted, or substituted between the source and the target. We extracted these atomic edits by aligning the characters between the source and the target by minimizing the edit distance, then by extracting contiguous edits that are insertion, deletion, or substitution."]}
{"question_id": "90dde59e1857a0d2b1ee4615ab017fee0741f29f", "predicted_answer": "The language of the text was identified based on the language codes provided in the GitHub Typo Corpus dataset. Specifically, the language codes used were:\n\n* eng for English\n* cmn-hans for Simplified Chinese\n* jpn for Japanese\n\nThese language codes were likely used to categorize and separate the text samples into their respective languages, allowing the researchers to analyze and annotate the edits for each language separately.", "predicted_evidence": ["Finally, after annotating a small amount of samples for the three languages, we computed some basic statistics about each edit that may help in classifying typo edits from non-typo ones. Specifically, we computed three statistics:", "See Figure FIGREF33 for an overview of the distributions of these computed statistics per category for English. We observed similar trends for other two languages (Chinese and Japanese), except for a slightly larger number of spell edits, mainly due to the non-Latin character conversion errors. We also confirmed that the difference of perplexities between the source and the target for typo edits (i.e., mechanical, spell, and grammatical edits) was statistically significant for all three languages (two-tailed t-test, $p < .01$). This means that these edits, on average, turn the source text into a more fluent text in the target.", "where $p(x)$ is determined by a trained language model. We hypothesize that perplexity captures the \u201cfluency\u201d of the input text to some degree, and by taking the ratio between the source and the target, the feature can represent the degree to which the fluency is improved before and after the edit.", "Table TABREF41 shows the statistics of the GitHub Typo Corpus, broken down per language. The distribution of languages is heavily skewed towards English, although we observe the dataset includes a diverse set of other languages. There are 15 languages that have 100 or more edits in the dataset.", "In order to investigate the characteristics of such edits empirically, we first extracted 200 edits for each one of the three largest languages in the GitHub Typo Corpus: English (eng), Simplified Chinese (cmn-hans), and Japanese (jpn). We then had fluent speakers of each language go over the list and annotate each edit with the following four edit categories:"]}
{"question_id": "811b67460e65232b8f363dc3f329ffecdfcc4ab2", "predicted_answer": "They collected repositories from GH Archive, which collects all the GitHub event data and makes them accessible through flexible APIs.", "predicted_evidence": ["This resulted in a total of 43,462 eligible repositories.", "Although GitHub provides a set of APIs (application programming interfaces) that allow end-users to access its data in a programmatic manner, it doesn't allow flexible querying on the repository meta data necessary for our data collection purposes. Therefore, we turn to GH Archive, which collects all the GitHub event data and make them accessible through flexible APIs. Specifically, we collected every repository from GH Archive that:", "Repository ... in git terms, a repository is a database of files whose versions are controlled under git. A single repository may contain multiple files and directories just like a computer file system.", "Note the \u201cand\u201d in the list above\u2014a repository needs to meet all the conditions mentioned above to be eligible. The first two criteria (pull request events and the number of starts) are a sign of a quality repository. As for the license, we allowed apache-2.0 (Apache License 2.0), mit (MIT License), bsd-3-clause (BSD 3-Clause License), bsd-2-clause (BSD 2-Clause License), cc0-1.0 (Creative Commons Zero v1.0 Universal), unlicense (Unlicense), cc-by-4.0 (Creative Commons Attribution 4.0), and bsl-1.0 (Boost Software License 1.0 (BSL-1.0). A repository's number of stars, size, and license are determined as of the event in the first condition.", "This section describes the process for collecting a large amount of typos from GitHub, which consists two steps: 1) collecting target repositories that meet some criteria and 2) collecting commits and edits from them. See Figure FIGREF15 for the overview of the typo-collecting process."]}
{"question_id": "68aa460ad357b4228b16b31b2cbec986215813bf", "predicted_answer": "The three features used are:\n\n1. Mechanical edits\n2. Spell edits\n3. Grammatical edits", "predicted_evidence": ["First, we define and clarify the terminology that we use throughout this paper. See Figure FIGREF3 for an illustration of the concepts and how they relate to each other.", "The rationale behind the third feature is that we observed that purely numerical changes always end up being tagged as semantic edits.", "The authors would like to thank Tomoya Mizumoto at RIKEN AIP/Future Corporation and Kentaro Inui at RIKEN AIP/Tohoku University for their useful comments and discussion on this project.", "In this section, we provide detailed quantitative and qualitative analyses of the GitHub Typo Corpus.", "See Figure FIGREF27 for some examples of different edit types on each language. If one edit contains more than one type of changes, the least superficial category is assigned. For example, if there are both spell and grammatical changes in a single edit, the \u201cgrammatical\u201d category is assigned to the edit. We note that the first three (mechanical, spell, and grammatical edits, also called typos) are within the scope of the dataset we build, while the last one (semantic edits) is not. Thus, our goal is to identify the last type of edits as accurately as possible in a scalable manner. We will show the statistics of the annotated data in Section 6."]}
{"question_id": "4542b162a5be00206fd14570898a7925cb267599", "predicted_answer": "According to the text, the GitHub Typo Corpus covers more than 15 languages, including:\n\n1. English (eng)\n2. Simplified Chinese (cmn-hans)\n3. Japanese (jpn)\n\nThese three languages are the largest in the corpus, and the text mentions that there are 15 languages that have 100 or more edits in the dataset, but it does not specify the other 12 languages.", "predicted_evidence": ["Table TABREF41 shows the statistics of the GitHub Typo Corpus, broken down per language. The distribution of languages is heavily skewed towards English, although we observe the dataset includes a diverse set of other languages. There are 15 languages that have 100 or more edits in the dataset.", "In order to investigate the characteristics of such edits empirically, we first extracted 200 edits for each one of the three largest languages in the GitHub Typo Corpus: English (eng), Simplified Chinese (cmn-hans), and Japanese (jpn). We then had fluent speakers of each language go over the list and annotate each edit with the following four edit categories:", "Finally, after annotating a small amount of samples for the three languages, we computed some basic statistics about each edit that may help in classifying typo edits from non-typo ones. Specifically, we computed three statistics:", "See Figure FIGREF27 for some examples of different edit types on each language. If one edit contains more than one type of changes, the least superficial category is assigned. For example, if there are both spell and grammatical changes in a single edit, the \u201cgrammatical\u201d category is assigned to the edit. We note that the first three (mechanical, spell, and grammatical edits, also called typos) are within the scope of the dataset we build, while the last one (semantic edits) is not. Thus, our goal is to identify the last type of edits as accurately as possible in a scalable manner. We will show the statistics of the annotated data in Section 6.", "We demonstrate that a very simple logistic regression model with only three features can classify typos and non-typo edits correctly with $F1 \\sim 0.9$. This resulted in a dataset containing more than 350k edits and 64M characters in more than 15 languages. To the best of our knowledge, this is the largest multilingual dataset of misspellings to date. We made the dataset publicly available (https://github.com/mhagiwara/github-typo-corpus) along with the automatically assigned typo labels as well as the source code to extract typos. We also provide the detailed analyses of the dataset, where we demonstrate that the F measure of existing spell checkers merely reaches $\\sim 0.5$, arguing that the GitHub Typo Corpus provides a new, rich source of naturally-occurring misspellings and grammatical errors that complement existing datasets."]}
{"question_id": "a17fc7b96753f85aee1d2036e2627570f4b50c30", "predicted_answer": "No, they do not report results only on English data. Although the language of the datasets is not explicitly mentioned, the names of the datasets (e.g. WikiPassageQA, InsuranceQA, Quasar-t, SearchQA) suggest that they are likely to be in English. However, the authors do not explicitly state that they only report results on English data, leaving open the possibility that they may have experimented with datasets in other languages as well.", "predicted_evidence": ["Datasets: We experimented on four datasets: (1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, and (4) SearchQA BIBREF16. They cover both factoid and non-factoid QA and different average passage length. The statistics of the four datasets are provided in the Appendix. To generate passage-level question-answering data from Quasart-t and SearchQA, we used the retrieved passages for each question from OpenQA, and generated question-passage relevance label based on whether the ground truth answer is contained in the passage.", "In this paper, we conducted an empirical study of layer-wise activations of BERT as general-purpose text embeddings. We want to understand to what extent does the BERT representation capture syntactic and semantic information. The sentence-level embeddings are evaluated on downstream and probing tasks using the SentEval toolkit BIBREF8, while the passage-level encodings are evaluated on four passage-level QA datasets (both factoid and non-factoid) under a learning-to-rank setting. Different methods of combining query embeddings with passage-level answer embeddings are examined.", "A logistic regression layer or an MLP layer is added on top of the embeddings to output a ranking score. We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$. Ranking metrics such as MRR (mean reciprocal rank), MAP (mean average precision), Precision@K and Recall@K are used to measure the performance. We compared BERT passage embeddings against the baseline of BM25, other state-of-the-art models, and a fine-tuned BERT on in-domain supervised data which serves as the upper bound. For in-domain BERT fine-tuning, we feed the hidden state of the [CLS] token from the top layer into a two-layer MLP which outputs a relevance score between the question and candidate answer passage.", "Experiment Setting: We use the same pooling methods as in the sentence embedding experiment to extract passage embeddings, and make sure that the passage length is within BERT's maximum sequence length. Different methods of combining query embeddings with answer passage embeddings were explored including: cosine similarity (no trainable parameter), bilinear function, concatenation, and $(u, v, u * v, |u - v|)$ where $u$ and $v$ are query embedding and answer embedding, respectively. A logistic regression layer or an MLP layer is added on top of the embeddings to output a ranking score. We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$. Ranking metrics such as MRR (mean reciprocal rank), MAP (mean average precision), Precision@K and Recall@K are used to measure the performance.", "Effect of Pooling Methods: We examined different methods of extracting BERT hidden state activations. The pooling methods we evaluated include: CLS-pooling (the hidden state corresponding to the [CLS] token), SEP-pooling (the hidden state corresponding to the [SEP] token), Mean-pooling (the average of the hidden state of the encoding layer on the time axis), and Max-pooling (the maximum of the hidden state of the encoding layer on the time axis). To eliminate the layer-wise effects, we averaged the performance of each pooling method over different layers. The results are summarized in Table TABREF2, where the score for each task category is calculated by averaging the normalized values for the tasks within each category. Although the activations of [CLS] token hidden states are often used in fine-tuning BERT for classification tasks, Mean-pooling of hidden states performs the best in all task categories among all the pooling methods."]}
{"question_id": "c6170bb09ba2a416f8fa9b542f0ab05a64dbf2e4", "predicted_answer": "The BM25 baseline refers to a baseline model that uses the BM25 (Best Match 25) algorithm, which is a ranking function used for search engines and information retrieval systems. It is used as a baseline to compare the performance of the BERT passage embeddings and other models in the experiment.", "predicted_evidence": ["Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM BIBREF18, but still far behind the state-of-the-art interaction-based model SUBMULT+NN BIBREF17 and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline significantly.", "A logistic regression layer or an MLP layer is added on top of the embeddings to output a ranking score. We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$. Ranking metrics such as MRR (mean reciprocal rank), MAP (mean average precision), Precision@K and Recall@K are used to measure the performance. We compared BERT passage embeddings against the baseline of BM25, other state-of-the-art models, and a fine-tuned BERT on in-domain supervised data which serves as the upper bound. For in-domain BERT fine-tuning, we feed the hidden state of the [CLS] token from the top layer into a two-layer MLP which outputs a relevance score between the question and candidate answer passage.", "We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$. Ranking metrics such as MRR (mean reciprocal rank), MAP (mean average precision), Precision@K and Recall@K are used to measure the performance. We compared BERT passage embeddings against the baseline of BM25, other state-of-the-art models, and a fine-tuned BERT on in-domain supervised data which serves as the upper bound. For in-domain BERT fine-tuning, we feed the hidden state of the [CLS] token from the top layer into a two-layer MLP which outputs a relevance score between the question and candidate answer passage. We fine-tune all BERT parameters except the word embedding layers.", "Different methods of combining query embeddings with answer passage embeddings were explored including: cosine similarity (no trainable parameter), bilinear function, concatenation, and $(u, v, u * v, |u - v|)$ where $u$ and $v$ are query embedding and answer embedding, respectively. A logistic regression layer or an MLP layer is added on top of the embeddings to output a ranking score. We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$. Ranking metrics such as MRR (mean reciprocal rank), MAP (mean average precision), Precision@K and Recall@K are used to measure the performance. We compared BERT passage embeddings against the baseline of BM25, other state-of-the-art models, and a fine-tuned BERT on in-domain supervised data which serves as the upper bound.", "Experiment Setting: We use the same pooling methods as in the sentence embedding experiment to extract passage embeddings, and make sure that the passage length is within BERT's maximum sequence length. Different methods of combining query embeddings with answer passage embeddings were explored including: cosine similarity (no trainable parameter), bilinear function, concatenation, and $(u, v, u * v, |u - v|)$ where $u$ and $v$ are query embedding and answer embedding, respectively. A logistic regression layer or an MLP layer is added on top of the embeddings to output a ranking score. We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$. Ranking metrics such as MRR (mean reciprocal rank), MAP (mean average precision), Precision@K and Recall@K are used to measure the performance."]}
{"question_id": "fe080c6393f126b55ae456b81133bfc8ecbe85c2", "predicted_answer": "According to the text, the combination of the top and bottom layer embeddings led to better performance.", "predicted_evidence": ["Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM BIBREF18, but still far behind the state-of-the-art interaction-based model SUBMULT+NN BIBREF17 and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline significantly.", "Effect of Encoder Layer: We compare the performance of embeddings extracted from different encoder layers of a pre-trained BERT using bert-as-service BIBREF10. Since we are interested in the linguistic information encoded in the embeddings, we only add a logistic regression layer on top of the embeddings for each classification task. The results of using [CLS] token activations as embeddings are presented in Figure FIGREF1. The raw values are provided in the Appendix. In the heatmap, the raw values of metrics are normalized by the best performance of a particular task from all the models we evaluated including BERT. The tasks in the figure are grouped by task category. For example, all semantic similarity related tasks are placed at the top of the figure.", "As can be seen from the figure, embeddings from top layers generally perform better than lower layers. However, for certain semantic probing tasks such as tense classification, subject, and object number classifications, middle layer embeddings perform the best. Intuitively, embeddings from top layer should be more biased towards the target of BERT pre-training tasks, while bottom layer embeddings should be close to the word embeddings. We observed a higher correlation in performance between bottom layer embeddings and GloVe embeddings than embeddings from other layers. Overall, pre-trained BERT embeddings perform well in text classification and syntactic probing tasks. The biggest limitation lies in the semantic similarity and sentence surface information probing tasks, where we observed a big gap between BERT and other state-of-the-art models.", "Effect of Pooling Methods: We examined different methods of extracting BERT hidden state activations. The pooling methods we evaluated include: CLS-pooling (the hidden state corresponding to the [CLS] token), SEP-pooling (the hidden state corresponding to the [SEP] token), Mean-pooling (the average of the hidden state of the encoding layer on the time axis), and Max-pooling (the maximum of the hidden state of the encoding layer on the time axis). To eliminate the layer-wise effects, we averaged the performance of each pooling method over different layers. The results are summarized in Table TABREF2, where the score for each task category is calculated by averaging the normalized values for the tasks within each category. Although the activations of [CLS] token hidden states are often used in fine-tuning BERT for classification tasks, Mean-pooling of hidden states performs the best in all task categories among all the pooling methods.", "Pre-trained vs. Fine-tuned BERT: All the models we considered in this paper benefit from supervised training on natural language inference datasets. In this section, we compare the performance of embeddings from pre-trained BERT and fine-tuned BERT. Two natural language inference datasets, MNLI BIBREF11 and SNLI, were considered in the experiment. Inspired by the fact that embeddings from different layers excel in different tasks, we also conducted experiments by concatenating embeddings from multiple layers. The results are presented in Table TABREF3, and the raw values are provided in the Appendix."]}
{"question_id": "53a8c3cf22d6bf6477bc576a85a83d8447ee0484", "predicted_answer": "The NLI data used to improve the quality of the embeddings is SNLI (Stanford Natural Language Inference) and MNLI (Multi-Genre Natural Language Inference).", "predicted_evidence": ["Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM BIBREF18, but still far behind the state-of-the-art interaction-based model SUBMULT+NN BIBREF17 and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline significantly.", "Pre-trained vs. Fine-tuned BERT: All the models we considered in this paper benefit from supervised training on natural language inference datasets. In this section, we compare the performance of embeddings from pre-trained BERT and fine-tuned BERT. Two natural language inference datasets, MNLI BIBREF11 and SNLI, were considered in the experiment. Inspired by the fact that embeddings from different layers excel in different tasks, we also conducted experiments by concatenating embeddings from multiple layers. The results are presented in Table TABREF3, and the raw values are provided in the Appendix.", "In this paper, we conducted an empirical investigation of BERT activations as universal text embeddings. We show that sentence embeddings from BERT perform strongly on SentEval tasks, and combining embeddings from the top and bottom layers of BERT fine-tuned on SNLI provides the best performance. At passage-level, we evaluated BERT embeddings on four QA datasets. Models based on BERT passage embeddings outperform BM25 baseline significantly on factoid QA datasets but fail to perform better than BM25 on non-factoid datasets. We observed a big gap between embedding-based models and in-domain the fully fine-tuned BERT on QA datasets. Future research is needed to better model the interactions between pairs of text embeddings.", "Effect of Encoder Layer: We compare the performance of embeddings extracted from different encoder layers of a pre-trained BERT using bert-as-service BIBREF10. Since we are interested in the linguistic information encoded in the embeddings, we only add a logistic regression layer on top of the embeddings for each classification task. The results of using [CLS] token activations as embeddings are presented in Figure FIGREF1. The raw values are provided in the Appendix. In the heatmap, the raw values of metrics are normalized by the best performance of a particular task from all the models we evaluated including BERT. The tasks in the figure are grouped by task category. For example, all semantic similarity related tasks are placed at the top of the figure.", "In this paper, we conducted an empirical study of layer-wise activations of BERT as general-purpose text embeddings. We want to understand to what extent does the BERT representation capture syntactic and semantic information. The sentence-level embeddings are evaluated on downstream and probing tasks using the SentEval toolkit BIBREF8, while the passage-level encodings are evaluated on four passage-level QA datasets (both factoid and non-factoid) under a learning-to-rank setting. Different methods of combining query embeddings with passage-level answer embeddings are examined."]}
{"question_id": "3a33512d253005ac280ee9ca4f9dfa69aa38d48f", "predicted_answer": "The four QA datasets examined are:\n\n1. WikiPassageQA\n2. InsuranceQA (version 1.0)\n3. Quasar-t\n4. SearchQA", "predicted_evidence": ["Datasets: We experimented on four datasets: (1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, and (4) SearchQA BIBREF16. They cover both factoid and non-factoid QA and different average passage length. The statistics of the four datasets are provided in the Appendix. To generate passage-level question-answering data from Quasart-t and SearchQA, we used the retrieved passages for each question from OpenQA, and generated question-passage relevance label based on whether the ground truth answer is contained in the passage.", "However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM BIBREF18, but still far behind the state-of-the-art interaction-based model SUBMULT+NN BIBREF17 and fully fine-tuned BERT.", "When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline.", "Results: The comparison between BERT embeddings and other models is presented in Table TABREF5. Overall, in-domain fine-tuned BERT delivers the best performance. We report new state-of-the-art results on WikiPassageQA ($33\\%$ improvement in MAP) and InsuranceQA (version 1.0) ($3.6\\%$ improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected.", "When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer."]}
{"question_id": "f7f2968feb28c2907266c892f051ae9f7d6286e6", "predicted_answer": "The sentence embeddings are evaluated against downstream and probing tasks using the SentEval toolkit.", "predicted_evidence": ["We use the SentEval toolkit to evaluate the quality of sentence representations from BERT activations. The evaluation encompasses a variety of downstream and probing tasks. Downstream tasks include text classification, natural language inference, paraphrase detection, and semantic similarity. Probing tasks use single sentence embedding as input, are designed to probe sentence-level linguistic phenomena, from superficial properties of sentences to syntactic information to semantic acceptability. For details about the tasks, please refer to BIBREF8 and BIBREF9. We compare the BERT embeddings against two state-of-the-art sentence embeddings, Universal Sentence Encoder BIBREF5, InferSent BIBREF2, and a baseline of averaging GloVe word embeddings.", "Passages that consist of multiple sentences are coherent units of natural languages that convey information at a pragmatic or discourse level. While there are many models for generating and evaluating sentence embeddings, there hasn't been a lot of work on passage level embedding generation and evaluation.", "In this paper, we conducted an empirical study of layer-wise activations of BERT as general-purpose text embeddings. We want to understand to what extent does the BERT representation capture syntactic and semantic information. The sentence-level embeddings are evaluated on downstream and probing tasks using the SentEval toolkit BIBREF8, while the passage-level encodings are evaluated on four passage-level QA datasets (both factoid and non-factoid) under a learning-to-rank setting. Different methods of combining query embeddings with passage-level answer embeddings are examined.", "In this paper, we conducted an empirical investigation of BERT activations as universal text embeddings. We show that sentence embeddings from BERT perform strongly on SentEval tasks, and combining embeddings from the top and bottom layers of BERT fine-tuned on SNLI provides the best performance. At passage-level, we evaluated BERT embeddings on four QA datasets. Models based on BERT passage embeddings outperform BM25 baseline significantly on factoid QA datasets but fail to perform better than BM25 on non-factoid datasets. We observed a big gap between embedding-based models and in-domain the fully fine-tuned BERT on QA datasets. Future research is needed to better model the interactions between pairs of text embeddings.", "Experiment Setting: We use the same pooling methods as in the sentence embedding experiment to extract passage embeddings, and make sure that the passage length is within BERT's maximum sequence length. Different methods of combining query embeddings with answer passage embeddings were explored including: cosine similarity (no trainable parameter), bilinear function, concatenation, and $(u, v, u * v, |u - v|)$ where $u$ and $v$ are query embedding and answer embedding, respectively. A logistic regression layer or an MLP layer is added on top of the embeddings to output a ranking score. We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$."]}
{"question_id": "38289bd9592db4d3670b65a0fef1fe8a309fee61", "predicted_answer": "According to the text, they trained the following classifiers:\n\n1. Multinomial logistic regression model with ridge estimator\n2. Multilayer perceptron\n3. Support vector machine learner (Sequential Minimal Optimization, SMO)\n4. Decision tree (J48)\n5. Majority classifier (baseline)\n6. Logistic regression (used for document-level classification models)", "predicted_evidence": ["Accuracy scores using other learning algorithms were significantly lower (see Table TABREF10 ), therefore, we report only the results of the logistic regression classifier in the subsequent sections.", "To make sure that our system was not biased towards the majority classes B1 and B2, we inspected the confusion matrix (Table TABREF11 ) after classification using All. We can observe from Table TABREF11 that the system performs better at A1 and C1 levels, where confusion occurred only with adjacent classes. Similar to the findings in BIBREF13 for French, classes in the middle of the scale were harder to distinguish. Most misclassifications in our material occurred at A2 level (23%) followed by B1 and B2 level, (20% and 17% respectively).", "We explored different classification algorithms for this task using the machine learning toolkit WEKA BIBREF24 . These included: (1) a multinomial logistic regression model with ridge estimator, (2) a multilayer perceptron, (3) a support vector machine learner, Sequential Minimal Optimization (SMO), and (4) a decision tree (J48). For each of these, the default parameter settings have been used as implemented in WEKA.", "Most misclassifications occurred, however, within a distance of one class only, thus the adjacent accuracy of the sentence-level model was still high, 92% (see Table TABREF14 ). Predictions were noticeably more accurate for classes A1, A2 and B1 which had a larger number of instances.", "We trained document-level classification models, comparing the performance between different subgroups of features. We had two baselines: a majority classifier (Majority), with B2 as majority class, and the LIX readability score. Table TABREF9 shows the type of subgroup (Type), the number of features (Nr) and three evaluation metrics using logistic regression."]}
{"question_id": "cb7a00233502c4b7801d34bc95d6d22d79776ae8", "predicted_answer": "The dataset used is the COCTAILL corpus, which contains sentences and language examples, and was used to build a sentence-level dataset consisting of 1874 instances.", "predicted_evidence": ["In the following sections, we first describe our datasets (section SECREF2 ) and features (section SECREF3 ), then we present the details and the results of our experiments in section SECREF4 . Finally, section SECREF5 concludes our work and outlines further directions of research within this area.", "texts, the COCTAILL corpus contains a number of sentences independent from each other, i.e. not forming a coherent text, in the form of lists of sentences and language examples. This latter category consists of sentences illustrating the use of specific grammatical patterns or lexical items. Collecting these sentences, we built a sentence-level dataset consisting of 1874 instances. The information encoded in the content-level annotation of COCTAILL (XML tags list, language_example and the attribute unit) enabled us to include only complete sentences and exclude sentences containing gaps and units larger or smaller than a sentence (e.g. texts, phrases, single words etc.). The CEFR level of both sentences and texts has been derived from the CEFR level of the lesson (chapter) they appeared in. In Table TABREF3 , columns 2-5 give an overview of the distribution of texts across levels and their mean length in sentences. The distribution of sentences per level is presented in the last two columns of Table TABREF3 .", "Besides reading passages, i.e. texts, the COCTAILL corpus contains a number of sentences independent from each other, i.e. not forming a coherent text, in the form of lists of sentences and language examples. This latter category consists of sentences illustrating the use of specific grammatical patterns or lexical items. Collecting these sentences, we built a sentence-level dataset consisting of 1874 instances. The information encoded in the content-level annotation of COCTAILL (XML tags list, language_example and the attribute unit) enabled us to include only complete sentences and exclude sentences containing gaps and units larger or smaller than a sentence (e.g. texts, phrases, single words etc.). The CEFR level of both sentences and texts has been derived from the CEFR level of the lesson (chapter) they appeared in. In Table TABREF3 , columns 2-5 give an overview of the distribution of texts across levels and their mean length in sentences.", "Besides reading passages, i.e. texts, the COCTAILL corpus contains a number of sentences independent from each other, i.e. not forming a coherent text, in the form of lists of sentences and language examples. This latter category consists of sentences illustrating the use of specific grammatical patterns or lexical items. Collecting these sentences, we built a sentence-level dataset consisting of 1874 instances. The information encoded in the content-level annotation of COCTAILL (XML tags list, language_example and the attribute unit) enabled us to include only complete sentences and exclude sentences containing gaps and units larger or smaller than a sentence (e.g. texts, phrases, single words etc.). The CEFR level of both sentences and texts has been derived from the CEFR level of the lesson (chapter) they appeared in.", "Although a direct comparison with other studies is difficult because of the target language, the nature of the datasets and the number of classes used, in terms of absolute numbers, our model achieves comparable performance with the state-of-the-art systems for English BIBREF9 , BIBREF12 . Other studies for non-English languages using CEFR levels include: BIBREF13 , reporting 49.1% accuracy for a French system distinguishing six classes; and BIBREF14 achieving 29.7% accuracy on a smaller Portuguese dataset with five levels."]}
{"question_id": "35d2eae3a7c9bed54196334a09344591f9cbb5c8", "predicted_answer": "The question is not explicitly answered in the provided text. The text describes the methodology and experiments performed, but it does not explicitly state which combination of features helped improve the classification. The results of the experiments are presented in tables (Table TABREF9 and Table TABREF13), but the text does not provide a clear answer to the question.", "predicted_evidence": ["In the following sections, we first describe our datasets (section SECREF2 ) and features (section SECREF3 ), then we present the details and the results of our experiments in section SECREF4 . Finally, section SECREF5 concludes our work and outlines further directions of research within this area.", "The complete set of 61 features is presented in Table TABREF6 . Throughout this paper we will refer to the machine learning models using this set of features, unless otherwise specified. Features for both document- and sentence-level analyses were extracted for each sentence, the values being averaged over all sentences in the text in the document-level experiments to ensure comparability.", "We trained document-level classification models, comparing the performance between different subgroups of features. We had two baselines: a majority classifier (Majority), with B2 as majority class, and the LIX readability score. Table TABREF9 shows the type of subgroup (Type), the number of features (Nr) and three evaluation metrics using logistic regression.", "We explored different classification algorithms for this task using the machine learning toolkit WEKA BIBREF24 . These included: (1) a multinomial logistic regression model with ridge estimator, (2) a multilayer perceptron, (3) a support vector machine learner, Sequential Minimal Optimization (SMO), and (4) a decision tree (J48). For each of these, the default parameter settings have been used as implemented in WEKA.", "With the same methodology (section SECREF7 ) and feature set (section SECREF3 ) used at the document level, we trained and tested classification models based on the sentence-level data (see section SECREF2 ). The results are shown in Table TABREF13 ."]}
{"question_id": "a70656fc61bf526dd21db7d2ec697b29a5a9c24e", "predicted_answer": "The linguistics features applied in this context include:\n\n**Morphological features:**\n\n1. Variation (ratio of a category to the ratio of lexical tokens) for nouns, verbs, adjectives, and adverbs.\n2. IncSc (incremental score) of all lexical categories, punctuations, particles, sub-, and conjunctions.\n3. Fine-grained inflectional information, such as:\n\t* IncSc of neuter gender nouns.\n\t* Ratio of different verb forms to all verbs.\n4. Bilogarithmic and square root type-token ratio (TTR).\n5. Nominal ratio.\n6. Ratio of pronouns to prepositions.\n7. Two lexical density features:\n\t* Ratio of lexical words to all non-lexical categories.\n\t* Ratio of lexical words to all tokens.\n\n**Syntactic features:**\n\n1. Length (depth) and direction of dependency arcs.\n2. IncSc of relative clauses in clefts.\n3. IncSc of pre- and postmodifiers (e.g., adjectives and prepositional phrases).\n\n**Other features:**\n\n1. Relative structures, including relative adverbs, determiners, pronouns, and possessives.", "predicted_evidence": ["Morphological (Morph): We included the variation (the ratio of a category to the ratio of lexical tokens - i.e. nouns, verbs, adjectives and adverbs) and the IncSc of all lexical categories together with the IncSc of punctuations, particles, sub- and conjunctions (#34, #51). Some additional features, using insights from L2 teaching material BIBREF20 , captured fine-grained inflectional information such as the IncSc of neuter gender nouns and the ratio of different verb forms to all verbs (#52 - #56). Instead of simple type-token ratio (TTR) we used a bilogarithmic and a square root TTR as in BIBREF3 .", "Morphological (Morph): We included the variation (the ratio of a category to the ratio of lexical tokens - i.e. nouns, verbs, adjectives and adverbs) and the IncSc of all lexical categories together with the IncSc of punctuations, particles, sub- and conjunctions (#34, #51). Some additional features, using insights from L2 teaching material BIBREF20 , captured fine-grained inflectional information such as the IncSc of neuter gender nouns and the ratio of different verb forms to all verbs (#52 - #56). Instead of simple type-token ratio (TTR) we used a bilogarithmic and a square root TTR as in BIBREF3 . Moreover, nominal ratio BIBREF4 , the ratio of pronouns to prepositions BIBREF13 , and two lexical density features were also included: the ratio of lexical words to all non-lexical categories (#48) and to all tokens (#49).", "Syntactic (Synt): Some of these features were based on the length (depth) and the direction of dependency arcs (#17 - #21). We complemented this, among others, with the IncSc of relative clauses in clefts (#26), and the IncSc of pre-and postmodifiers (e.g. adjectives and prepositional phrases) BIBREF4 .", "nouns, verbs, adjectives and adverbs) and the IncSc of all lexical categories together with the IncSc of punctuations, particles, sub- and conjunctions (#34, #51). Some additional features, using insights from L2 teaching material BIBREF20 , captured fine-grained inflectional information such as the IncSc of neuter gender nouns and the ratio of different verb forms to all verbs (#52 - #56). Instead of simple type-token ratio (TTR) we used a bilogarithmic and a square root TTR as in BIBREF3 . Moreover, nominal ratio BIBREF4 , the ratio of pronouns to prepositions BIBREF13 , and two lexical density features were also included: the ratio of lexical words to all non-lexical categories (#48) and to all tokens (#49). Relative structures (#57) consisted of relative adverbs, determiners, pronouns and possessives.", "Besides reading passages, i.e. texts, the COCTAILL corpus contains a number of sentences independent from each other, i.e. not forming a coherent text, in the form of lists of sentences and language examples. This latter category consists of sentences illustrating the use of specific grammatical patterns or lexical items. Collecting these sentences, we built a sentence-level dataset consisting of 1874 instances. The information encoded in the content-level annotation of COCTAILL (XML tags list, language_example and the attribute unit) enabled us to include only complete sentences and exclude sentences containing gaps and units larger or smaller than a sentence (e.g. texts, phrases, single words etc.). The CEFR level of both sentences and texts has been derived from the CEFR level of the lesson (chapter) they appeared in."]}
{"question_id": "f381b0ef693243d67657f6c34bbce015f6b1fd07", "predicted_answer": "The state of the art in English for predicting linguistic complexity of texts and sentences for second and foreign language (L2) learners is represented by the studies mentioned in BIBREF9, BIBREF10, BIBREF11, and BIBREF12.", "predicted_evidence": ["Linguistic information provided by Natural Language Processing (NLP) tools has good potential for turning the continuously growing amount of digital text into interactive and personalized language learning material. Our work aims at overcoming one of the fundamental obstacles in this domain of research, namely how to assess the linguistic complexity of texts and sentences from the perspective of second and foreign language (L2) learners.", "In the future, a more detailed investigation is needed to understand the performance drop between document and sentence level. Acquiring more sentence-level annotated data and exploring new features relying on lexical-semantic resources for Swedish would be interesting directions to pursue. Furthermore, we intend to test the utility of this approach in a real-world web application involving language learners and teachers.", "Our goal was to predict linguistic complexity of material written by teachers and course book writers for learners, rather than assessing learner-produced texts. We adopted the scale from the Common European Framework of Reference for Languages (CEFR) BIBREF18 which contains guidelines for the creation of teaching material and the assessment of L2 proficiency. CEFR proposes six levels of language proficiency: A1 (beginner), A2 (elementary), B1 (intermediate), B2 (upper intermediate), C1 (advanced) and C2 (proficient). Since sentences are a common unit in language exercises, but remain less explored in the readability literature, we also investigate the applicability of our approach to sentences, performing a 5-way classification (levels A1-C1). Our document-level model achieves a state-of-the-art performance (F-score of 0.8), however, there is room for improvement in sentence-level predictions. We plan to make our results available through the online intelligent computer-assisted language learning platform L\u00e4rka, both as corpus-based exercises for teachers and learners of L2 Swedish and as web-services for researchers and developers.", "The previously mentioned studies target mainly native language (L1) readers including people with low literacy levels or mild cognitive disabilities. Our focus, however, is on building a model for predicting the proficiency level of texts and sentences used in L2 teaching materials. This aspect has been explored for English BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , French BIBREF13 , Portuguese BIBREF14 and, without the use of NLP, for Dutch BIBREF15 .", "Our dataset is a subset of COCTAILL, a corpus of course books covering five CEFR levels (A1-C1) BIBREF19 . This corpus consists of twelve books (from four different publishers) whose usability and level have been confirmed by Swedish L2 teachers. The course books have been annotated both content-wise (e.g. exercises, lists) and linguistically (e.g. with POS and dependency tags) BIBREF19 . We collected a total of 867 texts (reading passages) from this corpus. We excluded texts that are primarily based on dialogues from the current experiments due to their specific linguistic structure, with the aim of scaling down differences connected to text genres rather than linguistic complexity. We plan to study the readability of dialogues and compare them to non-dialogue texts in the future."]}
{"question_id": "c176eb1ccaa0e50fb7512153f0716e60bf74aa53", "predicted_answer": "Yes, the results reported are only on English data, as the authors focus on accounts that use English as their main language and remove non-English tweets from the dataset.", "predicted_evidence": ["To understand more the NLI features performance, given their high performance comparing to the other features, we extract the top important tokens for each of the NLI feature subsets (see Figure FIGREF37). Some of the obtained results confirmed what was found previously. For instance, the authors in BIBREF19 found that Russians write English tweets with more prepositions comparing to native speakers of other languages (e.g. as, about, because in (c) Stop-words and RP in (a) POS in Figure FIGREF37). Further research must be conducted to investigate in depth the rest of the results.", "We used the IRA dataset that was released by Twitter after identifying the Russian trolls. The original dataset contains $3,841$ accounts, but we use a lower number of accounts and tweets after filtering them. We focus on accounts that use English as main language. In fact, our goal is to detect Russian accounts that mimic a regular US user. Then, we remove from these accounts non-English tweets, and maintain only tweets that were tweeted originally by them. Our final IRA accounts list contains 2,023 accounts.", "The result of the NLI feature in the table is interesting; we are able to detect IRA trolls from their writing style with a F1$_{macro}$ value of 0.91. Considering the results in Table TABREF32, we can notice that we are able to detect the IRA trolls effectively using only textual features (RQ2).", "Native Language Identification (NLI): This feature was inspired by earlier works on identifying native language of essays writers BIBREF22. We aim to detect IRA trolls by identifying their way of writing English tweets. As shown in BIBREF19, English tweets generated by non-English speakers have a different syntactic pattern . Thus, we use state-of-the-art NLI features to detect this unique pattern BIBREF23, BIBREF24, BIBREF25; the feature set consists of bag of stopwords, Part-of-speech tags (POS), and syntactic dependency relations (DEPREL). We extract the POS and the DEPREL information using spaCy, an off-the-shelf POS tagger. We clean the tweets from the special characters and maintained dots, commas, and first-letter capitalization of words. We use regular expressions to convert a sequence of dots to a single dot, and similarly for sequence of characters.", "Bias Cues: We rely on a set of lexicons to capture the bias in text. We model the presence of the words in one of the following cues categories: assertives verbs BIBREF14, bias BIBREF15, factive verbs BIBREF16, implicative verbs BIBREF17, hedges BIBREF18, report verbs BIBREF15. A previous work has used these bias cues to identify bias in suspicious news posts in Twitter BIBREF19."]}
{"question_id": "e0b54906184a4ad87d127bed22194e62de38222b", "predicted_answer": "The features were used in a classification model.", "predicted_evidence": ["For the theme-based features, we use the following features that we believe that they change based on the themes:", "Based on our thematic information, we model the users textual features w.r.t. each of these themes. In other words, we model a set of textual features independently for each of the former themes to capture the emotional, stance, and others changes in the users tweets.", "Table TABREF32 presents the classification results showing the performance of each feature set independently. Generally, we can see that the thematic information improves the performance of the proposed features clearly (RQ1), and with the largest amount in the Emotions features (see $-_{themes}$ and $+_{themes}$ columns). This result emphasizes the importance of the thematic information. Also, we see that the emotions performance increases with the largest amount considering F1$_{macro}$ value; this motivates us to analyze the emotions in IRA tweets (see the following section).", "Similar to the feature representation of the theme-based features, we represent each user's tweets by considering the average and standard deviation of her tweets' $V_{1,2,..N}$, given $V_i$ as the concatenation of the previous two features vectors of a tweet$_i$. A user $x$ final feature vector is defined as follows:", "In addition, we removed special characters (except HASH \"#\" sign for the hashtags) and lowercase the final tweet. To ensure the quality of the themes, we removed the hashtags we used in the collecting process where they may bias the modeling algorithm. We tested multiple number of themes and we chose seven of them. We manually observed the content of these themes to label them. The extracted themes are: Police shootings, Islam and War, Supporting Trump, Black People, Civil Rights, Attacking Hillary, and Crimes. In some themes, like Supporting Trump and Attacking Hillary, we found contradicted opinions, in favor and against the main themes, but we chose the final stance based on the most representative hashtags and words in each of them (see Figure FIGREF11). Also, the themes Police Shooting and Crimes are similar, but we found that some words such as: police, officers, cops, shooting, gun, shot, etc. are the most discriminative between these two themes."]}
{"question_id": "1f8044487af39244d723582b8a68f94750eed2cc", "predicted_answer": "The unsupervised approach used to deduce the thematic information is not explicitly stated in the given context. However, based on the description, it appears that the authors used a combination of manual analysis and clustering/ dimensionality reduction techniques to extract the themes.\n\nThe authors mention that they \"tested multiple number of themes and we chose seven of them\" and that they \"manually observed the content of these themes to label them\". This suggests that they used a manual approach to identify the themes, possibly by reading and categorizing the tweets into different topics.\n\nAdditionally, the authors mention that they removed the hashtags used in the collecting process to avoid biasing the modeling algorithm, which implies that they may have used a clustering or topic modeling algorithm to identify the themes.\n\nTherefore, the unsupervised approach used is likely a combination of manual analysis and clustering/topic modeling techniques, such as Latent Dirichlet Allocation (LDA) or Non-Negative Matrix Factorization (NMF).", "predicted_evidence": ["Why do the thematic information help? The Flip-Flop behavior. As an example, let's considering the fear and joy emotions in Figure FIGREF34. We can notice that all the themes that used to nudge the division issues have a decreasing dashed line, where others such as Supporting Trump theme has an extremely increasing dashed line. Therefore, we manually analyzed the tweets of some IRA accounts and we found this observation clear, as an example from user $x$:", "Based on our thematic information, we model the users textual features w.r.t. each of these themes. In other words, we model a set of textual features independently for each of the former themes to capture the emotional, stance, and others changes in the users tweets.", "To ensure the quality of the themes, we removed the hashtags we used in the collecting process where they may bias the modeling algorithm. We tested multiple number of themes and we chose seven of them. We manually observed the content of these themes to label them. The extracted themes are: Police shootings, Islam and War, Supporting Trump, Black People, Civil Rights, Attacking Hillary, and Crimes. In some themes, like Supporting Trump and Attacking Hillary, we found contradicted opinions, in favor and against the main themes, but we chose the final stance based on the most representative hashtags and words in each of them (see Figure FIGREF11). Also, the themes Police Shooting and Crimes are similar, but we found that some words such as: police, officers, cops, shooting, gun, shot, etc. are the most discriminative between these two themes. In addition, we found that the Crimes theme focuses more on raping crimes against children and women.", "Table TABREF32 presents the classification results showing the performance of each feature set independently. Generally, we can see that the thematic information improves the performance of the proposed features clearly (RQ1), and with the largest amount in the Emotions features (see $-_{themes}$ and $+_{themes}$ columns). This result emphasizes the importance of the thematic information. Also, we see that the emotions performance increases with the largest amount considering F1$_{macro}$ value; this motivates us to analyze the emotions in IRA tweets (see the following section).", "In addition, we removed special characters (except HASH \"#\" sign for the hashtags) and lowercase the final tweet. To ensure the quality of the themes, we removed the hashtags we used in the collecting process where they may bias the modeling algorithm. We tested multiple number of themes and we chose seven of them. We manually observed the content of these themes to label them. The extracted themes are: Police shootings, Islam and War, Supporting Trump, Black People, Civil Rights, Attacking Hillary, and Crimes. In some themes, like Supporting Trump and Attacking Hillary, we found contradicted opinions, in favor and against the main themes, but we chose the final stance based on the most representative hashtags and words in each of them (see Figure FIGREF11). Also, the themes Police Shooting and Crimes are similar, but we found that some words such as: police, officers, cops, shooting, gun, shot, etc. are the most discriminative between these two themes."]}
{"question_id": "595fe416a100bc7247444f25b11baca6e08d9291", "predicted_answer": "The profile features used are:\n\n1. Profile description\n2. Account creation date\n3. Number of followers\n4. Number of followees\n5. Location\n6. Account language", "predicted_evidence": ["For the theme-based features, we use the following features that we believe that they change based on the themes:", "Based on our thematic information, we model the users textual features w.r.t. each of these themes. In other words, we model a set of textual features independently for each of the former themes to capture the emotional, stance, and others changes in the users tweets.", "Table TABREF32 presents the classification results showing the performance of each feature set independently. Generally, we can see that the thematic information improves the performance of the proposed features clearly (RQ1), and with the largest amount in the Emotions features (see $-_{themes}$ and $+_{themes}$ columns). This result emphasizes the importance of the thematic information. Also, we see that the emotions performance increases with the largest amount considering F1$_{macro}$ value; this motivates us to analyze the emotions in IRA tweets (see the following section).", "IRA dataset provided by Twitter contains less information about the accounts details, and they limited to: profile description, account creation date, number of followers and followees, location, and account language. Therefore, as another baseline we use the number of followers and followees to assess their identification ability (we will mention them as Network Features in the rest of the paper).", "Similar to the feature representation of the theme-based features, we represent each user's tweets by considering the average and standard deviation of her tweets' $V_{1,2,..N}$, given $V_i$ as the concatenation of the previous two features vectors of a tweet$_i$. A user $x$ final feature vector is defined as follows:"]}
{"question_id": "1f011fa772ce802e74eda89f706cdb1aa2833686", "predicted_answer": "The textual features used in this context are:\n\n1. Stylistic features:\n\t* Count of special characters\n\t* Consecutive characters and letters\n\t* URLs\n\t* Hashtags\n\t* Users' mentions\n\t* Uppercase ratio\n\t* Tweet length\n2. Part-of-speech (POS) features\n3. Stop-words and rare phrases (RP) features\n4. NLI (Natural Language Inference) feature subsets\n5. Emotions features\n6. Theme-based features (not specified in detail, but mentioned as being used to capture emotional, stance, and other changes in users' tweets)", "predicted_evidence": ["Based on our thematic information, we model the users textual features w.r.t. each of these themes. In other words, we model a set of textual features independently for each of the former themes to capture the emotional, stance, and others changes in the users tweets.", "To understand more the NLI features performance, given their high performance comparing to the other features, we extract the top important tokens for each of the NLI feature subsets (see Figure FIGREF37). Some of the obtained results confirmed what was found previously. For instance, the authors in BIBREF19 found that Russians write English tweets with more prepositions comparing to native speakers of other languages (e.g. as, about, because in (c) Stop-words and RP in (a) POS in Figure FIGREF37). Further research must be conducted to investigate in depth the rest of the results.", "For the theme-based features, we use the following features that we believe that they change based on the themes:", "Table TABREF32 presents the classification results showing the performance of each feature set independently. Generally, we can see that the thematic information improves the performance of the proposed features clearly (RQ1), and with the largest amount in the Emotions features (see $-_{themes}$ and $+_{themes}$ columns). This result emphasizes the importance of the thematic information. Also, we see that the emotions performance increases with the largest amount considering F1$_{macro}$ value; this motivates us to analyze the emotions in IRA tweets (see the following section).", "Stylistic: We extract a set of stylistic features following previous works in the authorship attribution domain BIBREF27, BIBREF28, BIBREF29, such as: the count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions. In addition, we extract the uppercase ratio and the tweet length."]}
{"question_id": "181027f398a6b79b1ba44d8d41cc1aba0d6f5212", "predicted_answer": "According to the context, INLINEFORM0, INLINEFORM1, and INLINEFORM2 are compared with the following representations:\n\n* INLINEFORM0: a trainable weight matrix, a trainable weight vector, and weights specifying the importance of individual sentences.\n* INLINEFORM1: a trainable bias vector, a trainable bias scalar, LSTM's hidden states of Reader, and the document-level embedding.\n* INLINEFORM2: the log likelihood function of Encoder, and a transformation.", "predicted_evidence": ["where INLINEFORM0 is a trainable weight matrix, and INLINEFORM1 is a trainable bias vector.", "where INLINEFORM0 is a trainable weight vector, and INLINEFORM1 is a trainable bias scalar.", "The final output of Encoder is a document embedding INLINEFORM0 , derived from LSTM's hidden states INLINEFORM1 of Reader. Given our goal of developing a general-purpose model for embedding documents, we would like INLINEFORM2 to be semantically rich to encode as much key information as possible. To this end, we impose an additional objective on Encoder: the final document embedding needs to be able to reproduce the key words in the document, as illustrated in Figure FIGREF1 .", "where INLINEFORM0 and INLINEFORM1 denotes the log likelihood functions of Reader and Encoder, respectively.", "The Neural Encoder computes document-level embeddings based on the salient sentences identified by the Reader. In order to capture the topics of a document and the importance of its individual sentences, we perform a weighted pooling over the constituent sentences, with the weights specified by INLINEFORM0 , which gives the document-level embedding INLINEFORM1 through a INLINEFORM2 transformation:"]}
{"question_id": "ab097db03652b8b38edddc074f23e2adf9278cba", "predicted_answer": "The question is asking about the number of layers in the neural network, but the given context doesn't explicitly mention the number of layers. It describes the KeyVec model as consisting of two cascaded neural network components: a Neural Reader and a Neural Encoder. Each of these components has its own architecture, but the number of layers is not specified.\n\nTherefore, the answer is: The number of layers is not specified in the given context.", "predicted_evidence": ["Given a document INLINEFORM0 consisting of INLINEFORM1 sentences INLINEFORM2 , our KeyVec model aims to learn a fixed-length vectorial representation of INLINEFORM3 , denoted as INLINEFORM4 . Figure FIGREF1 illustrates an overview of the KeyVec model consisting of two cascaded neural network components: a Neural Reader and a Neural Encoder, as described below.", "In this section, we describe the learning process of the parameters of KeyVec. Similarly to most neural network models, KeyVec can be trained using Stochastic Gradient Descent (SGD), where the Neural Reader and Neural Encoder are jointly optimized. In particular, the parameters of Reader and Encoder are learned simultaneously by maximizing the joint likelihood of the two components:", "The Neural Encoder computes document-level embeddings based on the salient sentences identified by the Reader. In order to capture the topics of a document and the importance of its individual sentences, we perform a weighted pooling over the constituent sentences, with the weights specified by INLINEFORM0 , which gives the document-level embedding INLINEFORM1 through a INLINEFORM2 transformation:", "The Neural Reader learns to understand the topics of every given input document with paying attention to the salient sentences. It computes a dense representation for each sentence in the given document, and derives its probability of being a salient sentence. The identified set of salient sentences, together with the derived probabilities, will be used by the Neural Encoder to generate a document-level embedding.", "The log likelihood objective of the Neural Reader is then given by maximizing the probability of INLINEFORM0 being the set of key sentences, denoted as INLINEFORM1 :"]}
{"question_id": "5d4190403eb800bb17eec71e979788e11cf74e67", "predicted_answer": "The empirical evaluations performed in this study are:\n\n1. Document clustering task: Comparing embedding methods (Paragraph Vector, word2vec averagings, and KeyVec) using F1, V-measure, and ARI metrics.\n2. Document retrieval task: Comparing embedding methods (KeyVec, Paragraph Vector, and word2vec averagings) using P@10, MAP, and MRR metrics.\n\nThese evaluations aim to assess the performance of different embedding methods in capturing the semantics of documents and representing them as fixed-length vectors.", "predicted_evidence": ["In the document clustering task, we aim to cluster the academic papers by the venues in which they are published. There are a total of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation. Each academic paper is represented as a vector of 100 dimensions.", "To compare embedding methods in academic paper clustering, we calculate F1, V-measure (a conditional entropy-based clustering measure BIBREF11 ), and ARI (Adjusted Rand index BIBREF12 ). As shown in Table TABREF18 , similarly to document retrieval, Paragraph Vector performed better than word2vec averagings in clustering documents, while our KeyVec consistently performed the best among all the compared methods.", "To optimize Reader, we take a surrogate approach to heuristically generate a set of salient sentences from a document collection, which constitute a training dataset for learning the probabilities of salient sentences INLINEFORM0 parametrized by INLINEFORM1 . More specifically, given a training set INLINEFORM2 of documents (e.g., body-text of research papers) and their associated summaries (e.g., abstracts) INLINEFORM3 , where INLINEFORM4 is a gold summary of document INLINEFORM5 , we employ a state-of-the-art sentence similarity model, DSSM BIBREF9 , BIBREF10 , to find the set of top- INLINEFORM6 sentences INLINEFORM8 in INLINEFORM9 , such that the similarity between INLINEFORM10 and any sentence in the gold summary INLINEFORM11 is above a pre-defined threshold. Note that here we assume each training document is associated with a gold summary composed of sentences that might not come from INLINEFORM12 .", "Table TABREF15 presents P@10, MAP and MRR results of our KeyVec model and competing embedding methods in academic paper retrieval. word2vec averaging generates an embedding for a document by averaging the word2vec vectors of its constituent words. In the experiment, we used two different versions of word2vec: one from public release, and the other one trained specifically on our own academic corpus (113 GB). From Table TABREF15 , we observe that as a document-embedding model, Paragraph Vector gave better retrieval results than word2vec averagings did. In contrast, our KeyVec outperforms all the competitors given its unique capability of capturing and embedding the key information of documents.", "In recent years, the use of word representations, such as word2vec BIBREF0 , BIBREF1 and GloVe BIBREF2 , has become a key \u201csecret sauce\u201d for the success of many natural language processing (NLP), information retrieval (IR) and machine learning (ML) tasks. The empirical success of word embeddings raises an interesting research question: Beyond words, can we learn fixed-length distributed representations for pieces of texts? The texts can be of variable-length, ranging from paragraphs to documents. Such document representations play a vital role in a large number of downstream NLP/IR/ML applications, such as text clustering, sentiment analysis, and document retrieval, which treat each piece of text as an instance. Learning a good representation that captures the semantics of each document is thus essential for the success of such applications."]}
{"question_id": "56d41e0fcc288c1e65806ae77097d685c83e22db", "predicted_answer": "The two document understanding tasks that KeyVec was evaluated on are:\n\n1. Document retrieval\n2. Document clustering", "predicted_evidence": ["To verify the effectiveness, we evaluate the KeyVec model on two text understanding tasks that take continuous distributed vectors as the representations for documents: document retrieval and document clustering.", "In the document clustering task, we aim to cluster the academic papers by the venues in which they are published. There are a total of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation. Each academic paper is represented as a vector of 100 dimensions.", "We evaluate our KeyVec on two text understanding tasks: document retrieval and document clustering. As shown in the experimental section SECREF5 , KeyVec yields generic document representations that perform better than state-of-the-art embedding models.", "In another recent work BIBREF5 , Djuric et al. introduced a Hierarchical Document Vector (HDV) model to learn representations from a document stream. Our KeyVec differs from HDV in that we do not assume the existence of a document stream and HDV does not model sentences.", "The Neural Reader learns to understand the topics of every given input document with paying attention to the salient sentences. It computes a dense representation for each sentence in the given document, and derives its probability of being a salient sentence. The identified set of salient sentences, together with the derived probabilities, will be used by the Neural Encoder to generate a document-level embedding."]}
{"question_id": "1237b6fcc64b43901415f3ded17cc210a54ab698", "predicted_answer": "The dataset used is a collection of research papers with their associated summaries (abstracts).", "predicted_evidence": ["where INLINEFORM0 is implemented as a softmax function with output dimensionality being the size of the vocabulary.", "where INLINEFORM0 and INLINEFORM1 denotes the log likelihood functions of Reader and Encoder, respectively.", "where INLINEFORM0 is a trainable weight matrix, and INLINEFORM1 is a trainable bias vector.", "where INLINEFORM0 is a trainable weight vector, and INLINEFORM1 is a trainable bias scalar.", "To optimize Reader, we take a surrogate approach to heuristically generate a set of salient sentences from a document collection, which constitute a training dataset for learning the probabilities of salient sentences INLINEFORM0 parametrized by INLINEFORM1 . More specifically, given a training set INLINEFORM2 of documents (e.g., body-text of research papers) and their associated summaries (e.g., abstracts) INLINEFORM3 , where INLINEFORM4 is a gold summary of document INLINEFORM5 , we employ a state-of-the-art sentence similarity model, DSSM BIBREF9 , BIBREF10 , to find the set of top- INLINEFORM6 sentences INLINEFORM8 in INLINEFORM9 , such that the similarity between INLINEFORM10 and any sentence in the gold summary INLINEFORM11 is above a pre-defined threshold. Note that here we assume each training document is associated with a gold summary composed of sentences that might not come from INLINEFORM12 ."]}
{"question_id": "31cba86bc45970337ba035ecf36d8954a9a5206a", "predicted_answer": "The private companies that are members of the consortium are not explicitly listed in the given context. However, it is mentioned that the consortium consists of universities, institutions, associations, and private companies (nine in total) in Iceland, and that the list of members is provided in Table TABREF6. Unfortunately, the table is not provided in the given context.", "predicted_evidence": ["After studying somewhat similar national programmes in other European countries, we have defined the most important factors that in our opinion will help lead to the success of the programme: First, we have defined core projects that comprise the most important language resources and software tools necessary for various LT applications. Second, all deliverables will be published under as open licenses as possible and all resources and software will be easily accessible. The deliverables will be packaged and published for use in commercial applications, where applicable. Third, from the beginning of the programme, we encourage innovation projects from academia and industry through a competitive R&D fund, and fourth, constant communication with users and industry through conferences, events and direct interaction will be maintained, with the aim of putting deliverables to use in products as soon as possible. The cooperation between academia and industry is also reflected in the consortium of universities, institutions, associations, and private companies, performing the R&D work for all core projects.", "The Icelandic Ministry of Education, Science and Culture signed an agreement with Almannar\u00f3mur in August 2018, giving Almannar\u00f3mur officially the function of organising the execution of the LT programme for Icelandic. Following a European Tender published in March 2019, Almannar\u00f3mur decided to make an agreement with a consortium of universities, institutions, associations, and private companies (nine in total) in Iceland (listed in Table TABREF6) to perform the research and development part of the programme. This Consortium for Icelandic LT (Samstarf um \u00edslenska m\u00e1lt\u00e6kni \u2013 S\u00cdM) is a joint effort of LT experts in Iceland from academia and industry. S\u00cdM is not a legal entity but builds the cooperation on a consortium agreement signed by all members. During the preparation of the project, an expert panel of three experienced researchers from Denmark, the Netherlands, and Estonia was established to oversee the project planning and to evaluate deliverables at predefined milestones during the project.", "The Icelandic Ministry of Education, Science and Culture signed an agreement with Almannar\u00f3mur in August 2018, giving Almannar\u00f3mur officially the function of organising the execution of the LT programme for Icelandic. Following a European Tender published in March 2019, Almannar\u00f3mur decided to make an agreement with a consortium of universities, institutions, associations, and private companies (nine in total) in Iceland (listed in Table TABREF6) to perform the research and development part of the programme. This Consortium for Icelandic LT (Samstarf um \u00edslenska m\u00e1lt\u00e6kni \u2013 S\u00cdM) is a joint effort of LT experts in Iceland from academia and industry. S\u00cdM is not a legal entity but builds the cooperation on a consortium agreement signed by all members.", "S\u00cdM has created teams across the member organisations, each taking charge of a core project and/or defined subtasks. This way the best use of resources is ensured, since the team building is not restricted to one organisation per project. One project manager coordinates the work and handles communication and reporting to Almannar\u00f3mur and the expert panel.", "In October 2016, the Minister of Education, Science and Culture appointed a special LT steering group, consisting of representatives from the Ministry, from academia, and from the Confederation of Icelandic Enterprise (CIE). The steering group commissioned three LT experts to work out a detailed five-year Project Plan for Icelandic LT. The experts delivered their proposals, Language Technology for Icelandic 2018\u20132022 \u2013 Project Plan BIBREF0 to the Minister in June 2017."]}
{"question_id": "3a25f82512d56d9e1ffba72f977f515ae3ba3cca", "predicted_answer": "Yes, the programme plans to gather and open source large datasets for the Icelandic language. This is mentioned in the context of the text-to-speech project, which will produce language resources that enable voice building for Icelandic. Additionally, the programme aims to make language resources available at the repository m\u00e1lf\u00f6ng and the CLARIN-IS website, which will be integrated into the CLARIN Virtual Language Observatory.", "predicted_evidence": ["As mentioned above, a number of language resources have been made available at the repository m\u00e1lf\u00f6ng. Most of these are now also available at the CLARIN-IS website and will be integrated into the CLARIN Virtual Language Observatory. Below we give a brief and non-exhaustive overview of language resources for Icelandic which will be developed in the programme.", "While the government-funded programme for the development of resources and infrastructure software builds the backbone of the Icelandic LT programme, another branch is a competitive fund for research and development. This Strategic Research and Development Programme for Language Technology is managed by the Icelandic Centre for Research, Rann\u00eds, which publishes calls for applications on a regular basis.", "The Icelandic Government decided in 2017 to fund a five-year programme for Icelandic LT, based on a report written by a group of LT experts BIBREF0. After more than two years of preparation, a consortium consisting of universities, institutions, associations, and private companies started the work on the programme on the 1st of October 2019. The goal of the programme is to ensure that Icelandic can be made available in LT applications, and thus will be usable in all areas of communication. Furthermore, that access to information and other language-based communication and interaction in Icelandic will be accessible to all, e.g. via speech synthesis or speech-to-text systems.", ". The text-to-speech project will produce language resources that enable voice building for Icelandic.", "Icelandic is an example of a language with almost a negligible number of speakers, in terms of market size, since only about 350,000 people speak Icelandic as their native language. Icelandic is therefore seldom on the list of supported languages in LT software and applications."]}
{"question_id": "b59f3a58939f7ac007d3263a459c56ebefc4b49a", "predicted_answer": "According to the text, the concrete software planned to be developed by the end of the programme is a working open source correction system for spell and grammar checking, based on the Greynir system for processing Icelandic text.", "predicted_evidence": ["The second phase of the programme ran from 2011 to 2017 with a total budget of approx. 5.5 million euros. It focused on the implementation and integration of existing resources and software prototypes in public services. Project proposals were called for, funding several types of actions in an open competition. The main drawback of this method is that it does not fully cover the objectives, and LT support for Estonian is thus not systematically developed. Researchers were also often mostly interested in results using prototypes rather than stable applications. As most of the projects were instigated at public institutes, relation to IT business was weak. Furthermore, the programme does not deal explicitly with LT education. On the other hand, the state of LT in Estonia soon become relatively good compared to languages with similar number of speakers BIBREF6.", "Our MT project in the new LT programme consists of the following sub-parts:", "The National Programme for Estonian Language Technology was launched in 2006. The first phase ran from 2006 to 2010. All results of this first phase, language resources and software prototypes, were released as public domain. All such resources and tools are preserved long term and available from the Center of Estonian Language Resources. 33 projects were funded, which included the creation of reusable language resources and development of essential linguistic software, as well as bringing the relevant infrastructure up to date BIBREF5. The programme managed to significantly improve upon existing Estonian language resources, both in size, annotation and standardisation. In creating software, most noticeable results were in speech technology. Reporting on the results of the programme BIBREF5 stress that the first phase of the programme created favourable conditions for LT development in Estonia. According to an evaluation of the success of the programme, at least 84% of the projects had satisfactory results. The total budged for this first phase was 3.4 million euros.", "The National Programme for Estonian Language Technology was launched in 2006. The first phase ran from 2006 to 2010. All results of this first phase, language resources and software prototypes, were released as public domain. All such resources and tools are preserved long term and available from the Center of Estonian Language Resources. 33 projects were funded, which included the creation of reusable language resources and development of essential linguistic software, as well as bringing the relevant infrastructure up to date BIBREF5. The programme managed to significantly improve upon existing Estonian language resources, both in size, annotation and standardisation. In creating software, most noticeable results were in speech technology. Reporting on the results of the programme BIBREF5 stress that the first phase of the programme created favourable conditions for LT development in Estonia. According to an evaluation of the success of the programme, at least 84% of the projects had satisfactory results.", "Software development. The software development tasks of the spell and grammar checking project will build a working open source correction system whose development is informed by the analysis of the data sets created within the project. The spell and grammar checker will be based on the foundation for processing Icelandic text provided by the Greynir system."]}
{"question_id": "b4b7333805cb6fdde44907747887a971422dc298", "predicted_answer": "According to the paper, the other national language technology programs described are:\n\n1. The STEVIN programme in the Netherlands (2004-2011)\n2. The Plan for the Advancement of Language Technology in Spain (2016-2020)", "predicted_evidence": ["In recent years, there has been much international discussion on how the future of languages depends on them being usable in the digital world. This concern has led to a number of national LT programmes. We studied three of these national programmes: the STEVIN programme in the Netherlands which ran between 2004 and 2011, the Plan for the Advancement of Language Technology in Spain, and, in particular, the Estonian LT programmes that have been running since 2006.", "In Estonia, three consecutive national programmes have been launched. The third national programme, Estonian Language Technology 2018\u20132027, is currently under way. While the Estonian Ministry of Education and Research has been responsible for the programmes, the universities in Tallinn and Tartu, together with the Institute of the Estonian Language, led the implementation.", "During the last decade, we have witnessed enormous advances in language technology (LT). Applications that allow users to interact with technology via spoken or written natural language are emerging in all areas, and access to language resources and open-source software libraries enables faster development for new domains and languages.", "The Spanish LT programme Plan for Advancement of Language Technology started in 2016, and is scheduled to finish in 2020. Its aims are to develop infrastructure for LT in Spain, specifically for Spanish and the co-official languages, Basque, Catalan, Galician and Aranese. Furthermore, to promote the LT industry by boosting knowledge transfer between research and industry actors, and to improve the quality and capacity of public services by employing NLP and machine translation (MT) technology. Government should be the leading participant in LT with high-profile projects in healthcare, as well as in the judicial and educational systems, and in tourism BIBREF4.", "The National Programme for Estonian Language Technology was launched in 2006. The first phase ran from 2006 to 2010. All results of this first phase, language resources and software prototypes, were released as public domain. All such resources and tools are preserved long term and available from the Center of Estonian Language Resources. 33 projects were funded, which included the creation of reusable language resources and development of essential linguistic software, as well as bringing the relevant infrastructure up to date BIBREF5. The programme managed to significantly improve upon existing Estonian language resources, both in size, annotation and standardisation. In creating software, most noticeable results were in speech technology. Reporting on the results of the programme BIBREF5 stress that the first phase of the programme created favourable conditions for LT development in Estonia. According to an evaluation of the success of the programme, at least 84% of the projects had satisfactory results."]}
{"question_id": "871f7661f5a3da366b0b5feaa36f54fd3dedae8e", "predicted_answer": "The history of Icelandic Language Technology (LT) is usually considered to have begun around the turn of the century, with a few LT resources and products developed in the years leading up to that. However, the Icelandic Government launched a special LT Programme in the year 2000, which marked the beginning of more focused efforts in the field.", "predicted_evidence": ["We have described a five-year, national LT programme for Icelandic. The goal is to make Icelandic useable in communication and interactions in the digital world. Further, to establish graduate and post-graduate education in LT in Iceland to enable the building of strong knowledge centres in LT in the country.", ". The text-to-speech project will produce language resources that enable voice building for Icelandic.", "As mentioned above, a number of language resources have been made available at the repository m\u00e1lf\u00f6ng. Most of these are now also available at the CLARIN-IS website and will be integrated into the CLARIN Virtual Language Observatory. Below we give a brief and non-exhaustive overview of language resources for Icelandic which will be developed in the programme.", "Icelandic is an example of a language with almost a negligible number of speakers, in terms of market size, since only about 350,000 people speak Icelandic as their native language. Icelandic is therefore seldom on the list of supported languages in LT software and applications.", "The history of Icelandic LT is usually considered to have begun around the turn of the century, even though a couple of LT resources and products were developed in the years leading up to that. Following the report of an expert group appointed by the Minister of Education, Science and Culture BIBREF7, the Icelandic Government launched a special LT Programme in the year 2000, with the aim of supporting institutions and companies to create basic resources for Icelandic LT work. This initiative resulted in a few projects which laid the ground for future work in the field. The most important of these were a 25 million token, balanced, tagged corpus, a full-form database of Icelandic inflections, a training model for PoS taggers, an improved speech synthesiser, and an isolated word speech recogniser BIBREF8."]}
{"question_id": "3fafde90eebc1c00ba6c3fb4c5b984009393ce7f", "predicted_answer": "The accuracy result is not explicitly stated in the given context. However, it is mentioned that the precision value was doubled after removing a sentiment filtering step of analysis, but the exact precision value is not provided. Additionally, the recall value is mentioned to be decreasing slowly with increasing importance factor value, but the exact recall value is not given.", "predicted_evidence": ["Figure FIGREF19 shows the agreement between our aspects and that of the dataset. We assumed two aspects as equal when they were textually the same. We made some experiments using text distance metrics, such as the Jaro-Winkler distance, but the results did not differ significantly from an exact matching. We fitted the importance factor value (on the X axis) so as to enrich final aspects set: a higher factor resulted in a larger aspects set and a higher value of precision metric, with slowly decreasing recall. First results (blue line on charts) were not satisfactory, so we removed a sentiment filtering step of analysis (orange line on chart), which doubled the precision value, with nearly the same value of recall. The level of precision for whole dataset (computer, router, and speaker) was most of the time at the same level. However, the recall of router was significantly worse than speaker and computer sets.", "We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually.", "The work was partially supported by the National Science Centre grants DEC-2016/21/N/ST6/02366 and DEC-2016/21/D/ST6/02948, and from the European Union\u2019s Horizon 2020 research and innovation programme under the Marie Sk\u0142odowska-Curie grant agreement No 691152 (RENOIR project).", "In Table TABREF18 there are presented some examples of the results of our approach compared with the annotated data from Bing Liu's dataset. In the first sentence, the results of the analysis differ because we decided to treat only nouns or noun phrases as aspects, while annotators also accepted verbs. In some cases, such as sentences 2 or 4, our approach generated more valuable aspects than the annotators found, but in some cases, like sentence 5, we found fewer. This is possibly the result of our method of filtering valuable aspects - if some aspects were not frequent enough in the dataset, we can treat them as void. In cases where there is neither aspect nor sentiment in the dataset, such as sentence 6, we measure sentiment as well, as one of our analysis steps.", "The biggest disadvantage of document level analysis is an assumption that each document expresses views on a single entity. Thus, it is not applicable to documents which evaluate or compare multiple objects. As for sentence level analysis BIBREF10 - The task at this level relates to sentences and determines whether each sentence expressed a positive, negative, or neutral opinion. This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level."]}
{"question_id": "e6bc11bd6cfd4b2138c29602b9b56fc5378a4293", "predicted_answer": "The opinions fall under the domain of **Natural Language Processing (NLP)**, specifically in the area of **Sentiment Analysis** and **Opinion Mining**.", "predicted_evidence": ["However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level. Instead of looking at language constructs (documents, paragraphs, sentences, clauses or phrases), aspect level directly looks at the opinion itself. It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion). As a result, we can aggregate the opinions. For example, the phone display gathers positive feedback, but the battery is often rated negatively. The aspect-based level of analysis is much more complex since it requires more advanced knowledge representation than at the level of entire documents only.", "However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level. Instead of looking at language constructs (documents, paragraphs, sentences, clauses or phrases), aspect level directly looks at the opinion itself. It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion). As a result, we can aggregate the opinions. For example, the phone display gathers positive feedback, but the battery is often rated negatively. The aspect-based level of analysis is much more complex since it requires more advanced knowledge representation than at the level of entire documents only. Also, the documents often consist of multiple sentences, so saying that the document is positive provides only partial information.", "Regarding document level analysis BIBREF8 , BIBREF9 - the task at this level is to classify whether a full opinion expresses a positive, negative or neutral attitude. For example, given a product review, the model determines whether the text shows an overall positive, negative or neutral opinion about the product. The biggest disadvantage of document level analysis is an assumption that each document expresses views on a single entity. Thus, it is not applicable to documents which evaluate or compare multiple objects. As for sentence level analysis BIBREF10 - The task at this level relates to sentences and determines whether each sentence expressed a positive, negative, or neutral opinion. This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions.", "This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level. Instead of looking at language constructs (documents, paragraphs, sentences, clauses or phrases), aspect level directly looks at the opinion itself. It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion). As a result, we can aggregate the opinions. For example, the phone display gathers positive feedback, but the battery is often rated negatively.", "This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level. Instead of looking at language constructs (documents, paragraphs, sentences, clauses or phrases), aspect level directly looks at the opinion itself. It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion). As a result, we can aggregate the opinions."]}
{"question_id": "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521", "predicted_answer": "The baseline is not explicitly stated in the given context. The text discusses the results of experiments and comparisons between different approaches, but it does not provide a clear baseline against which these results are compared.", "predicted_evidence": ["Figure FIGREF19 shows the agreement between our aspects and that of the dataset. We assumed two aspects as equal when they were textually the same. We made some experiments using text distance metrics, such as the Jaro-Winkler distance, but the results did not differ significantly from an exact matching. We fitted the importance factor value (on the X axis) so as to enrich final aspects set: a higher factor resulted in a larger aspects set and a higher value of precision metric, with slowly decreasing recall. First results (blue line on charts) were not satisfactory, so we removed a sentiment filtering step of analysis (orange line on chart), which doubled the precision value, with nearly the same value of recall. The level of precision for whole dataset (computer, router, and speaker) was most of the time at the same level. However, the recall of router was significantly worse than speaker and computer sets.", "A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level. Instead of looking at language constructs (documents, paragraphs, sentences, clauses or phrases), aspect level directly looks at the opinion itself. It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion). As a result, we can aggregate the opinions. For example, the phone display gathers positive feedback, but the battery is often rated negatively. The aspect-based level of analysis is much more complex since it requires more advanced knowledge representation than at the level of entire documents only. Also, the documents often consist of multiple sentences, so saying that the document is positive provides only partial information. In the literature, there exists some initial work related to aspects. There exist initial solutions that use SVM-based algorithms BIBREF12 or conditional random field classifiers BIBREF13 with manually engineered features.", "The biggest disadvantage of document level analysis is an assumption that each document expresses views on a single entity. Thus, it is not applicable to documents which evaluate or compare multiple objects. As for sentence level analysis BIBREF10 - The task at this level relates to sentences and determines whether each sentence expressed a positive, negative, or neutral opinion. This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level.", "Regarding document level analysis BIBREF8 , BIBREF9 - the task at this level is to classify whether a full opinion expresses a positive, negative or neutral attitude. For example, given a product review, the model determines whether the text shows an overall positive, negative or neutral opinion about the product. The biggest disadvantage of document level analysis is an assumption that each document expresses views on a single entity. Thus, it is not applicable to documents which evaluate or compare multiple objects. As for sentence level analysis BIBREF10 - The task at this level relates to sentences and determines whether each sentence expressed a positive, negative, or neutral opinion. This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like.", "For example, given a product review, the model determines whether the text shows an overall positive, negative or neutral opinion about the product. The biggest disadvantage of document level analysis is an assumption that each document expresses views on a single entity. Thus, it is not applicable to documents which evaluate or compare multiple objects. As for sentence level analysis BIBREF10 - The task at this level relates to sentences and determines whether each sentence expressed a positive, negative, or neutral opinion. This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level."]}
{"question_id": "4748a50c96acb1aa03f7efd1b43376c193b2450a", "predicted_answer": "The dataset used is Bing Liu's dataset BIBREF20, which contains three review datasets of three domains: computers, wireless routers, and speakers.", "predicted_evidence": ["We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually.", "Figure FIGREF19 shows the agreement between our aspects and that of the dataset. We assumed two aspects as equal when they were textually the same. We made some experiments using text distance metrics, such as the Jaro-Winkler distance, but the results did not differ significantly from an exact matching. We fitted the importance factor value (on the X axis) so as to enrich final aspects set: a higher factor resulted in a larger aspects set and a higher value of precision metric, with slowly decreasing recall. First results (blue line on charts) were not satisfactory, so we removed a sentiment filtering step of analysis (orange line on chart), which doubled the precision value, with nearly the same value of recall. The level of precision for whole dataset (computer, router, and speaker) was most of the time at the same level. However, the recall of router was significantly worse than speaker and computer sets.", "In Table TABREF18 there are presented some examples of the results of our approach compared with the annotated data from Bing Liu's dataset. In the first sentence, the results of the analysis differ because we decided to treat only nouns or noun phrases as aspects, while annotators also accepted verbs. In some cases, such as sentences 2 or 4, our approach generated more valuable aspects than the annotators found, but in some cases, like sentence 5, we found fewer. This is possibly the result of our method of filtering valuable aspects - if some aspects were not frequent enough in the dataset, we can treat them as void. In cases where there is neither aspect nor sentiment in the dataset, such as sentence 6, we measure sentiment as well, as one of our analysis steps.", "Modern society is an information society bombarded from all sides by an increasing number of different pieces of information. The 21st century has brought us the rapid development of media, especially in the internet ecosystem. This change has caused the transfer of many areas of our lives to virtual reality. New forms of communication have been established. Their development has created the need for analysis of related data. Nowadays, unstructured information is available in digital form, but how can we analyse and summarise billions of newly created texts that appear daily on the internet? Natural language analysis techniques, statistics and machine learning have emerged as tools to help us. In recent years, particular attention has focused on sentiment analysis. This area is defined as the study of opinions expressed by people as well as attitudes and emotions about a particular topic, product, event, or person. Sentiment analysis determines the polarisation of the text. It answers the question as to whether a particular text is a positive, negative, or neutral one.", "Then, an Aspect-Rhetorical Relation Graph based on a set of these rules is created. Each node represents an aspect and each edge is one of the relations between the EDU\u2019s aspects. A graph will be created for all documents used in the experiment. The graph can be represented with weighted edges (association rules confidence, a number of such relations in the whole graph etc.), but there is a need to check and compare different types of graph representations. Then, it is possible to characterise the whole graph and each node (aspect) with graph metrics (PageRank BIBREF16 , degree, betweenness or other metrics). These metrics will be used for estimating the cut threshold \u2013 removing uninformative or redundant aspects. Hence, we will end up with only the most important aspects derived from analysed corpora. Then the graph will be transformed into an Aspect Hierarchical Tree. This represents the correlation between aspects and enables us to generate natural language-based descriptions."]}
{"question_id": "acac0606aab83cae5d13047863c7af542d58e54c", "predicted_answer": "Based on the provided context, it appears that this is not the first dataset with a grading scaling rather than binary. The text mentions that the authors convert their controversiality estimates to a binary classification by classifying the higher-scored half as controversial and the lower half as non-controversial, implying that the original dataset has a grading scaling rather than binary labels. Additionally, the authors mention that they compare their methodology to those reported in BIBREF1 and BIBREF4, which suggests that previous work has also used grading scaling rather than binary labels.", "predicted_evidence": ["We consider three datasets, two of which are a contribution of this work.", "Table TABREF14 presents results obtained when models trained on Dataset I are applied to Dataset III. For this experiment we also included a BERT network BIBREF14 fine tuned on Dataset I. The Pearson correlation between the scores obtained via manual annotation and the scores generated by our automatic estimators suggests a rather strong linear relationship between the two. Accuracy was computed as for previous datasets, by taking here as positive examples the concepts receiving 6 or more positive votes, and as negative a random sample of 670 concepts out of the 1182 concepts receiving no positive vote.", "Table TABREF14 compares the accuracy reported on Dataset I for the methods suggested in BIBREF1, BIBREF4 with the accuracy obtained by our methods, as well as the latter on Dataset II, using 10-fold cross-validation in all cases. Table TABREF14 reports accuracy results of the more stringent analysis described in section SECREF13.", "Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4. 240 are positive examples, titles from the Wikipedia list of controversial issues, and 240 are negative examples chosen at random and exclusive of the positives. Over this dataset, we compare the methodology suggested here to those reported by BIBREF1, BIBREF4. As the latter report overall accuracy of their binary prediction, we convert our controversiality estimates to a binary classification by classifying the higher-scored half as controversial, and the lower half as non-controversial.", "BIBREF4 review several controversy classifiers. The most accurate one, the Structure classifier, builds, among others, collaboration networks by considering high-level behavior of editors both in their individual forms, and their pairwise interactions. A collaboration profile containing these individual and pairwise features is built for each two interacting editors and is classified based on the agreement or disagreement relation between them. This intensive computation renders that classifier impractical. Table TABREF14 therefore also includes the most accurate classifier BIBREF4 consider practical."]}
{"question_id": "2ee4ecf98ef7d02c9e4d103968098fe35f067bbb", "predicted_answer": "The existing datasets for this task are:\n\n1. Dataset I (not described in detail, but used for training models)\n2. Dataset II (based on the Wikipedia list of controversial issues from May 2017, with 608 positive examples and an equal number of negative examples)\n3. Dataset III (extracted from 3561 Wikipedia concepts under edit protection, crowd-annotated with controversy scores on a scale of 0-10)\n\nAdditionally, the paper mentions Project Debater by IBM, which may have its own dataset of controversial topics, but this is not explicitly stated as a dataset for this task.", "predicted_evidence": ["We consider three datasets, two of which are a contribution of this work.", "Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017). As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia. This leaves 608 controversial Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our estimation into a binary classification, and report accuracy.", "Table TABREF14 presents results obtained when models trained on Dataset I are applied to Dataset III. For this experiment we also included a BERT network BIBREF14 fine tuned on Dataset I. The Pearson correlation between the scores obtained via manual annotation and the scores generated by our automatic estimators suggests a rather strong linear relationship between the two. Accuracy was computed as for previous datasets, by taking here as positive examples the concepts receiving 6 or more positive votes, and as negative a random sample of 670 concepts out of the 1182 concepts receiving no positive vote.", "Recently, IBM introduced Project Debater BIBREF15, an AI system that debates humans on controversial topics. Training and evaluating such a system undoubtedly requires an extensive supply of such topics, which can be enabled by the automatic extraction methods suggested here as well as the new datasets.", "Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I."]}
{"question_id": "82f8843b59668567bba09fc8f93963ca7d1fe107", "predicted_answer": "The sizes of the introduced datasets are:\n\n* Dataset III: 3561 concepts\n* Dataset II: 608 controversial Wikipedia concepts + a like number of negative examples (so, approximately 1216 concepts in total)\n* Dataset I: not explicitly stated, but it is used for training models that are tested on Dataset III, so it is likely to be of similar size or larger than Dataset III.", "predicted_evidence": ["We consider three datasets, two of which are a contribution of this work.", "Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I.", "Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017). As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia. This leaves 608 controversial Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our estimation into a binary classification, and report accuracy.", "Table TABREF14 presents results obtained when models trained on Dataset I are applied to Dataset III. For this experiment we also included a BERT network BIBREF14 fine tuned on Dataset I. The Pearson correlation between the scores obtained via manual annotation and the scores generated by our automatic estimators suggests a rather strong linear relationship between the two. Accuracy was computed as for previous datasets, by taking here as positive examples the concepts receiving 6 or more positive votes, and as negative a random sample of 670 concepts out of the 1182 concepts receiving no positive vote.", "We first examined the estimators in $k$-fold cross-validation scheme on the datasets I and II with $k=10$: the set of positive (controversial) concepts was split into 10 equal size sets, and the corresponding sentences were split accordingly. Each set was matched with similarly sized sets of negative (non-controversial) concepts and corresponding sentences. For each fold, a model was generated from the training sentences and used to score the test concepts. Scores were converted into a binary classification, as described in SECREF3, and accuracy was computed accordingly. Finally, the accuracy over the $k$ folds was averaged."]}
{"question_id": "376e8ed6e039e07c892c77b7525778178d56acb7", "predicted_answer": "Yes, they crowd-annotated the dataset, with 10 or more annotators per concept.", "predicted_evidence": ["Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I.", "Recently, IBM introduced Project Debater BIBREF15, an AI system that debates humans on controversial topics. Training and evaluating such a system undoubtedly requires an extensive supply of such topics, which can be enabled by the automatic extraction methods suggested here as well as the new datasets.", "Content analysis of controversial Wikipedia articles has been used to evaluate the level of controversy of other documents (e.g., web pages) by mapping them to related Wikipedia articles BIBREF5. BIBREF6 further build a language model, which enhances predictions made by existing classifiers, by inferring word probabilities from Wikipedia articles prominent in Wikipedia controversy features (mainly signals in edit history as discussed above) and from articles retrieved by manually selected query terms, believed to indicate controversy.", "Indicating that a web page is controversial, or disputed - for example, in a search result - facilitates an educated consumption of the information therein, suggesting the content may not represent the \u201cfull picture\u201d. Here, we consider the problem of estimating the level of controversiality associated with a given Wikipedia concept (title). We demonstrate that the textual contexts in which the concept is referenced can be leveraged to facilitate this.", "Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017). As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia. This leaves 608 controversial Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our estimation into a binary classification, and report accuracy."]}
{"question_id": "4de6bcddd46726bf58326304b0490fdb9e7e86ec", "predicted_answer": "According to the text, labeling was done through crowd-annotation, where 10 or more annotators per concept were asked to mark if a topic is likely to be argued about, given its Wikipedia page and description. The annotation instructions were: \"Given a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\" The annotations were then normalized to controversiality scores on an integer scale of 0-10.", "predicted_evidence": ["Nearest neighbors (NN) Estimator: We used the pre-trained GloVe embeddings BIBREF11 of concepts to implement a nearest-neighbor estimator as follows. Given a concept $c$, we extract all labeled concepts within a given radius $r$ (cosine similarity $0.3$). In one variant, $c$'s controversiality score is taken to be the fraction of controversial concepts among them. In another variant, labeled concepts are weighted by their cosine similarity to $c$.", "Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I.", "Recently, IBM introduced Project Debater BIBREF15, an AI system that debates humans on controversial topics. Training and evaluating such a system undoubtedly requires an extensive supply of such topics, which can be enabled by the automatic extraction methods suggested here as well as the new datasets.", "The major advantages of our estimation scheme are its simplicity and reliance on abundantly accessible features. At the same time, its accuracy is similar to state-of-the-art classifiers, which depend on complex meta-data, and rely on sophisticated - in some cases impractical - algorithmic techniques. Because the features herein are so simple, our estimators are convertible to any corpus, in any language, even of moderate size.", "Focusing here on Wikipedia concepts, we adopt as an initial \u201cground truth\u201d the titles listed on the Wikipedia list of controversial issues, which is curated based on so-called \u201cedit wars\u201d. We then manually annotate a set of Wikipedia titles which are locked for editing, and evaluate our system on this much larger and more challenging dataset."]}
{"question_id": "e831ce6c406bf5d1c493162732e1b320abb71b6f", "predicted_answer": "The datasets come from the following sources:\n\n* Dataset I: Not specified in the given context, but it is one of the two datasets contributed by this work.\n* Dataset II: Based on a May 2017 version of the Wikipedia list of controversial issues.\n* Dataset III: Extracted from Wikipedia pages under edit protection, and crowd-annotated by 10 or more annotators per concept. This dataset was shared by Shiri Dori-Hacohen and Hoda Sepehri Rad.", "predicted_evidence": ["We consider three datasets, two of which are a contribution of this work.", "Table TABREF14 presents results obtained when models trained on Dataset I are applied to Dataset III. For this experiment we also included a BERT network BIBREF14 fine tuned on Dataset I. The Pearson correlation between the scores obtained via manual annotation and the scores generated by our automatic estimators suggests a rather strong linear relationship between the two. Accuracy was computed as for previous datasets, by taking here as positive examples the concepts receiving 6 or more positive votes, and as negative a random sample of 670 concepts out of the 1182 concepts receiving no positive vote.", "Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017). As positive examples we take, from this list, all concepts which appear more than 50 times in Wikipedia. This leaves 608 controversial Wikipedia concepts. For negative examples, we follow BIBREF1, BIBREF4 and select a like number of concepts at random. Here too, since each concept only has a binary label, we convert our estimation into a binary classification, and report accuracy.", "Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I.", "We are grateful to Shiri Dori-Hacohen and Hoda Sepehri Rad for sharing their data with us and giving us permission to use it."]}
{"question_id": "634a071b13eb7139e77872ecfdc135a2eb2f89da", "predicted_answer": "The baselines are not explicitly mentioned in the given context. However, based on the description of the experiment, it can be inferred that the baseline accuracy might be the accuracy of a random classifier or a majority class classifier, which would be around 50% since the positive and negative sets are of equal size.", "predicted_evidence": ["We first examined the estimators in $k$-fold cross-validation scheme on the datasets I and II with $k=10$: the set of positive (controversial) concepts was split into 10 equal size sets, and the corresponding sentences were split accordingly. Each set was matched with similarly sized sets of negative (non-controversial) concepts and corresponding sentences. For each fold, a model was generated from the training sentences and used to score the test concepts. Scores were converted into a binary classification, as described in SECREF3, and accuracy was computed accordingly. Finally, the accuracy over the $k$ folds was averaged.", "We consider three datasets, two of which are a contribution of this work.", "In a preliminary task, we looked for words which may designate sentences associated with controversial concepts. To this end, we ranked the words appearing in positive sentences according to their information gain for this task. The top of the list comprises the following: that, sexual, people, movement, religious, issues, rights.", "In all datasets, to obtain the sentence-level context of the concepts (positive and negative), we randomly select two equal-sized sets of Wikipedia sentences, that explicitly reference these concepts \u2013 i.e., that contain a hyperlink to the article titled by the concept. Importantly, in each sentence we mask the words that reference the concept \u2013 i.e., the surface form of the hyperlink leading to the concept \u2013 by a fixed, singular token, thus focusing solely on the context within which the concepts are mentioned.", "Focusing here on Wikipedia concepts, we adopt as an initial \u201cground truth\u201d the titles listed on the Wikipedia list of controversial issues, which is curated based on so-called \u201cedit wars\u201d. We then manually annotate a set of Wikipedia titles which are locked for editing, and evaluate our system on this much larger and more challenging dataset."]}
{"question_id": "8861138891669a45de3955c802c55a37be717977", "predicted_answer": "The tools used are not explicitly mentioned in the given context. However, it can be inferred that the following tools or techniques were used:\n\n* 10-fold cross-validation\n* Collaboration networks\n* Classification algorithms (e.g. the Structure classifier)\n* AI system (Project Debater)\n\nNote that the context does not provide detailed information about the specific tools or software used, but rather mentions the techniques and methods employed in the research.", "predicted_evidence": ["We consider three datasets, two of which are a contribution of this work.", "We are grateful to Shiri Dori-Hacohen and Hoda Sepehri Rad for sharing their data with us and giving us permission to use it.", "Table TABREF14 compares the accuracy reported on Dataset I for the methods suggested in BIBREF1, BIBREF4 with the accuracy obtained by our methods, as well as the latter on Dataset II, using 10-fold cross-validation in all cases. Table TABREF14 reports accuracy results of the more stringent analysis described in section SECREF13.", "BIBREF4 review several controversy classifiers. The most accurate one, the Structure classifier, builds, among others, collaboration networks by considering high-level behavior of editors both in their individual forms, and their pairwise interactions. A collaboration profile containing these individual and pairwise features is built for each two interacting editors and is classified based on the agreement or disagreement relation between them. This intensive computation renders that classifier impractical. Table TABREF14 therefore also includes the most accurate classifier BIBREF4 consider practical.", "Recently, IBM introduced Project Debater BIBREF15, an AI system that debates humans on controversial topics. Training and evaluating such a system undoubtedly requires an extensive supply of such topics, which can be enabled by the automatic extraction methods suggested here as well as the new datasets."]}
{"question_id": "267d70d9f3339c56831ea150d2213643fbc5129b", "predicted_answer": "The performance of NJM (Neural Joke Machine) is that it generates captions that are considered funnier than those generated by the baseline STAIR caption, but less funny than those generated by humans. According to user evaluations on the Bokete Ogiri website, NJM captions averaged 3.23 stars, while STAIR captions averaged 1.71 stars. In a questionnaire, NJM captions were ranked \"funniest\" 22.59% of the time, while human-generated captions were ranked \"funniest\" 67.99% of the time.", "predicted_evidence": ["Finally, we present the visual results in Figure FIGREF14 , which includes examples of funny captions obtained using NJM. Although the original caption is in Japanese, we also translated the captions into English. Enjoy!", "We are currently posting funny captions generated by the NJM to the Bokete Ogiri website in order to evaluate the proposed method. Here, we compare the proposed method with STAIR captions. As reported by Bokete users, the funny captions generated by STAIR caption averaged 1.71 stars, whereas the NJM averaged 3.23 stars. Thus, the NJM is funnier than the baseline STAIR caption according to Bokete users. We believe that this difference is the result of using (i) Funny Score to effectively train the generator regarding funny captions and (ii) the self-collected BoketeDB, which is a large-scale database for funny captions.", "In this subsection, we present the experimental results along with a discussion. Table TABREF10 shows the experimental results of the questionnaire. A total of 16 personal questionnaires were completed. Table TABREF10 shows the percentages of captions of each rank for each method of caption generation considered herein. Captions generated by humans were ranked \u201cfunniest\u201d 67.99% of the time, followed by the NJM at 22.59%. The baseline captions, STAIR caption, were ranked \u201cfunniest\u201d 9.41% of the time. These results suggest that captions generated by the NJM are less funny than those generated by humans. However, the NJM is ranked much higher than STAIR caption.", "The Bokete Ogiri website uses the number of stars to evaluate the degree of funniness of a caption. The user evaluates the \u201cfunniness\u201d of a posted caption and assigns one to three stars to the caption. Therefore, funnier captions tend to be assigned a lot of stars. We focus on the number of stars in order to propose an effective training method, in which the Funny Score enables us to evaluate the funniness of a caption. Based on the results of our pre-experiment, a Funny Score of 100 stars is treated as a threshold. In other words, the Funny Score outputs a loss value INLINEFORM0 when #star is less than 100. In contrast, the Funny Score returns INLINEFORM1 when #star is over 100. The loss value INLINEFORM2 is calculated with the LSTM as an average of each mini-batch.", "Here, we describe the experimental method used to validate the effectiveness of the NJM. We compare the proposed method with two other methods of generating funny captions: 1) human generated captions, which are highly ranked on Bokete (indicated by \u201cHuman\" in Table TABREF10 ), and 2) Japanese image caption generation using CNN+LSTM pre-trained by STAIR caption BIBREF7 . Based on the captions provided by MS COCO, the STAIR caption is translated from English to Japanese (indicated by \u201cSTAIR caption\u201d in Table TABREF10 ). We use a questionnaire as the evaluation method. We selected a total of 30 themes from the Bokete Ogiri website that included \u201cpeople\u201d, \u201ctwo or more people\u201d, \u201canimals\u201d, \u201clandscape\u201d, \u201cinorganics\u201d, and \u201cillustrations\u201d. The questionnaire asks respondents to rank the captions provided by humans, the NJM, and STAIR caption in order of \u201cfunniness\u201d."]}
{"question_id": "477da8d997ff87400c6aad19dcc74f8998bc89c3", "predicted_answer": "The results are evaluated based on the number of stars assigned to a caption by users on the Bokete Ogiri website. The Funny Score is used to evaluate the funniness of a caption, with a threshold of 100 stars. If a caption receives less than 100 stars, it outputs a loss value of INLINEFORM0, and if it receives more than 100 stars, it returns INLINEFORM1. The loss value INLINEFORM2 is calculated as an average of each mini-batch using the LSTM. In the experiment, the proposed method (NJM) is compared to the baseline STAIR caption, and the results show that the NJM receives an average of 3.23 stars, while the STAIR caption receives an average of 1.71 stars, indicating that the NJM is considered funnier by Bokete users.", "predicted_evidence": ["The Bokete Ogiri website uses the number of stars to evaluate the degree of funniness of a caption. The user evaluates the \u201cfunniness\u201d of a posted caption and assigns one to three stars to the caption. Therefore, funnier captions tend to be assigned a lot of stars. We focus on the number of stars in order to propose an effective training method, in which the Funny Score enables us to evaluate the funniness of a caption. Based on the results of our pre-experiment, a Funny Score of 100 stars is treated as a threshold. In other words, the Funny Score outputs a loss value INLINEFORM0 when #star is less than 100. In contrast, the Funny Score returns INLINEFORM1 when #star is over 100. The loss value INLINEFORM2 is calculated with the LSTM as an average of each mini-batch.", "We conducted evaluations to confirm the effectiveness of the proposed method. We describe the experimental method in Section SECREF11 , and the experimental results are presented in Section SECREF12 .", "The flow of the proposed method is shown in Figure FIGREF2 . Basically, we adopted the CNN+LSTM model used in Show and Tell, but the CNN is replaced by ResNet-152 as an image feature extraction method. In the next subsection, we describe in detail how to calculate a loss function with a Funny Score. The function appropriately evaluates the number of stars and its \u201cfunniness\u201d.", "Laughter is a special, higher-order function that only humans possess. In the analysis of laughter, as Wikipedia says, \u201cLaughter is thought to be a shift of composition (schema)\", and laughter frequently occurs when there is a change from a composition of receiver. However, the viewpoint of laughter differs greatly depending on the position of the receiver. Therefore, the quantitative measurement of laughter is very difficult. Image Ogiri on web services such as \"Bokete\" BIBREF0 have recently appeared, where users post funny captions for thematic images and the captions are evaluated in an SNS-like environment. Users compete to obtain the greatest number of \u201cstars\u201d. Although quantification of laughter is considered to be a very difficult task, the correspondence between evaluations and images on Bokete allows us to treat laughter quantitatively. Image captioning is an active topic in computer vision, and we believe that humorous image captioning can be realized. The main contributions of the present paper are as follows:", "We are currently posting funny captions generated by the NJM to the Bokete Ogiri website in order to evaluate the proposed method. Here, we compare the proposed method with STAIR captions. As reported by Bokete users, the funny captions generated by STAIR caption averaged 1.71 stars, whereas the NJM averaged 3.23 stars. Thus, the NJM is funnier than the baseline STAIR caption according to Bokete users. We believe that this difference is the result of using (i) Funny Score to effectively train the generator regarding funny captions and (ii) the self-collected BoketeDB, which is a large-scale database for funny captions."]}
{"question_id": "4485e32052741972877375667901f61e602ec4de", "predicted_answer": "The self-collected corpus, BoketeDB, contains approximately 70,981 images and 999,571 funny captions.", "predicted_evidence": ["We are currently posting funny captions generated by the NJM to the Bokete Ogiri website in order to evaluate the proposed method. Here, we compare the proposed method with STAIR captions. As reported by Bokete users, the funny captions generated by STAIR caption averaged 1.71 stars, whereas the NJM averaged 3.23 stars. Thus, the NJM is funnier than the baseline STAIR caption according to Bokete users. We believe that this difference is the result of using (i) Funny Score to effectively train the generator regarding funny captions and (ii) the self-collected BoketeDB, which is a large-scale database for funny captions.", "Comparison with MS COCO BIBREF5 . MS COCO contains a correspondence for each of 160,000 images to one of five types of captions. In comparison with MS COCO, BoketeDB has approximately half the number of the images and 124% the number of captions.", "We have downloaded pairs of images and funny captions in order to construct a Bokete Database (BoketeDB). As of March 2018, 60M funny captions and 3.4M images have been posted on the Bokete Ogiri website. In the present study, we consider 999,571 funny captions for 70,981 images. A number of pair between image and funny caption is posted in temporal order on the Ogiri website Bokete. We collected images and funny captions to make corresponding image and caption pairs. Thus, we obtained a database for generating funny captions like an image caption one.", "The Bokete Ogiri website uses the number of stars to evaluate the degree of funniness of a caption. The user evaluates the \u201cfunniness\u201d of a posted caption and assigns one to three stars to the caption. Therefore, funnier captions tend to be assigned a lot of stars. We focus on the number of stars in order to propose an effective training method, in which the Funny Score enables us to evaluate the funniness of a caption. Based on the results of our pre-experiment, a Funny Score of 100 stars is treated as a threshold. In other words, the Funny Score outputs a loss value INLINEFORM0 when #star is less than 100. In contrast, the Funny Score returns INLINEFORM1 when #star is over 100. The loss value INLINEFORM2 is calculated with the LSTM as an average of each mini-batch.", "Laughter is a special, higher-order function that only humans possess. In the analysis of laughter, as Wikipedia says, \u201cLaughter is thought to be a shift of composition (schema)\", and laughter frequently occurs when there is a change from a composition of receiver. However, the viewpoint of laughter differs greatly depending on the position of the receiver. Therefore, the quantitative measurement of laughter is very difficult. Image Ogiri on web services such as \"Bokete\" BIBREF0 have recently appeared, where users post funny captions for thematic images and the captions are evaluated in an SNS-like environment. Users compete to obtain the greatest number of \u201cstars\u201d. Although quantification of laughter is considered to be a very difficult task, the correspondence between evaluations and images on Bokete allows us to treat laughter quantitatively. Image captioning is an active topic in computer vision, and we believe that humorous image captioning can be realized. The main contributions of the present paper are as follows:"]}
{"question_id": "df4895c6ae426006e75c511a304d56d37c42a1c7", "predicted_answer": "The Funny Score is calculated based on the number of stars assigned to a caption by users on the Bokete Ogiri website. Specifically, if the number of stars (#star) is less than 100, the Funny Score outputs a loss value of INLINEFORM0. If the number of stars is over 100, the Funny Score returns INLINEFORM1.", "predicted_evidence": ["The Bokete Ogiri website uses the number of stars to evaluate the degree of funniness of a caption. The user evaluates the \u201cfunniness\u201d of a posted caption and assigns one to three stars to the caption. Therefore, funnier captions tend to be assigned a lot of stars. We focus on the number of stars in order to propose an effective training method, in which the Funny Score enables us to evaluate the funniness of a caption. Based on the results of our pre-experiment, a Funny Score of 100 stars is treated as a threshold. In other words, the Funny Score outputs a loss value INLINEFORM0 when #star is less than 100. In contrast, the Funny Score returns INLINEFORM1 when #star is over 100. The loss value INLINEFORM2 is calculated with the LSTM as an average of each mini-batch.", "Laughter is a special, higher-order function that only humans possess. In the analysis of laughter, as Wikipedia says, \u201cLaughter is thought to be a shift of composition (schema)\", and laughter frequently occurs when there is a change from a composition of receiver. However, the viewpoint of laughter differs greatly depending on the position of the receiver. Therefore, the quantitative measurement of laughter is very difficult. Image Ogiri on web services such as \"Bokete\" BIBREF0 have recently appeared, where users post funny captions for thematic images and the captions are evaluated in an SNS-like environment. Users compete to obtain the greatest number of \u201cstars\u201d. Although quantification of laughter is considered to be a very difficult task, the correspondence between evaluations and images on Bokete allows us to treat laughter quantitatively. Image captioning is an active topic in computer vision, and we believe that humorous image captioning can be realized. The main contributions of the present paper are as follows:", "We are currently posting funny captions generated by the NJM to the Bokete Ogiri website in order to evaluate the proposed method. Here, we compare the proposed method with STAIR captions. As reported by Bokete users, the funny captions generated by STAIR caption averaged 1.71 stars, whereas the NJM averaged 3.23 stars. Thus, the NJM is funnier than the baseline STAIR caption according to Bokete users. We believe that this difference is the result of using (i) Funny Score to effectively train the generator regarding funny captions and (ii) the self-collected BoketeDB, which is a large-scale database for funny captions.", "In the present paper, we proposed a method by which to generate captions that draw laughter. We built the BoketeDB, which contains pairs comprising a theme (image) and a corresponding funny caption (text) posted on the Bokete Ogiri website. We effectively trained a funny caption generator with the proposed Funny Score by weight evaluation. Although we adopted CNN+LSTM as a baseline, we have been exploring an effective scoring function and database construction. The experiments of the present study suggested that the NJM was much funnier than the baseline STAIR caption.", "We effectively train a funny caption generator by using the proposed Funny Score by weight evaluation. We adopt CNN+LSTM as a baseline, but we have been exploring an effective scoring function and database construction. We refer to the proposed method as the Neural Joking Machine (NJM), which is combined with the BoketeDB pre-trained model, as described in Section SECREF4 ."]}
{"question_id": "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e", "predicted_answer": "The dataset used is the U.S. Census Bureau's Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013.", "predicted_evidence": ["Each of these 250 audio utterances were passed through 4 different ASR engines, namely, Google ASR (Ga), Kaldi with US acoustic models (Ku), Kaldi with Indian Acoustic models (Ki) and PocketSphinx ASR (Ps). In particular, that audio utterances were in wave format (.wav) with a sampling rate of 8 kHz and 16 bit. In case of Google ASR (Ga), each utterance was first converted into .flac format using the utility sound exchange (sox) commonly available on Unix machines. The .flac audio files were sent to the cloud based Google ASR (Ga) one by one in a batch mode and the text string returned by Ga was stored. In all 7 utterances did not get any text output, presumably Ga was unable to recognize the utterance. For all the other 243 utterances a text output was received.", "Figure FIGREF11 and Table TABREF14 capture the performance of the different speech recognition engines. The performance of the ASR engines varied, with Ki performing the best with 127 of the 250 utterances being correctly recognized while Ps returned only 44 correctly recognized utterances (see Table TABREF14 , Column 4 named \"Correct\") of 250 utterances. The accuracy of the ASR varied widely. For instance, in case of Ps there were as many as 97 instances of the 206 erroneously recognized utterances which had an accuracy of less than 70%.", "We present the results of our experiments with both the Evo-Devo and the Machine Learning mechanisms described earlier using the U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12 .", "We downloaded this survey data and hand crafted a total of 293 textual questions BIBREF13 which could answer the survey data. A set of 6 people (L2 English) generated 50 queries each with the only constraint that these queries should be able to answer the survey data. In all a set of 300 queries were crafted of which duplicate queries were removed to leave 293 queries in all. Of these, we chose 250 queries randomly and distributed among 5 Indian speakers, who were asked to read aloud the queries into a custom-built audio data collecting application. So, in all we had access to 250 audio queries spoken by 5 different Indian speakers; each speaking 50 queries.", "In the machine learning technique of adaptation, we considers INLINEFORM0 pairs as the predominant entity and tests the accuracy of classification of errors."]}
{"question_id": "54b0d2df6ee27aaacdaf7f9c76c897b27e534667", "predicted_answer": "No, based on the provided context, it does not appear that the authors compare their proposed domain adaptation methods to existing methods. The text discusses the limitations of existing confidence-based methods and presents two new mechanisms for adaptation/repair of ASR output, but it does not provide a comparison with other existing methods.", "predicted_evidence": ["In the machine learning technique of adaptation, we considers INLINEFORM0 pairs as the predominant entity and tests the accuracy of classification of errors.", "Most work on ASR error detection and correction has focused on using confidence measures, generally called the log-likelihood score, provided by the speech recognition engine; the text with lower confidence is assumed to be incorrect and subjected to correction. Such confidence based methods are useful only when we have access to the internals of a speech recognition engine built for a specific domain. As mentioned earlier, use of domain-specific engine requires one to rebuild the interface every time the domain is updated, or a new domain is introduced. As mentioned earlier, our focus is to avoid rebuilding the interface each time the domain changes by using an existing ASR. As such our method is specifically a post-ASR system. A post-ASR system provides greater flexibility in terms of absorbing domain variations and adapting the output of ASR in ways that are not possible during training a domain-specific ASR system BIBREF2 .", "General-purpose ASR engines when used for enterprise domains may output erroneous text, especially when encountering domain-specific terms. One may have to adapt/repair the ASR output in order to do further natural language processing such as question-answering. We have presented two mechanisms for adaptation/repair of ASR-output with respect to a domain. The Evo-Devo mechanism provides a bio-inspired abstraction to help structure the adaptation and repair process. This is one of the main contribution of this paper. The machine learning mechanism provides a means of adaptation and repair by examining the feature-space of the ASR output. The results of the experiments show that both these mechanisms are promising and may need further development.", "Table TABREF16 clearly demonstrates the promise of the evo-devo mechanism for adaptation/repair. In our experiments we observed that the adaptation/repair of sub-parts in ASR-output ( INLINEFORM0 ) that most probably referred to domain terms occurred well and were easily repaired, thus contributing to increase in accuracy. For non-domain-specific linguistic terms the method requires one to build very good linguistic repair rules, without which the method could lead to a decrease in accuracy. One may need to fine-tune the repair, match and fitness functions for linguistic terms. However, we find the abstraction of evo-devo mechanism is very apt to use.", "In the machine learning based mechanism of adaptation, we assume the availability of example pairs of INLINEFORM0 namely (ASR output, the actual transcription of the spoken sentence) for training. We further assume that such a machine-learnt model can help repair an unseen ASR output to its intended correct sentence. We address the following hypothesis"]}
{"question_id": "b9a3836cff16af7454c7a8b0e5ff90206d0db1f5", "predicted_answer": "The answer is not explicitly stated in the text, as the text presents two mechanisms for adaptation/repair of ASR-output (Evo-Devo and machine learning) and mentions that both are promising and may need further development, but does not provide a direct comparison or conclusion on which one is the best overall.", "predicted_evidence": ["In the machine learning technique of adaptation, we considers INLINEFORM0 pairs as the predominant entity and tests the accuracy of classification of errors.", "Most work on ASR error detection and correction has focused on using confidence measures, generally called the log-likelihood score, provided by the speech recognition engine; the text with lower confidence is assumed to be incorrect and subjected to correction. Such confidence based methods are useful only when we have access to the internals of a speech recognition engine built for a specific domain. As mentioned earlier, use of domain-specific engine requires one to rebuild the interface every time the domain is updated, or a new domain is introduced. As mentioned earlier, our focus is to avoid rebuilding the interface each time the domain changes by using an existing ASR. As such our method is specifically a post-ASR system. A post-ASR system provides greater flexibility in terms of absorbing domain variations and adapting the output of ASR in ways that are not possible during training a domain-specific ASR system BIBREF2 .", "General-purpose ASR engines when used for enterprise domains may output erroneous text, especially when encountering domain-specific terms. One may have to adapt/repair the ASR output in order to do further natural language processing such as question-answering. We have presented two mechanisms for adaptation/repair of ASR-output with respect to a domain. The Evo-Devo mechanism provides a bio-inspired abstraction to help structure the adaptation and repair process. This is one of the main contribution of this paper. The machine learning mechanism provides a means of adaptation and repair by examining the feature-space of the ASR output. The results of the experiments show that both these mechanisms are promising and may need further development.", "An alternative to domain-specific speech recognition engines has been to re-purpose general-purpose speech recognition engines, such as Google Speech API, IBM Watson Speech to text API which can be used across domains with natural language question answering systems. Such general-purpose automatic speech engines (gp-ASR) are deep trained on very large general corpus using deep neural network (DNN) techniques. The deep learnt acoustic and language models enhance the performance of a ASR. However, this comes with its own limitations. For freely spoken natural language sentences, the typical recognition accuracy achievable even for state-of-the-art speech recognition systems have been observed to be about 60% to 90% in real-world environments BIBREF0 . The recognition is worse if we consider factors such as domain-specific words, environmental noise, variations in accent, poor ability to express on the part of the user, or inadequate speech and language resources from the domain to train such speech recognition systems. The subsequent natural language processing, such as that in a question answering system, of such erroneously and partially recognized text becomes rather problematic, as the domain terms may be inaccurately recognized or linguistic errors may creep into the sentence.", "The process of converting human spoken speech into text is performed by an Automatic Speech Recognition (ASR) engine. While functional examples of ASR with enterprise systems can be seen in day-to-day use, most of these work under constraints of a limited domain, and/or use of additional domain-specific cues to enhance the speech-to-text conversion process. Prior speech-and-natural language interfaces for such purposes have been rather restricted to either Interactive Voice Recognition (IVR) technology, or have focused on building a very specialized speech engine with domain specific terminology that recognizes key-words in that domain through an extensively customized language model, and trigger specific tasks in the enterprise application system. This makes the interface extremely specialized, rather cumbersome and non-adaptable for other domains. Further, every time a new enterprise application requires a speech and natural language interface, one has to redevelop the entire interface again."]}
{"question_id": "99554d0c76fbaef90bce972700fa4c315f961c31", "predicted_answer": "No, the text does not mention the use of evolutionary-based optimization algorithms as a domain adaptation approach. However, it does mention an \"evolutionary development based artificial development mechanism\" (Evo-Devo) as one of the two mechanisms for adaptation or repair of the ASR output. This mechanism is inspired by biological evolution and development, but it is not an optimization algorithm.", "predicted_evidence": ["We present two mechanisms for adaptation or repair of the ASR output, namely INLINEFORM0 , in this paper: (a) an evolutionary development based artificial development mechanism, and (b) a machine-learning mechanism.", "The ASR output ( INLINEFORM0 ) are then given as input in the Evo-Devo and Machine Learning mechanism of adaptation.", "General-purpose ASR engines when used for enterprise domains may output erroneous text, especially when encountering domain-specific terms. One may have to adapt/repair the ASR output in order to do further natural language processing such as question-answering. We have presented two mechanisms for adaptation/repair of ASR-output with respect to a domain. The Evo-Devo mechanism provides a bio-inspired abstraction to help structure the adaptation and repair process. This is one of the main contribution of this paper. The machine learning mechanism provides a means of adaptation and repair by examining the feature-space of the ASR output. The results of the experiments show that both these mechanisms are promising and may need further development.", "In the machine learning technique of adaptation, we considers INLINEFORM0 pairs as the predominant entity and tests the accuracy of classification of errors.", "Table TABREF16 clearly demonstrates the promise of the evo-devo mechanism for adaptation/repair. In our experiments we observed that the adaptation/repair of sub-parts in ASR-output ( INLINEFORM0 ) that most probably referred to domain terms occurred well and were easily repaired, thus contributing to increase in accuracy. For non-domain-specific linguistic terms the method requires one to build very good linguistic repair rules, without which the method could lead to a decrease in accuracy. One may need to fine-tune the repair, match and fitness functions for linguistic terms. However, we find the abstraction of evo-devo mechanism is very apt to use."]}
{"question_id": "5370a0062aae7fa4e700ae47aa143be5c5fc6b22", "predicted_answer": "According to the given context, they experiment with 15 languages in the multilingual setup.", "predicted_evidence": ["Our experiments therefore aim to evaluate on a wider range of target languages, and to explore the effects of both the amount of labeled data, and the number of languages from which it is obtained.", "In the second study BIBREF34 , the authors built multilingual systems using either seven or ten high-resource languages, and evaluated on the three \u201cdevelopment\u201d and two \u201csurprise\u201d languages of the zrsc 2017. However, they included transcribed training data from four out of the five evaluation languages, so only one language's results (Wolof) were truly zero-resource.", "To determine whether these gains come from the diversity of training languages or just the larger amount of training data, we trained models on the 15 hour subset and the full 81 hours of the English wsj corpus, which corresponds to the amount of data of four GlobalPhone languages. More data does help to some degree, as Figure FIGREF21 shows. But, except for Mandarin, training on just two languages (46 hours) already works better.", "In the previous experiments, we used data from GlobalPhone, which provides corpora collected and formatted similarly for a wide range of languages. However, GlobalPhone is not freely available and no previous zero-resource studies have used these corpora, so in this section we also provide results on the zrsc 2015 BIBREF0 data sets, which have been widely used in other work. The target languages are English (from the Buckeye corpus BIBREF38 ) and Xitsonga (NCHLT corpus BIBREF39 ). Table TABREF8 includes the corpus statistics. These corpora are not split into train/dev/test; since training is unsupervised, the system is simply trained directly on the unlabeled test set (which could also be done in deployment). Importantly, no hyperparameter tuning is done on the Buckeye or Xitsonga data, so these results still provide a useful test of generalization.", "In the previous experiments, we used data from GlobalPhone, which provides corpora collected and formatted similarly for a wide range of languages. However, GlobalPhone is not freely available and no previous zero-resource studies have used these corpora, so in this section we also provide results on the zrsc 2015 BIBREF0 data sets, which have been widely used in other work. The target languages are English (from the Buckeye corpus BIBREF38 ) and Xitsonga (NCHLT corpus BIBREF39 ). Table TABREF8 includes the corpus statistics. These corpora are not split into train/dev/test; since training is unsupervised, the system is simply trained directly on the unlabeled test set (which could also be done in deployment). Importantly, no hyperparameter tuning is done on the Buckeye or Xitsonga data, so these results still provide a useful test of generalization. Notably, the Buckeye English corpus contains conversational speech and is therefore different in style from the rest of our data."]}
{"question_id": "9a52a33d0ae5491c07f125454aea9a41b9babb82", "predicted_answer": "The text does not explicitly state how they extract target language bottleneck features. However, it mentions that they train multilingual bottleneck features (bnfs) on between one and ten languages from the GlobalPhone collection and evaluate on six other languages (simulating different zero-resource targets). It also mentions that they use a convolutional autoencoder (cae) to fine-tune the bnfs in an unsupervised fashion, but it does not provide details on how the bnfs are extracted in the first place.", "predicted_evidence": ["Our experiments therefore aim to evaluate on a wider range of target languages, and to explore the effects of both the amount of labeled data, and the number of languages from which it is obtained.", "In the previous experiments, we used data from GlobalPhone, which provides corpora collected and formatted similarly for a wide range of languages. However, GlobalPhone is not freely available and no previous zero-resource studies have used these corpora, so in this section we also provide results on the zrsc 2015 BIBREF0 data sets, which have been widely used in other work. The target languages are English (from the Buckeye corpus BIBREF38 ) and Xitsonga (NCHLT corpus BIBREF39 ). Table TABREF8 includes the corpus statistics. These corpora are not split into train/dev/test; since training is unsupervised, the system is simply trained directly on the unlabeled test set (which could also be done in deployment). Importantly, no hyperparameter tuning is done on the Buckeye or Xitsonga data, so these results still provide a useful test of generalization.", "Next, we explore how multilingual annotated data can be used to improve feature extraction for a zero-resource target language. We train multilingual bnfs on between one and ten languages from the GlobalPhone collection and evaluate on six other languages (simulating different zero-resource targets). We show that training on more languages consistently improves performance on word discrimination, and that the improvement is not simply due to more training data: an equivalent amount of data from one language fails to give the same benefit. In fact, we observe the largest gain in performance when adding the second training language, which is already better than adding three times as much data from the same language. Moreover, when compared to our best results from training unsupervised on target language data only, we find that bnfs trained on just a single other language already outperform the target-language-only training, with multilingual bnfs doing better by a wide margin.", "In the previous experiments, we used data from GlobalPhone, which provides corpora collected and formatted similarly for a wide range of languages. However, GlobalPhone is not freely available and no previous zero-resource studies have used these corpora, so in this section we also provide results on the zrsc 2015 BIBREF0 data sets, which have been widely used in other work. The target languages are English (from the Buckeye corpus BIBREF38 ) and Xitsonga (NCHLT corpus BIBREF39 ). Table TABREF8 includes the corpus statistics. These corpora are not split into train/dev/test; since training is unsupervised, the system is simply trained directly on the unlabeled test set (which could also be done in deployment). Importantly, no hyperparameter tuning is done on the Buckeye or Xitsonga data, so these results still provide a useful test of generalization. Notably, the Buckeye English corpus contains conversational speech and is therefore different in style from the rest of our data.", "So far we have shown that multilingual bnfs work better than any of the features trained using only the target language data. However, in principle it could be possible to use the target language data to fine tune the bnfs in an unsupervised fashion, improving performance further. We explored this possibility by simply training a cae using bnfs as input rather than PLPs. That is, we trained the cae with the same word pairs as before, but replaced VTLN-adapted MFCCs with the 10-lingual bnfs as input features, without any other changes in the training procedure. Table TABREF23 (penultimate row) shows that the cae trained with utd pairs is able to slightly improve on the bnfs in some cases, but this is not consistent across all languages and for Croatian the cae features are much worse."]}
{"question_id": "8c46a26f9b0b41c656b5b55142d491600663defa", "predicted_answer": "The datasets used are:\n\n1. GlobalPhone corpus\n2. zrsc 2015 dataset\n3. Buckeye corpus (for English)\n4. NCHLT corpus (for Xitsonga)\n\nNote that the GlobalPhone corpus is not freely available, while the zrsc 2015 dataset has been widely used in other work.", "predicted_evidence": ["In the previous experiments, we used data from GlobalPhone, which provides corpora collected and formatted similarly for a wide range of languages. However, GlobalPhone is not freely available and no previous zero-resource studies have used these corpora, so in this section we also provide results on the zrsc 2015 BIBREF0 data sets, which have been widely used in other work. The target languages are English (from the Buckeye corpus BIBREF38 ) and Xitsonga (NCHLT corpus BIBREF39 ). Table TABREF8 includes the corpus statistics. These corpora are not split into train/dev/test; since training is unsupervised, the system is simply trained directly on the unlabeled test set (which could also be done in deployment). Importantly, no hyperparameter tuning is done on the Buckeye or Xitsonga data, so these results still provide a useful test of generalization.", "In the previous experiments, we used data from GlobalPhone, which provides corpora collected and formatted similarly for a wide range of languages. However, GlobalPhone is not freely available and no previous zero-resource studies have used these corpora, so in this section we also provide results on the zrsc 2015 BIBREF0 data sets, which have been widely used in other work. The target languages are English (from the Buckeye corpus BIBREF38 ) and Xitsonga (NCHLT corpus BIBREF39 ). Table TABREF8 includes the corpus statistics. These corpora are not split into train/dev/test; since training is unsupervised, the system is simply trained directly on the unlabeled test set (which could also be done in deployment). Importantly, no hyperparameter tuning is done on the Buckeye or Xitsonga data, so these results still provide a useful test of generalization. Notably, the Buckeye English corpus contains conversational speech and is therefore different in style from the rest of our data.", "Next we investigate how labeled data from high-resource languages can be used to obtain improved features on a target zero-resource language for which no labeled data is available.", "We use the GlobalPhone corpus of speech read from news articles BIBREF20 . We chose 6 languages from different language families as zero-resource languages on which we evaluate the new feature representations. That means our models do not have any access to the transcriptions of the training data, although transcriptions still need to be available to run the evaluation. The selected languages and dataset sizes are shown in Table TABREF8 . Each GlobalPhone language has recordings from around 100 speakers, with 80% of these in the training sets and no speaker overlap between training, development, and test sets.", "Figure FIGREF21 shows ap on the same-different task of multilingual bnfs trained from scratch on an increasing number of languages in two randomly chosen orders. We provide two baselines for comparison, drawn from our results in Table TABREF13 . Firstly, our best cae features trained with utd pairs (row 4, Table TABREF13 ) are a reference for a fully unsupervised system. Secondly, the best cae features trained with gold standard pairs (row 6, Table TABREF13 ) give an upper bound on the cae performance."]}
{"question_id": "e5f8d2fc1332e982a54ee4b4c1f7f55e900d0b86", "predicted_answer": "The intrinsic measures used to evaluate the obtained representations are:\n\n1. Same-different measures\n2. ABX phone discriminability task, which includes:\n\t* Within-speaker error rate\n\t* Cross-speaker error rate", "predicted_evidence": ["For training the cae on the Buckeye English and Xitsonga corpora, we use the same sets of utd pairs as in BIBREF23 , which were discovered from fdlp features. We evaluate using both the same-different measures from above, as well as the ABX phone discriminability task BIBREF40 used in the zrsc and other recent work BIBREF0 , BIBREF1 . The ABX task evaluates phoneme discriminability using minimal pairs: sequences of three phonemes where the central phoneme differs between the two sequences INLINEFORM0 and INLINEFORM1 in the pair, such as b ih n and b eh n. Feature representations are then evaluated on how well they can identify a third triplet INLINEFORM2 as having the same phoneme sequence as either INLINEFORM3 or INLINEFORM4 . See BIBREF0 , BIBREF1 for details on how the scores are computed and averaged over speakers and phonemes to obtain the final ABX error rate.", "Our experiments therefore aim to evaluate on a wider range of target languages, and to explore the effects of both the amount of labeled data, and the number of languages from which it is obtained.", "We evaluate using both the same-different measures from above, as well as the ABX phone discriminability task BIBREF40 used in the zrsc and other recent work BIBREF0 , BIBREF1 . The ABX task evaluates phoneme discriminability using minimal pairs: sequences of three phonemes where the central phoneme differs between the two sequences INLINEFORM0 and INLINEFORM1 in the pair, such as b ih n and b eh n. Feature representations are then evaluated on how well they can identify a third triplet INLINEFORM2 as having the same phoneme sequence as either INLINEFORM3 or INLINEFORM4 . See BIBREF0 , BIBREF1 for details on how the scores are computed and averaged over speakers and phonemes to obtain the final ABX error rate. One usually distinguishes between the within-speaker error rate where all three triplets belong to the same speaker, and the cross-speaker error rate where INLINEFORM5 and INLINEFORM6 are from the same and INLINEFORM7 from a different speaker.", "Table TABREF36 compares MFCCs, cae features (with and without vtln) and bnfs as input to the system of BIBREF4 . It shows that both vtln and bnfs help on all metrics, with improvements ranging from small to more substantial and bnfs clearly giving the most benefit. The effects of vtln are mostly confined to reducing both gender and speaker purity of the identified clusters (which is desirable) while maintaining the performance on other metrics. This means that the learned representations have become more invariant to variation in speaker and gender, which is exactly what vtln aims to do. However, this appears to be insufficient to also help other metrics, aligning with the experiments in BIBREF4 that indicate that improvements on the other metrics are hard to obtain.", "For training the cae on the Buckeye English and Xitsonga corpora, we use the same sets of utd pairs as in BIBREF23 , which were discovered from fdlp features. We evaluate using both the same-different measures from above, as well as the ABX phone discriminability task BIBREF40 used in the zrsc and other recent work BIBREF0 , BIBREF1 . The ABX task evaluates phoneme discriminability using minimal pairs: sequences of three phonemes where the central phoneme differs between the two sequences INLINEFORM0 and INLINEFORM1 in the pair, such as b ih n and b eh n. Feature representations are then evaluated on how well they can identify a third triplet INLINEFORM2 as having the same phoneme sequence as either INLINEFORM3 or INLINEFORM4 ."]}
{"question_id": "19578949108ef72603afe538059ee55b4dee0751", "predicted_answer": "The answer is: No, they do not use pre-trained embeddings in their model. \n\nThe text does not mention anything about pre-trained embeddings. It talks about training models on different datasets (CNNDM, DUC 2004, and Multi-News) and using different architectures (Bi-LSTM, Transformer), but it does not mention using pre-trained embeddings.", "predicted_evidence": ["In Table TABREF30 and Table TABREF31 we report ROUGE scores on DUC 2004 and Multi-News datasets respectively. We use DUC 2004, as results on this dataset are reported in lebanoff18mds, although this dataset is not the focus of this work. For results on DUC 2004, models were trained on the CNNDM dataset, as in lebanoff18mds. PG-BRNN and CopyTransformer models, which were pretrained by OpenNMT on CNNDM, were applied to DUC without additional training, analogous to PG-Original. We also experimented with training on Multi-News and testing on DUC data, but we did not see significant improvements. We attribute the generally low performance of pointer-generator, CopyTransformer and Hi-MAP to domain differences between DUC and CNNDM as well as DUC and Multi-News.", "In Table TABREF30 and Table TABREF31 we report ROUGE scores on DUC 2004 and Multi-News datasets respectively. We use DUC 2004, as results on this dataset are reported in lebanoff18mds, although this dataset is not the focus of this work. For results on DUC 2004, models were trained on the CNNDM dataset, as in lebanoff18mds. PG-BRNN and CopyTransformer models, which were pretrained by OpenNMT on CNNDM, were applied to DUC without additional training, analogous to PG-Original. We also experimented with training on Multi-News and testing on DUC data, but we did not see significant improvements. We attribute the generally low performance of pointer-generator, CopyTransformer and Hi-MAP to domain differences between DUC and CNNDM as well as DUC and Multi-News. These domain differences are evident in the statistics and extractive metrics discussed in Section 3.", "To expand our model into a hierarchical one, we compute sentence representations on both the encoder and decoder. The input is a collection of sentences INLINEFORM0 from all the source documents, where a given sentence INLINEFORM1 is made up of input word tokens. Word tokens from the whole document are treated as a single sequential input to a Bi-LSTM encoder as in the original encoder of the pointer-generator network from see2017ptrgen (see bottom of Figure FIGREF19 ). For each time step, the output of an input word token INLINEFORM2 is INLINEFORM3 (we use superscript INLINEFORM4 to indicate word-level LSTM cells, INLINEFORM5 for sentence-level).", "where INLINEFORM0 is a learned parameter used to transform INLINEFORM1 and INLINEFORM2 into a common feature space.", "CopyTransformer Instead of using an LSTM, the CopyTransformer model used in Gehrmann:18 uses a 4-layer Transformer of 512 dimensions for encoder and decoder. One of the attention heads is chosen randomly as the copy distribution. This model and the PG-BRNN are run without the bottom-up masked attention for inference from Gehrmann:18 as we did not find a large improvement when reproducing the model on this data."]}
{"question_id": "44435fbd4087fa711835d267036b6a1f82336a22", "predicted_answer": "The results obtained by their model are reported in Table TABREF30 and Table TABREF31, which show ROUGE scores on DUC 2004 and Multi-News datasets respectively.", "predicted_evidence": ["In this section we describe additional methods we compare with and present our assumptions and experimental process.", "Additionally, for both DUC and Multi-News testing, we experimented with using the output of 500 tokens from extractive methods (LexRank, TextRank and MMR) as input to the abstractive model. However, this did not improve results. We believe this is because our truncated input mirrors the First-3 baseline, which outperforms these three extractive methods and thus may provide more information as input to the abstractive model.", "For the second term of Equation SECREF21 , instead of choosing the maximum score from all candidates except for INLINEFORM0 , which is intended to find the candidate most similar to INLINEFORM1 , we choose to apply a self-attention model on INLINEFORM2 and all the other candidates INLINEFORM3 . We then choose the largest weight as the final score:", "PG-Original, PG-MMR These are the original pointer-generator network models reported by BIBREF11 .", "In Table TABREF30 and Table TABREF31 we report ROUGE scores on DUC 2004 and Multi-News datasets respectively. We use DUC 2004, as results on this dataset are reported in lebanoff18mds, although this dataset is not the focus of this work. For results on DUC 2004, models were trained on the CNNDM dataset, as in lebanoff18mds. PG-BRNN and CopyTransformer models, which were pretrained by OpenNMT on CNNDM, were applied to DUC without additional training, analogous to PG-Original. We also experimented with training on Multi-News and testing on DUC data, but we did not see significant improvements. We attribute the generally low performance of pointer-generator, CopyTransformer and Hi-MAP to domain differences between DUC and CNNDM as well as DUC and Multi-News. These domain differences are evident in the statistics and extractive metrics discussed in Section 3."]}
{"question_id": "86656aae3c27c6ea108f5600dd09ab7e421d876a", "predicted_answer": "The news articles in the Multi-News dataset come from a diverse set of over 1,500 news sources, with each source document appearing at least 5 times in the dataset.", "predicted_evidence": ["Our dataset, which we call Multi-News, consists of news articles and human-written summaries of these articles from the site newser.com. Each summary is professionally written by editors and includes links to the original articles cited. We will release stable Wayback-archived links, and scripts to reproduce the dataset from these links. Our dataset is notably the first large-scale dataset for MDS on news articles. Our dataset also comes from a diverse set of news sources; over 1,500 sites appear as source documents 5 times or greater, as opposed to previous news datasets (DUC comes from 2 sources, CNNDM comes from CNN and Daily Mail respectively, and even the Newsroom dataset BIBREF6 covers only 38 news sources). A total of 20 editors contribute to 85% of the total summaries on newser.com. Thus we believe that this dataset allows for the summarization of diverse source documents and summaries.", "Multi-document summarization of news events offers the challenge of outputting a well-organized summary which covers an event comprehensively while simultaneously avoiding redundancy. The input documents may differ in focus and point of view for an event. We present an example of multiple input news documents and their summary in Figure TABREF2 . The three source documents discuss the same event and contain overlaps in content: the fact that Meng Wanzhou was arrested is stated explicitly in Source 1 and 3 and indirectly in Source 2. However, some sources contain information not mentioned in the others which should be included in the summary: Source 3 states that (Wanzhou) is being sought for extradition by the US while only Source 2 mentioned the attitude of the Chinese side.", "The number of collected Wayback links for summaries and their corresponding cited articles totals over 250,000. We only include examples with between 2 and 10 source documents per summary, as our goal is MDS, and the number of examples with more than 10 sources was minimal. The number of source articles per summary present, after downloading and processing the text to obtain the original article text, varies across the dataset, as shown in Table TABREF4 . We believe this setting reflects real-world situations; often for a new or specialized event there may be only a few news articles. Nonetheless, we would like to summarize these events in addition to others with greater news coverage.", "As discussed above, large scale datasets for multi-document news summarization are lacking. There have been several attempts to create MDS datasets in other domains. zopf18mds introduce a multi-lingual MDS dataset based on English and German Wikipedia articles as summaries to create a set of about 7,000 examples. liu18wikisum use Wikipedia as well, creating a dataset of over two million examples. That paper uses Wikipedia references as input documents but largely relies on Google search to increase topic coverage. We, however, are focused on the news domain, and the source articles in our dataset are specifically cited by the corresponding summaries. Related work has also focused on opinion summarization in the multi-document setting; angelidis18opinions introduces a dataset of 600 Amazon product reviews.", "Summarization is a central problem in Natural Language Processing with increasing applications as the desire to receive content in a concise and easily-understood format increases. Recent advances in neural methods for text summarization have largely been applied in the setting of single-document news summarization and headline generation BIBREF0 , BIBREF1 , BIBREF2 . These works take advantage of large datasets such as the Gigaword Corpus BIBREF3 , the CNN/Daily Mail (CNNDM) dataset BIBREF4 , the New York Times dataset BIBREF5 and the Newsroom corpus BIBREF6 , which contain on the order of hundreds of thousands to millions of article-summary pairs."]}
{"question_id": "22488c8628b6db5fd708b6471c31a8eac31f66df", "predicted_answer": "The size of the Multi-News dataset is:\n\n* Training set: 44,972 examples (80% of the total dataset)\n* Validation set: 5,622 examples (10% of the total dataset)\n* Test set: 5,622 examples (10% of the total dataset)\n\nIn total, the dataset contains 56,216 examples.", "predicted_evidence": ["Our dataset, which we call Multi-News, consists of news articles and human-written summaries of these articles from the site newser.com. Each summary is professionally written by editors and includes links to the original articles cited. We will release stable Wayback-archived links, and scripts to reproduce the dataset from these links. Our dataset is notably the first large-scale dataset for MDS on news articles. Our dataset also comes from a diverse set of news sources; over 1,500 sites appear as source documents 5 times or greater, as opposed to previous news datasets (DUC comes from 2 sources, CNNDM comes from CNN and Daily Mail respectively, and even the Newsroom dataset BIBREF6 covers only 38 news sources). A total of 20 editors contribute to 85% of the total summaries on newser.com. Thus we believe that this dataset allows for the summarization of diverse source documents and summaries.", "We split our dataset into training (80%, 44,972), validation (10%, 5,622), and test (10%, 5,622) sets. Table TABREF5 compares Multi-News to other news datasets used in experiments below. We choose to compare Multi-News with DUC data from 2003 and 2004 and TAC 2011 data, which are typically used in multi-document settings. Additionally, we compare to the single-document CNNDM dataset, as this has been recently used in work which adapts SDS to MDS BIBREF11 . The number of examples in our Multi-News dataset is two orders of magnitude larger than previous MDS news data. The total number of words in the concatenated inputs is shorter than other MDS datasets, as those consist of 10 input documents, but larger than SDS datasets, as expected. Our summaries are notably longer than in other works, about 260 words on average.", "We split our dataset into training (80%, 44,972), validation (10%, 5,622), and test (10%, 5,622) sets. Table TABREF5 compares Multi-News to other news datasets used in experiments below. We choose to compare Multi-News with DUC data from 2003 and 2004 and TAC 2011 data, which are typically used in multi-document settings. Additionally, we compare to the single-document CNNDM dataset, as this has been recently used in work which adapts SDS to MDS BIBREF11 . The number of examples in our Multi-News dataset is two orders of magnitude larger than previous MDS news data. The total number of words in the concatenated inputs is shorter than other MDS datasets, as those consist of 10 input documents, but larger than SDS datasets, as expected.", "As discussed above, large scale datasets for multi-document news summarization are lacking. There have been several attempts to create MDS datasets in other domains. zopf18mds introduce a multi-lingual MDS dataset based on English and German Wikipedia articles as summaries to create a set of about 7,000 examples. liu18wikisum use Wikipedia as well, creating a dataset of over two million examples. That paper uses Wikipedia references as input documents but largely relies on Google search to increase topic coverage. We, however, are focused on the news domain, and the source articles in our dataset are specifically cited by the corresponding summaries. Related work has also focused on opinion summarization in the multi-document setting; angelidis18opinions introduces a dataset of 600 Amazon product reviews.", "Table TABREF5 compares Multi-News to other news datasets used in experiments below. We choose to compare Multi-News with DUC data from 2003 and 2004 and TAC 2011 data, which are typically used in multi-document settings. Additionally, we compare to the single-document CNNDM dataset, as this has been recently used in work which adapts SDS to MDS BIBREF11 . The number of examples in our Multi-News dataset is two orders of magnitude larger than previous MDS news data. The total number of words in the concatenated inputs is shorter than other MDS datasets, as those consist of 10 input documents, but larger than SDS datasets, as expected. Our summaries are notably longer than in other works, about 260 words on average. While compressing information into a shorter text is the goal of summarization, our dataset tests the ability of abstractive models to generate fluent text concise in meaning while also coherent in the entirety of its generally longer output, which we consider an interesting challenge."]}
{"question_id": "1f2952cd1dc0c891232fa678b6c219f6b4d31958", "predicted_answer": "According to the text, the optimal vocabulary size for BPE (Byte Pair Encoding) is not too small and not too large, but rather an intermediate size that achieves a balance between $D$ and $\\mu$. The experiments show that a vocabulary size of $1k$ is too small, and sizes of $32k$ and $64k$ are too large, resulting in lower BLEU scores. The optimal size is not specified exactly, but it is implied to be somewhere in between, as it can be tuned to reduce $D$ while keeping $\\mu$ short enough.", "predicted_evidence": ["Experiments #7, #8, #9 and #10 with comparison to #6 showed that reducing vocabulary too much also negatively affects BLEU. Though Experiment #9 with $1k$ target vocabulary has the lowest $D$ favoring the $C$, in comparison to others, the BLEU is still lower than the others. An explanation for this reduction is that $\\mu $ is higher and unfavorable to $R$. Hence a strictly smaller vocabulary is not the best choice either.", "Experiments #1 and #2 use a word vocabulary, while #3 and #4 use a BPE vocabulary. The results show that with BPE, increasing the vocabulary size at this range reduces BLEU. Experiment #3 with a vocabulary as large as $64k$ BPE types even fails to reach the comparable Word model's (#1) BLEU score, which raises the need for a systematic understanding of `Why BPE model reduced BLEU when vocabulary increased from $32k$ to $64k$?'. With increase in BPE vocabulary, $\\mu $ is reduced which is favorable to $R$. An explanation is that the $D$ increased which is unfavorable to $C$. For Word models, there is an effect of OOVs along with $D$, and it is beyond the scope of this work.", "Experiments #3, #4, #5, #6 show that with BPE, decreasing the vocabulary indeed improves BLEU. Hence the larger BPE vocabulary such as $32k$ and $64k$ are not the best choice.", "We use character, word, and BPE subword encoding with various vocabulary sizes to analyze the effect of $D$ and $\\mu $. Each experiment is run twice and we report the mean of BLEU scores in Table TABREF15. The BLEU scores were computed using SacreBLEU BIBREF11 . All results are in Table TABREF15. We observe the following:", "Figure FIGREF9 shows the relation between the BPE vocabulary size on both $D$ and $\\mu $. A smaller vocabulary of BPE, after merging a few extremely frequent pairs, has smallest $D$ which is a desired setting for $C$, but at the same point $\\mu $ is large and undesired for $R$. When BPE vocabulary is set to a large one, the effect is reversed i.e. $D$ is large and unfavorable to $C$ while $\\mu $ small and favorable to $R$. As seen with evidence in Section SECREF4, there exists optimal vocabulary size of BPE that achieve the best setting for both $C$ and $R$. Hence, BPE vocabulary size is not arbitrary since it can be tuned to reduce $D$ while keeping $\\mu $ short enough as well."]}
{"question_id": "23fe8431058f2a7b7588745766fc715f271aad07", "predicted_answer": "The languages explored in this context are German (De) and English (En).", "predicted_evidence": ["We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.", "We summarize sequence lengths using Mean Sequence Length, $\\mu $, computed trivially as the arithmetic mean of the lengths of target language sequences after encoding them:", "Experiments #7, #8, #9 and #10 with comparison to #6 showed that reducing vocabulary too much also negatively affects BLEU. Though Experiment #9 with $1k$ target vocabulary has the lowest $D$ favoring the $C$, in comparison to others, the BLEU is still lower than the others. An explanation for this reduction is that $\\mu $ is higher and unfavorable to $R$. Hence a strictly smaller vocabulary is not the best choice either.", "sennrich-etal-2016-bpe introduced byte pair encoding (BPE) as a simplified way for solving OOV words without using back-off models. They noted that BPE improved the translation of not only the OOV words, but also some of rare in-vocabulary words. In their work, the vocabulary size was arbitrary, and large as $60k$ and $100k$.", "Subwords obtained through e.g. byte pair encoding (BPE) BIBREF5 addresses the open-ended vocabulary problem by using only a finite set of subwords. Due to the benefit and simplicity of BPE, it is rightfully part of the majority of current NMT models. However, the choice of vocabulary size used for BPE is a hyperparameter whose effect is not well understood. In practice, BPE vocabulary choice is either arbitrary or chosen from several trial-and-errors."]}
{"question_id": "e5b2eb6a49c163872054333f8670dd3f9563046a", "predicted_answer": "The paper does not explicitly mention the specific datasets used in the research. However, it mentions that the research is focused on Natural Language Processing (NLP) tasks, particularly Neural Machine Translation (NMT), and that word types in natural language models follow a Zipfian distribution. It can be inferred that the datasets used are likely to be NLP datasets, possibly related to machine translation, but the specific datasets are not mentioned.", "predicted_evidence": ["In this work, we attempt to find answers to these questions: `What value of BPE vocabulary size is best for NMT?', and more crucially an explanation for `Why that value?'. As we will see, the answers and explanations for those are an immediate consequence of a broader question, namely `What is the impact of Zipfian imbalance on classifier-based NLG?'", "Since the parameters of ML classification models are estimated from training data, certain biases in the training data affect the final performance of model. Among those biases, class imbalance is a topic of our interest. Class imbalance is said to exist when one or more classes are not of approximately equal frequency in data. The effect of class imbalance has been extensively studied in several domains where classifiers are used (see Section SECREF32). With neural networks, the imbalanced learning is mostly targeted to computer vision tasks; NLP tasks are underexplored BIBREF4. Word types in natural language models follow a Zipfian distribution, i.e. in any natural language corpus, we observe that a few types are extremely frequent and the vast number of others lie on the long tail of infrequency. The Zipfian distribution thus causes two problems to the classifier based NLG systems:", "This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C-9116, and by research sponsored by Air Force Research Laboratory (AFRL) under agreement number FA8750-19-1-1000. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, Air Force Laboratory, DARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.", "We categorize the related work into the subsections as following:", "The contributions of this paper are as follows: We offer a simplified view of NMT architectures by re-envisioning them as two high-level components: a classifier and an autoregressor (Section SECREF2). For the best performance of the classifier, we argue that the balanced class distribution is desired, and describe a method to measure class imbalance in a Zipfian distribution (Section SECREF6). For the best performance of the autoregressor, we argue that it is desired to have shorter sequences (Section SECREF7). In Section SECREF8, we describe how BPE vocabulary relates with the desired settings for both classifier and autoregressor. Our experimental setup is described in Section SECREF3, followed by the analysis of results in Section SECREF4 that offers an explanation with evidence for why some vocabulary sizes are better than others. Section SECREF5 uncovers the impact of class imbalance, particularly the discrimination on classes based on their frequency."]}
{"question_id": "73760a45b23b2ec0cab181f82953fb296bb6cd19", "predicted_answer": "The vocabulary sizes explored in the experiments are:\n\n* 1k (Experiment #9)\n* 32k (Experiment #3)\n* 64k (Experiment #4)\n* 16k (mentioned in the context of morishita-etal-2018-improving's analysis)\n\nThese vocabulary sizes are used in the context of Byte Pair Encoding (BPE) and word vocabularies.", "predicted_evidence": ["Experiments #7, #8, #9 and #10 with comparison to #6 showed that reducing vocabulary too much also negatively affects BLEU. Though Experiment #9 with $1k$ target vocabulary has the lowest $D$ favoring the $C$, in comparison to others, the BLEU is still lower than the others. An explanation for this reduction is that $\\mu $ is higher and unfavorable to $R$. Hence a strictly smaller vocabulary is not the best choice either.", "Subwords obtained through e.g. byte pair encoding (BPE) BIBREF5 addresses the open-ended vocabulary problem by using only a finite set of subwords. Due to the benefit and simplicity of BPE, it is rightfully part of the majority of current NMT models. However, the choice of vocabulary size used for BPE is a hyperparameter whose effect is not well understood. In practice, BPE vocabulary choice is either arbitrary or chosen from several trial-and-errors.", "Experiments #1 and #2 use a word vocabulary, while #3 and #4 use a BPE vocabulary. The results show that with BPE, increasing the vocabulary size at this range reduces BLEU. Experiment #3 with a vocabulary as large as $64k$ BPE types even fails to reach the comparable Word model's (#1) BLEU score, which raises the need for a systematic understanding of `Why BPE model reduced BLEU when vocabulary increased from $32k$ to $64k$?'. With increase in BPE vocabulary, $\\mu $ is reduced which is favorable to $R$. An explanation is that the $D$ increased which is unfavorable to $C$. For Word models, there is an effect of OOVs along with $D$, and it is beyond the scope of this work.", "morishita-etal-2018-improving viewed BPE more generally in the sense that both character and word vocabularies as two special cases of BPE vocabulary. Their analysis was different than ours in a way that they viewed BPE with varied vocabulary sizes as hierarchical features which were used in addition to a fixed BPE vocabulary size of $16k$ on the target language. DBLP:journals/corr/abs-1810-08641 offer an efficient way to search BPE vocabulary size for NMT. kudo-2018-subword used BPE segmentation as a regularization by introducing sampling based randomness to the BPE segmentation. For the best of our knowledge, no previous work exists that analyzed BPE's effect on class imbalance or answered `why certain BPE vocabularies are better than others?'.", "Figure FIGREF9 shows the relation between the BPE vocabulary size on both $D$ and $\\mu $. A smaller vocabulary of BPE, after merging a few extremely frequent pairs, has smallest $D$ which is a desired setting for $C$, but at the same point $\\mu $ is large and undesired for $R$. When BPE vocabulary is set to a large one, the effect is reversed i.e. $D$ is large and unfavorable to $C$ while $\\mu $ small and favorable to $R$. As seen with evidence in Section SECREF4, there exists optimal vocabulary size of BPE that achieve the best setting for both $C$ and $R$. Hence, BPE vocabulary size is not arbitrary since it can be tuned to reduce $D$ while keeping $\\mu $ short enough as well."]}
{"question_id": "ec990c16896793a819766bc3168c02556ef69971", "predicted_answer": "The best performer in terms of vocabulary size is not explicitly stated in the text. However, it can be inferred that the optimal vocabulary size for BPE (Byte Pair Encoding) is not too small (e.g. 1k) and not too large (e.g. 64k), but rather somewhere in between. The text suggests that there exists an optimal vocabulary size for BPE that achieves the best setting for both $C$ and $R$, but the exact size is not specified.", "predicted_evidence": ["Experiments #7, #8, #9 and #10 with comparison to #6 showed that reducing vocabulary too much also negatively affects BLEU. Though Experiment #9 with $1k$ target vocabulary has the lowest $D$ favoring the $C$, in comparison to others, the BLEU is still lower than the others. An explanation for this reduction is that $\\mu $ is higher and unfavorable to $R$. Hence a strictly smaller vocabulary is not the best choice either.", "Experiments #1 and #2 use a word vocabulary, while #3 and #4 use a BPE vocabulary. The results show that with BPE, increasing the vocabulary size at this range reduces BLEU. Experiment #3 with a vocabulary as large as $64k$ BPE types even fails to reach the comparable Word model's (#1) BLEU score, which raises the need for a systematic understanding of `Why BPE model reduced BLEU when vocabulary increased from $32k$ to $64k$?'. With increase in BPE vocabulary, $\\mu $ is reduced which is favorable to $R$. An explanation is that the $D$ increased which is unfavorable to $C$. For Word models, there is an effect of OOVs along with $D$, and it is beyond the scope of this work.", "Experiments #3, #4, #5, #6 show that with BPE, decreasing the vocabulary indeed improves BLEU. Hence the larger BPE vocabulary such as $32k$ and $64k$ are not the best choice.", "We use character, word, and BPE subword encoding with various vocabulary sizes to analyze the effect of $D$ and $\\mu $. Each experiment is run twice and we report the mean of BLEU scores in Table TABREF15. The BLEU scores were computed using SacreBLEU BIBREF11 . All results are in Table TABREF15. We observe the following:", "Figure FIGREF9 shows the relation between the BPE vocabulary size on both $D$ and $\\mu $. A smaller vocabulary of BPE, after merging a few extremely frequent pairs, has smallest $D$ which is a desired setting for $C$, but at the same point $\\mu $ is large and undesired for $R$. When BPE vocabulary is set to a large one, the effect is reversed i.e. $D$ is large and unfavorable to $C$ while $\\mu $ small and favorable to $R$. As seen with evidence in Section SECREF4, there exists optimal vocabulary size of BPE that achieve the best setting for both $C$ and $R$. Hence, BPE vocabulary size is not arbitrary since it can be tuned to reduce $D$ while keeping $\\mu $ short enough as well."]}
{"question_id": "11c4071d9d7efeede84f47892b1fa0c6a93667eb", "predicted_answer": "The datasets used in this research are not explicitly mentioned. However, it can be inferred that the research involves natural language processing (NLP) tasks, specifically neural machine translation (NMT) architectures, and that the datasets used are likely to be natural language corpora that follow a Zipfian distribution.", "predicted_evidence": ["This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C-9116, and by research sponsored by Air Force Research Laboratory (AFRL) under agreement number FA8750-19-1-1000. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, Air Force Laboratory, DARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.", "The contributions of this paper are as follows: We offer a simplified view of NMT architectures by re-envisioning them as two high-level components: a classifier and an autoregressor (Section SECREF2). For the best performance of the classifier, we argue that the balanced class distribution is desired, and describe a method to measure class imbalance in a Zipfian distribution (Section SECREF6). For the best performance of the autoregressor, we argue that it is desired to have shorter sequences (Section SECREF7). In Section SECREF8, we describe how BPE vocabulary relates with the desired settings for both classifier and autoregressor. Our experimental setup is described in Section SECREF3, followed by the analysis of results in Section SECREF4 that offers an explanation with evidence for why some vocabulary sizes are better than others. Section SECREF5 uncovers the impact of class imbalance, particularly the discrimination on classes based on their frequency.", "The contributions of this paper are as follows: We offer a simplified view of NMT architectures by re-envisioning them as two high-level components: a classifier and an autoregressor (Section SECREF2). For the best performance of the classifier, we argue that the balanced class distribution is desired, and describe a method to measure class imbalance in a Zipfian distribution (Section SECREF6). For the best performance of the autoregressor, we argue that it is desired to have shorter sequences (Section SECREF7). In Section SECREF8, we describe how BPE vocabulary relates with the desired settings for both classifier and autoregressor. Our experimental setup is described in Section SECREF3, followed by the analysis of results in Section SECREF4 that offers an explanation with evidence for why some vocabulary sizes are better than others. Section SECREF5 uncovers the impact of class imbalance, particularly the discrimination on classes based on their frequency. Section SECREF6 provides an overview of the related work, followed by a conclusion in Section SECREF7.", "A classifier that does not discriminate classes based on their frequencies is the one that exhibits no correlation between class rank vs precision and class rank vs recall. However, in the top rows of Table TABREF15 where larger vocabularies such as $64k$ are used, we make two observations:", "Since the parameters of ML classification models are estimated from training data, certain biases in the training data affect the final performance of model. Among those biases, class imbalance is a topic of our interest. Class imbalance is said to exist when one or more classes are not of approximately equal frequency in data. The effect of class imbalance has been extensively studied in several domains where classifiers are used (see Section SECREF32). With neural networks, the imbalanced learning is mostly targeted to computer vision tasks; NLP tasks are underexplored BIBREF4. Word types in natural language models follow a Zipfian distribution, i.e. in any natural language corpus, we observe that a few types are extremely frequent and the vast number of others lie on the long tail of infrequency. The Zipfian distribution thus causes two problems to the classifier based NLG systems:"]}
{"question_id": "9aa751aebf6a449d95fb04ceec71688f2ed2cea2", "predicted_answer": "The text does not explicitly mention specific vocabulary sizes that were analyzed. However, it mentions that the authors experimented with \"various vocabulary sizes\" and that the optimal vocabulary size was found to be not arbitrary, but rather it can be tuned to achieve the best setting for both $C$ and $R$. Additionally, it mentions that morishita-etal-2018-improving used a fixed BPE vocabulary size of $16k$ on the target language.", "predicted_evidence": ["In this work, we attempt to find answers to these questions: `What value of BPE vocabulary size is best for NMT?', and more crucially an explanation for `Why that value?'. As we will see, the answers and explanations for those are an immediate consequence of a broader question, namely `What is the impact of Zipfian imbalance on classifier-based NLG?'", "Subwords obtained through e.g. byte pair encoding (BPE) BIBREF5 addresses the open-ended vocabulary problem by using only a finite set of subwords. Due to the benefit and simplicity of BPE, it is rightfully part of the majority of current NMT models. However, the choice of vocabulary size used for BPE is a hyperparameter whose effect is not well understood. In practice, BPE vocabulary choice is either arbitrary or chosen from several trial-and-errors.", "We use character, word, and BPE subword encoding with various vocabulary sizes to analyze the effect of $D$ and $\\mu $. Each experiment is run twice and we report the mean of BLEU scores in Table TABREF15. The BLEU scores were computed using SacreBLEU BIBREF11 . All results are in Table TABREF15. We observe the following:", "Figure FIGREF9 shows the relation between the BPE vocabulary size on both $D$ and $\\mu $. A smaller vocabulary of BPE, after merging a few extremely frequent pairs, has smallest $D$ which is a desired setting for $C$, but at the same point $\\mu $ is large and undesired for $R$. When BPE vocabulary is set to a large one, the effect is reversed i.e. $D$ is large and unfavorable to $C$ while $\\mu $ small and favorable to $R$. As seen with evidence in Section SECREF4, there exists optimal vocabulary size of BPE that achieve the best setting for both $C$ and $R$. Hence, BPE vocabulary size is not arbitrary since it can be tuned to reduce $D$ while keeping $\\mu $ short enough as well.", "morishita-etal-2018-improving viewed BPE more generally in the sense that both character and word vocabularies as two special cases of BPE vocabulary. Their analysis was different than ours in a way that they viewed BPE with varied vocabulary sizes as hierarchical features which were used in addition to a fixed BPE vocabulary size of $16k$ on the target language. DBLP:journals/corr/abs-1810-08641 offer an efficient way to search BPE vocabulary size for NMT. kudo-2018-subword used BPE segmentation as a regularization by introducing sampling based randomness to the BPE segmentation. For the best of our knowledge, no previous work exists that analyzed BPE's effect on class imbalance or answered `why certain BPE vocabularies are better than others?'."]}
{"question_id": "2929e92f9b4939297b4d0f799d464d46e8d52063", "predicted_answer": "The improvement on OntoNotes is significantly smaller compared to improvement on WNUT 2017 because WNUT 2017 is a more challenging dataset that requires models to generalize to emerging contexts and entities, rather than relying on familiar surface cues. The cross-structures are more crucial for emerging contexts and entities, which might not be memorized by their surface forms. In contrast, OntoNotes may contain more familiar entities that can be memorized by their surface forms, making the improvement from cross-structures less significant.", "predicted_evidence": ["Table TABREF14 shows overall results on the two datasets spanning broad domains of newswires, broadcast, telephone, and social media. The models proposed in this paper significantly surpassed previous comparable models by 1.4% on OntoNotes and 4.6% on WNUT. Compared to the re-implemented Baseline-BiLSTM-CNN, the cross-structures brought 0.7% and 2.2% improvements on OntoNotes and WNUT. More substantial improvements were achieved for WNUT 2017 emerging NER, suggesting that cross-context patterns were even more crucial for emerging contexts and entities than familiar entities, which might often be memorized by their surface forms.", "Table TABREF22 shows the entity-chunking ablation results on OntoNotes 5.0 development set. Both Att and Baseline models were taken without re-training for this subtask. The $HC^{all}$ column lists the performance of Att-BiLSTM-CNN on each chunking tag. Other columns list the performance compared to $HC^{all}$. Columns $H$ to $C^5$ are when the full model is deprived of all other information in testing time by forcefully zeroing all vectors except the one specified by the column header. The figures shown in the table are per-token recalls for each chunking tag, which tells if a part of the model is responsible for signaling the whole model to predict that tag. Colors mark relatively high and low values of interest.", "Table TABREF17 shows results among different entity lengths. It could be seen that cross-structures were much better at dealing with multi-token mentions (1.8%/2.3%/8.7%/2.6%) compared to the prevalently used, problematic Baseline.", "WNUT 2017 Emerging NER \u2013 a dataset providing maximally diverse, noisy, and drifting user-generated text BIBREF22. The training set consists of previously annotated tweets \u2013 social media text with non-standard spellings, abbreviations, and unreliable capitalization BIBREF23; the development set consists of newly sampled YouTube comments; the test set includes text newly drawn from Twitter, Reddit, and StackExchange. Besides drawing new samples from diverse topics across different sources, the shared task also filtered out text containing surface forms of entities seen in the training set. The resulting dataset requires models to generalize to emerging contexts and entities instead of relying on familiar surface cues.", "Then, breaking context vectors to each attention head reveals that they have worked in cooperation: $C^2$, $C^3$ focused more on scoring E (-36.45, -39.19) than I (-60.56, -50.19), while $C^4$ focused more on scoring B (-12.21) than I (-57.19). It was when information from all these heads were combined was Att able to better identify a token as being Inside a multi-token mention than Baseline."]}
{"question_id": "1dcfcfa46dbcffc2fc7be92dd57df9620258097b", "predicted_answer": "In this work, \"complexity\" and \"confusability\" of entity mentions are defined as follows:\n\n* Complexity refers to the difficulty of identifying entity mentions and their boundaries due to various factors such as:\n\t+ Unseen surface forms (e.g., work-of-art/creative-work entities that can take any surface form)\n\t+ Unreliable capitalizations (e.g., \"the White house\" with unreliable capitalization)\n\t+ Abbreviations\n\t+ Similarity to other entity types (e.g., language and NORP entities that take the same surface forms)\n* Confusability refers to the difficulty of disambiguating fine-grained entity types, such as distinguishing between language and NORP entities that have the same surface forms (e.g., \"Dutch\" and \"English\").", "predicted_evidence": ["Entity-chunking is a subtask of NER concerned with locating entity mentions and their boundaries without disambiguating their types. For sequence-labeling models, this means correct O, S, B, I, E tagging for each token. In addition to showing that cross-structures achieved superior performance on multi-token entity mentions (Section SECREF18), an ablation study focused on the chunking tags was performed to better understand how it was achieved.", "Moreover, disambiguating fine-grained entity types is also a challenging task. For example, entities of language and NORP often take the same surface forms. Figure FIGREF19 shows an example containing \"Dutch\" and \"English\". While \"English\" was much more frequently used as a language and was identified correctly, the \"Dutch\" mention was tricky for Baseline. The attention heat map (Figure FIGREF24) further tells the story that Att has relied on its attention head to make context-aware decisions. Overall, both cross-structures were much better at disambiguating these fine-grained types (4.1%/0.8%/3.3%/3.4%).", "Following BIBREF1, the 5 chunk labels O, S, B, I, E denote if a word token is Outside any entity mentions, the Sole token of a mention, the Beginning token of a multi-token mention, In the middle of a multi-token mention, or the Ending token of a multi-token mention. Hence when there are $P$ types of named entities, the actual number of token classes $d_p=P\\times 4+1$ for sequence labeling NER.", "Table TABREF16 shows significant results per entity type compared to Baseline ($>$3% absolute F1 differences for either Cross or Att). It could be seen that harder entity types generally benefitted more from the cross-structures. For example, work-of-art/creative-work entities could in principle take any surface forms \u2013 unseen, the same as a person name, abbreviated, or written with unreliable capitalizations on social media. Such mentions require models to learn a deep, generalized understanding of their context to accurately identify their boundaries and disambiguate their types. Both cross-structures were more capable in dealing with such hard entities (2.1%/5.6%/3.2%/2.0%) than the prevalently used, problematic Baseline.", "Figure FIGREF19 shows a sentence with multiple entity mentions. Among them, \"the White house\" is a triple-token facility mention with unreliable capitalization, resulting in an emerging surface form. Without usual strong hints given by a seen surface form, Baseline predicted a false single-token mention \"White\". In contrast, Att utilized its multiple attention heads (Figure FIGREF24, FIGREF24, FIGREF24) to consider the preceding and succeeding tokens for each token and correctly tagged the three tokens as facility:B, facility:I, facility:E."]}
{"question_id": "77bbe1698e001c5889217be3164982ea36e85752", "predicted_answer": "The baseline models are the Baseline-BiLSTM-CNN models, which use the same set of raw features (character embedding, character type, word embedding, and word capitalization) and the same model parameters (Nadam optimizer, uniform learning rate 0.001, batch size 32, and 35% dropout) as the Cross- and Att-BiLSTM-CNN models, but without the cross-structures.", "predicted_evidence": ["Table TABREF16 shows significant results per entity type compared to Baseline ($>$3% absolute F1 differences for either Cross or Att). It could be seen that harder entity types generally benefitted more from the cross-structures. For example, work-of-art/creative-work entities could in principle take any surface forms \u2013 unseen, the same as a person name, abbreviated, or written with unreliable capitalizations on social media. Such mentions require models to learn a deep, generalized understanding of their context to accurately identify their boundaries and disambiguate their types. Both cross-structures were more capable in dealing with such hard entities (2.1%/5.6%/3.2%/2.0%) than the prevalently used, problematic Baseline.", "All models in the experiments use the same set of raw features: character embedding, character type, word embedding, and word capitalization.", "All experiments for Baseline-, Cross-, and Att-BiLSTM-CNN used the same model parameters given in Section SECREF3. The training minimized per-token cross-entropy loss with the Nadam optimizer BIBREF24 with uniform learning rate 0.001, batch size 32, and 35% dropout. Each training lasted 400 epochs when using GloVe embedding (OntoNotes), and 1600 epochs when using Twitter embedding (WNUT). The development set of each dataset was used to select the best epoch to restore model weights for testing. Following previous work on NER, model performances were evaluated with strict mention F1 score. Training of each model on each dataset repeated 6 times to report the mean score and standard deviation.", "Table TABREF22 shows the entity-chunking ablation results on OntoNotes 5.0 development set. Both Att and Baseline models were taken without re-training for this subtask. The $HC^{all}$ column lists the performance of Att-BiLSTM-CNN on each chunking tag. Other columns list the performance compared to $HC^{all}$. Columns $H$ to $C^5$ are when the full model is deprived of all other information in testing time by forcefully zeroing all vectors except the one specified by the column header. The figures shown in the table are per-token recalls for each chunking tag, which tells if a part of the model is responsible for signaling the whole model to predict that tag. Colors mark relatively high and low values of interest.", "Section SECREF3 formulates the three Baseline, Cross, and Att-BiLSTM-CNN models. The section gives a concrete proof that patterns forming an XOR cannot be modeled by Baseline-BiLSTM-CNN used in all previous work. Cross-BiLSTM-CNN and Att-BiLSTM-CNN are shown to have additive and multiplicative cross-structures respectively to deal with the problem. Section SECREF4 evaluates the approaches on two challenging NER datasets spanning a wide range of domains with complex, noisy, and emerging entities. The cross-structures bring consistent improvements over the prevalently used Baseline-BiLSTM-CNN without additional gazetteers, POS taggers, language-modeling, or multi-task supervision. The improved core module surpasses comparable previous models on OntoNotes 5.0 and WNUT 2017 by 1.4% and 4.6% respectively."]}
{"question_id": "b537832bba2eb6d34702a9d71138e661c05a7c3a", "predicted_answer": "The text classification tasks considered are:\n\n1. News classification (AGNews)\n2. Sentiment analysis (Yelp, Amazon)\n3. Wikipedia article classification (DBPedia)\n4. Questions and answers categorization (Yahoo)", "predicted_evidence": ["In this section, we evaluate our proposed model against several baselines on text classification and question answering tasks.", "We use the following dataset orders (chosen randomly) for text classification:", "We use publicly available text classification datasets from BIBREF22 to evaluate our models (http://goo.gl/JyCnZq). This collection of datasets includes text classification datasets from diverse domains such as news classification (AGNews), sentiment analysis (Yelp, Amazon), Wikipedia article classification (DBPedia), and questions and answers categorization (Yahoo). Specifically, we use AGNews (4 classes), Yelp (5 classes), DBPedia (14 classes), Amazon (5 classes), and Yahoo (10 classes) datasets. Since classes for Yelp and Amazon datasets have similar semantics (product ratings), we merge the classes for these two datasets. In total, we have 33 classes in our experiments. These datasets have varying sizes. For example, AGNews is ten times smaller than Yahoo.", "We show per-dataset breakdown of results in Table TABREF33 in Table TABREF54 and Table TABREF55 for text classification and question answering respectively.", "For text classification, our key is an encoded representation of the first token of the document to be classified, so INLINEFORM0 (i.e., the special beginning-of-document symbol). For question answering, we first take the question part of the input INLINEFORM1 . We encode it using the key network and take the first token as the key vector INLINEFORM2 . For both tasks, we store the input and the label INLINEFORM3 as its associated memory value."]}
{"question_id": "1002bd01372eba0f3078fb4a951505278ed45f2e", "predicted_answer": "Yes, they compare MbPA++ against other models, including MbPA, A-GEM, and other episodic memory models.", "predicted_evidence": ["We compare the following models in our experiments:", "Figure FIGREF34 shows INLINEFORM0 score and accuracy of various models on the test set corresponding to the first dataset seen during training as the models are trained on more datasets. The figure illustrates how well each model retains its previously acquired knowledge as it learns new knowledge. We can see that MbPA++ is consistently better compared to other methods.", "Comparing MBpA++ to other episodic memory models, MBpA has roughly the same time and space complexity as MBpA++. A-GEM, on the other hand, is faster at prediction time (no local adaptation), although at training time it is slower due to extra projection steps and uses more memory since it needs to store two sets of gradients (one from the current batch, and one from samples from the memory). We find that this cost is not negligible when using a large encoder such as BERT.", "We use publicly available text classification datasets from BIBREF22 to evaluate our models (http://goo.gl/JyCnZq). This collection of datasets includes text classification datasets from diverse domains such as news classification (AGNews), sentiment analysis (Yelp, Amazon), Wikipedia article classification (DBPedia), and questions and answers categorization (Yahoo). Specifically, we use AGNews (4 classes), Yelp (5 classes), DBPedia (14 classes), Amazon (5 classes), and Yahoo (10 classes) datasets. Since classes for Yelp and Amazon datasets have similar semantics (product ratings), we merge the classes for these two datasets. In total, we have 33 classes in our experiments. These datasets have varying sizes. For example, AGNews is ten times smaller than Yahoo.", "At inference time, MbPA++ requires a local adaptation phase and is thus slower than methods without local adaptation. This can be seen as a limitation of MbPA++ (and MbPA). One way to speed it up is to parallelize predictions across test examples, since each prediction is independent of others. We set the number of local adaptation steps INLINEFORM0 in our experiments. Figure FIGREF44 shows INLINEFORM1 is needed to converge to an optimal performance."]}
{"question_id": "3450723bf66956486de777f141bde5073e4a7694", "predicted_answer": "Episodic memory refers to a type of memory that stores previously seen examples or experiences throughout an individual's (or in this case, a model's) lifetime. In the context of the MbPA++ model, the episodic memory module is used to store examples that can be retrieved and used for sparse experience replay and local adaptation, helping to prevent catastrophic forgetting and encourage positive transfer.", "predicted_evidence": ["MbPA++: our episodic memory model described in \u00a7 SECREF2 .", "Our model is augmented with an episodic memory module that stores previously seen examples throughout its lifetime. The episodic memory module is used for sparse experience replay and local adaptation to prevent catastrophic forgetting and encourage positive transfer. We first describe the architecture of our episodic memory module, before discussing how it is used at training and inference (prediction) time in \u00a7 SECREF3 .", "Our model consists of three main components: (i) an example encoder, (ii) a task decoder, and (iii) an episodic memory module. Figure FIGREF6 shows an illustration of our complete model. We describe each component in detail in the following.", "We show examples of retrieved neighbors from our episodic memory model in Appendix SECREF9 . We observe that the model manages to retrieve examples that are both syntactically and semantically related to a given query derived from a test example.", "MbPA INLINEFORM0 : an episodic memory model with randomly retrieved examples for local adaptation (no key network)."]}
{"question_id": "36cb7ebdd39e0b8a89ff946d3a3aef8a76a6bb43", "predicted_answer": "No, LSA-reduced n-gram features are not considered hand-crafted features in this context.\n\nThe text states that the improved model (RNNwA + n-gram) combines neural representation with \"hand-crafted features\", but it is clear that the LSA-reduced n-gram features are not considered hand-crafted features. Instead, they are a type of feature that is extracted and processed using a specific method (LSA reduction), and then concatenated with the neural representation.\n\nThe term \"hand-crafted features\" typically refers to features that are manually designed and engineered by humans, often using domain-specific knowledge and expertise. In this context, the LSA-reduced n-gram features are more akin to automatically extracted features, rather than hand-crafted features.", "predicted_evidence": ["For this model (denoted RNNwA + n-gram on results), n-gram features are collected with the same method described in BIBREF3. At the beginning, word level and character level n-gram features are obtained and concatenated. Then they are normalized with tf-idf transformation. For reducing the number of features and sparsity in n-gram vectors, tuples that have frequency less than 2 are ignored. For character level n-gram $N$ is selected as $3,4$, and 5 and for word level n-gram, $N$ is $1,2$ for Spanish and Arabic; $1,2,3$ for English. The dimension of the vector is reduced by LSA to 300. Then the vector is concatenated with neural representation which is produced right after tweet level attention in RNNwA model. The resultant representation is fed to a fully- connected layer that produces predictions.", "On the other hand, the improved model (RNNwA + n-gram), where neural and hand-crafted features are concatenated, increases the accuracy of the proposed model by approximately $0,5$% on English and approximately 2% in Spanish and Arabic. This also supports our intuition that the performance of neural models can be improved by hand-crafted features, which is based on the study of BIBREF14. As can be seen in Table TABREF11, the improved model outperforms the state-of-the-art method of BIBREF3 in English and produces competitive results in Spanish and Arabic.", "Most of the work on gender prediction rely on n-gram features BIBREF2. BIBREF3 give Latent Semantic Analysis (LSA)-reduced forms of word and character n-grams into Support Vector Machine (SVM) and achieve state-of-the-art performance. Apart from exploiting n-gram frequencies, there are studies BIBREF4, BIBREF5, BIBREF6 to extract cross-lingual features to determine gender from tweets. Some other work BIBREF4, BIBREF7 exploit user metadata besides using just tweets.", "In this work, we propose a model that relies on RNN with attention mechanism (RNNwA). A bidirectional RNN with attention mechanism both on word level and tweet level is trained with word embeddings. The final representation of the user is fed to a fully connected layer for prediction. Since combining some hand-crafted features with a learned linear layer has shown to perform well in complex tasks like Semantic Role Labeling (SRL) BIBREF14, an improved version of the model (RNNwA + n-gram) is also tested with hand-crafted features. In the improved version, LSA-reduced n-gram features are concatenated with the neural representation of the user. Then the result is fed into a fully-connected layer to make prediction. Models are tested in three languages; English, Spanish, and Arabic, and the improved version achieves state-of-the-art accuracy on English, and competitive results on Spanish and Arabic corpus.", "Recently, neural network-based models have been proposed to solve this problem. Rather than explicitly extracting features, the aim is to develop an architecture that implicitly learns. In author profiling, both style and content-based features were proved useful BIBREF8 and neural networks are able to capture both syntactic and semantic regularities. In general, syntactic information is drawn from the local context. On the other hand, semantic information is often captured with larger window sizes. Thus, CNNs are preferred to obtain style-based features while RNNs are the methods of choice for addressing content-based features BIBREF9. In literature, CNN BIBREF10 or RNN BIBREF11, BIBREF12, BIBREF13 is used on this task. BIBREF11 obtain state-of-the-art performance among neural methods by proposing a model architecture where they process text through RNN with GRU cells."]}
{"question_id": "28e50459da60ceda49fe1578c12f3f805b288bd0", "predicted_answer": "The performance of the model on English, Spanish, and Arabic is as follows:\n\n* English: The improved model (RNNwA + n-gram) increases the accuracy by approximately 0.5% compared to the baseline model.\n* Spanish: The improved model increases the accuracy by approximately 2% compared to the baseline model, producing competitive results.\n* Arabic: The improved model increases the accuracy by approximately 2% compared to the baseline model, producing competitive results.\n\nNote that the exact accuracy values are not provided in the given context, but the relative improvements are mentioned.", "predicted_evidence": ["On the other hand, the improved model (RNNwA + n-gram), where neural and hand-crafted features are concatenated, increases the accuracy of the proposed model by approximately $0,5$% on English and approximately 2% in Spanish and Arabic. This also supports our intuition that the performance of neural models can be improved by hand-crafted features, which is based on the study of BIBREF14. As can be seen in Table TABREF11, the improved model outperforms the state-of-the-art method of BIBREF3 in English and produces competitive results in Spanish and Arabic.", "[1]In their paper, authors report a result of 82.21 in English but we couldn't verify their accuracy in our repetitions by using their software and the same dataset. [2]Since their software is not provided, we directly take the accuracy values from their paper.", "Also, compared to the best neural model BIBREF11 where max pooling is used instead of an attention mechanism on the outputs of RNN, the proposed model (RNNwA) gives better results in terms of accuracy on English and Arabic datasets, and produces similar accuracy levels on Spanish dataset (Table TABREF11). These results show that an attention layer is able to learn \"where/how to look\" for features that are helpful in identifying the gender of a user.", "In Table TABREF10, user level accuracy results for the proposed model (RNNwA) along with the baseline models are given. As can be seen in the results, tweet level attention mechanism increases the score of all baseline models with the only exception of the CNNwA model in Arabic.", "For this model (denoted RNNwA + n-gram on results), n-gram features are collected with the same method described in BIBREF3. At the beginning, word level and character level n-gram features are obtained and concatenated. Then they are normalized with tf-idf transformation. For reducing the number of features and sparsity in n-gram vectors, tuples that have frequency less than 2 are ignored. For character level n-gram $N$ is selected as $3,4$, and 5 and for word level n-gram, $N$ is $1,2$ for Spanish and Arabic; $1,2,3$ for English. The dimension of the vector is reduced by LSA to 300. Then the vector is concatenated with neural representation which is produced right after tweet level attention in RNNwA model. The resultant representation is fed to a fully- connected layer that produces predictions."]}
{"question_id": "e1f61500eb733f2b95692b6a9a53f8aaa6f1e1f6", "predicted_answer": "This model is different from a traditional LSTM (Long Short-Term Memory) in several ways:\n\n1. **Bidirectional RNN with GRU cells**: The model uses a bidirectional RNN with GRU (Gated Recurrent Unit) cells, whereas LSTM is a specific type of RNN cell. GRU cells are similar to LSTM cells but have fewer parameters and are computationally more efficient.\n2. **Two-level attention mechanism**: The model has two attention layers: one at the word level and one at the tweet level. This is different from a traditional LSTM, which does not have an attention mechanism. The attention mechanism allows the model to focus on important words and tweets when making predictions.\n3. **Tweet-level attention**: The model uses a tweet-level attention mechanism to combine the signals from each tweet and create a final representation of a user. This is not typically done in LSTM models, which process sequences of data (e.g., words or characters) but do not have a built-in mechanism to combine outputs from multiple sequences.\n\nOverall, while the model uses a type of RNN (bidirectional RNN with GRU cells), the addition of the two-level attention mechanism and the tweet-level attention layer make it distinct from a traditional LSTM model.", "predicted_evidence": ["There are two different attention layers on the model. One is a word level attention where it amplifies the signal coming from important words, the other one is on tweet level where it combines the signals coming from each tweet and creates the final representation of a user.", "In order to measure the effectiveness of the attention mechanism, in addition to the CNN baseline model (CNNwA) and RNNwA, two new models (denoted as CNN and RNN) are created by removing the tweet level attention layer (word level attention stays the same) and generating a prediction for each tweet then just simply taking an average to give a user level prediction. Tweet level accuracies for these models are shown in Table TABREF9.", "In Table TABREF10, user level accuracy results for the proposed model (RNNwA) along with the baseline models are given. As can be seen in the results, tweet level attention mechanism increases the score of all baseline models with the only exception of the CNNwA model in Arabic.", "A bidirectional RNN with GRU BIBREF19 cells are used in this model where the number of cells is a hyperparameter. Among the tested range (50-150 with intervals of 25), best accuracy on validation set is obtained by 150 cells in English and 100 cells in Spanish and Arabic. An attention mechanism is used on word-level in addition to tweet-level to capture the important parts of each tweet as shown in Figure FIGREF2.", "where $W_\\alpha $ is a learnable weight matrix that is used to multiply each output of the RNN, $t_i$ is the feature vector of $i$th tweet, $b$ is a learnable bias vector, $w_i$ is a learnable attention weight, $A_i$ is the attention context vector, $v_i$ is the attention value for $i$th tweet, $o_i$ is attention output vector for the corresponding tweet, $K$ is the output vector for user. Matrix $W_\\alpha $ and vectors $w_i$ and $b$ are learned parameters."]}
{"question_id": "da4d07645edaf7494a8cb5216150a00690da01f7", "predicted_answer": "The cache consists of two layers: a static public cache ($T_P$) and a private cache ($T_D$). The static public cache stores the most frequent states, while the private cache is constructed on-the-fly and stores personalized states that are specific to a particular user and dialog session.", "predicted_evidence": ["where $T_D$ is the dynamic cache built on top of $T_P$. $T_D$ may need to copy some states from $T_P$ if we need to update information for those states in $T_P$.", "Furthermore, the private cache will not be freed after decoding a single utterance. The lifetime of a private cache actually can last for the entire dialog section for a specific user. The private cache keeps updating during the dialog session, making processing the subsequent utterances faster as more states are composed and stored in $T_D$. With this accumulated dynamic cache, a longer dialog can expect a better RTF in theory. In general, the static public cache serves all threads, while the private cache boosts the performance within a dialog session. The private cache will be freed at the end of the dialog.", "The static public cache stores the most frequent states, which greatly reduces the run time factor (RTF) of online decoding. Since $T_D$ has a smaller size than a fully dynamic graph, the marginal memory efficiency for multi-threaded service will be better.", "In order to support this mechanism, we use a two-layered cached FST for decoding. The first layer is public cache which represents $T_P$. It is a static cache created by pre-initialization. The second layer is the private cache, which is owned by a particular user and constructed on-the-fly. Figure FIGREF9 shows the architecture of our two-layer FST. The solid box denotes the static graph and the dashed ones show the dynamic graph. Personalized states will appear only in $T_D$.", "The lower graph of Figure FIGREF22 shows the memory usage as a function of the number of concurrent requests. Though the pre-composed graph may use more memory when we have only a small number of threads, the marginal memory cost for additional requests for a fully dynamic graph is roughly 1.5 times larger than for the pre-composed graph. The data-driven method has the best marginal memory efficiency for a large number of concurrent requests."]}
{"question_id": "c0cebef0e29b9d13c165b6f19f6ca8393348c671", "predicted_answer": "The model is not explicitly tested on specific languages. The text discusses speech recognition decoders and language models in a general sense, without mentioning specific languages. The use of IPA (International Phonetic Alphabet) phone set and graphemes suggests that the model is designed to be language-agnostic, but it is not clear which languages are actually supported.", "predicted_evidence": ["is the class language model transducer obtained by replacing the class labels in generic root FST $G_c$ with class FSTs $G_p$ for different classes, where $\\mathcal {C}$ denotes the set of all supported classes.", "Many state-of-the-art speech recognition decoders are based on the weighted finite state transducer (WFST) paradigm BIBREF0, BIBREF1. A conventional WFST decoder searches a statically composed $H C L G$ graph, where $H$ is the graph that translates HMM states to CD phones, $C$ translates CD phones to graphemes, $L$ translates graphemes to words and $G$ is graph that represents the language model. Using a statically composed graph has two limitations. First, it is both compute and memory intensive when the vocabulary and LM are large. Second, the static graph approach makes it hard to handle personalized language models BIBREF2. Many common tasks a user may want to perform with a voice assistant such as making phone calls, messaging to a specific contact or playing favorite music require a personalized language model. A dynamic WFST decoder is better suited for such cases.", "In Figure FIGREF17, the contact name is represented as monophone words using IPA phone set. SIL is added after each name in contact FST. Names with the same pronunciation also need to be handled using disambiguation symbols. In practice, because of accent and pronunciation variability, we have found that multiple pronunciations of OOV names are required in the personalized class FST.", "In previous work, a method was proposed to do a pre-initialized composition for a non-class LM BIBREF3. However, it the dynamic part is still expanded on-the-fly. In this work, we propose two improvements in order to best leverage class language models. First, we use simpler methods for pre-initialization which do not need to pre-generate decoder state statistics. Second, we propose a two-layer pre-initialization mechanism that also avoids performing dynamic expansion on per user basis. In the two-layer pre-initialization method, we make use of a class LM with class tag. We build a personalized FST that contains the members of the class for each user. Using the FST replacement algorithm, we obtain a personalized language transducer BIBREF4. We perform a pre-composition for all FST states whose transitions do not contain class tags.", "Many state-of-the-art speech recognition decoders are based on the weighted finite state transducer (WFST) paradigm BIBREF0, BIBREF1. A conventional WFST decoder searches a statically composed $H C L G$ graph, where $H$ is the graph that translates HMM states to CD phones, $C$ translates CD phones to graphemes, $L$ translates graphemes to words and $G$ is graph that represents the language model. Using a statically composed graph has two limitations. First, it is both compute and memory intensive when the vocabulary and LM are large. Second, the static graph approach makes it hard to handle personalized language models BIBREF2. Many common tasks a user may want to perform with a voice assistant such as making phone calls, messaging to a specific contact or playing favorite music require a personalized language model. A dynamic WFST decoder is better suited for such cases. As denoted in Eq (DISPLAY_FORM1), in a dynamic WFST decoder, $HCL$ is composed and optimized offline, while $G$ is composed on the fly with lazy (on-demand) composition, denoted as $\\circ $."]}
{"question_id": "5695908a8c6beb0e3863a1458a1b93aab508fd34", "predicted_answer": "A personalized language model (LM) is a language model that contains expected user-specific entities, such as names from a contact list, locations, or music titles, tailored to an individual user. This allows for improved speech recognition performance when the user's query involves infrequent or out-of-vocabulary entities.", "predicted_evidence": ["Speech input is now a common feature for smart devices. In many cases, the user's query involves entities such as a name from a contact list, a location, or a music title. Recognizing entities is particularly challenging for speech recognition because many entities are infrequent or out of the main vocabulary of the system. One way to improve performance is such cases is through the use of a personal language model (LM) which contains the expected user-specific entities. Because each user can have their own personalized LM, it is vital that the speech decoder be able to efficiently load the model on the fly, so it can be used in decoding, without any noticeable increase in latency.", "Many state-of-the-art speech recognition decoders are based on the weighted finite state transducer (WFST) paradigm BIBREF0, BIBREF1. A conventional WFST decoder searches a statically composed $H C L G$ graph, where $H$ is the graph that translates HMM states to CD phones, $C$ translates CD phones to graphemes, $L$ translates graphemes to words and $G$ is graph that represents the language model. Using a statically composed graph has two limitations. First, it is both compute and memory intensive when the vocabulary and LM are large. Second, the static graph approach makes it hard to handle personalized language models BIBREF2. Many common tasks a user may want to perform with a voice assistant such as making phone calls, messaging to a specific contact or playing favorite music require a personalized language model. A dynamic WFST decoder is better suited for such cases.", "In this work, we propose two improvements in order to best leverage class language models. First, we use simpler methods for pre-initialization which do not need to pre-generate decoder state statistics. Second, we propose a two-layer pre-initialization mechanism that also avoids performing dynamic expansion on per user basis. In the two-layer pre-initialization method, we make use of a class LM with class tag. We build a personalized FST that contains the members of the class for each user. Using the FST replacement algorithm, we obtain a personalized language transducer BIBREF4. We perform a pre-composition for all FST states whose transitions do not contain class tags. By doing so, the actual on-demand composition is only required for the states in personalized FST. For a multi-threaded service, the pre-composed FST can be shared by all threads, since it does not contain personalized FST states (non-terminals). The personalized part will be shared for all utterances from the same user, which will take full advantage of memory usage.", "In these experiments, speech recognition was performed using a hybrid LSTM-HMM framework. The acoustic model is an LSTM that consumes 40-dimensional log filterbank coefficients as the input and generates the posterior probabilities of 8000 tied context-dependent states as the output. The LM is a pruned 4-gram model trained using various semantic patterns that include a class label as well as a general purpose text corpus. The LM contains $@contact$ as an entity word, which will be replaced by the personalized contact FST. After pruning, the LM has 26 million n-grams.", "In previous work, a method was proposed to do a pre-initialized composition for a non-class LM BIBREF3. However, it the dynamic part is still expanded on-the-fly. In this work, we propose two improvements in order to best leverage class language models. First, we use simpler methods for pre-initialization which do not need to pre-generate decoder state statistics. Second, we propose a two-layer pre-initialization mechanism that also avoids performing dynamic expansion on per user basis. In the two-layer pre-initialization method, we make use of a class LM with class tag. We build a personalized FST that contains the members of the class for each user. Using the FST replacement algorithm, we obtain a personalized language transducer BIBREF4. We perform a pre-composition for all FST states whose transitions do not contain class tags."]}
{"question_id": "fa800a21469a70fa6490bfc67cabdcc8bf086fb5", "predicted_answer": "The answer is not explicitly stated in the given context, but it can be inferred that the dataset has been used in other work since the authors mention that the dataset was \"released\" and that they were able to retrieve most of the tweets using the provided tweet IDs. This suggests that the dataset was previously used or published in another study.", "predicted_evidence": ["The dataset was released as a list of $16,907$ tweet IDs along with their corresponding annotations. Using python's Tweepy library, we could only retrieve $16,202$ of the tweets since some of them have now been deleted or their visibility limited. Of the ones retrieved, 1,939 (12%) are labelled as racism, 3,148 (19.4%) as sexism, and the remaining 11,115 (68.6%) as none; this distribution follows the original dataset very closely (11.7%, 20.0%, 68.3%).", "Figure 3 further provides visualizations for authors from the sexism and none classes separately. While the authors from the none class are spread out in the embedding space, the ones from the sexism class are more tightly clustered. Note that we do not visualize the 5 authors from the racism class since 4 of them are already covered in the sexism class.", "Author profile (auth). In order to test whether community-based information of authors is in itself sufficient to correctly classify the content produced by them, we utilize just the author profiles we generated to train a gbdt classifier.", "Figure 1 shows example snippets from the community graph. Some authors belong to densely-connected communities (left figure), while others are part of more sparse ones (right figure). In either case, node2vec generates embeddings that capture the authors' neighborhood.", "Hidden-state + author profile (hs + auth) and Word-sum + author profile (ws + auth). These methods are identical to the char n-grams + author profile method except that here we append the author profiling features on to features derived from the hidden-state and word-sum baselines respectively and feed them to a gbdt classifier."]}
{"question_id": "6883767bbdf14e124c61df4f76335d3e91bfcb03", "predicted_answer": "The drawback to methods that rely on textual cues is that they may not be able to detect hate speech that is part of a larger hateful discourse or contains links to hateful content, but does not explicitly contain textual cues indicative of hate speech.", "predicted_evidence": ["We conduct a qualitative analysis of system errors and the cases where author profiling leads to the correct classification of previously misclassified examples. Table 3 shows examples of hateful tweets from the dataset that are misclassified by the lr method, but are correctly classified upon the addition of author profiling features, i.e., by the lr + auth method. It is worth noting that some of the wins scored by the latter are on tweets that are part of a larger hateful discourse or contain links to hateful content while not explicitly having textual cues that are indicative of hate speech per se. The addition of author profiling features may then be viewed as a proxy for wider discourse information, thus allowing us to correctly resolve the cases where lexical and semantic features alone are insufficient.", "The authors first utilize an lstm to task-tune glove-initialized word embeddings by propagating the error back from an lr layer. They then train a gradient boosted decision tree (gbdt) classifier to classify texts based on the average of the embeddings of constituent words. We make two minor modifications to this method: we use a 2-layer gru instead of the lstm to tune the embeddings, and we train the gbdt classifier on the l $_2$ -normalized sum of the embeddings instead of their average. Although the authors achieved state-of-the-art results on Twitter by initializing embeddings randomly rather than with glove (which is what we do here), we found the opposite when performing a 10-fold stratified cross-validation (cv). A possible explanation of this lies in the authors' decision to not use stratification, which for such a highly imbalanced dataset can lead to unexpected outcomes BIBREF18 . Furthermore, the authors train their lstm on the entire dataset (including the test set) without any early stopping criterion, which leads to over-fitting of the randomly-initialized embeddings.", "License details: http://creativecommons.org/licenses/by/4.0/. Hate speech, a term used to collectively refer to offensive language, racist comments, sexist remarks, etc., is omnipresent in social media. Users on social media platforms are at risk of being exposed to content that may not only be degrading but also harmful to their mental health in the long term. Pew Research Center highlighted the gravity of the situation via a recently released report BIBREF0 . As per the report, 40% of adult Internet users have personally experienced harassment online, and 60% have witnessed the use of offensive names and expletives. Expectedly, the majority (66%) of those who have personally faced harassment have had their most recent incident occur on a social networking website or app. While most of these websites and apps provide ways of flagging offensive and hateful content, only 8.8% of the victims have actually considered using such provisions. These statistics suggest that passive or manual techniques for curbing propagation of hateful content (such as flagging) are neither effective nor easily scalable BIBREF1 .", "Djuric et al. Djuric:2015:HSD:2740908.2742760 experimented with comments extracted from the Yahoo Finance portal and showed that distributional representations of comments learned using paragraph2vec BIBREF11 outperform simpler bag-of-words (bow) representations in a supervised classification setting for hate speech detection. Nobata et al. Nobata:2016:ALD:2872427.2883062 improved upon the results of Djuric et al. by training their classifier on a combination of features drawn from four different categories: linguistic (e.g., count of insult words), syntactic (e.g., pos tags), distributional semantic (e.g., word and comment embeddings) and bow-based (word and characters n-grams). They reported that while the best results were obtained with all features combined, character n-grams contributed more to performance than all the other features.", "Users on social media platforms are at risk of being exposed to content that may not only be degrading but also harmful to their mental health in the long term. Pew Research Center highlighted the gravity of the situation via a recently released report BIBREF0 . As per the report, 40% of adult Internet users have personally experienced harassment online, and 60% have witnessed the use of offensive names and expletives. Expectedly, the majority (66%) of those who have personally faced harassment have had their most recent incident occur on a social networking website or app. While most of these websites and apps provide ways of flagging offensive and hateful content, only 8.8% of the victims have actually considered using such provisions. These statistics suggest that passive or manual techniques for curbing propagation of hateful content (such as flagging) are neither effective nor easily scalable BIBREF1 . Consequently, the efforts to automate the detection and moderation of such content have been gaining popularity in natural language processing (nlp) BIBREF2 , BIBREF3 ."]}
{"question_id": "11679d1feba747c64bbbc62939a20fbb69ada0f3", "predicted_answer": "The community-based profiling features used are based on the similarity criterion that defines a community, such as:\n\n* Location\n* Age\n* Language\n\nThese features are used to generate author profiles, which are then used to train a classifier to predict the use of hateful or non-hateful language by an author.", "predicted_evidence": ["Author profiling has emerged as a powerful tool for NLP applications, leading to substantial performance improvements in several downstream tasks, such as text classification, sentiment analysis and author attribute identification BIBREF8 , BIBREF9 , BIBREF10 . The relevance of information gained from it is best explained by the idea of homophily, i.e., the phenomenon that people, both in real life as well as on the Internet, tend to associate more with those who appear similar. Here, similarity can be defined along various axes, e.g., location, age, language, etc. The strength of author profiling lies in that if we have information about members of a community $c$ defined by some similarity criterion, and we know that the person $p$ belongs to $c$ , we can infer information about $p$ .", "Author profiling has emerged as a powerful tool for NLP applications, leading to substantial performance improvements in several downstream tasks, such as text classification, sentiment analysis and author attribute identification BIBREF8 , BIBREF9 , BIBREF10 . The relevance of information gained from it is best explained by the idea of homophily, i.e., the phenomenon that people, both in real life as well as on the Internet, tend to associate more with those who appear similar. Here, similarity can be defined along various axes, e.g., location, age, language, etc. The strength of author profiling lies in that if we have information about members of a community $c$ defined by some similarity criterion, and we know that the person $p$ belongs to $c$ , we can infer information about $p$ . This concept has a straightforward application to our task: knowing that members of a particular community are prone to creating hateful content, and knowing that the author p is connected to this community, we can leverage information beyond linguistic cues and more accurately predict the use of hateful/non-hateful language from $p$ .", "Author profile (auth). In order to test whether community-based information of authors is in itself sufficient to correctly classify the content produced by them, we utilize just the author profiles we generated to train a gbdt classifier.", "In this paper, we answer these questions and develop novel methods that take into account community-based profiling features of authors when examining their tweets for hate speech. Experimenting with a dataset of $16k$ tweets, we show that the addition of such profiling features to the current state-of-the-art methods for hate speech detection significantly enhances their performance. We also release our code (including code that replicates previous work), pre-trained models and the resources we used in the public domain.", "The results are presented in Table 1 . For all three baseline methods (lr, ws, and hs), the addition of author profiling features significantly improves performance ( $p < 0.05$ under 10-fold cv paired t-test). The lr + auth method yields the highest performance of f $_1$ $=87.57$ , exceeding its respective baseline by nearly 4 points. A similar trend can be observed for the other methods as well. These results point to the importance of community-based information and author profiling in hate speech detection and demonstrate that our approach can further improve the performance of existing state-of-the-art methods."]}
{"question_id": "e0c80d31d590df46d33502169b1d32f0aa1ea6e3", "predicted_answer": "Based on the given context, the user traits that are taken into account include:\n\n1. Self-identity (e.g. \"republican\" or \"atheist\")\n2. Interests\n3. Personality\n4. Physical health\n5. Personal values (e.g. Family, Nature, Work-Ethic)\n6. Interpersonal affect\n7. Self-esteem\n\nThese traits are inferred from the user's profile, which is a sequence of tokens representing their plain text user description, as well as their activity history and the types of activities they engage in.", "predicted_evidence": ["A user's profile is a single document, also represented as a sequence of tokens. For each user, we populate the profile input using the plain text user description associated with their account, which often contains terms which express self-identity such as \u201crepublican\u201d or \u201cathiest.\u201d", "What a person does says a lot about who they are. Information about the types of activities that a person engages in can provide insights about their interests BIBREF0 , personality BIBREF1 , physical health BIBREF2 , the activities that they are likely to do in the future BIBREF3 , and other psychological phenomena like personal values BIBREF4 . For example, it has been shown that university students who exhibit traits of interpersonal affect and self-esteem are more likely to attend parties BIBREF5 , and those that value stimulation are likely to watch movies that can be categorized as thrillers BIBREF6 .", "Further, we explore the types of activity clusters that contain activities reported by users with high scores for various value dimensions. For a given value, we compute a score for each cluster INLINEFORM0 by taking the average INLINEFORM1 of all users who tweeted about doing activities in the cluster. For each value INLINEFORM2 , we can then rank all clusters by their INLINEFORM3 score. Examples of those with the highest scores are presented in Table TABREF28 . We observe that users whose profiles had high scores for Family were likely to report doing activities including family members, those with high scores for Nature tweeted about travel, and those with high Work-Ethic scores reported performing writing related tasks.", "While the attributes vector INLINEFORM0 can be used to encode any information of interest about a user, we choose to experiment with the use of personal values because of their theoretical connection to human activities BIBREF6 . In order to get a representation of a user's values, we turn to the hierarchical personal values lexicon from BIBREF24 . In this lexicon, there are 50 value dimensions, represented as sets of words and phrases that characterize that value. Since users' profiles often contain value-related content, we use the Distributed Dictionary Representations (DDR) method BIBREF25 to compute a score, INLINEFORM1 for each value dimension, INLINEFORM2 , using cosine similarity as follows: INLINEFORM3", "While our models are able to make predictions indicating that learning has taken place, it is clear that this prediction task is difficult. In the 50-class setup, the INLINEFORM0 model consistently had the strongest average per-class accuracy for all values of INLINEFORM1 and the lowest (best) ACR score (Table TABREF31 ). The INLINEFORM2 model performed nearly as well, showing that using only the human-activity relevant content from a user's history gives similar results to using the full set of content available. When including the attributes and profile for a user, the model typically overfits quickly and generalization deteriorates."]}
{"question_id": "7a8b24062a5bb63a8b4c729f6247a7fd2fec7f07", "predicted_answer": "According to the text, incorporating user traits (such as personal values) does not necessarily help the task. In fact, the model tends to overfit quickly and generalization deteriorates when including attributes and profile for a user. This suggests that using only the human-activity relevant content from a user's history is sufficient and may even be more effective than incorporating additional user traits.", "predicted_evidence": ["While our models are able to make predictions indicating that learning has taken place, it is clear that this prediction task is difficult. In the 50-class setup, the INLINEFORM0 model consistently had the strongest average per-class accuracy for all values of INLINEFORM1 and the lowest (best) ACR score (Table TABREF31 ). The INLINEFORM2 model performed nearly as well, showing that using only the human-activity relevant content from a user's history gives similar results to using the full set of content available. When including the attributes and profile for a user, the model typically overfits quickly and generalization deteriorates.", "A user's profile is a single document, also represented as a sequence of tokens. For each user, we populate the profile input using the plain text user description associated with their account, which often contains terms which express self-identity such as \u201crepublican\u201d or \u201cathiest.\u201d", "While the attributes vector INLINEFORM0 can be used to encode any information of interest about a user, we choose to experiment with the use of personal values because of their theoretical connection to human activities BIBREF6 . In order to get a representation of a user's values, we turn to the hierarchical personal values lexicon from BIBREF24 . In this lexicon, there are 50 value dimensions, represented as sets of words and phrases that characterize that value. Since users' profiles often contain value-related content, we use the Distributed Dictionary Representations (DDR) method BIBREF25 to compute a score, INLINEFORM1 for each value dimension, INLINEFORM2 , using cosine similarity as follows: INLINEFORM3", "Further, we explore the types of activity clusters that contain activities reported by users with high scores for various value dimensions. For a given value, we compute a score for each cluster INLINEFORM0 by taking the average INLINEFORM1 of all users who tweeted about doing activities in the cluster. For each value INLINEFORM2 , we can then rank all clusters by their INLINEFORM3 score. Examples of those with the highest scores are presented in Table TABREF28 . We observe that users whose profiles had high scores for Family were likely to report doing activities including family members, those with high scores for Nature tweeted about travel, and those with high Work-Ethic scores reported performing writing related tasks.", "Finally, our model allows the inclusion of any additional attributes that might be known or inferred in order to aid the prediction task, which can be passed to the model as a INLINEFORM0 dimensional real-valued vector. For instance, we can use personal values as a set of attributes, as described in Section SECREF26 ."]}
{"question_id": "cab082973e1648b0f0cc651ab4e0298a5ca012b5", "predicted_answer": "According to the text, there are nearly 30,000 queries (or activities) in the dataset, which were collected from two sources: the Event2Mind dataset and a set of short activity surveys.", "predicted_evidence": ["We make the activity clusters, models, and code for the prediction task available at http://lit.eecs.umich.edu/downloads.html", "Given that the space of possible phrases describing human activities is nearly limitless, we propose a set of human activity clusters that summarize a large set of several hundred-thousand self-reported activities. We then construct predictive models that are able to estimate the likelihood that a user has reported that they have performed an activity from any cluster.", "Given a set of activity clusters and knowledge about the users who have reported to have participated in these activities, we explore the ability of machine learning models to make inferences about which activities are likely to be next performed by a user. Here we describe the supervised learning setup, evaluation, and neural architecture used for the prediction task.", "We evaluate our activity prediction models using a number of metrics that consider not only the most likely cluster, but also the set of INLINEFORM0 most likely clusters. First, we evaluate the average per-class accuracy of the model's ability to rank INLINEFORM1 , the target cluster, within the top INLINEFORM2 clusters. These scores tell us how well the model is able to make predictions about the kinds of activities that each user is likely to do.", "Therefore, in order to target only those tweets that are rich in human activity content, we formulate a set of queries that allows us to use the Twitter Search API to find instances of users tweeting about common human activities. Each query contains a first-person, past-tense verb within a phrase that describes a common activity that people do. Using this approach, we are able to retrieve a set of tweets that contains a high concentration of human activity content, and we also find that users who wrote these tweets are much more likely to have written other tweets that describe human activities (Table TABREF1 ). We build our set of human activity queries from two sources: the Event2Mind dataset BIBREF15 and a set of short activity surveys, which we collect ourselves, to obtain nearly 30K queries (Table TABREF2 ) ."]}
{"question_id": "1cc394bdfdfd187fc0af28500ad47a0a764d5645", "predicted_answer": "The annotators of the Event2Mind dataset are not explicitly mentioned in the given context. The text only describes how the dataset was processed and filtered, but it does not provide information about who performed the annotation.", "predicted_evidence": ["where INLINEFORM0 is a representation of a set of vectors, which, for the DDR method, is defined as the mean vector of the set; INLINEFORM1 is a set of word embeddings, one for each token in the user's profile; and INLINEFORM2 is another set of word embeddings, one for each token in the lexicon for value dimension INLINEFORM3 . Finally, we set INLINEFORM4 where INLINEFORM5 , the number of value dimensions in the lexicon. Examples of profiles with high scores for sample value dimensions are shown in Table TABREF27 .", "This layer takes the sequence INLINEFORM0 as input and produces a single INLINEFORM1 dimensional vector, INLINEFORM2 , as output, intended to represent high-level features extracted from the entire history of the user.", "where INLINEFORM0 is the number of training instances belonging to class INLINEFORM1 . We evaluate our model on the development data after each epoch and save the model with the highest per-class accuracy. Finally, we compute the results on the test data using this model, and report these results.", "The Event2Mind dataset contains a large number of event phrases which are annotated for intent and reaction. The events themselves come from four sources of phrasal events (stories, common n-grams found in web data, blogs, and English idioms), and many of them fall under our classification of human activities, making Event2Mind a great resource in our search for concrete examples of human activities. We consider events for which a person is the subject (e.g, \u201cPersonX listens to PersonX's music\u201d) to be human activities, and remove the rest (e.g., \u201cIt is Christmas morning\u201d). We then use several simple rules to convert the Event2Mind instances into first-person past-tense activities. Since all events were already filtered so that they begin with \u201cPersonX\u201d, we replace the first occurrence of \u201cPersonX\u201d in each event with \u201cI\u201d and all subsequent occurrences with \u201cme\u201d.", "A user's profile is a single document, also represented as a sequence of tokens. For each user, we populate the profile input using the plain text user description associated with their account, which often contains terms which express self-identity such as \u201crepublican\u201d or \u201cathiest.\u201d"]}
{"question_id": "16cc37e4f8e2db99eaf89337a3d9ada431170d5b", "predicted_answer": "The data instances were chosen by splitting the data at the user-level, with 200,000 instances used for training, 10,000 for testing, and the rest for validation.", "predicted_evidence": ["We split our data at the user-level, and from our set of valid users we use 200,000 instances for training data, 10,000 as test data, and the rest as our validation set.", "This research was supported in part through computational resources and services provided by the Advanced Research Computing at the University of Michigan. This material is based in part upon work supported by the Michigan Institute for Data Science, by the National Science Foundation (grant #1815291), by the John Templeton Foundation (grant #61156), and by DARPA (grant #HR001117S0026-AIDA-FP-045). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the Michigan Institute for Data Science, the National Science Foundation, the John Templeton Foundation, or DARPA. Many thanks to the anonymous reviewers who provided helpful feedback.", "where INLINEFORM0 is the number of training instances belonging to class INLINEFORM1 . We evaluate our model on the development data after each epoch and save the model with the highest per-class accuracy. Finally, we compute the results on the test data using this model, and report these results.", "where INLINEFORM0 is a set of activity clusters, INLINEFORM1 , INLINEFORM2 , and INLINEFORM3 are vectors that represent the user's history, profile, and attributes, respectively, and INLINEFORM4 is the target cluster. The target cluster is the cluster label of an activity cluster that contains an activity that is known to have been performed by the user.", "In practice, however, we find that these metrics are strongly correlated (either positively or negatively) with the INLINEFORM3 , making it difficult to quantitatively compare the results of using a different number of clusters, and we therefore make a decision based on a qualitative analysis of the clusters. For the purpose of making these kinds of predictions about clusters, it is beneficial to have a smaller number of larger clusters, but clusters that are too large are no longer meaningful since they contain sets of activities that are less strongly related to one another. In the end, we find that using INLINEFORM4 clusters leads to a good balance between cluster size and specificity, and we use this configuration for our prediction experiments moving forward. Examples of activities that were assigned the same cluster label are shown in Table TABREF15 , and Table TABREF16 illustrates the notion of distance within our newly defined semantic space of human activities."]}
{"question_id": "cc78a08f5bfe233405c99cb3dac1f11f3a9268b1", "predicted_answer": "The social media platform from which the data was collected is Twitter.", "predicted_evidence": ["While we do not expect to know exactly what a person is doing at any given time, it is fairly common for people to publicly share the types of activities that they are doing by making posts, written in natural language, on social media platforms like Twitter. However, when taking a randomly sampled stream of tweets, we find that only a small fraction of the content was directly related to activities that the users were doing in the real world \u2013 instead, most instances are more conversational in nature, or contain the sharing of opinions about the world or links to websites or images. Using such a random sample would require us to filter out a large percentage of the total data collected, making the data collection process inefficient.", "Therefore, in order to target only those tweets that are rich in human activity content, we formulate a set of queries that allows us to use the Twitter Search API to find instances of users tweeting about common human activities. Each query contains a first-person, past-tense verb within a phrase that describes a common activity that people do. Using this approach, we are able to retrieve a set of tweets that contains a high concentration of human activity content, and we also find that users who wrote these tweets are much more likely to have written other tweets that describe human activities (Table TABREF1 ). We build our set of human activity queries from two sources: the Event2Mind dataset BIBREF15 and a set of short activity surveys, which we collect ourselves, to obtain nearly 30K queries (Table TABREF2 ) .", "Several studies have applied computational approaches to the understanding and modeling of human behavior at scale BIBREF7 and in real time BIBREF8 . However, this previous work has mainly relied on specific devices or platforms that require structured definitions of behaviors to be measured. While this leads to an accurate understanding of the types of activities being done by the involved users, these methods capture a relatively narrow set of behaviors compared to the huge range of things that people do on a day-to-day basis. On the other hand, publicly available social media data provide us with information about an extremely rich and diverse set of human activities, but the data are rarely structured or categorized, and they mostly exist in the form of natural language. Recently, however, natural language processing research has provided several examples of methodologies for extracting and representing human activities from text BIBREF9 , BIBREF10 and even multimodal data BIBREF11 .", "For our final dataset, we also filter our set of users. From the set of users who posted at least one valid queried tweet, we remove those who had empty user profiles, those with less than 25 additional tweets, and those with less than 5 additional activities (Table TABREF12 ).", "Further, we use a simple pattern-matching approach to extract additional activities from these additional tweets. We search for strings that match I <VBD> .* <EOS> where <VBD> is any past-tense verb, .* matches any string (non-greedy), and <EOS> matches the end of a sentence. We then perform the same filtering as before for indications that the person did not actually do the activity, and we refer to these filtered matches as our set of additional activities (see Table TABREF11 for more information). Note that since these additional activities can contain any range of verbs, they are naturally noisier than our set of valid query tweets, and we therefore do not treat them as a reliable \u201cground truth\u201d source of self-reported human activities, but as a potentially useful signal of activity-related information that can be associated with users in our dataset."]}
{"question_id": "101d7a355e8bf6d1860917876ee0b9971eae7a2f", "predicted_answer": "The answer is not explicitly stated in the given context, but based on the mention of \"tweets\" and \"hashtags\", it can be inferred that the results are likely reported only for English data, as Twitter is a platform that is predominantly used in English. However, without further information, it cannot be confirmed with certainty.", "predicted_evidence": ["This test essentially captures the importance of \u201cnatural word order\u201d. We found that LDA was invariant to the reordering of the words in the tweet for most of the tasks. This result is not surprising as LDA considers each word in the tweet independently. CNN, LSTM and BLSTM rely on the word order significantly to perform well for most of the prediction tasks.", "In this section we list down the set of models considered in the study.", "[11] Ritter, A., Clark, S., Mausam, & Etzioni, O.: Named entity recognition in tweets: an experimental study. In: EMNLP. (2011) 1524-1534", "We summarize the results of property prediction tasks in Table TABREF31 . Length prediction turns out to be a difficult task for most of the models. Models which rely on the recurrent architectures such as LSTM, STV, T2V have sufficient capacity to perform well in modeling the tweet length. Also BLSTM is the best in modeling slang words. BLSTM outperforms the LSTM variant in all the tasks except `Content', which signifies the power of using the information flowing from both the directions of the tweet. T2V which is expected to perform well in this task because of its ability to work at a more fine level (i.e., characters) performs the worst. In fact T2V does not outperform other models in any task, which could be mainly due to the fact that the hashtags which are used for supervision in learning tweet representations reduces the generalization capability of the tweets beyond hashtag prediction.", "We summarize the results of property prediction tasks in Table TABREF31 . Length prediction turns out to be a difficult task for most of the models. Models which rely on the recurrent architectures such as LSTM, STV, T2V have sufficient capacity to perform well in modeling the tweet length. Also BLSTM is the best in modeling slang words. BLSTM outperforms the LSTM variant in all the tasks except `Content', which signifies the power of using the information flowing from both the directions of the tweet. T2V which is expected to perform well in this task because of its ability to work at a more fine level (i.e., characters) performs the worst. In fact T2V does not outperform other models in any task, which could be mainly due to the fact that the hashtags which are used for supervision in learning tweet representations reduces the generalization capability of the tweets beyond hashtag prediction. Prediction tasks such as `Content' and `Hashtag' seem to be less difficult as all the models perform nearly optimal for them."]}
{"question_id": "4288621e960ffbfce59ef1c740d30baac1588b9b", "predicted_answer": "The authors do not draw conclusions from their experiments in this section. This section appears to be an introduction or overview of the paper, and the conclusions are likely to be presented in Section 5, which is mentioned as the \"result analysis\" section.", "predicted_evidence": ["In this section we list down the set of models considered in the study.", "Our main contributions are summarized below.", "In this section we perform an extensive evaluation of all the models in an attempt to find the significance of different representation models. Essentially we study every model (with optimal settings reported in the corresponding paper) with respect to the following three perspectives.", "[6] Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun, R., Torralba, A., & Fidler, S.: Skip-thought vectors. In: NIPS. (2015) 3294-3302", "The paper is organized as follows. Sections 2 and 3 discuss the set of proposed elementary property prediction tasks and the models considered for this study respectively. Section 4 and 5 presents the experiment setup and result analysis respectively. We conclude the work with a brief summary in Section 5."]}
{"question_id": "c3befe7006ca81ce64397df654c31c11482dafbe", "predicted_answer": "Each classifier evaluates one of the syntactic or social properties by attempting to predict the property based on the tweet representation. If the classifier performs well in predicting the property, it indicates that the property is encoded in the representation. For example, a classifier that predicts tweet length would evaluate the syntactic property of tweet length, while a classifier that predicts whether a tweet is a reply would evaluate the social property of \"is reply\".", "predicted_evidence": ["Essentially we ask the following question: \u201cWhat are the core properties encoded in the given tweet representation?\u201d. We explicitly group the set of these properties into two categories: syntactic and social. Syntactic category includes properties such as tweet length, the order of words in it, the words themselves, slang words, hashtags and named entities in the tweet. On the other hand, social properties consist of `is reply', and `reply time'. We investigate the degree to which the tweet representations encode these properties. We assume that if we cannot train a classifier to predict a property based on its tweet representation, then this property is not encoded in this representation. For example, the model which preserves the tweet length should perform well in predicting the length given the representation generated from the model.", "Essentially we ask the following question: \u201cWhat are the core properties encoded in the given tweet representation?\u201d. We explicitly group the set of these properties into two categories: syntactic and social. Syntactic category includes properties such as tweet length, the order of words in it, the words themselves, slang words, hashtags and named entities in the tweet. On the other hand, social properties consist of `is reply', and `reply time'. We investigate the degree to which the tweet representations encode these properties. We assume that if we cannot train a classifier to predict a property based on its tweet representation, then this property is not encoded in this representation. For example, the model which preserves the tweet length should perform well in predicting the length given the representation generated from the model. Though these elementary property prediction tasks are not directly related to any downstream application, knowing that the model is good at modeling a particular property (e.g., the social properties) indicates that it could excel in correlated applications (e.g., user profiling task).", "We explicitly group the set of these properties into two categories: syntactic and social. Syntactic category includes properties such as tweet length, the order of words in it, the words themselves, slang words, hashtags and named entities in the tweet. On the other hand, social properties consist of `is reply', and `reply time'. We investigate the degree to which the tweet representations encode these properties. We assume that if we cannot train a classifier to predict a property based on its tweet representation, then this property is not encoded in this representation. For example, the model which preserves the tweet length should perform well in predicting the length given the representation generated from the model. Though these elementary property prediction tasks are not directly related to any downstream application, knowing that the model is good at modeling a particular property (e.g., the social properties) indicates that it could excel in correlated applications (e.g., user profiling task). In this work we perform an extensive evaluation of 9 unsupervised and 4 supervised tweet representation models, using 8 different properties.", "On the other hand, social properties consist of `is reply', and `reply time'. We investigate the degree to which the tweet representations encode these properties. We assume that if we cannot train a classifier to predict a property based on its tweet representation, then this property is not encoded in this representation. For example, the model which preserves the tweet length should perform well in predicting the length given the representation generated from the model. Though these elementary property prediction tasks are not directly related to any downstream application, knowing that the model is good at modeling a particular property (e.g., the social properties) indicates that it could excel in correlated applications (e.g., user profiling task). In this work we perform an extensive evaluation of 9 unsupervised and 4 supervised tweet representation models, using 8 different properties. The most relevant work is that of Adi et al.", "On the other hand, social properties consist of `is reply', and `reply time'. We investigate the degree to which the tweet representations encode these properties. We assume that if we cannot train a classifier to predict a property based on its tweet representation, then this property is not encoded in this representation. For example, the model which preserves the tweet length should perform well in predicting the length given the representation generated from the model. Though these elementary property prediction tasks are not directly related to any downstream application, knowing that the model is good at modeling a particular property (e.g., the social properties) indicates that it could excel in correlated applications (e.g., user profiling task). In this work we perform an extensive evaluation of 9 unsupervised and 4 supervised tweet representation models, using 8 different properties. The most relevant work is that of Adi et al. [5], which investigates three sentence properties in comparing unsupervised sentence representation models such as average of words vectors and LSTM auto-encoders."]}
{"question_id": "5d0a3f8ca3882f87773cf8c2ef1b4f72b9cc241e", "predicted_answer": "The per-word reward is tuned using a perceptron-like tuning method, which is a quick post-training step. This method is used to optimize the word reward score, and the results show that the optimal value is sensitive to both task and beam size, implying that it is important to tune for every model trained.", "predicted_evidence": ["Finally, instead of tuning the word reward using grid search, we introduce a way to learn it using a perceptron-like tuning method. We show that the optimal value is sensitive both to task and beam size, implying that it is important to tune for every model trained. Fortunately, tuning is a quick post-training step.", "Above, we have shown that fixing the length problem with a word reward score fixes the beam problem. However these results are contingent upon choosing an adequate word reward score, which we have done in our experiments by optimization using a perceptron loss. Here, we show the sensitivity of systems to the value of this penalty, as well as the fact that there is not one correct penalty for all tasks. It is dependent on a myriad of factors including, beam size, dataset, and language pair.", "However, when we tune our word reward score with a beam of 1000, the problem mostly goes away. Over the uncorrected baseline, we see a 22.0 BLEU point difference for a beam of 1000. Over the uncorrected baseline with a beam of 10, the corrected beam of 1000 gets a BLEU gain of 0.8 BLEU. However, the beam of 1000 still sees a drop of less than 1.0 BLEU over the best corrected version. The word reward method beats the uncorrected baseline and the length normalization correction in almost all cases.", "The results of tuning the word reward, INLINEFORM0 , as described in Section SECREF6 , is shown in the second section of Tables TABREF10 , TABREF11 , and TABREF12 . In contrast to our baseline systems, our tuned word reward always fixes the brevity problem (length ratios are approximately 1.0), and generally fixes the beam problem. An optimized word reward score always leads to improvements in METEOR scores over any of the best baselines. Across all language pairs, reward and norm have close METEOR scores, though the reward method wins out slightly. BLEU scores for reward and norm also increase over the baseline in most cases, despite BLEU's inherent bias towards shorter sentences. Most notably, whereas the baseline Russian\u2013English system lost more than 20 BLEU points when the beam was increased to 1000, our tuned reward score resulted in a BLEU gain over any baseline beam size. Whereas in our baseline systems, the length ratio decreases with larger beam sizes, our tuned word reward results in length ratios of nearly 1.0 across all language pairs, mitigating many of the issues of the brevity problem.", "The results of tuning the word reward, INLINEFORM0 , as described in Section SECREF6 , is shown in the second section of Tables TABREF10 , TABREF11 , and TABREF12 . In contrast to our baseline systems, our tuned word reward always fixes the brevity problem (length ratios are approximately 1.0), and generally fixes the beam problem. An optimized word reward score always leads to improvements in METEOR scores over any of the best baselines. Across all language pairs, reward and norm have close METEOR scores, though the reward method wins out slightly. BLEU scores for reward and norm also increase over the baseline in most cases, despite BLEU's inherent bias towards shorter sentences. Most notably, whereas the baseline Russian\u2013English system lost more than 20 BLEU points when the beam was increased to 1000, our tuned reward score resulted in a BLEU gain over any baseline beam size."]}
{"question_id": "dce27c49b9bf1919ca545e04663507d83bb42dbe", "predicted_answer": "According to the text, there are three methods in common use to correct the brevity problem:\n\n1. Reward method: This involves using a tuned constant word reward to encourage the model to generate longer sentences.\n2. Normalization method (Norm): This involves dividing a hypothesis' score by its length to normalize the scores and favor longer sentences.\n3. Globally-normalized models: This is a more general solution to the label bias problem, which includes the brevity problem as a special case.", "predicted_evidence": ["Another way to demonstrate that the beam problem is the same as the brevity problem is to look at the translations generated by baseline systems on shorter sentences. Figure FIGREF18 shows the BLEU scores of the Russian\u2013English system for beams of size 10 and 1000 on sentences of varying lengths, with and without correcting lengths. The x-axes of the figure are cumulative: length 20 includes sentences of length 0\u201320, while length 10 includes 0\u201310. It is worth noting that BLEU is a word-level metric, but the systems were built using BPE; so the sequences actually generated are longer than the x-axes would suggest.", "Here, we first show that the beam problem is indeed the brevity problem. We then demonstrate that solving the length problem does solve the beam problem. Tables TABREF10 , TABREF11 , and TABREF12 show the results of our German\u2013English, Russian\u2013English, and French\u2013English systems respectively. Each table looks at the impact on BLEU, METEOR, and the ratio of the lengths of generated sentences compared to the gold lengths BIBREF22 , BIBREF23 . The baseline method is a standard model without any length correction. The reward method is the tuned constant word reward discussed in the previous section. Norm refers to the normalization method, where a hypothesis' score is divided by its length.", "We have argued that the brevity problem is an example of label bias, and that the solution is a very limited form of globally-normalized model. These can be seen as the simplest case of the more general problem of label bias and the more general solution of globally-normalized models for NMT BIBREF24 , BIBREF25 , BIBREF26 , BIBREF13 . Some questions for future research are:", "To our knowledge, there are three methods in common use for adjusting the model to favor longer sentences.", "We have explored simple and effective ways to alleviate or eliminate the beam problem. We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model. We compared two corrections to the model and introduced a method to learn the parameters of these corrections. Because this method is helpful and easy, we hope to see it included to make stronger baseline NMT systems."]}
{"question_id": "991ea04072b3412928be5e6e903cfa54eeac3951", "predicted_answer": "Wider beam search hurts NMT because it exacerbates the brevity problem, which is a result of the locally-normalized structure of the model. When the beam size is increased, more partial translations are retained, including shorter ones that may have higher probabilities due to the brevity problem. These shorter translations can then block longer, more accurate translations from being generated, leading to a decrease in translation accuracy.", "predicted_evidence": ["We have explored simple and effective ways to alleviate or eliminate the beam problem. We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model. We compared two corrections to the model and introduced a method to learn the parameters of these corrections. Because this method is helpful and easy, we hope to see it included to make stronger baseline NMT systems.", "In this example, INLINEFORM0 , even though overestimated, is still lower than INLINEFORM1 , and wins only because its suffixes have higher probability. Greedy search would prune the incorrect prefix an and yield the correct output. In general, then, we might expect greedy or beam search to alleviate some symptoms of label bias. Namely, a prefix with a low-entropy suffix distribution can be pruned if its probability is, even though overestimated, not among the highest probabilities. Such an observation was made by BIBREF11 in the context of dependency parsing, and we will see next that precisely such a situation affects output length in NMT.", "To address the brevity problem, many designers of NMT systems add corrections to the model. These corrections are often presented as modifications to the search procedure. But, in our view, the brevity problem is essentially a modeling problem, and these corrections should be seen as modifications to the model (Section SECREF5 ). Furthermore, since the root of the problem is local normalization, our view is that these modifications should be trained as globally-normalized models (Section SECREF6 ).", "As in our label-bias example, greedy search would prune the incorrect empty translation. More generally, consider beam search: at time step INLINEFORM0 , only the top INLINEFORM1 partial or complete translations are retained while the rest are pruned. (Implementations of beam search vary in the details, but this variant is simplest for the sake of argument.) Even if a translation ending at time INLINEFORM2 scores higher than a longer translation, as long as it does not fall within the top INLINEFORM3 when compared with partial translations of length INLINEFORM4 (or complete translations of length at most INLINEFORM5 ), it will be pruned and unable to block the longer translation. But if we widen the beam ( INLINEFORM6 ), then translation accuracy will suffer. We call this problem (which is BIBREF0 's sixth challenge) the beam problem. Our claim, hinted at by BIBREF0 , is that the brevity problem and the beam problem are essentially the same, and that solving one will solve the other.", "We note that the beam problem in NMT exists for relatively small beam sizes \u2013 especially when compared to traditional beam sizes in SMT systems. On our medium-resource Russian\u2013English system, we investigate the full impact of this problem using a much larger beam size of 1000. In Table TABREF10 , we can see that the beam problem is particularly pronounced. The first row of the table shows the uncorrected, baseline score. From a beam of 10 to a beam of 1000, the drop in BLEU scores is over 20 points. This is largely due to the brevity problem discussed earlier. The second row of the table shows the length of the translated outputs compared to the lengths of the correct translations. Though the problem persists even at a beam size of 10, at a beam size of 1000, our baseline system generates less than one third the number of words that are in the correct translations. Furthermore, 37.3% of our translated outputs have sentences of length 0."]}
{"question_id": "a82a12a22a45d9507bc359635ffe9574f15e0810", "predicted_answer": "The conventional method uses a binary classification model.", "predicted_evidence": ["We collected INLINEFORM0 TED Talk transcripts. An example transcription is given in Figure FIGREF4 . The collected transcripts were split into sentences using the Stanford CoreNLP tool BIBREF11 . In this study, sentences containing or immediately followed by `(Laughter)' were used as `Laughter' sentences, as shown in Figure FIGREF4 ; all other sentences were defined as `No-Laughter' sentences. Following BIBREF1 and BIBREF3 , we selected the same numbers ( INLINEFORM1 ) of `Laughter' and `No-Laughter' sentences. To minimize possible topic shifts between positive and negative instances, for each positive instance, we picked one negative instance nearby (the context window was 7 sentences in this study). For example, in Figure FIGREF4 , a negative instance (corresponding to `sent-2') was selected from the nearby sentences ranging from `sent-7' to `sent+7'.", "We used two corpora: the TED Talk corpus (denoted as TED) and the Pun of the Day corpus (denoted as Pun). Note that we normalized words in the Pun data to lowercase to avoid a possibly elevated result caused by a special pattern: in the original format, all negative instances started with capital letters. The Pun data allows us to verify that our implementation is consistent with the work reported in yang-EtAl:2015:EMNLP2.", "In most of the previous studies BIBREF1 , BIBREF2 , BIBREF3 , humor recognition was modeled as a binary classification task. In the seminal work BIBREF1 , a corpus of INLINEFORM0 \u201cone-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website. BIBREF3 explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style. In addition, Word2Vec BIBREF4 distributed representations were utilized in the model building.", "Stemming from the present study, we envision that more research is worth pursuing: (a) for presentations, cues from other modalities such as audio or video will be included, similar to Bertero2016LREC; (b) context information from multiple utterances will be modeled by using sequential modeling methods.", "The remainder of the paper is organized as follows: Section SECREF2 briefly reviews the previous related research; Section SECREF3 describes the corpus we collected from TED talks; Section SECREF4 describes the text classification methods; Section SECREF5 reports on our experiments; finally, Section SECREF6 discusses the findings of our study and plans for future work."]}
{"question_id": "355cf303ba61f84b580e2016fcb24e438abeafa7", "predicted_answer": "What is novel about the newly emerging CNN method, in comparison to well-established conventional method, is its ability to automatically learn its optimal feature representation and adjust the features automatically across data sets, making it quite versatile for supporting different tasks and data domains. This is in contrast to the conventional method, which requires manual feature engineering and takes days or even weeks to generate and implement features.", "predicted_evidence": ["Next, the embedding matrix was fed into a INLINEFORM5 convolution network with multiple filters. To cover varied reception fields, we used filters of sizes of INLINEFORM6 , INLINEFORM7 , and INLINEFORM8 . For each filter size, INLINEFORM9 filters were utilized. Then, max pooling, which stands for finding the largest value from a vector, was applied to each feature map (total INLINEFORM10 feature maps) output by the INLINEFORM11 convolution. Finally, maximum values from all of INLINEFORM12 filters were formed as a flattened vector to go through a fully connected (FC) layer to predict two possible labels (Laughter vs. No-Laughter). Note that for INLINEFORM13 convolution and FC layer's input, we applied `dropout' BIBREF10 regularization, which entails randomly setting a proportion of network weights to be zero during model training, to overcome over-fitting. By using cross-entropy as the learning metric, the whole sequential network (all weights and bias) could be optimized by using any SGD optimization, e.g., Adam BIBREF13 , Adadelta BIBREF14 , and so on.", "On the Pun data, the CNN model shows consistent improved performance over the conventional model, as suggested in BIBREF3 . In particular, precision has been greatly increased from INLINEFORM0 to INLINEFORM1 . On the TED data, we also observed that the CNN model helps to increase precision (from INLINEFORM2 to INLINEFORM3 ) and accuracy (from INLINEFORM4 to INLINEFORM5 ). The empirical evaluation results suggest that the CNN-based model has an advantage on the humor recognition task. In addition, focusing on the system development time, generating and implementing those features in the conventional model would take days or even weeks. However, the CNN model automatically learns its optimal feature representation and can adjust the features automatically across data sets. This makes the CNN model quite versatile for supporting different tasks and data domains. Compared with the humor recognition results on the Pun data, the results on the TED data are still quite low, and more research is needed to fully handle humor in authentic presentations.", "Our CNN-based text classification's setup follows Kim2014. Figure FIGREF17 depicts the model's details. From the left side's input texts to the right side's prediction labels, different shapes of tensors flow through the entire network for solving the classification task in an end-to-end mode.", "From the brief review, it is clear that corpora used in humor research so far are limited to one-line puns or jokes and conversations from TV comedy shows. There is a great need for an open corpus that can support investigating humor in presentations. CNN-based text categorization methods have been applied to humor recognition (e.g., in BIBREF5 ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in yang-EtAl:2015:EMNLP2 is missing; (b) CNN's performance in the previous research is not quite clear; and (c) some important techniques that can improve CNN performance (e.g., using varied-sized filters and dropout regularization BIBREF10 ) were not applied. Therefore, the present study is meant to address these limitations.", "For the purpose of monitoring how well speakers can use humor during their presentations, we have created a corpus from TED talks. Compared to the existing (albeit limited) corpora for humor recognition research, ours has the following advantages: (a) it was collected from authentic talks, rather than from TV shows performed by professional actors based on scripts; (b) it contains about 100 times more speakers compared to the limited number of actors in existing corpora. We compared two types of leading text-based humor recognition methods: a conventional classifier (e.g., Random Forest) based on human-engineered features vs. an end-to-end CNN method, which relies on its inherent representation learning. We found that the CNN method has better performance. More importantly, the representation learning of the CNN method makes it very efficient when facing new data sets."]}
{"question_id": "88757bc49ccab76e587fba7521f0981d6a1af2f7", "predicted_answer": "According to the given context, the lexical cues used for humor recognition include:\n\n1. Alliteration\n2. Antonymy\n3. Adult slang\n4. Latent semantic structural features, covering four categories:\n\t* Incongruity (2)\n\t* Ambiguity (6)\n\t* Interpersonal Effect (4)\n\t* Phonetic Pattern (4)\n5. Semantic distance features, including:\n\t* Humor label classes from 5 sentences in the training set that are closest to the sentence (found by using a k-Nearest Neighbors (kNN) method)\n\t* Averaged Word2Vec representations of each sentence", "predicted_evidence": ["Beyond lexical cues from text inputs, other research has also utilized speakers' acoustic cues BIBREF2 , BIBREF5 . These studies have typically used audio tracks from TV shows and their corresponding captions in order to categorize characters' speaking turns as humorous or non-humorous. Utterances prior to canned laughter that was manually inserted into the shows were treated as humorous, while other utterances were treated as negative cases.", "Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies BIBREF1 , BIBREF2 , BIBREF3 , humor recognition was modeled as a binary classification task. In the seminal work BIBREF1 , a corpus of INLINEFORM0 \u201cone-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website. BIBREF3 explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style.", "Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies BIBREF1 , BIBREF2 , BIBREF3 , humor recognition was modeled as a binary classification task. In the seminal work BIBREF1 , a corpus of INLINEFORM0 \u201cone-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website.", "Following yang-EtAl:2015:EMNLP2, we applied Random Forest BIBREF12 to perform humor recognition by using the following two groups of features. The first group are latent semantic structural features covering the following 4 categories: Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4). The second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to this sentence (found by using a k-Nearest Neighbors (kNN) method), and each sentence's averaged Word2Vec representations ( INLINEFORM0 ). More details can be found in BIBREF3 .", "Automatically simulating an audience's reactions to humor will not only be useful for presentation training, but also improve conversational systems by giving machines more empathetic power. The present study reports our efforts in recognizing utterances that cause laughter in presentations. These include building a corpus from TED talks and using Convolutional Neural Networks (CNNs) in the recognition."]}
{"question_id": "2f9a31f5a2b668acf3bce8958f5daa67ab8b2c83", "predicted_answer": "No, they do not evaluate only on English data. Although the text does not explicitly mention non-English data, the fact that they use TED Talk transcripts, which are multilingual, suggests that they may be evaluating on data in multiple languages.", "predicted_evidence": ["In our experiment, we firstly divided each corpus into two parts. The smaller part (the Dev set) was used for setting various hyper-parameters used in text classifiers. The larger portion (the CV set) was then formulated as a 10-fold cross-validation setup for obtaining a stable and comprehensive model evaluation result. For the PUN data, the Dev contains 482 sentences, while the CV set contains 4344 sentences. For the TED data, the Dev set contains 1046 utterances, while the CV set contains 8406 utterances. Note that, with a goal of building a speaker-independent humor detector, when partitioning our TED data set, we always kept all utterances of a single talk within the same partition. To our knowledge, this is the first time that such a strict experimental setup has been used in recognizing humor in conversations, and it makes the humor recognition task on the TED data quite challenging.", "In most of the previous studies BIBREF1 , BIBREF2 , BIBREF3 , humor recognition was modeled as a binary classification task. In the seminal work BIBREF1 , a corpus of INLINEFORM0 \u201cone-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website. BIBREF3 explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style. In addition, Word2Vec BIBREF4 distributed representations were utilized in the model building.", "We used two corpora: the TED Talk corpus (denoted as TED) and the Pun of the Day corpus (denoted as Pun). Note that we normalized words in the Pun data to lowercase to avoid a possibly elevated result caused by a special pattern: in the original format, all negative instances started with capital letters. The Pun data allows us to verify that our implementation is consistent with the work reported in yang-EtAl:2015:EMNLP2.", "We collected INLINEFORM0 TED Talk transcripts. An example transcription is given in Figure FIGREF4 . The collected transcripts were split into sentences using the Stanford CoreNLP tool BIBREF11 . In this study, sentences containing or immediately followed by `(Laughter)' were used as `Laughter' sentences, as shown in Figure FIGREF4 ; all other sentences were defined as `No-Laughter' sentences. Following BIBREF1 and BIBREF3 , we selected the same numbers ( INLINEFORM1 ) of `Laughter' and `No-Laughter' sentences. To minimize possible topic shifts between positive and negative instances, for each positive instance, we picked one negative instance nearby (the context window was 7 sentences in this study). For example, in Figure FIGREF4 , a negative instance (corresponding to `sent-2') was selected from the nearby sentences ranging from `sent-7' to `sent+7'.", "Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies BIBREF1 , BIBREF2 , BIBREF3 , humor recognition was modeled as a binary classification task. In the seminal work BIBREF1 , a corpus of INLINEFORM0 \u201cone-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website. BIBREF3 explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style."]}
{"question_id": "4830459e3d1d204e431025ce7e596ef3f8d757d2", "predicted_answer": "The number of speakers is not explicitly mentioned in the dataset. The dataset is based on TED Talks, which are recordings from TED conferences and other special TED programs. The transcripts of the talks are used, and the markup `(Laughter)' is used to determine utterance labels. However, the number of speakers is not provided.", "predicted_evidence": ["Stemming from the present study, we envision that more research is worth pursuing: (a) for presentations, cues from other modalities such as audio or video will be included, similar to Bertero2016LREC; (b) context information from multiple utterances will be modeled by using sequential modeling methods.", "In our experiment, we firstly divided each corpus into two parts. The smaller part (the Dev set) was used for setting various hyper-parameters used in text classifiers. The larger portion (the CV set) was then formulated as a 10-fold cross-validation setup for obtaining a stable and comprehensive model evaluation result. For the PUN data, the Dev contains 482 sentences, while the CV set contains 4344 sentences. For the TED data, the Dev set contains 1046 utterances, while the CV set contains 8406 utterances. Note that, with a goal of building a speaker-independent humor detector, when partitioning our TED data set, we always kept all utterances of a single talk within the same partition. To our knowledge, this is the first time that such a strict experimental setup has been used in recognizing humor in conversations, and it makes the humor recognition task on the TED data quite challenging.", "Beyond lexical cues from text inputs, other research has also utilized speakers' acoustic cues BIBREF2 , BIBREF5 . These studies have typically used audio tracks from TV shows and their corresponding captions in order to categorize characters' speaking turns as humorous or non-humorous. Utterances prior to canned laughter that was manually inserted into the shows were treated as humorous, while other utterances were treated as negative cases.", "TED Talks are recordings from TED conferences and other special TED programs. In the present study, we focused on the transcripts of the talks. Most transcripts of the talks contain the markup `(Laughter)', which represents where audiences laughed aloud during the talks. This special markup was used to determine utterance labels.", "Next, the embedding matrix was fed into a INLINEFORM5 convolution network with multiple filters. To cover varied reception fields, we used filters of sizes of INLINEFORM6 , INLINEFORM7 , and INLINEFORM8 . For each filter size, INLINEFORM9 filters were utilized. Then, max pooling, which stands for finding the largest value from a vector, was applied to each feature map (total INLINEFORM10 feature maps) output by the INLINEFORM11 convolution. Finally, maximum values from all of INLINEFORM12 filters were formed as a flattened vector to go through a fully connected (FC) layer to predict two possible labels (Laughter vs. No-Laughter). Note that for INLINEFORM13 convolution and FC layer's input, we applied `dropout' BIBREF10 regularization, which entails randomly setting a proportion of network weights to be zero during model training, to overcome over-fitting. By using cross-entropy as the learning metric, the whole sequential network (all weights and bias) could be optimized by using any SGD optimization, e.g., Adam BIBREF13 , Adadelta BIBREF14 , and so on."]}
{"question_id": "74ebfba06f37cc95dfe59c3790ebe6165e6be19c", "predicted_answer": "The positive instances (i.e. \"Laughter\" sentences) are annotated based on the presence of \"(Laughter)\" in the transcript, which indicates that the audience laughed in response to the sentence.", "predicted_evidence": ["We collected INLINEFORM0 TED Talk transcripts. An example transcription is given in Figure FIGREF4 . The collected transcripts were split into sentences using the Stanford CoreNLP tool BIBREF11 . In this study, sentences containing or immediately followed by `(Laughter)' were used as `Laughter' sentences, as shown in Figure FIGREF4 ; all other sentences were defined as `No-Laughter' sentences. Following BIBREF1 and BIBREF3 , we selected the same numbers ( INLINEFORM1 ) of `Laughter' and `No-Laughter' sentences. To minimize possible topic shifts between positive and negative instances, for each positive instance, we picked one negative instance nearby (the context window was 7 sentences in this study). For example, in Figure FIGREF4 , a negative instance (corresponding to `sent-2') was selected from the nearby sentences ranging from `sent-7' to `sent+7'.", "Beyond lexical cues from text inputs, other research has also utilized speakers' acoustic cues BIBREF2 , BIBREF5 . These studies have typically used audio tracks from TV shows and their corresponding captions in order to categorize characters' speaking turns as humorous or non-humorous. Utterances prior to canned laughter that was manually inserted into the shows were treated as humorous, while other utterances were treated as negative cases.", "Automatically simulating an audience's reactions to humor will not only be useful for presentation training, but also improve conversational systems by giving machines more empathetic power. The present study reports our efforts in recognizing utterances that cause laughter in presentations. These include building a corpus from TED talks and using Convolutional Neural Networks (CNNs) in the recognition.", "For the purpose of monitoring how well speakers can use humor during their presentations, we have created a corpus from TED talks. Compared to the existing (albeit limited) corpora for humor recognition research, ours has the following advantages: (a) it was collected from authentic talks, rather than from TV shows performed by professional actors based on scripts; (b) it contains about 100 times more speakers compared to the limited number of actors in existing corpora. We compared two types of leading text-based humor recognition methods: a conventional classifier (e.g., Random Forest) based on human-engineered features vs. an end-to-end CNN method, which relies on its inherent representation learning. We found that the CNN method has better performance. More importantly, the representation learning of the CNN method makes it very efficient when facing new data sets.", "From the brief review, it is clear that corpora used in humor research so far are limited to one-line puns or jokes and conversations from TV comedy shows. There is a great need for an open corpus that can support investigating humor in presentations. CNN-based text categorization methods have been applied to humor recognition (e.g., in BIBREF5 ) but with limitations: (a) a rigorous comparison with the state-of-the-art conventional method examined in yang-EtAl:2015:EMNLP2 is missing; (b) CNN's performance in the previous research is not quite clear; and (c) some important techniques that can improve CNN performance (e.g., using varied-sized filters and dropout regularization BIBREF10 ) were not applied. Therefore, the present study is meant to address these limitations."]}
{"question_id": "3a01dc85ac983002fd631f1c28fc1cbe16094c24", "predicted_answer": "To incorporate commonsense into an LSTM, you can use an additional LSTM to encode commonsense assertions, which are transformed into a sequence of tokens by chunking concepts into multi-word phrases. This allows the model to use the commonsense knowledge as an external memory module to provide additional clues for response selection.", "predicted_evidence": ["(2) Integrating commonsense knowledge into conversational models boosts model performance, as Tri-LSTM outperforms Dual-LSTM by a certain margin.", "Our main approach to integrating commonsense knowledge into the conversational model involves using another LSTM for encoding all assertions $a$ in $A_x$ , as illustrated in Figure 3 . Each $a$ , originally in the form of $<c_1,r,c_2 >$ , is transformed into a sequence of tokens by chunking $c_1$ , $c_2$ , concepts which are potentially multi-word phrases, into $[c_{11},c_{12},c_{13}...]$ and $[c_{21},c_{22},c_{23}...]$ . Thus, $a=[c_{11},c_{12},c_{13}...,r,c_{21},c_{22},c_{23}...]$ .", "We also analyze samples from the test set to gain an insight on how commonsense knowledge supplements the message itself in response selection by comparing Tri-LSTM encoder and Dual-LSTM encoder.", "As illustrated in Table 2 , instances 1,2 represent cases where commonsense assertions as an external memory module provide certain clues that the other model failed to capture. For example in instance 2, Tri-LSTM selects the response \u201c...improve your french\u201d to message \u201cbonjour madame\u201d based on a retrieved assertion \u201c $bonjour, IsA, hello\\_in\\_french$ \u201d, while Dual-LSTM selects an irrelevant response. Unsurprisingly, Dual-LSTM is also able to select the correct response in some cases where certain commonsense knowledge is necessary, as illustrated in instance 3. Both models select \u201c... pink or black\u201d in response to message \u201c...what color shoes...\u201d, even though Dual-LSTM does not have access to a helpful assertion \u201c $pink, RelatedTo,\ncolor$ \u201d.", "Informally speaking, such cases suggest that to some extent, Dual-LSTM (models with no memory) is able to encode certain commonsense knowledge in model parameters (e.g., word embeddings) in an implicit way. In other cases, e.g., instance 4, the message itself is enough for the selection of the correct response, where both models do equally well."]}
{"question_id": "00ffe2c59a3ba18d6d2b353d6ab062a152c88526", "predicted_answer": "The conversations in this context are in the open-domain human-computer conversation domain.", "predicted_evidence": ["In human-to-human conversations, however, people respond to each other's utterances in a meaningful way not only by paying attention to the latest utterance of the conversational partner itself, but also by recalling relevant information about the concepts covered in the dialogue and integrating it into their responses. Such information may contain personal experience, recent events, commonsense knowledge and more (Figure 1 ). As a result, it is speculated that a conversational model with a \u201cmemory look-up\u201d module can mimic human conversations more closely BIBREF1 , BIBREF2 . In open-domain human-computer conversation, where the model is expected to respond to human utterances in an interesting and engaging way, commonsense knowledge has to be integrated into the model effectively.", "We gratefully acknowledge the help of Alan Ritter for sharing the twitter dialogue dataset and the NTU PDCC center for providing computing resources.", "In recent years, data-driven approaches to building conversation models have been made possible by the proliferation of social media conversation data and the increase of computing power. By relying on a large number of message-response pairs, the Seq2Seq framework BIBREF0 attempts to produce an appropriate response based solely on the message itself, without any memory module.", "Data-driven conversational models generally fall into two categories: retrieval-based methods BIBREF6 , BIBREF7 , BIBREF8 , which select a response from a predefined repository, and generation-based methods BIBREF9 , BIBREF10 , BIBREF11 , which employ an encoder-decoder framework where the message is encoded into a vector representation and, then, fed to the decoder to generate the response. The latter is more natural (as it does not require a response repository) yet suffers from generating dull or vague responses and generally needs a great amount of training data.", "Our future work includes extending the commonsense knowledge with common (or factual) knowledge, e.g., to extend the knowledge base coverage by linking more named entities to commonsense knowledge concepts BIBREF34 , and developing a better mechanism for utilizing such knowledge instead of the simple max-pooling scheme used in this paper. We would also like to explore the memory of the model for multiple message response pairs in a long conversation."]}
{"question_id": "042800c3336ed5f4826203616a39747c61382ba6", "predicted_answer": "The commonsense knowledge base being used is ConceptNet.", "predicted_evidence": ["Several commonsense knowledge bases have been constructed during the past decade, such as ConceptNet BIBREF17 and SenticNet BIBREF18 . The aim of commonsense knowledge representation and reasoning is to give a foundation of real-world knowledge to a variety of AI applications, e.g., sentiment analysis BIBREF19 , handwriting recognition BIBREF20 , e-health BIBREF21 , aspect extraction BIBREF22 , and many more. Typically, a commonsense knowledge base can be seen as a semantic network where concepts are nodes in the graph and relations are edges (Figure 2 ). Each $<concept1, relation, concept2 >$ triple is termed an assertion.", "Our future work includes extending the commonsense knowledge with common (or factual) knowledge, e.g., to extend the knowledge base coverage by linking more named entities to commonsense knowledge concepts BIBREF34 , and developing a better mechanism for utilizing such knowledge instead of the simple max-pooling scheme used in this paper. We would also like to explore the memory of the model for multiple message response pairs in a long conversation.", "In this paper, we assume that a commonsense knowledge base is composed of assertions $A$ about concepts $C$ . Each assertion $a \\in A$ takes the form of a triple $<c_1,r,c_2 >$ , where $r \\in R$ is a relation between $c_1$ and $c_2$ , such as IsA, CapableOf, etc. $c_1,c_2$ are concepts in $C$ . The relation set $R$ is typically much smaller than $C$0 . $C$1 can either be a single word (e.g., \u201cdog\u201d and \u201cbook\u201d) or a multi-word expression (e.g., \u201ctake_a_stand\u201d and \u201cgo_shopping\u201d).", "In our experiment, ConceptNet is used as the commonsense knowledge base. Preprocessing of this knowledge base involves removing assertions containing non-English characters or any word outside vocabulary $V$ . 1.4M concepts remain. 0.8M concepts are unigrams, 0.43M are bi-grams and the other 0.17M are tri-grams or more. Each concept is associated with an average of 4.3 assertions. More than half of the concepts are associated with only one assertion.", "In this paper, we assume that a commonsense knowledge base is composed of assertions $A$ about concepts $C$ . Each assertion $a \\in A$ takes the form of a triple $<c_1,r,c_2 >$ , where $r \\in R$ is a relation between $c_1$ and $c_2$ , such as IsA, CapableOf, etc. $c_1,c_2$ are concepts in $C$ . The relation set $R$ is typically much smaller than $C$0 . $C$1 can either be a single word (e.g., \u201cdog\u201d and \u201cbook\u201d) or a multi-word expression (e.g., \u201ctake_a_stand\u201d and \u201cgo_shopping\u201d). We build a dictionary $C$2 out of $C$3 where every concept $C$4 is a key and a list of all assertions in $C$5 concerning $C$6 , i.e., $C$7 or $C$8 , is the value."]}
{"question_id": "52868394eb2b3b37eb5f47f51c06ad53061f4495", "predicted_answer": "Unfortunately, the text does not explicitly state how the HotelRec dataset was obtained. It only discusses the characteristics of the dataset, its size, and how it was preprocessed, but it does not provide information on the source of the data or how it was collected.", "predicted_evidence": ["In this section, we first discuss about the data collection process (Section SECREF8), followed by general descriptive statistics (Section SECREF12). Finally, Section SECREF18 analyzes the overall rating and sub-ratings.", "To the best of our knowledge, only a few number of datasets for hotel reviews have been created: 35k BIBREF9, 68k BIBREF18, 140k BIBREF19, 142k BIBREF20, 235k BIBREF9, 435k BIBREF13, and 870k BIBREF11. However, the number of users, items, and interactions is limited compared to traditional recommendation datasets. In contrast, the HotelRec dataset has at least two orders of magnitude more examples. Statistics of HotelRec is available in Table TABREF2.", "The results of the baselines are available in Table TABREF36. HR@$k$ and NDCG@$k$ correspond to the Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG), evaluated on the top-$k$ computed ranked items for a particular test user, and then averaged over all test users.", "From each review, we kept the corresponding \"userID\", \"itemID\", rating (from 1 to 5 stars), written text, and date. We preprocessed the text by lowering and tokenizing it. Statistics of both subsets are shown in Table TABREF2.", "Due to the large size of the HotelRec dataset, especially in the 5-core setting (around 20 million reviews), running an extensive hyper-parameter tuning for each neural model would require a high time and resource budget. Therefore, for the neural model, we used the default parameters from the original implementation and a random search of three trials. For all other models (i.e., HFT, ItemKNN, UserKNN, PureSVD), we ran a standard grid search over the parameter sets."]}
{"question_id": "59dc6b1d3da74a2e67a6fb1ce940b28d9e3d8de0", "predicted_answer": "The answer is no, the recommendations are not specific to a region. The text mentions that the dataset can be increased with other languages, implying that the recommendations are not limited to a specific region or language. Additionally, the examples of recommendation systems mentioned, such as Amazon products, beers, restaurants, images, music, and movies, are not region-specific.", "predicted_evidence": ["Recommendation is an old problem that has been studied from a wide range of areas, such as Amazon products BIBREF14, beers BIBREF15, restaurants, images BIBREF16, music BIBREF4, and movies BIBREF1. The size of the datasets generally varies from hundreds of thousands to tens of millions of user-item interactions; an interaction always contains a rating and could have additional attributes, such as a user-written text, sub-ratings, the date, or whether the review was helpful. At the time of writing, and to the best of our knowledge, the largest available recommendation corpus on a specific domain and with textual reviews, is based on Amazon Books and proposed by he2016ups. It contains a total of 22 million book reviews. In comparison, HotelRec has $2.3$ times more reviews and is based on hotels. Consequently, HotelRec is the largest domain-specific public recommendation dataset with textual reviews and on a single domain.", "Recommendation is an old problem that has been studied from a wide range of areas, such as Amazon products BIBREF14, beers BIBREF15, restaurants, images BIBREF16, music BIBREF4, and movies BIBREF1. The size of the datasets generally varies from hundreds of thousands to tens of millions of user-item interactions; an interaction always contains a rating and could have additional attributes, such as a user-written text, sub-ratings, the date, or whether the review was helpful. At the time of writing, and to the best of our knowledge, the largest available recommendation corpus on a specific domain and with textual reviews, is based on Amazon Books and proposed by he2016ups. It contains a total of 22 million book reviews. In comparison, HotelRec has $2.3$ times more reviews and is based on hotels.", "POP BIBREF24: Another non-personalized recommender method, where items are recommended based on their popularity (i.e., the number of interactions with users). It is a common baseline to benchmark the recommendation performance;", "In future work, we could easily increase the dataset with other languages and use it for multilingual recommendation. We release HotelRec for further research: https://github.com/Diego999/HotelRec.", "The increasing flood of information on the web creates a need for selecting content according to the end user's preferences. Today, recommender systems are deployed on most internet platforms and play an important role in everybody's daily digital routine, including e-commerce websites, social networks, music streaming, or hotel booking."]}
{"question_id": "713e1c7b0ab17759ba85d7cd2041e387831661df", "predicted_answer": "No, they did not experiment on this dataset. They are discussing the dataset and its characteristics, and comparing it to other datasets, but they are not presenting the results of their own experiments on this dataset.", "predicted_evidence": ["In this section, we first discuss about the data collection process (Section SECREF8), followed by general descriptive statistics (Section SECREF12). Finally, Section SECREF18 analyzes the overall rating and sub-ratings.", "To the best of our knowledge, only a few number of datasets for hotel reviews have been created: 35k BIBREF9, 68k BIBREF18, 140k BIBREF19, 142k BIBREF20, 235k BIBREF9, 435k BIBREF13, and 870k BIBREF11. However, the number of users, items, and interactions is limited compared to traditional recommendation datasets. In contrast, the HotelRec dataset has at least two orders of magnitude more examples. Statistics of HotelRec is available in Table TABREF2.", "MLP BIBREF8: Similar than GMF, but it models the interaction of latent features with a neural network instead of a linear kernel;", "The results of the baselines are available in Table TABREF36. HR@$k$ and NDCG@$k$ correspond to the Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG), evaluated on the top-$k$ computed ranked items for a particular test user, and then averaged over all test users.", "Due to the large size of the HotelRec dataset, especially in the 5-core setting (around 20 million reviews), running an extensive hyper-parameter tuning for each neural model would require a high time and resource budget. Therefore, for the neural model, we used the default parameters from the original implementation and a random search of three trials. For all other models (i.e., HFT, ItemKNN, UserKNN, PureSVD), we ran a standard grid search over the parameter sets."]}
{"question_id": "00db191facf903cef18fb1727d1cab638c277e0a", "predicted_answer": "They use character 2-gram, 3-gram, and 4-gram embeddings. Specifically, they found that character 3-gram embeddings (char3-MS-vec) are the most useful.", "predicted_evidence": ["To compute INLINEFORM0 , we apply an encoder to character INLINEFORM1 -gram embeddings. Previous studies demonstrated that additive composition, which computes the (weighted) sum of embeddings, is a suitable method for embedding construction BIBREF13 , BIBREF0 . Thus, we adopt (simplified) multi-dimensional self-attention BIBREF11 , which computes weights for each dimension of given embeddings and sums up the weighted embeddings (i.e., element-wise weighted sum) as an encoder. Let INLINEFORM2 be the character INLINEFORM3 -gram embeddings of an input word, let INLINEFORM4 be the number of character INLINEFORM5 -grams extracted from the word, and let INLINEFORM6 be the matrix whose INLINEFORM7 -th column corresponds to INLINEFORM8 , that is, INLINEFORM9 . The multi-dimensional self-attention constructs the word embedding INLINEFORM10 by the following equations: DISPLAYFORM0", "Let us consider the case where an input word is `the' and we use character 3-gram in Figure FIGREF4 . We prepare special characters `' and `$' to represent the beginning and end of the word, respectively. Then, `the' is composed of three character 3-grams: `th', `the', and `he$'. We multiply the embeddings of these 3-grams by transformation matrix INLINEFORM0 and apply the softmax function to each row as in Equation . As a result of the softmax, we obtain a matrix that contains weights for each embedding. The size of the computed matrix is identical to the input embedding matrix: INLINEFORM1 . We then compute Equation EQREF7 , i.e., the weighted sum of the embeddings, and add the resulting vector to the word embedding of `the'. Finally, we input the vector into an RNN to predict the next word.", "Figure FIGREF4 is the overview of the proposed method using character 3-gram embeddings (char3-MS-vec). As illustrated in this figure, our proposed method regards the sum of char3-MS-vec and the standard word embedding as an input of an RNN. In other words, let INLINEFORM0 be char INLINEFORM1 -MS-vec and we replace Equation with the following: DISPLAYFORM0", "Moreover, to investigate the effect of only char INLINEFORM0 -MS-vec, we ignore INLINEFORM1 in Equation EQREF5 . We refer to this setting as \u201cRemove word embeddings INLINEFORM2 \u201d in Table TABREF24 . Table TABREF24 shows cahr3-MS-vec and char4-MS-vec are superior to char2-MS-vec. In the view of perplexity, char3-MS-vec and char4-MS-vec achieved comparable scores to each other. On the other hand, char3-MS-vec is composed of much smaller parameters. Furthermore, we decreased the embedding size INLINEFORM3 to adjust the number of parameters to the same size as the baseline (\u201cSame #Params as baseline\u201d in Table TABREF24 ). In this setting, char3-MS-vec achieved the best perplexity. Therefore, we consider that char3-MS-vec is more useful than char4-MS-vec, which is the answer to the fourth research question.", "On the other hand, in the field of word embedding construction, some previous researchers found that character INLINEFORM0 -grams are more useful than single characters BIBREF0 , BIBREF10 . In particular, BIBREF0 demonstrated that constructing word embeddings from character INLINEFORM1 -gram embeddings outperformed the methods that construct word embeddings from character embeddings by using CNN or a Long Short-Term Memory (LSTM)."]}
{"question_id": "1edfe390828f02a2db9a88454421c7f3d4cdd611", "predicted_answer": "No, the text does not mention experimenting with fine-tuning their embeddings. The experiments focus on comparing different methods for constructing word embeddings from character embeddings, such as using CNN, LSTM, or multi-dimensional self-attention, and varying the size of character n-grams used.", "predicted_evidence": ["Table TABREF24 shows that the proposed char INLINEFORM0 -MS-vec outperformed charCNN even though the original settings of charCNN had much larger parameters. Moreover, we trained charCNN with two additional settings but CNN did not improve the baseline performance. This result implies that char INLINEFORM1 -MS-vec is better embeddings than ones constructed by applying CNN to character embeddings. Table TABREF24 also indicates that char INLINEFORM2 -Sum-vec was harmful to the performance. Moreover, char INLINEFORM3 -SS-vec did not have a positive effect on the baseline. These results answer yes to the third research question; our use of multi-dimensional self-attention is more appropriate for constructing word embeddings from character INLINEFORM4 -gram embeddings.", "On the other hand, in the field of word embedding construction, some previous researchers found that character INLINEFORM0 -grams are more useful than single characters BIBREF0 , BIBREF10 . In particular, BIBREF0 demonstrated that constructing word embeddings from character INLINEFORM1 -gram embeddings outperformed the methods that construct word embeddings from character embeddings by using CNN or a Long Short-Term Memory (LSTM).", "For the baseline, we also applied two word embeddings to investigate the performance in the case where we use more kinds of word embeddings. In detail, we prepared INLINEFORM2 and used INLINEFORM3 instead of INLINEFORM4 in Equation . Table TABREF15 also shows the number of character INLINEFORM5 -grams in each dataset. This table indicates that char INLINEFORM6 -MS-vec improved the performance of state-of-the-art models except for char4-MS-vec on WT103. These results indicate that char INLINEFORM7 -MS-vec can raise the quality of word-level language models. In particular, Table TABREF15 shows that char3-MS-vec achieved the best scores consistently. In contrast, an additional word embedding did not improve the performance. This fact implies that the improvement of char INLINEFORM8 -MS-vec is caused by using character INLINEFORM9 -grams. Thus, we answer yes to the first research question.", "In these experiments, we only applied char3-MS-vec to EncDec but BIBREF38 indicated that combining multiple kinds of subword units can improve the performance. We will investigate the effect of combining several character INLINEFORM0 -gram embeddings in future work.", "We varied INLINEFORM0 for char INLINEFORM1 -MS-vec from 2 to 4. For the baseline, we also applied two word embeddings to investigate the performance in the case where we use more kinds of word embeddings. In detail, we prepared INLINEFORM2 and used INLINEFORM3 instead of INLINEFORM4 in Equation . Table TABREF15 also shows the number of character INLINEFORM5 -grams in each dataset. This table indicates that char INLINEFORM6 -MS-vec improved the performance of state-of-the-art models except for char4-MS-vec on WT103. These results indicate that char INLINEFORM7 -MS-vec can raise the quality of word-level language models. In particular, Table TABREF15 shows that char3-MS-vec achieved the best scores consistently. In contrast, an additional word embedding did not improve the performance. This fact implies that the improvement of char INLINEFORM8 -MS-vec is caused by using character INLINEFORM9 -grams."]}
{"question_id": "3dad6b792044018bb968ac0d0fd4628653f9e4b7", "predicted_answer": "The text does not explicitly mention the specific word embeddings that are being compared against. However, it mentions that two word embeddings were applied as a baseline to investigate the performance when using more kinds of word embeddings. The INLINEFORM2 and INLINEFORM3 are likely placeholders for the names of these word embeddings, but their actual values are not provided in the given context.", "predicted_evidence": ["For the baseline, we also applied two word embeddings to investigate the performance in the case where we use more kinds of word embeddings. In detail, we prepared INLINEFORM2 and used INLINEFORM3 instead of INLINEFORM4 in Equation . Table TABREF15 also shows the number of character INLINEFORM5 -grams in each dataset. This table indicates that char INLINEFORM6 -MS-vec improved the performance of state-of-the-art models except for char4-MS-vec on WT103. These results indicate that char INLINEFORM7 -MS-vec can raise the quality of word-level language models. In particular, Table TABREF15 shows that char3-MS-vec achieved the best scores consistently. In contrast, an additional word embedding did not improve the performance. This fact implies that the improvement of char INLINEFORM8 -MS-vec is caused by using character INLINEFORM9 -grams. Thus, we answer yes to the first research question.", "We varied INLINEFORM0 for char INLINEFORM1 -MS-vec from 2 to 4. For the baseline, we also applied two word embeddings to investigate the performance in the case where we use more kinds of word embeddings. In detail, we prepared INLINEFORM2 and used INLINEFORM3 instead of INLINEFORM4 in Equation . Table TABREF15 also shows the number of character INLINEFORM5 -grams in each dataset. This table indicates that char INLINEFORM6 -MS-vec improved the performance of state-of-the-art models except for char4-MS-vec on WT103. These results indicate that char INLINEFORM7 -MS-vec can raise the quality of word-level language models. In particular, Table TABREF15 shows that char3-MS-vec achieved the best scores consistently. In contrast, an additional word embedding did not improve the performance. This fact implies that the improvement of char INLINEFORM8 -MS-vec is caused by using character INLINEFORM9 -grams.", "Table TABREF15 shows perplexities of the baselines and the proposed method. We varied INLINEFORM0 for char INLINEFORM1 -MS-vec from 2 to 4. For the baseline, we also applied two word embeddings to investigate the performance in the case where we use more kinds of word embeddings. In detail, we prepared INLINEFORM2 and used INLINEFORM3 instead of INLINEFORM4 in Equation . Table TABREF15 also shows the number of character INLINEFORM5 -grams in each dataset. This table indicates that char INLINEFORM6 -MS-vec improved the performance of state-of-the-art models except for char4-MS-vec on WT103. These results indicate that char INLINEFORM7 -MS-vec can raise the quality of word-level language models. In particular, Table TABREF15 shows that char3-MS-vec achieved the best scores consistently.", "On the other hand, in the field of word embedding construction, some previous researchers found that character INLINEFORM0 -grams are more useful than single characters BIBREF0 , BIBREF10 . In particular, BIBREF0 demonstrated that constructing word embeddings from character INLINEFORM1 -gram embeddings outperformed the methods that construct word embeddings from character embeddings by using CNN or a Long Short-Term Memory (LSTM).", "Table TABREF15 shows perplexities of the baselines and the proposed method. We varied INLINEFORM0 for char INLINEFORM1 -MS-vec from 2 to 4. For the baseline, we also applied two word embeddings to investigate the performance in the case where we use more kinds of word embeddings. In detail, we prepared INLINEFORM2 and used INLINEFORM3 instead of INLINEFORM4 in Equation . Table TABREF15 also shows the number of character INLINEFORM5 -grams in each dataset. This table indicates that char INLINEFORM6 -MS-vec improved the performance of state-of-the-art models except for char4-MS-vec on WT103. These results indicate that char INLINEFORM7 -MS-vec can raise the quality of word-level language models. In particular, Table TABREF15 shows that char3-MS-vec achieved the best scores consistently. In contrast, an additional word embedding did not improve the performance."]}
{"question_id": "a28c73a6a8c46a43a1eec2b42b542dd7fde1e30e", "predicted_answer": "The dataset used for headline generation is the annotated English Gigaword corpus, specifically the test sets constructed by BIBREF33 and BIBREF34.", "predicted_evidence": ["For headline generation, we used sentence-headline pairs extracted from the annotated English Gigaword corpus BIBREF35 in the same manner as BIBREF2 . The training set contains about 3.8M sentence-headline pairs. For evaluation, we exclude the test set constructed by BIBREF2 because it contains some invalid instances, as reported in BIBREF33 . We instead used the test sets constructed by BIBREF33 and BIBREF34 .", "We conduct experiments on the well-known benchmark datasets: Penn Treebank, WikiText-2, and WikiText-103. Our experiments indicate that the proposed method outperforms neural language models trained with well-tuned hyperparameters and achieves state-of-the-art scores on each dataset. In addition, we incorporate our proposed method into a standard neural encoder-decoder model and investigate its effect on machine translation and headline generation. We indicate that the proposed method also has a positive effect on such tasks.", "As described in Section SECREF1 , neural encoder-decoder models can be interpreted as conditional neural language models. Therefore, to investigate if the proposed method contributes to encoder-decoder models, we conduct experiments on machine translation and headline generation tasks.", "We set the embedding size and dimension of the LSTM hidden state to 500 for machine translation and 400 for headline generation. The mini-batch size is 64 for machine translation and 256 for headline generation. For other hyperparameters, we followed the configurations described in BIBREF34 . We constructed the vocabulary set by using Byte-Pair-Encoding (BPE) BIBREF37 because BPE is a currently widely-used technique for vocabulary construction. We set the number of BPE merge operations to 16K for machine translation and 5K for headline generation.", "In this paper, we incorporated character information with RNN language models. Based on the research in the field of word embedding construction BIBREF0 , we focused on character INLINEFORM0 -gram embeddings to construct word embeddings. We used multi-dimensional self-attention BIBREF11 to encode character INLINEFORM1 -gram embeddings. Our proposed char INLINEFORM2 -MS-vec improved the performance of state-of-the-art RNN language models and achieved the best perplexities on Penn Treebank, WikiText-2, and WikiText-103. Moreover, we investigated the effect of char INLINEFORM3 -MS-vec on application tasks, specifically, machine translation and headline generation. Our experiments show that char INLINEFORM4 -MS-vec also improved the performance of a neural encoder-decoder on both tasks."]}
{"question_id": "5f1ffaa738fedd5b6668ec8b58a027ddea6867ce", "predicted_answer": "The results of their embeddings on machine translation are not explicitly stated in the given context. The text only mentions that their experiments show that char INLINEFORM4 -MS-vec improved the performance of a neural encoder-decoder on machine translation, but it does not provide specific numbers or metrics (e.g. BLEU score, accuracy) to quantify the improvement.", "predicted_evidence": ["For machine translation, we used two kinds of language pairs: English-French and English-German sentences in the IWSLT 2016 dataset. The dataset contains about 208K English-French pairs and 189K English-German pairs. We conducted four translation tasks: from English to each language (En-Fr and En-De), and their reverses (Fr-En and De-En).", "On the other hand, in the field of word embedding construction, some previous researchers found that character INLINEFORM0 -grams are more useful than single characters BIBREF0 , BIBREF10 . In particular, BIBREF0 demonstrated that constructing word embeddings from character INLINEFORM1 -gram embeddings outperformed the methods that construct word embeddings from character embeddings by using CNN or a Long Short-Term Memory (LSTM).", "We incorporate char INLINEFORM0 -MS-vec, which is an embedding constructed from character INLINEFORM1 -gram embeddings, into RNN language models since, as discussed earlier, previous studies revealed that we can construct better word embeddings by using character INLINEFORM2 -gram embeddings BIBREF0 , BIBREF10 . In particular, we expect char INLINEFORM3 -MS-vec to help represent infrequent words by taking advantage of the internal structure.", "Let us consider the case where an input word is `the' and we use character 3-gram in Figure FIGREF4 . We prepare special characters `' and `$' to represent the beginning and end of the word, respectively. Then, `the' is composed of three character 3-grams: `th', `the', and `he$'. We multiply the embeddings of these 3-grams by transformation matrix INLINEFORM0 and apply the softmax function to each row as in Equation . As a result of the softmax, we obtain a matrix that contains weights for each embedding. The size of the computed matrix is identical to the input embedding matrix: INLINEFORM1 . We then compute Equation EQREF7 , i.e., the weighted sum of the embeddings, and add the resulting vector to the word embedding of `the'. Finally, we input the vector into an RNN to predict the next word.", "In this paper, we incorporated character information with RNN language models. Based on the research in the field of word embedding construction BIBREF0 , we focused on character INLINEFORM0 -gram embeddings to construct word embeddings. We used multi-dimensional self-attention BIBREF11 to encode character INLINEFORM1 -gram embeddings. Our proposed char INLINEFORM2 -MS-vec improved the performance of state-of-the-art RNN language models and achieved the best perplexities on Penn Treebank, WikiText-2, and WikiText-103. Moreover, we investigated the effect of char INLINEFORM3 -MS-vec on application tasks, specifically, machine translation and headline generation. Our experiments show that char INLINEFORM4 -MS-vec also improved the performance of a neural encoder-decoder on both tasks."]}
{"question_id": "8e26c471ca0ee1b9779da04c0b81918fd310d0f3", "predicted_answer": "According to the text, they combine ordinary word embeddings and ones constructed from character n-grams by adding the weighted sum of the character n-gram embeddings to the word embedding. Specifically, they compute the weighted sum of the character n-gram embeddings using a transformation matrix and softmax function, and then add the resulting vector to the word embedding of the input word.", "predicted_evidence": ["On the other hand, in the field of word embedding construction, some previous researchers found that character INLINEFORM0 -grams are more useful than single characters BIBREF0 , BIBREF10 . In particular, BIBREF0 demonstrated that constructing word embeddings from character INLINEFORM1 -gram embeddings outperformed the methods that construct word embeddings from character embeddings by using CNN or a Long Short-Term Memory (LSTM).", "Let us consider the case where an input word is `the' and we use character 3-gram in Figure FIGREF4 . We prepare special characters `' and `$' to represent the beginning and end of the word, respectively. Then, `the' is composed of three character 3-grams: `th', `the', and `he$'. We multiply the embeddings of these 3-grams by transformation matrix INLINEFORM0 and apply the softmax function to each row as in Equation . As a result of the softmax, we obtain a matrix that contains weights for each embedding. The size of the computed matrix is identical to the input embedding matrix: INLINEFORM1 . We then compute Equation EQREF7 , i.e., the weighted sum of the embeddings, and add the resulting vector to the word embedding of `the'. Finally, we input the vector into an RNN to predict the next word.", "To compute INLINEFORM0 , we apply an encoder to character INLINEFORM1 -gram embeddings. Previous studies demonstrated that additive composition, which computes the (weighted) sum of embeddings, is a suitable method for embedding construction BIBREF13 , BIBREF0 . Thus, we adopt (simplified) multi-dimensional self-attention BIBREF11 , which computes weights for each dimension of given embeddings and sums up the weighted embeddings (i.e., element-wise weighted sum) as an encoder. Let INLINEFORM2 be the character INLINEFORM3 -gram embeddings of an input word, let INLINEFORM4 be the number of character INLINEFORM5 -grams extracted from the word, and let INLINEFORM6 be the matrix whose INLINEFORM7 -th column corresponds to INLINEFORM8 , that is, INLINEFORM9 . The multi-dimensional self-attention constructs the word embedding INLINEFORM10 by the following equations: DISPLAYFORM0", "We incorporate char INLINEFORM0 -MS-vec, which is an embedding constructed from character INLINEFORM1 -gram embeddings, into RNN language models since, as discussed earlier, previous studies revealed that we can construct better word embeddings by using character INLINEFORM2 -gram embeddings BIBREF0 , BIBREF10 . In particular, we expect char INLINEFORM3 -MS-vec to help represent infrequent words by taking advantage of the internal structure.", "In these experiments, we only applied char3-MS-vec to EncDec but BIBREF38 indicated that combining multiple kinds of subword units can improve the performance. We will investigate the effect of combining several character INLINEFORM0 -gram embeddings in future work."]}
{"question_id": "a398c9b061f28543bc77c2951d0dfc5d1bee9e87", "predicted_answer": "The dataset used is the one curated by BIBREF4, which consists of 19538 tweets annotated with various levels of clickbait-y nature, including title, text, target description, target keywords, and linked images. The model was trained on 17000 records from this dataset and tested on 2538 disjoint instances.", "predicted_evidence": ["We start with an explanation of the various types of embeddings we have used and proceed to describe the various components of our model, both individually and together. Finally, we cover how the parameters are learned.", "BIBREF4 crowdsourced the annotation of 19538 tweets they had curated, into various levels of their clickbait-y nature. These tweets contained the title and text of the article and also included supplementary information such as target description, target keywords and linked images. We trained our model over 17000 records in the described dataset and test it over 2538 disjoint instances from the same. We performed our experiments with the aim of increasing the accuracy and F1 score of the model. Other metrics like mean squared error (MSE) were also considered.", "(1) Generate a random data sample (vectors, trajectories etc.) according to a specified mechanism.", "here INLINEFORM0 is the logistic sigmoid function, INLINEFORM1 , INLINEFORM2 , INLINEFORM3 represent the forget, input and output gates respectively. INLINEFORM4 denotes the input at time INLINEFORM5 and INLINEFORM6 denotes the latent state, INLINEFORM7 and INLINEFORM8 represent the bias terms. The forget, input and output gates control the flow of information throughout the sequence. INLINEFORM9 and INLINEFORM10 are matrices which represent the weights associated with the connections.", "where, INLINEFORM0 ,..., INLINEFORM1 represents the sequence of annotations to which the encoder maps the post title vector and each INLINEFORM2 represents the respective weight corresponding to each annotation INLINEFORM3 . This is represented as the left most component in Figure FIGREF14 ."]}
{"question_id": "dae9caf8434ce43c9bc5913ebf062bc057a27cfe", "predicted_answer": "The answer is not explicitly stated in the given context. The context discusses the importance of detecting clickbait headlines, the methodology used, and the dataset employed, but it does not provide a specific comparison with previous state-of-the-art approaches or mention by how much the current approach outperforms them.", "predicted_evidence": ["Keeping up with the times, news agencies have expanded their digital presence, increasing their reach exponentially. They generate revenue by (1) advertisements on their websites, or (2) a subscription based model for articles that might interest users. Since multiple agencies offer similar content, the user has his pick. To lure in more readers and increase the number of clicks on their content, subsequently enhancing their agency's revenue, writers have begun adopting a new technique - clickbait.", "In recent years, content delivery has changed drastically, shifting from offline methods to the Internet. It is now the primary source of information for a majority of the populace, especially for ever-changing news updates. This has also caused a shift in users' preferred sources. Previously, these preferences were static, sticking to a particular news source. Now, with the plethora of information available easily, there is no differentiation in the source it has been gathered from, with users opting to go for whatever is convenient.", "We use binary cross-entropy as the loss optimization function for our model. The cross-entropy method BIBREF18 is an iterative procedure where each iteration can be divided into two stages:", "BIBREF4 crowdsourced the annotation of 19538 tweets they had curated, into various levels of their clickbait-y nature. These tweets contained the title and text of the article and also included supplementary information such as target description, target keywords and linked images. We trained our model over 17000 records in the described dataset and test it over 2538 disjoint instances from the same. We performed our experiments with the aim of increasing the accuracy and F1 score of the model. Other metrics like mean squared error (MSE) were also considered.", "The importance of detecting clickbait headlines has increased exponentially in recent years. Initial work in this domain can be traced back to BIBREF2 , relying on heavy feature engineering on a specific news dataset. These works define the various types of clickbait and focus on the presence of linguistic peculiarities in the headline text, including various informality metrics and the use of forward references. Applying such techniques over a social media stream was first attempted by BIBREF3 as the authors crowdsourced a dataset of tweets BIBREF4 and performed feature engineering to accomplish the task. BIBREF5 have tried to expand the work done for news headlines they collected from trusted sources."]}
{"question_id": "e9b6b14b8061b71d73a73d8138c8dab8eda4ba3f", "predicted_answer": "The answer is yes, they do analyze attention outputs to determine which terms in general contribute to clickbait titles. This is mentioned in the sentence: \"Attaching an attention mechanism to it helps us identify the surprise associated with each representation within the clickbait.\"", "predicted_evidence": ["Previous work in this field that has exploited the power of embeddings has considered either word vectors, for their ability to create context-sensitive word representations, or character-level word embeddings to model the orthographic features of a word. We propose the use of sub-word level representations since it incorporates the word's morphological features. Attaching an attention mechanism to it helps us identify the surprise associated with each representation within the clickbait. One of the identifying characteristics of clickbait is that the article title differs from the text attached to it. For this reason, we define a component to capture the interaction between these attributes and augment our model.", "Merriam-Webster defines clickbait as something (such as a headline) to encourage readers to click on hyperlinks based on snippets of information accompanying it, especially when those links lead to content of dubious value or interest. It is built to create, and consequently capitalise, on the Loewenstein information gap BIBREF0 by purposefully misrepresenting or promising what can be expected while reading a story on the web, be it through a headline, image or related text.", "We now describe our approach to clickbait detection and the reasons behind devising such a model. Our approach is a fusion of multiple components, each exploiting a particular type of embedding: (1) BiLSTM with attention, and (2) Doc2Vec enrichment. Figure FIGREF14 lays out our proposed architecture.", "The importance of detecting clickbait headlines has increased exponentially in recent years. Initial work in this domain can be traced back to BIBREF2 , relying on heavy feature engineering on a specific news dataset. These works define the various types of clickbait and focus on the presence of linguistic peculiarities in the headline text, including various informality metrics and the use of forward references. Applying such techniques over a social media stream was first attempted by BIBREF3 as the authors crowdsourced a dataset of tweets BIBREF4 and performed feature engineering to accomplish the task. BIBREF5 have tried to expand the work done for news headlines they collected from trusted sources.", "Each record in the dataset has a target description attached with it. This is the entire text of the article whose title has been given. By definition, clickbait articles differ from the content described in their headline. We generate document embeddings for both the title and the article text and perform element wise multiplication over the two. This allows us to capture the interaction between the two, something which has not been used before. Since the title is supposed to mislead the reader with respect to the content, modeling this interaction in terms of their similarity gives an added dimenstion to our approach. It augments the output obtained from the first component."]}
{"question_id": "76e17e648a4d1f386eb6bf61b0c24f134af872be", "predicted_answer": "The bias mitigation methods discussed in the text can be applied to other scenarios beyond gender bias in natural language processing, such as:\n\n1. Racial bias: Similar to gender bias, these methods can be used to reduce racial bias in language models and classifiers.\n2. Age bias: Bias mitigation methods can be applied to reduce age-related biases in language models and classifiers.\n3. Disability bias: These methods can be used to reduce biases related to disabilities in language models and classifiers.\n4. Religious bias: Bias mitigation methods can be applied to reduce religious biases in language models and classifiers.\n5. Geographical bias: These methods can be used to reduce geographical biases in language models and classifiers.\n6. Other latent variables: The methods can be applied to reduce biases related to other latent variables, such as socioeconomic status, education level, or occupation.\n\nAdditionally, the bias mitigation methods can be applied to other areas beyond natural language processing, such as:\n\n1. Computer vision: Bias mitigation methods can be used to reduce biases in image classification and object detection models.\n2. Recommendation systems: These methods can be applied to reduce biases in recommendation systems, ensuring that users are not discriminated against based on certain characteristics.\n3. Healthcare: Bias mitigation methods can be used to reduce biases in healthcare models and algorithms, ensuring that patients are not discriminated against based on certain characteristics.\n\nOverall, the bias mitigation methods discussed in the text have the potential to be applied to a wide range of scenarios where biases may exist, and can help to promote fairness and equality in AI systems.", "predicted_evidence": ["All methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks (abusive & sexist). However, the decrease was marginal (less than 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important \u201cunbiased\u201d features.", "As shown in Section SECREF13 , some classification performance drop happens when mitigation methods. We believe that a meaningful extension of our work can be developing bias mitigation methods that maintain (or even increase) the classification performance and reduce the bias at the same time. Some previous works BIBREF17 , BIBREF18 employ adversarial training methods to make the classifiers unbiased toward certain variables. However, those works do not deal with natural language where features like gender and race are latent variables inside the language. Although those approaches are not directly comparable to our methods, it would be interesting to explore adversarial training to tackle this problem in the future.", "We experiment and discuss various methods to reduce gender biases identified in Section SECREF13 .", "Based on this criterion and results from Section SECREF13 , we choose the abt dataset as source and st dataset as target for bias fine-tuning experiments.", "To our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of \u201cattending\u201d model architectures on biases as discussed in Section SECREF13 . On the other hand, using the three methods together improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance."]}
{"question_id": "7572f6e68a2ed2c41b87c5088ba8680afa0c0a0b", "predicted_answer": "Yes, the three bias mitigation methods (debiased embedding, gender swap, and fine-tuning) were combined in one model, which improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance.", "predicted_evidence": ["All methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks (abusive & sexist). However, the decrease was marginal (less than 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important \u201cunbiased\u201d features.", "Based on this criterion and results from Section SECREF13 , we choose the abt dataset as source and st dataset as target for bias fine-tuning experiments.", "To our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of \u201cattending\u201d model architectures on biases as discussed in Section SECREF13 . On the other hand, using the three methods together improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance.", "We experiment and discuss various methods to reduce gender biases identified in Section SECREF13 .", "After identifying the degree of gender bias of each dataset, we select a source with less bias and a target with more bias. Vocabulary is extracted from training split of both sets. The model is first trained by the source dataset. We then remove final softmax layer and attach a new one initialized for training the target. The target is trained with a slower learning rate. Early stopping is decided by the valid set of the respective dataset."]}
{"question_id": "5d2bbcc3aa769e639dc21893890bc36b76597a33", "predicted_answer": "The most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance.", "predicted_evidence": ["All methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks (abusive & sexist). However, the decrease was marginal (less than 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important \u201cunbiased\u201d features.", "Based on this criterion and results from Section SECREF13 , we choose the abt dataset as source and st dataset as target for bias fine-tuning experiments.", "We experiment and discuss various methods to reduce gender biases identified in Section SECREF13 .", "To our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of \u201cattending\u201d model architectures on biases as discussed in Section SECREF13 . On the other hand, using the three methods together improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance.", "After identifying the degree of gender bias of each dataset, we select a source with less bias and a target with more bias. Vocabulary is extracted from training split of both sets. The model is first trained by the source dataset. We then remove final softmax layer and attach a new one initialized for training the target. The target is trained with a slower learning rate. Early stopping is decided by the valid set of the respective dataset."]}
{"question_id": "4ddc53afffaf1622d97695347dd1b3190d156dee", "predicted_answer": "The model architectures used are:\n\n1. GRU (Gated Recurrent Unit)\n2. CNN (Convolutional Neural Network) with the following specifications:\n\t* Convolution layers with 3 filters with sizes [3, 4, 5]\n\t* Feature map size = 100\n\t* Embedding Size = 300\n\t* Max-pooling\n\t* Dropout = 0.5", "predicted_evidence": ["This work is partially funded by ITS/319/16FP of Innovation Technology Commission, HKUST, and 16248016 of Hong Kong Research Grants Council.", "While the two AUC scores show the performances of the models in terms of accuracy, the equality difference scores show them in terms of fairness, which we believe is another dimension for evaluating the model's generalization ability.", "To our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of \u201cattending\u201d model architectures on biases as discussed in Section SECREF13 . On the other hand, using the three methods together improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance.", "CNN: Convolution layers with 3 filters with the size of [3,4,5], feature map size=100, Embedding Size=300, Max-pooling, Dropout=0.5", "We discussed model biases, especially toward gender identity terms, in abusive language detection. We found out that pre-trained word embeddings, model architecture, and different datasets all can have influence. Also, we found our proposed methods can reduce gender biases up to 90-98%, improving the robustness of the models."]}
{"question_id": "5d93245832d90b31aee42ea2bf1e7704c22ebeca", "predicted_answer": "The pre-trained word embeddings used in this work are:\n\n1. Word2Vec (trained on Google News corpus)\n2. FastText (trained on Wikipedia corpus)\n3. Debiased Word Embeddings (DE) (published embeddings from BIBREF9)\n4. Randomly initialized embeddings (random)", "predicted_evidence": ["Tables TABREF12 and TABREF14 show the bias measurement experiment results for st and abt, respectively. As expected, pre-trained embeddings improved task performance. The score on the unbiased generated test set (Gen. ROC) also improved since word embeddings can provide prior knowledge of words.", "Debiased Word Embeddings (DE) BIBREF9 proposed an algorithm to correct word embeddings by removing gender stereotypical information. All the other experiments used pretrained word2vec to initialized the embedding layer but we substitute the pretrained word2vec with their published embeddings to verify their effectiveness in our task.", "We also compare different pre-trained embeddings, word2vec BIBREF10 trained on Google News corpus, FastText BIBREF16 ) trained on Wikipedia corpus, and randomly initialized embeddings (random) to analyze their effects on the biases. Experiments were run 10 times and averaged.", "In this work, we address model biases specific to gender identities (gender bias) existing in abusive language datasets by measuring them with a generated unbiased test set and propose three reduction methods: (1) debiased word embedding, (2) gender swap data augmentation, (3) fine-tuning with a larger corpus. Moreover, we compare the effects of different pre-trained word embeddings and model architectures on gender bias.", "Addressing biases in NLP models/systems have recently started to gain more interest in the research community, not only because fairness in AI is important but also because bias correction can improve the robustness of the models. BIBREF9 is one of the first works to point out the gender stereotypes inside word2vec BIBREF10 and propose an algorithm to correct them. BIBREF11 also propose a method called Word Embedding Association Test (WEAT) to measure model bias inside word embeddings and finds that many of those pretrained embeddings contain problematic bias toward gender or race. BIBREF1 is one of the first works that point out existing \u201cunintended\u201d bias in abusive language detection models. BIBREF12 compare 219 sentiment analysis systems participating in SemEval competition with their proposed dataset, which can be used for evaluating racial and gender bias of those systems."]}
{"question_id": "c0dbf3f1957f3bff3ced5b48aff60097f3eac7bb", "predicted_answer": "The metrics used to measure gender biases are:\n\n1. Equality differences: This metric measures the difference in performance between male and female genders.\n2. Unbiased test set performance: This metric evaluates the model's performance on a separate unbiased test set generated for each gender using the identity term template method.\n\nThese metrics are used to assess the effectiveness of the methods proposed to reduce gender biases in NLP systems.", "predicted_evidence": ["We experiment and discuss various methods to reduce gender biases identified in Section SECREF13 .", "All methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks (abusive & sexist). However, the decrease was marginal (less than 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important \u201cunbiased\u201d features.", "Gender bias cannot be measured when evaluated on the original dataset as the test sets will follow the same biased distribution, so normal evaluation set will not suffice. Therefore, we generate a separate unbiased test set for each gender, male and female, using the identity term template method proposed in BIBREF1 .", "Although our work is preliminary, we hope that our work can further develop the discussion of evaluating NLP systems in different directions, not merely focusing on performance metrics like accuracy or AUC. The idea of improving models by measuring and correcting gender bias is still unfamiliar but we argue that they can be crucial in building systems that are not only ethical but also practical. Although this work focuses on gender terms, the methods we proposed can easily be extended to other identity problems like racial and to different tasks like sentiment analysis by following similar steps, and we hope to work on this in the future.", "To our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of \u201cattending\u201d model architectures on biases as discussed in Section SECREF13 . On the other hand, using the three methods together improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance."]}
{"question_id": "ed7ce13cd95f7664a5e4fc530dcf72dc3808dced", "predicted_answer": "The answer is yes, they ensure that the architecture is differentiable everywhere after adding the Hungarian layer. This is achieved by dynamically constructing the computational graph in the forward pass and propagating the gradients through the dynamically constructed links in the backward pass, as shown in Figure FIGREF13. This allows the optimization framework to adjust the parameters of the neural architecture in an end-to-end manner, without any branching or non-differentiated issues.", "predicted_evidence": ["Hungarian Layer. This layer, which is the matching component of our model, extracts the aligned unmatched parts from the source and target sentences. This layer is composed by two sequential stages.", "As Figure FIGREF13 shows, in the forward propagation, Hungarian algorithm works out the aligned position pairs, according to which, neural components are dynamically connected to the next layer. For the example of Figure FIGREF13 , the 1st source and 2nd target word representations are jointly linked to the 1st aligned position of concatenation layer. Once the computational graph has been dynamically constructed in the forward pass, the backward process could propagate through the dynamically constructed links between layers, without any branching and non-differentiated issues. For the example in Figure FIGREF13 , the backward pass firstly propagates to the 1st aligned position of concatenation layer, then respectively propagates to 1st source and 2nd target word representations. In this way, the optimization framework could still adjust the parameters of neural architectures in an end-to-end manner.", "First, we introduce the basic components of our neural architecture. Then, we analyze the training process of Hungarian layer, that how to dynamically construct the computational graph.", "where INLINEFORM0 is the INLINEFORM1 -th output of Hungarian layer, INLINEFORM2 is the INLINEFORM3 -th aligned source/target hidden representation generated by Algorithm SECREF7 and INLINEFORM4 is the scalar-vector multiplication. Actually in the practical setting, most of cosine dissimilarity approach 0 and the remaining hidden representations indicate the aligned unmatched parts.", "Previously discussed, Hungarian algorithm is embedded into neural architecture, making a challenge for learning process. We tackle this issue by modifying the back-propagation algorithm in a dynamically graph-constructing manner. In the forward pass, we dynamically construct the links between Hungarian layer and the next layer, according to the aligned position pairs, while in the backward process, the back-propagation is performed through the dynamically constructed links. Next, we illustratively exemplify how the computational graph is dynamically constructed in Hungarian layer as Figure FIGREF13 shows."]}
{"question_id": "26eceba0e6e4c0b6dfa94e5708dd74b63f701731", "predicted_answer": "The dataset they train on is the \"Quora Question Pairs\" dataset, which contains over 400,000 question pairs annotated with a binary value indicating whether the two questions are paraphrases of each other or not.", "predicted_evidence": ["First, we introduce the basic components of our neural architecture. Then, we analyze the training process of Hungarian layer, that how to dynamically construct the computational graph.", "Our neural architecture is illustrated in Figure FIGREF6 . Basically our model is composed by four components, namely, word embedding, bi-directional LSTM (BiLSTM), Hungarian layer and cosine similarity.", "Dataset. Actually, to demonstrate the effectiveness of our model, we perform our experiments on the famous public benchmark dataset of \u201cQuora Question Pairs\u201d . For a fair comparison, we follow the splitting rules of BIBREF2 . Specifically, there are over 400,000 question pairs in this dataset, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other or not. We randomly select 5,000 paraphrases and 5,000 non-paraphrases as the development set, and sample another 5,000 paraphrases and 5,000 non-paraphrases as the test set. We keep the remaining instances as the training set. Baselines. To make a sufficient comparison, we choose five state-of-the-art baselines: Siamese CNN, Multi-Perspective CNN, Siamese LSTM, Multi-Perspective LSTM, and L.D.C.", "Dataset. Actually, to demonstrate the effectiveness of our model, we perform our experiments on the famous public benchmark dataset of \u201cQuora Question Pairs\u201d . For a fair comparison, we follow the splitting rules of BIBREF2 . Specifically, there are over 400,000 question pairs in this dataset, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other or not. We randomly select 5,000 paraphrases and 5,000 non-paraphrases as the development set, and sample another 5,000 paraphrases and 5,000 non-paraphrases as the test set. We keep the remaining instances as the training set. Baselines. To make a sufficient comparison, we choose five state-of-the-art baselines: Siamese CNN, Multi-Perspective CNN, Siamese LSTM, Multi-Perspective LSTM, and L.D.C. Specifically, Siamese CNN and LSTM encode the two input sentences into two sentence vectors by CNN and LSTM, respectively, BIBREF24 .", "Algorithm SECREF7 demonstrates the first stage. The objective of this stage is to align the source and target hidden representations. The inputs of this stage are INLINEFORM0 source hidden representation vectors INLINEFORM1 and INLINEFORM2 target hidden representation vectors INLINEFORM3 , while the outputs of this stage are INLINEFORM4 aligned hidden representation vector pairs INLINEFORM5 , assuming INLINEFORM6 , where INLINEFORM7 corresponds to the INLINEFORM8 -th aligned source/target hidden representation vector, respectively."]}
{"question_id": "ff69b363ca604f80b2aa7afdc6a32d2ffd2d1f85", "predicted_answer": "The answer is not explicitly stated in the given context. The text only mentions that their model outperforms the state-of-the-art baselines, but it does not provide a specific metric or value by which their model outperforms the baselines.", "predicted_evidence": ["Our method outperforms all the baselines, which illustrates the effectiveness of our model.", "Dataset. Actually, to demonstrate the effectiveness of our model, we perform our experiments on the famous public benchmark dataset of \u201cQuora Question Pairs\u201d . For a fair comparison, we follow the splitting rules of BIBREF2 . Specifically, there are over 400,000 question pairs in this dataset, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other or not. We randomly select 5,000 paraphrases and 5,000 non-paraphrases as the development set, and sample another 5,000 paraphrases and 5,000 non-paraphrases as the test set. We keep the remaining instances as the training set. Baselines. To make a sufficient comparison, we choose five state-of-the-art baselines: Siamese CNN, Multi-Perspective CNN, Siamese LSTM, Multi-Perspective LSTM, and L.D.C.", "Dataset. Actually, to demonstrate the effectiveness of our model, we perform our experiments on the famous public benchmark dataset of \u201cQuora Question Pairs\u201d . For a fair comparison, we follow the splitting rules of BIBREF2 . Specifically, there are over 400,000 question pairs in this dataset, and each question pair is annotated with a binary value indicating whether the two questions are paraphrase of each other or not. We randomly select 5,000 paraphrases and 5,000 non-paraphrases as the development set, and sample another 5,000 paraphrases and 5,000 non-paraphrases as the test set. We keep the remaining instances as the training set. Baselines. To make a sufficient comparison, we choose five state-of-the-art baselines: Siamese CNN, Multi-Perspective CNN, Siamese LSTM, Multi-Perspective LSTM, and L.D.C. Specifically, Siamese CNN and LSTM encode the two input sentences into two sentence vectors by CNN and LSTM, respectively, BIBREF24 .", "Notably, L.D.C. is a very complex model, which is beaten by our simple model within a statistically significant improvement. This comparison illustrates our model is indeed simple but effective. Thus it is very suitable for industrial applications.", "In this section, we verify our model performance on the famous public benchmark dataset of \u201cQuora Question Pairs\u201d. First, we introduce the experimental settings, in Section 4.1. Then, in Section 4.2, we conduct the performance evaluation. Last, in order to further test our assumptions, that the aligned unmatched parts are semantically critical, we conduct a case study for illustration in Section 4.3."]}
{"question_id": "ee19fd54997f2eec7c87c7d4a2169026fe208285", "predicted_answer": "No, the text does not compare the results to previous work. It presents the results of an experiment comparing different systems for generating movie summaries, but it does not discuss how these results compare to previous research or studies in the field.", "predicted_evidence": ["), and Grammaticality (i.e., is the summary fluent and grammatical?). Examples of system summaries are shown in Figure FIGREF1 and Figure FIGREF37. We randomly selected 50 movies from the test set and compared all possible combinations of summary triples for each movie. We collected three judgments for each comparison. The order of summaries and movies was randomized per participant. The score of a system was computed as the percentage of times it was chosen as best minus the percentage of times it was selected as worst. The scores range from -1 (worst) to 1 (best) and are shown in Table TABREF36. Perhaps unsurprisingly, the human-generated gold summaries were considered best, whereas our model (AE+Att+Copy+Salient) was ranked second, indicating that humans find its output more informative, correct, and grammatical compared to other systems. SummaRunner was ranked third followed by SummaRunner+S2S+Copy.", "We collected three judgments for each comparison. The order of summaries and movies was randomized per participant. The score of a system was computed as the percentage of times it was chosen as best minus the percentage of times it was selected as worst. The scores range from -1 (worst) to 1 (best) and are shown in Table TABREF36. Perhaps unsurprisingly, the human-generated gold summaries were considered best, whereas our model (AE+Att+Copy+Salient) was ranked second, indicating that humans find its output more informative, correct, and grammatical compared to other systems. SummaRunner was ranked third followed by SummaRunner+S2S+Copy. We inspected the summaries produced by the latter system and found they were factually incorrect bearing little correspondence to the movie (examples shown in Figure FIGREF37), possibly due to the huge information loss at the extraction stage. All pairwise system differences are statistically significant using a one-way ANOVA with posthoc Tukey HSD tests ($p < 0.01$).", "Table TABREF40 shows what participants thought of summaries produced by non-customized systems (see column No) and systems which had customization switched on (see column Yes). Overall, we observe that AE+Att+Copy is able to customize summaries to a great extent. In all cases, crowdworkers perceive a significant increase in the proportion of aspect $x$ when using Cust($x$). AE+Att+Copy+Salient is unable to generate need-specific summaries, showing no discernible difference between generic and customized summaries. This shows that the use of an extractive module, which is used as one of the main components of EA-based approaches, limits the flexibility of the abstractive model to customize summaries based on a user need.", "Examples of system summaries are shown in Figure FIGREF1 and Figure FIGREF37. We randomly selected 50 movies from the test set and compared all possible combinations of summary triples for each movie. We collected three judgments for each comparison. The order of summaries and movies was randomized per participant. The score of a system was computed as the percentage of times it was chosen as best minus the percentage of times it was selected as worst. The scores range from -1 (worst) to 1 (best) and are shown in Table TABREF36. Perhaps unsurprisingly, the human-generated gold summaries were considered best, whereas our model (AE+Att+Copy+Salient) was ranked second, indicating that humans find its output more informative, correct, and grammatical compared to other systems. SummaRunner was ranked third followed by SummaRunner+S2S+Copy. We inspected the summaries produced by the latter system and found they were factually incorrect bearing little correspondence to the movie (examples shown in Figure FIGREF37), possibly due to the huge information loss at the extraction stage.", "), Correctness (i.e., is the information in the summary factually accurate and does it correspond to the information given about the movie?), and Grammaticality (i.e., is the summary fluent and grammatical?). Examples of system summaries are shown in Figure FIGREF1 and Figure FIGREF37. We randomly selected 50 movies from the test set and compared all possible combinations of summary triples for each movie. We collected three judgments for each comparison. The order of summaries and movies was randomized per participant. The score of a system was computed as the percentage of times it was chosen as best minus the percentage of times it was selected as worst. The scores range from -1 (worst) to 1 (best) and are shown in Table TABREF36. Perhaps unsurprisingly, the human-generated gold summaries were considered best, whereas our model (AE+Att+Copy+Salient) was ranked second, indicating that humans find its output more informative, correct, and grammatical compared to other systems."]}
{"question_id": "74fcb741d29892918903702dbb145fef372d1de3", "predicted_answer": "The model is trained to optimize the generation probability distribution $p(y^{\\prime }_t)$ based on gold summaries $Y$ provided at training time.", "predicted_evidence": ["The auto-encoder is trained with a maximum likelihood loss:", "We use two objective functions to train the Abstract model. Firstly, we use a maximum likelihood loss to optimize the generation probability distribution $p(y^{\\prime }_t)$ based on gold summaries $Y=\\lbrace y_1,y_2,...,y_L\\rbrace $ provided at training time:", "where $W$, $v$, and $b$ are learned parameters, and $t$ is the current timestep.", "At each time step $t$, we use an attention mechanism over word-level encodings to output the attention weight vector $a_t$ and context vector $c_t$:", "An advantage of using a separate encoder is increased training data, since we treat a single target with $N$ input documents as $N$ different instances. Once training has taken place, we use the Condense model to obtain $N$ pairs of document encodings $\\lbrace d_i\\rbrace $ and word-level encodings $\\lbrace h_{i,1}, h_{i,2}, ..., h_{i,M}\\rbrace $, $1 \\le i \\le N$ as representations for the documents in $\\mathcal {D}$."]}
{"question_id": "de0d135b94ba3b3a4f4a0fb03df38a84f9dc9da4", "predicted_answer": "The dataset used contains 3,731 movies, with an average of 99.8 reviews per movie, and a total of 2,458 movies for training, 536 movies for development, and 737 movies for testing.", "predicted_evidence": ["We performed experiments on the Rotten Tomatoes dataset provided in BIBREF16. It contains 3,731 movies; for each movie we are given a large set of reviews (99.8 on average) written by professional critics and users and a gold-standard consensus, i.e. a summary written by an editor (see an example in Figure FIGREF1). On average, reviews are 19.7 tokens long, while the summary length is 19.6 tokens. The dataset is divided into 2,458 movies for training, 536 movies for development, and 737 movies for testing. Following previous work BIBREF16, we used a generic label for movie titles during training which we replace with the original movie names at test time.", "An advantage of using a separate encoder is increased training data, since we treat a single target with $N$ input documents as $N$ different instances. Once training has taken place, we use the Condense model to obtain $N$ pairs of document encodings $\\lbrace d_i\\rbrace $ and word-level encodings $\\lbrace h_{i,1}, h_{i,2}, ..., h_{i,M}\\rbrace $, $1 \\le i \\le N$ as representations for the documents in $\\mathcal {D}$.", "The $N$ pairs of document encodings $\\lbrace d_i\\rbrace $ and word-level encodings $\\lbrace h_{i,1}, h_{i,2}, ..., h_{i,M}\\rbrace $, $1 \\le i \\le N$ are aggregated into a single pair of document encoding $d^{\\prime }$ and word-level encodings $h^{\\prime }_1, h^{\\prime }_2, ..., h^{\\prime }_V$, where $V$ is the number of total unique tokens in the input. We fuse document encodings, using an attentive pooling method which gives more weight to important documents. Specifically, we learn a set of weight vectors $a_i \\in \\mathbb {R}^{D_d}$, where $D_d$ is the dimension of $d_i$, to weight-sum the document encodings:", "They respectively measure word-overlap, bigram-overlap, and the longest common subsequence between the reference and system summaries. Our results are presented in Table TABREF28. The first block shows one-pass systems, both supervised (SubModular, SummaRunner) and unsupervised (LexRank, Opinosis). We can see that SummaRunner is the best performing system in this block; despite being extractive, it benefits from training data and the ability of neural models to learn task-specific representations. The second block in Table TABREF28 shows several two-pass abstractive systems based on the EA framework. Our implementation of an EA-based system, SummaRunner+S2S+Copy, improves over the purely extractive SummaRunner and the previously reported best EA-based system, Regress+S2S. The third block presents two models using the proposed CA framework. Both systems outperform all other models across all metrics; AE+Att+Copy+Salient is the best model overall which exploits information about all documents and most salient ones.", "EA-based methods include (g) Regress+S2S BIBREF16, an instantiation of the EA framework where a ridge regression model with hand-engineered features implements the Extract model, while an attention-based sequence-to-sequence neural network is the Abstract model; (h) SummaRunner+S2S, our implementation of an EA-based system which uses SummaRunner instead of Regress as the Extract model; and (i) SummaRunner+S2S+Copy, the same model as (h) but enhanced with a copy mechanism BIBREF32. For all EA-based systems, we set $k=5$, which is tuned on the development set. Larger $k$ leads to worse performance, possibly because the Abstract model becomes harder to optimize."]}
{"question_id": "6a20a3220c4edad758b912e2d3e5b99b0b295d96", "predicted_answer": "Based on the context, it appears that the weights are assigned to each sentence using the INLINEFORM1 models, and then the weights are combined across models using the INLINEFORM2 weights.\n\nIn other words, the INLINEFORM2 weights are used to weigh the importance of each model, and the INLINEFORM0 function is used to calculate the weight for each sentence within each model. The final weight for each sentence is then a combination of the model-specific weights, weighted by the INLINEFORM2 weights.\n\nFor example, if we have two models, M1 and M2, with INLINEFORM2 weights of 0.6 and 0.4, respectively, and a sentence S with weights of 0.8 and 0.2 from M1 and M2, respectively, the final weight for S would be:\n\ncWeight(S) = 0.6 \\* 0.8 + 0.4 \\* 0.2 = 0.56\n\nThis allows the system to combine the outputs of multiple models, giving more importance to the models that are considered more reliable or accurate.", "predicted_evidence": ["We discuss, in detail, the steps that are common for both statistical and semantic models.", "Here, INLINEFORM0 is a function which returns 1 if sentence is in summary of INLINEFORM1 model, otherwise zero. INLINEFORM2 is weight assigned to each model without training, INLINEFORM3", "Now for our list of sentences INLINEFORM0 we define cWeight as weight obtained for each sentence using INLINEFORM1 models.", "For each model, assign the weights using INLINEFORM0", "We use two similarity measures :"]}
{"question_id": "c2745e44ebe7dd57126b784ac065f0b7fc2630f1", "predicted_answer": "No, the given context does not explicitly compare their hybrid summarization approach against state-of-the-art summarization approaches. However, it does mention that abstractive summarization is still in a developing phase and relies on natural language processing techniques, which are still evolving to match human standards. It also mentions a state-of-the-art method proposed by Wenyuan Zeng that produces summaries with a length restricted to 75.", "predicted_evidence": ["There is a void of hybrid summarizers; there haven't been many studies made in the area.Wong BIBREF13 conducted some preliminary research but there isn't much there on benchmark tests to our knowledge. We use a mixture of statistical and semantic models, assign weights among them by training on field-specific corpora. As there is a significant variation in choices among different fields. We support our proposal with expectations that shortcomings posed by one model can be filled with positives from others. We deploy experimental analysis to test our proposition.", "Summarize using top ranking sentences.", "It is mostly distributed into two distinct domains, i.e., Abstractive Summarization and Extractive Summarization. Abstractive summarization( Dejong et al. ,1978) involves models to deduce the crux of the document. It then presents a summary consisting of words and phrases that were not there in the actual document, sometimes even paraphrasing BIBREF1 . A state of art method proposed by Wenyuan Zeng BIBREF2 produces such summaries with length restricted to 75. There have been many recent developments that produce optimal results, but it is still in a developing phase. It highly relies on natural language processing techniques, which is still evolving to match human standards. These shortcomings make abstractive summarization highly domain selective. As a result, their application is skewed to the areas where NLP techniques have been superlative. Extractive Summarization, on the other hand, uses different methods to identify the most informative/dominant sentences through the text, and then present the results, ranking them accordingly.", "Automatic Text Summarization deals with the task of condensing documents into a summary, whose level is similar to a human-generated summary. It is mostly distributed into two distinct domains, i.e., Abstractive Summarization and Extractive Summarization. Abstractive summarization( Dejong et al. ,1978) involves models to deduce the crux of the document. It then presents a summary consisting of words and phrases that were not there in the actual document, sometimes even paraphrasing BIBREF1 . A state of art method proposed by Wenyuan Zeng BIBREF2 produces such summaries with length restricted to 75. There have been many recent developments that produce optimal results, but it is still in a developing phase. It highly relies on natural language processing techniques, which is still evolving to match human standards. These shortcomings make abstractive summarization highly domain selective. As a result, their application is skewed to the areas where NLP techniques have been superlative.", "Automatic Text Summarization deals with the task of condensing documents into a summary, whose level is similar to a human-generated summary. It is mostly distributed into two distinct domains, i.e., Abstractive Summarization and Extractive Summarization. Abstractive summarization( Dejong et al. ,1978) involves models to deduce the crux of the document. It then presents a summary consisting of words and phrases that were not there in the actual document, sometimes even paraphrasing BIBREF1 . A state of art method proposed by Wenyuan Zeng BIBREF2 produces such summaries with length restricted to 75. There have been many recent developments that produce optimal results, but it is still in a developing phase. It highly relies on natural language processing techniques, which is still evolving to match human standards. These shortcomings make abstractive summarization highly domain selective."]}
{"question_id": "d5dcc89a08924bed9772bc431090cbb52fb7836f", "predicted_answer": "According to the text, the best performing combination of semantic and statistical models on the summarization task in terms of ROUGE score is the InferSent-based Summarizer, which performed exceptionally well.", "predicted_evidence": ["There is a void of hybrid summarizers; there haven't been many studies made in the area.Wong BIBREF13 conducted some preliminary research but there isn't much there on benchmark tests to our knowledge. We use a mixture of statistical and semantic models, assign weights among them by training on field-specific corpora. As there is a significant variation in choices among different fields. We support our proposal with expectations that shortcomings posed by one model can be filled with positives from others. We deploy experimental analysis to test our proposition.", "In the Table 1, we try different model pairs with weights trained on corpus for Task 2. We have displayed mean ROUGE-2 scores for base Models. We have calculated final scores taking into consideration all normalizations, stemming, lemmatizing and clustering techniques, and the ones providing best results were used. We generally expected WordNet, Glove based semantic models to perform better given they better capture crux of the sentence and compute similarity using the same, but instead, they performed average. This is attributed to the fact they assigned high similarity scores to not so semantically related sentences. We also observe that combinations with TF/IDF and Similarity Matrices(Jaccard/Cosine) offer nearly same results. The InferSent based Summarizer performed exceptionally well. We initially used pre-trained features to generate sentence vectors through InferSent.", "Summarize using top ranking sentences.", "We evaluate our approaches on 2004 DUC(Document Understanding Conferences) dataset(https://duc.nist.gov/). The Dataset has 5 Tasks in total. We work on Task 2. It (Task 2) contains 50 news documents cluster for multi-document summarization. Only 665-character summaries are provided for each cluster. For evaluation, we use ROGUE, an automatic summary evaluation metric. It was firstly used for DUC 2004 data-set. Now, it has become a benchmark for evaluation of automated summaries. ROUGE is a correlation metric for fixed-length summaries populated using n-gram co-occurrence. For comparison between model summary and to-be evaluated summary, separate scores for 1, 2, 3, and 4-gram matching are kept. We use ROUGE-2, a bi-gram based matching technique for our task.", "We here use machine learning based approach to further increase the quality of our summarization technique. The elemental concept is that we use training set of INLINEFORM0 domain specific documents, with gold standard/human-composed summaries, provided we fine tune our weights INLINEFORM1 for different models taking F1-score/F-measure. BIBREF23 as factor. INLINEFORM2"]}
{"question_id": "d418bf6595b1b51a114f28ac8a6909c278838aeb", "predicted_answer": "The QA system used in this work is a retrieval-based QA system that provides a top-10 ranking of answer candidates.", "predicted_evidence": ["With this work, we want to answer the question whether a deployed QA system that is difficult to adapt and that provides a top-10 ranking of answer candidates, can be improved by an additional re-ranking step that corresponds to the operationalisation step of the augmented CRISP cycle. It is also important to know the potential gain and the limitations of such a method that works on top of an existing system. We hypothesise that our proposed re-ranking approach can effectively improve ranking-based QA systems.", "In this work, we include two corpora: one for training the baseline system and another for evaluating the performance of the QA pipeline and our re-ranking approach. In the following, we describe the creation of the training corpus and the structure of the test corpus. Both corpora have been anonymised.", "We propose a re-ranking algorithm similar to BIBREF17 : we train a similarity model using n-gram based features of QA pairs for improving the answer selection of a retrieval-based QA system.", "The broad field of QA includes research ranging from retrieval-based BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 to generative BIBREF6 , BIBREF7 , as well as, from closed-domain BIBREF8 , BIBREF9 to open-domain QA BIBREF7 , BIBREF10 , BIBREF11 , BIBREF12 . We focus on the notion of improving an already deployed system.", "We implemented a simple re-ranking method and showed that it can effectively improve the performance of QA systems after deployment. Our approach includes the top-10 answer candidates and confidences of the initial ranking for selecting better answers. Promising directions for future work include the investigation of more advanced ranking approaches for increasing the performance gain and continuous improvements through crowdsourcing and active learning."]}
{"question_id": "6d6b0628d8a942c57d7af1447a563021be79bc64", "predicted_answer": "No, the re-ranking approach described in this paper is not a transductive learning technique.\n\nTransductive learning involves training a model on a specific test set, without seeing the labels, and then making predictions on that same test set. In contrast, the re-ranking approach described in this paper takes the top-10 answer candidates from an existing QA system and re-ranks them using a separate model. The re-ranking model is not trained on the test set, but rather uses the output of the existing QA system as input. This is a form of post-processing or refinement, rather than transductive learning.", "predicted_evidence": ["Further, the re-ranking model is based on very simple features. It would be interesting to investigate the impact of more advanced features, or models, on the ranking performance (e.g., word embeddings BIBREF26 and deep neural networks for learning similarity functions BIBREF3 , BIBREF4 ). Nevertheless, as can be seen in examples 1, 2 and 4 in Table TABREF1 , high-ranked but incorrect answers are often meaningful with respect to the query: the setting in our evaluation is overcritical, because we count incorrect, but meaningful answers as negative result. A major limitation is that the re-ranking algorithm cannot choose answer candidates beyond the top-10 results. It would be interesting to classify whether an answer is present in the top-10 or not. If not, the algorithm could search outside the top-10 results. Such a meta-model can also be used to estimate weaknesses of the QA model: it can determine topics that regularly fail, for instance, to guide data labelling for a targeted improvement of the model, also known as active learning BIBREF27 , and in combination with techniques from semi-supervised learning BIBREF5 , BIBREF28 .", "With this work, we want to answer the question whether a deployed QA system that is difficult to adapt and that provides a top-10 ranking of answer candidates, can be improved by an additional re-ranking step that corresponds to the operationalisation step of the augmented CRISP cycle. It is also important to know the potential gain and the limitations of such a method that works on top of an existing system. We hypothesise that our proposed re-ranking approach can effectively improve ranking-based QA systems.", "Our results indicate that the accuracy of the described QA system benefits from our re-ranking approach. Hence, it can be applied to improve the performance of already deployed QA systems that provide a top-10 ranking with confidences as output. However, the performance gain is small, which might have several reasons. For example, we did not integrate spell-checking in our re-ranking method which proved to be effective in our baseline evaluation. Further, the re-ranking model is based on very simple features. It would be interesting to investigate the impact of more advanced features, or models, on the ranking performance (e.g., word embeddings BIBREF26 and deep neural networks for learning similarity functions BIBREF3 , BIBREF4 ). Nevertheless, as can be seen in examples 1, 2 and 4 in Table TABREF1 , high-ranked but incorrect answers are often meaningful with respect to the query: the setting in our evaluation is overcritical, because we count incorrect, but meaningful answers as negative result.", "We implemented a simple re-ranking method and showed that it can effectively improve the performance of QA systems after deployment. Our approach includes the top-10 answer candidates and confidences of the initial ranking for selecting better answers. Promising directions for future work include the investigation of more advanced ranking approaches for increasing the performance gain and continuous improvements through crowdsourcing and active learning.", "Hence, it can be applied to improve the performance of already deployed QA systems that provide a top-10 ranking with confidences as output. However, the performance gain is small, which might have several reasons. For example, we did not integrate spell-checking in our re-ranking method which proved to be effective in our baseline evaluation. Further, the re-ranking model is based on very simple features. It would be interesting to investigate the impact of more advanced features, or models, on the ranking performance (e.g., word embeddings BIBREF26 and deep neural networks for learning similarity functions BIBREF3 , BIBREF4 ). Nevertheless, as can be seen in examples 1, 2 and 4 in Table TABREF1 , high-ranked but incorrect answers are often meaningful with respect to the query: the setting in our evaluation is overcritical, because we count incorrect, but meaningful answers as negative result. A major limitation is that the re-ranking algorithm cannot choose answer candidates beyond the top-10 results."]}
{"question_id": "b21245212244ad7adf7d321420f2239a0f0fe56b", "predicted_answer": "The test set used for evaluating the proposed re-ranking approach is 1/10 of the evaluation corpus, since 10-fold cross-validation is used, where 9/10 of the data is used for training and 1/10 for testing, with 10 different train-test splits.", "predicted_evidence": ["We compare our data-driven QA system with a version that re-ranks resulting top-10 candidates using the additional ranking model. We want to answer the question whether our re-ranking approach can improve the performance of the baseline QA pipeline after deployment. For that, we use the evaluation corpus ( INLINEFORM0 ) for training and evaluating our re-ranking method using 10-fold cross-validation, i.e., INLINEFORM1 of the data is used for training and INLINEFORM2 for testing with 10 different train-test splits.", "We evaluate the baseline model using all training configurations in Table TABREF4 to find a well-performing baseline for our re-ranking experiment. We use the evaluation corpus as reference data and report the top-1 to top-10 accuracies and the mean reciprocal rank for the top-10 results (MRR@10) as performance metrics. For computing the top-n accuracy, we count all queries for which the QA pipeline contains a correct answer on rank 1 to n and divide the result by the number of test queries. The MRR is computed as the mean of reciprocal ranks over all test queries. The reciprocal rank for one query is defined as INLINEFORM0 : The RR is 1 if the correct answer is ranked first, INLINEFORM1 if it is at the second rank and so on. We set RR to zero, if the answer is not contained in the top-10 results.", "With this work, we want to answer the question whether a deployed QA system that is difficult to adapt and that provides a top-10 ranking of answer candidates, can be improved by an additional re-ranking step that corresponds to the operationalisation step of the augmented CRISP cycle. It is also important to know the potential gain and the limitations of such a method that works on top of an existing system. We hypothesise that our proposed re-ranking approach can effectively improve ranking-based QA systems.", "Further, the re-ranking model is based on very simple features. It would be interesting to investigate the impact of more advanced features, or models, on the ranking performance (e.g., word embeddings BIBREF26 and deep neural networks for learning similarity functions BIBREF3 , BIBREF4 ). Nevertheless, as can be seen in examples 1, 2 and 4 in Table TABREF1 , high-ranked but incorrect answers are often meaningful with respect to the query: the setting in our evaluation is overcritical, because we count incorrect, but meaningful answers as negative result. A major limitation is that the re-ranking algorithm cannot choose answer candidates beyond the top-10 results. It would be interesting to classify whether an answer is present in the top-10 or not. If not, the algorithm could search outside the top-10 results. Such a meta-model can also be used to estimate weaknesses of the QA model: it can determine topics that regularly fail, for instance, to guide data labelling for a targeted improvement of the model, also known as active learning BIBREF27 , and in combination with techniques from semi-supervised learning BIBREF5 , BIBREF28 .", "Hence, it can be applied to improve the performance of already deployed QA systems that provide a top-10 ranking with confidences as output. However, the performance gain is small, which might have several reasons. For example, we did not integrate spell-checking in our re-ranking method which proved to be effective in our baseline evaluation. Further, the re-ranking model is based on very simple features. It would be interesting to investigate the impact of more advanced features, or models, on the ranking performance (e.g., word embeddings BIBREF26 and deep neural networks for learning similarity functions BIBREF3 , BIBREF4 ). Nevertheless, as can be seen in examples 1, 2 and 4 in Table TABREF1 , high-ranked but incorrect answers are often meaningful with respect to the query: the setting in our evaluation is overcritical, because we count incorrect, but meaningful answers as negative result. A major limitation is that the re-ranking algorithm cannot choose answer candidates beyond the top-10 results."]}
{"question_id": "4a201b8b9cc566b56aedb5ab45335f202bc41845", "predicted_answer": "The new metric is the Neighbour Similarity Test (NST) metric, which is defined as the Jaccard index between the sets of characteristics of two entities.", "predicted_evidence": ["The second metric is the Type and Category Test (TCT), based on the assumption that two entities which share types and categories should be close in the vector space. This assumption is suggested by the human bias for which rdf:type and dct:subject would be predicates with a higher weight than the others. Although this does not happen, we compute it for a mere sake of comparison with the NST metric. The TCT formula is equal to Equation 19 except for sets $C_K(e)$ , which are replaced by sets of types and categories $TC_K(e)$ .", "In this paper, we introduce two metrics inspired by The Identity of Indiscernibles BIBREF24 to gain insights over the distributional quality of the learned embeddings. The more characteristics two entities share, the more similar they are and so should be their vector representations. Considering the set of characteristics $C_K(s)=\\lbrace (p_1,o_1),\\dots ,(p_m,o_m)\\rbrace $ of a subject $s$ in a triple, we can define a metric that expresses the similarity among two entities $e_1,e_2$ as the Jaccard index between their sets of characteristics $C_K(e_1)$ and $C_K(e_2)$ . Given a set of entities $\\tilde{E}$ and their $N$ nearest neighbours in the vector space, the overall Neighbour Similarity Test (NST) metric is defined as:", "However, we approximate the final value by tracing the partial values of NST and TCT over time. In other words, at each iteration $i$ , we compute the metrics over $\\tilde{E}_i = \\lbrace e_1, \\dots , e_i\\rbrace $ . Figure 2 shows the partial TCT value on the most important 10,000 entities for $N=\\lbrace 1,10\\rbrace $ according to the ranks computed by BIBREF26 . Here, KG2Vec maintains a higher index than the other two approaches, despite these are steadily increasing after the $\\sim 2,000$ th entity. We interpret the lower TCT for the top $2,000$ entities as noise produced by the fact that these nodes are hyperconnected to the rest of the graph, therefore it is hard for them to remain close to their type peers. In Figures 2 and 3 , the TCT and NST metrics respectively are computed on 10,000 random entities.", "Computing the NST and TCT distributional quality metrics on the entire DBpedia dataset is time-demanding, since for each entity, the model and the graph need to be queried for the $N$ nearest neighbours and their respective sets. However, we approximate the final value by tracing the partial values of NST and TCT over time. In other words, at each iteration $i$ , we compute the metrics over $\\tilde{E}_i = \\lbrace e_1, \\dots , e_i\\rbrace $ . Figure 2 shows the partial TCT value on the most important 10,000 entities for $N=\\lbrace 1,10\\rbrace $ according to the ranks computed by BIBREF26 . Here, KG2Vec maintains a higher index than the other two approaches, despite these are steadily increasing after the $\\sim 2,000$ th entity.", "where $\\epsilon $ is an arbitrarily small positive value. In words, given a predicate $\\bar{p}$ , we select all triples where it occurs. For each triple, we compute the relation vector as the difference between the object and the subject vectors. We then count a match whenever the vector sum of subject $\\bar{s}$ and relation is close to object $\\bar{o}$ within a radius $\\epsilon $ . The score is equal to the rate of matches over the number of selected triples."]}
{"question_id": "6a90135bd001be69a888076aff1b149b78adf443", "predicted_answer": "According to the text, other state-of-the-art models take significantly longer to process the same amount of data. Specifically:\n\n* RDF2Vec and KGloVe were evaluated with their best settings, but no specific processing time is mentioned.\n* fastText embeddings were computed, but the process had to be halted after three days, implying that it would take much longer than KG2Vec to process the data.\n\nIn contrast, KG2Vec takes approximately 12.7 hours (2.7 hours for token counting, 7.7 hours for learning, and 2.3 hours for saving the model) to process the DBpedia 2016-04 dataset.", "predicted_evidence": ["In Figure 3 , we show the CPU, Memory, and disk consumption for KG2Vec on the larger model of DBpedia 2016-04. All three subphases of the algorithm are visible in the plot. For 2.7 hours, tokens are counted; then, the learning proceeds for 7.7 hours; finally in the last 2.3 hours, the model is saved.", "In this study, we aim at generating embeddings at a high rate while preserving accuracy. In Table 1 , we already showed that our simple pipeline can achieve a rate of almost $11,000$ triples per second on a large dataset such as DBpedia 2016-04. In Table 2 , we compare KG2Vec with three other scalable approaches for embedding knowledge bases. We selected the best settings of RDF2Vec and KGloVe according to their respective articles, since both algorithms had already been successfully evaluated on DBpedia BIBREF4 , BIBREF18 . We also tried to compute fastText embeddings on our machine, however we had to halt the process after three days. As the goal of our investigation is efficiency, we discarded any other KGE approach that would have needed more than three days of computation to deliver the final model BIBREF18 .", "Computing the NST and TCT distributional quality metrics on the entire DBpedia dataset is time-demanding, since for each entity, the model and the graph need to be queried for the $N$ nearest neighbours and their respective sets. However, we approximate the final value by tracing the partial values of NST and TCT over time. In other words, at each iteration $i$ , we compute the metrics over $\\tilde{E}_i = \\lbrace e_1, \\dots , e_i\\rbrace $ . Figure 2 shows the partial TCT value on the most important 10,000 entities for $N=\\lbrace 1,10\\rbrace $ according to the ranks computed by BIBREF26 . Here, KG2Vec maintains a higher index than the other two approaches, despite these are steadily increasing after the $\\sim 2,000$ th entity.", "Computing the NST and TCT distributional quality metrics on the entire DBpedia dataset is time-demanding, since for each entity, the model and the graph need to be queried for the $N$ nearest neighbours and their respective sets. However, we approximate the final value by tracing the partial values of NST and TCT over time. In other words, at each iteration $i$ , we compute the metrics over $\\tilde{E}_i = \\lbrace e_1, \\dots , e_i\\rbrace $ . Figure 2 shows the partial TCT value on the most important 10,000 entities for $N=\\lbrace 1,10\\rbrace $ according to the ranks computed by BIBREF26 . Here, KG2Vec maintains a higher index than the other two approaches, despite these are steadily increasing after the $\\sim 2,000$ th entity. We interpret the lower TCT for the top $2,000$ entities as noise produced by the fact that these nodes are hyperconnected to the rest of the graph, therefore it is hard for them to remain close to their type peers.", "The dataset used in the experiments are described in Table 1 . The AKSW-bib dataset \u2013 employed for the link prediction evaluation \u2013 was created using information from people and projects on the AKSW.org website and bibliographical data from Bibsonomy. We built a model on top of the English 2015-10 version of the DBpedia knowledge graph BIBREF25 ; Figure 1 shows a 3-dimensional plot of selected entities. For the English DBpedia 2016-04 dataset, we built two models. In the first, we set a threshold to embed only the entities occurring at least 5 times in the dataset; we chose this setting to be aligned to the related works' models. In the second model, all 36 million entities in DBpedia are associated a vector. More insights about the first model can be found in the next two subsections, while the resource consumption for creating the second model can be seen in Figure 3 ."]}
{"question_id": "1f40adc719d8ccda81e7e90525b577f5698b5aad", "predicted_answer": "The context used when computing the embedding for an entity is the set of triples in which the entity appears as a subject, i.e., `{s \u2208 E : \u2203 (s, p, o) \u2208 K}`.", "predicted_evidence": ["assigns a vector of dimensionality $d$ to an entity, a relation, or a literal. However, some approaches consider only the vector representations of entities or subjects (i.e, $\\lbrace s \\in E : \\exists (s, p, o) \\in K \\rbrace $ ). For instance, in approaches based on Tensor Factorization, given a relation, its subjects and objects are processed and transformed into sparse matrices; all the matrices are then combined into a tensor whose depth is the number of relations. For the final embedding, current approaches rely on dimensionality reduction to decrease the overall complexity BIBREF9 , BIBREF12 , BIBREF2 . The reduction is performed through an embedding map $\\Phi : \\mathbb {R}^d \\rightarrow \\mathbb {R}^k$ , which is a homomorphism that maps the initial vector space into a smaller, reduced space. The positive value $k < d$ is called the rank of the embedding.", "assigns a vector of dimensionality $d$ to an entity, a relation, or a literal. However, some approaches consider only the vector representations of entities or subjects (i.e, $\\lbrace s \\in E : \\exists (s, p, o) \\in K \\rbrace $ ). For instance, in approaches based on Tensor Factorization, given a relation, its subjects and objects are processed and transformed into sparse matrices; all the matrices are then combined into a tensor whose depth is the number of relations. For the final embedding, current approaches rely on dimensionality reduction to decrease the overall complexity BIBREF9 , BIBREF12 , BIBREF2 . The reduction is performed through an embedding map $\\Phi : \\mathbb {R}^d \\rightarrow \\mathbb {R}^k$ , which is a homomorphism that maps the initial vector space into a smaller, reduced space.", "However, some approaches consider only the vector representations of entities or subjects (i.e, $\\lbrace s \\in E : \\exists (s, p, o) \\in K \\rbrace $ ). For instance, in approaches based on Tensor Factorization, given a relation, its subjects and objects are processed and transformed into sparse matrices; all the matrices are then combined into a tensor whose depth is the number of relations. For the final embedding, current approaches rely on dimensionality reduction to decrease the overall complexity BIBREF9 , BIBREF12 , BIBREF2 . The reduction is performed through an embedding map $\\Phi : \\mathbb {R}^d \\rightarrow \\mathbb {R}^k$ , which is a homomorphism that maps the initial vector space into a smaller, reduced space. The positive value $k < d$ is called the rank of the embedding. Note that each dimension of the reduced common space does not necessarily have an explicit connection with a particular relation.", "Several methods have been proposed to evaluate word embeddings. The most common ones are based on analogies BIBREF22 , BIBREF23 , where word vectors are summed up together, e.g.:", "For instance, in approaches based on Tensor Factorization, given a relation, its subjects and objects are processed and transformed into sparse matrices; all the matrices are then combined into a tensor whose depth is the number of relations. For the final embedding, current approaches rely on dimensionality reduction to decrease the overall complexity BIBREF9 , BIBREF12 , BIBREF2 . The reduction is performed through an embedding map $\\Phi : \\mathbb {R}^d \\rightarrow \\mathbb {R}^k$ , which is a homomorphism that maps the initial vector space into a smaller, reduced space. The positive value $k < d$ is called the rank of the embedding. Note that each dimension of the reduced common space does not necessarily have an explicit connection with a particular relation. Dimensionality reduction methods include Principal Component Analysis techniques BIBREF9 and generative statistical models such as Latent Dirichlet Allocation BIBREF19 , BIBREF20 ."]}
{"question_id": "f92c344e9b1a986754277fd0f08a47dc3e5f9feb", "predicted_answer": "The limitations of currently used quantitative metrics are:\n\n1. **Insufficient complexity**: Metrics like BLEU, which are based on word-overlap, require a large number of ground-truth responses to find a significant match, making them inadequate for modeling sentence-level compositionality in dialogue.\n\n2. **Oversimplification**: Embedding-based metrics, which use basic averages of vectors obtained through distributional semantics, are too simplistic to capture the complexity of dialogue responses.\n\n3. **Lack of context**: Current metrics do not take into account the context of the conversation, leading to inaccurate evaluations.\n\n4. **Inability to compare conversational agents**: There is no perfect quantitative method to compare two conversational agents, relying on qualitative measures or borrowed metrics from machine translation, such as BLEU and perplexity.\n\n5. **Limited practical applicability**: The accuracy of current models is not sufficient for practical applications, and they often suffer from issues like the cold start problem, where the agent fails to learn from a random initialization.", "predicted_evidence": ["The metrics that take into account the context can also be considered. Such metrics can come in the form of an evaluation model that is learned from data. This model can be either a discriminative model that attempts to distinguish between model and human responses or a model that uses data collected from the human survey in order to provide human-like scores to proposed responses.", "According to them, the metrics (like Kiros et al, 2015 BIBREF32 ) that are based on distributed sentence representations hold the most promise for the future. It is because word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses. Similarly, the metrics that are embedding-based consist of basic averages of vectors obtained through distributional semantics and so they are also insufficiently complex for modeling sentence-level compositionality in dialogue.", "For each utterance, they calculated features like n-grams of the words and their POS tags, dialog act and task/subtask label. Then they put those features in the binary MaxEnt classifier. For this, their model was able to achieve an error rate of 25.1% for the dialog act classification which was better than the best performing models at that time. Although, according to the modern standards, the results are not that great but the approach they suggested (of using data to build machine learning models) forms the basis of the techniques that are currently used in this area.", "Despite such huge advancements in the field, the way these models are evaluated is something that needs to be dramatically altered. Currently there exists no perfect quantitative method to compare two conversational agents. The field has to rely on qualitative measures or measures like BLeU and perplexity borrowed from machine translation. In section SECREF8 we discuss this problem in detail.", "However, the system has a few limitations. The accuracy is not enough for using for the practical applications. The agent suffers from the cold start issue. In the case of end-to-end learning, they found that for a moderately sized knowledge base, the agent almost always fails if starting from random initialization."]}
{"question_id": "b10388e343868ca8e5c7c601ebb903f52e756e61", "predicted_answer": "According to the text, the following metrics are typically used to compare models:\n\n1. BLEU (Bilingual Evaluation Understudy)\n2. Perplexity (borrowed from machine translation)\n3. Distributed sentence representations (e.g. Kiros et al, 2015)\n4. Embedding-based metrics (e.g. averages of vectors obtained through distributional semantics)\n5. N-grams of words and their POS tags, dialog act and task/subtask label (used in a binary MaxEnt classifier)\n6. Human ratings (e.g. average human rating out of 5)\n\nNote that the text also mentions that these metrics have limitations and that there is no perfect quantitative method to compare two conversational agents.", "predicted_evidence": ["The metrics that take into account the context can also be considered. Such metrics can come in the form of an evaluation model that is learned from data. This model can be either a discriminative model that attempts to distinguish between model and human responses or a model that uses data collected from the human survey in order to provide human-like scores to proposed responses.", "Despite such huge advancements in the field, the way these models are evaluated is something that needs to be dramatically altered. Currently there exists no perfect quantitative method to compare two conversational agents. The field has to rely on qualitative measures or measures like BLeU and perplexity borrowed from machine translation. In section SECREF8 we discuss this problem in detail.", "According to them, the metrics (like Kiros et al, 2015 BIBREF32 ) that are based on distributed sentence representations hold the most promise for the future. It is because word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses. Similarly, the metrics that are embedding-based consist of basic averages of vectors obtained through distributional semantics and so they are also insufficiently complex for modeling sentence-level compositionality in dialogue.", "For each utterance, they calculated features like n-grams of the words and their POS tags, dialog act and task/subtask label. Then they put those features in the binary MaxEnt classifier. For this, their model was able to achieve an error rate of 25.1% for the dialog act classification which was better than the best performing models at that time. Although, according to the modern standards, the results are not that great but the approach they suggested (of using data to build machine learning models) forms the basis of the techniques that are currently used in this area.", "Their model scores an average human rating of 3.3919 out of 5 in comparison to the baseline which is 3.3638 out of 5. Hence, their model generates more informative responses. However, they found the external knowledge plays a negative role in the procedure of response generation when there is more noise in the information. Exploring how to obtain credible knowledge of a given dialog history can be a future generation of their model."]}
{"question_id": "e8cdeb3a081d51cc143c7090a54c82d393f1a2ca", "predicted_answer": "No, there is no perfect quantitative method to compare two conversational agents, and the field relies on qualitative measures or measures borrowed from machine translation, such as BLeU and perplexity.", "predicted_evidence": ["Despite such huge advancements in the field, the way these models are evaluated is something that needs to be dramatically altered. Currently there exists no perfect quantitative method to compare two conversational agents. The field has to rely on qualitative measures or measures like BLeU and perplexity borrowed from machine translation. In section SECREF8 we discuss this problem in detail.", "We can see in the figure FIGREF11 that the success rate reaches at around 95% for the case of policy trained with corpus data and using reinforcement learning which is impressive. Also, they train very quickly. For instance, for training just around 500-1000 dialogues, eNACER has a success rate of around 95% and TRACER has a success rate of around 92%. However, the authors noted that performance falls off rather rapidly in noise as the uncertainty estimates are not handled well by neural networks architectures. This can also be a topic for future research.", "After this they tried to increase the performance of the system (Raux et al, 2006 BIBREF10 ). They retrained their acoustic models by performing Baum-Welch optimization on the transcribed data (starting from their original models). Unfortunately, this only brought marginal improvement because the models (semi-continuous HMMs) and algorithms they were using were too simplistic for this task. They improved the turn-taking management abilities of the system by closely analysing the feedback they received. They added more specific strategies, aiming at dealing with problems like noisy environments, too loud or too long utterances, etc. They found that they were able to get a success rate of 79% for the complete dialogues (which was great).", "However, the system has a few limitations. The accuracy is not enough for using for the practical applications. The agent suffers from the cold start issue. In the case of end-to-end learning, they found that for a moderately sized knowledge base, the agent almost always fails if starting from random initialization.", "According to them, the metrics (like Kiros et al, 2015 BIBREF32 ) that are based on distributed sentence representations hold the most promise for the future. It is because word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses. Similarly, the metrics that are embedding-based consist of basic averages of vectors obtained through distributional semantics and so they are also insufficiently complex for modeling sentence-level compositionality in dialogue."]}
{"question_id": "833d3ae7613500f2867ed8b33d233d71781014e7", "predicted_answer": "According to the given context, the GAN and RL approaches used are:\n\n* Generative Adversarial Networks (GANs) are used to generate dialogues similar to humans.\n* Reinforcement Learning (RL) methods, specifically:\n\t+ On-policy Monte Carlo method (used by English et al in 2005)\n\t+ Deep Reinforcement Learning (RL) methods for dialogue policy optimization\n\t+ Hierarchical representation using a context RNN to represent common ground between speakers and improve the stability of the objective function.", "predicted_evidence": ["Recently, generative adversarial networks are being explored and how they can be used in the dialog agents. Although generative adversarial networks are a topic in itself to explore. However, the paper mentioned below used uses reinforcement learning along with generative adversarial network so we cover it here inside the reinforcement learning methods. They can be used by the applications to generate dialogues similar to humans.", "After exploring the neural methods in a lot of detail, the researchers have also begun exploring, in the current decade, how to use the reinforcement learning methods in the dialogue and personal agents.", "Deep reinforcement learning (RL) methods have significant potential for dialogue policy optimisation. However, they suffer from a poor performance in the early stages of learning as we saw in the paper in the above section. This is especially problematic for on-line learning with real users.", "The advantages of using a hierarchical representation are two-fold. First, the context RNN allows the model to represent a form of common ground between speakers, e.g. to represent topics and concepts shared between the speakers using a distributed vector representation. Second, because the number of computational steps between utterances is reduced. This makes the objective function more stable w.r.t. the model parameters, and helps propagate the training signal for first-order optimization methods.", "One of the first main papers that thought of using reinforcement learning for this came in 2005 by English et al BIBREF25 . They used an on-policy Monte Carlo method and the objective function they used was a linear combination of the solution quality (S) and the dialog length (L), taking the form: o(S,I) = INLINEFORM0 - INLINEFORM1 ."]}
{"question_id": "a1a0365bf6968cbdfd1072cf3923c26250bc955c", "predicted_answer": "The type of neural models used are:\n\n1. RNN (Recurrent Neural Network)\n2. LSTM (Long Short-Term Memory)\n3. GRU (Gated Recurrent Unit)\n4. Sequence-to-Sequence model", "predicted_evidence": ["The advantages of using a hierarchical representation are two-fold. First, the context RNN allows the model to represent a form of common ground between speakers, e.g. to represent topics and concepts shared between the speakers using a distributed vector representation. Second, because the number of computational steps between utterances is reduced. This makes the objective function more stable w.r.t. the model parameters, and helps propagate the training signal for first-order optimization methods.", "After exploring the neural methods in a lot of detail, the researchers have also begun exploring, in the current decade, how to use the reinforcement learning methods in the dialogue and personal agents.", "The model represents each individual speaker as a vector or embedding, which encodes speaker-specific information (e.g.dialect, register, age, gender, personal information) that influences the content and style of her responses. Most importantly these traits do not need to be explicitly annotated, which would be really tedious and limit the applications of the model. Instead the model manages to cluster users along some of these traits (e.g. age, country of residence) based on the responses alone. The model first encodes message INLINEFORM0 into a vector representation INLINEFORM1 using the source LSTM. Then for each step in the target side, hidden units are obtained by combining the representation produced by the target LSTM at the previous time step, the word representations at the current time step, and the speaker embedding INLINEFORM2 . In this way, speaker information is encoded and injected into the hidden layer at each time step and thus helps predict personalized responses throughout the generation process. The process described here is visualizes in figure FIGREF13 below.", "Due to their large parameter space, the estimation of neural conversation models requires considerable amounts of dialogue data. Large online corpora are helpful for this. However several dialogue corpora, most notably those extracted from subtitles, do not include any explicit turn segmentation or speaker identification.The neural conversation model may therefore inadvertently learn responses that remain within the same dialogue turn instead of starting a new turn. Lison et al BIBREF17 overcome these limitations by introduce a weighting model into the neural architecture. The weighting model, which is itself estimated from dialogue data, associates each training example to a numerical weight that reflects its intrinsic quality for dialogue modelling. At training time, these sample weights are included into the empirical loss to be minimized. The purpose of this model is to associate each \u27e8context, response\u27e9 example pair to a numerical weight that reflects the intrinsic \u201cquality\u201d of each example. The instance weights are then included in the empirical loss to minimize when learning the parameters of the neural conversation model.", "In this survey paper we explored the exciting and rapidly changing field of conversational agents. We talked about the early rule-based methods that depended on hand-engineered features. These methods laid the ground work for the current models. However these models were expensive to create and the features depended on the domain that the conversational agent was created for. It was hard to modify these models for a new domain. As computation power increased, and we developed neural networks that were able to capture long range dependencies (RNNs,GRUs,LSTMs) the field moved towards neural models for building these agents. Sequence to sequence model created in 2015 was capable of handling utterances of variable lengths, the application of sequence to sequence to conversation agents truly revolutionized the domain. After this advancement the field has literally exploded with numerous application in the last couple of years."]}
{"question_id": "64f7337970e8d1989b2e1f7106d86f73c4a3d0af", "predicted_answer": "The type of statistical models used initially were n-gram statistical models.", "predicted_evidence": ["Their model scores an average human rating of 3.3919 out of 5 in comparison to the baseline which is 3.3638 out of 5. Hence, their model generates more informative responses. However, they found the external knowledge plays a negative role in the procedure of response generation when there is more noise in the information. Exploring how to obtain credible knowledge of a given dialog history can be a future generation of their model.", "The metrics that take into account the context can also be considered. Such metrics can come in the form of an evaluation model that is learned from data. This model can be either a discriminative model that attempts to distinguish between model and human responses or a model that uses data collected from the human survey in order to provide human-like scores to proposed responses.", "The speech recognition was done using n-gram statistical model which is then passed to a robust parser based on an extended Context Free Grammar allowing the system to skip unknown words and perform partial parsing. They wrote the grammar based on a combination of their own intuition and a small scale Wizard-of-Oz experiment they ran. The grammar rules used to identify bus stops were generated automatically from the schedule database. After this, they trained a statistical language model on the artificial corpus. In order to make the parsing grammar robust enough to parse fairly ungrammatical, yet understandable sentences, it was kept as general as possible. On making it public, they initially achieved a task success rate of 43.3% for the whole corpus and 43.6 when excluding sessions that did not contain any system-directed speech.", "The previous papers (like the ones which we discussed in the above paragraph) did not attempt to use data-driven techniques for the dialog agents because such data was not available in large amount at that time. But then there was a high increase in the collection of spoken dialog corpora which made it possible to use data-driven techniques to build and use models of task-oriented dialogs and possibly get good results. In the paper by Srinivas et al,2008 BIBREF11 , the authors proposed using data-driven techniques to build task structures for individual dialogs and use the dialog task structures for dialog act classification, task/subtask classification, task/subtask prediction and dialog act prediction.", "The problem with rule-based models was that they were often domain dependent and could not be easily ported to a new domain. They also depended on hand crafted rules which was both expensive and required domain expertise. Two factors which when combined spell doom for scalbility. All of this changed in 2015 when Vinyals et al proposed an approach BIBREF2 inspired from the recent progress in machine translation BIBREF1 . Vinyals et al used the sequence to sequence learning architecture for conversation agents. Their model was the first model which could be trained end-to-end, and could generate a new output utterance based on just the input sentence and no other hand crafted features."]}
{"question_id": "8fdb4f521d3ba4179f8ccc4c28ba399aab6c3550", "predicted_answer": "The proposed use of conversational agents in pioneering work was to build machines that can converse with humans, with the ultimate goal of passing the Turing test, which would demonstrate true artificial intelligence.", "predicted_evidence": ["Next came the era of using machine learning methods in the area of conversation agents which totally revolutionized this field.", "The goal of capturing emotions and having consistent personalities for a conversational agent is an important one. The field is still nascent but advances in the domain will have far reaching consequences for conversational models in general. People tend to trust agents that are emotionally consistent, and in the long term trust is what will decide the fate of large scale adoption of conversational agents.", "Another feature that has been traditionally lacking in conversation agents is a personality. O Vinayal et al BIBREF2 hypothesis that not having a consistent personality is one of the main reasons that is stopping us from passing the turing test. Conversational agents also lack emotional consistency in their responses. These features are vital if we want humans to trust conversational agents. In section SECREF7 we discuss state of the art approaches to overcome these problems.", "A lack of a coherent personality in conversational agents that most of these models propose has been identified as one of the primary reasons that these agents have not been able to pass the Turing test BIBREF0 BIBREF2 . Aside from such academic motivations, making conversational agents more like their human interlocutors which posses both a persona and are capable of parsing emotions is of great practical and commercial use. Consequently in the last couple of years different approaches have been tried to achieve this goal.", "One of the earliest goals of Artificial Intelligence (AI) has been to build machines that can converse with us. Whether in early AI literature or the current popular culture, conversational agents have captured our imagination like no other technology has. In-fact the ultimate test of whether true artificial intelligence has been achieved, the Turing test BIBREF0 proposed by Alan Turing the father of artificial intelligence in 1950, revolves around the concept of a good conversational agent. The test is deemed to have been passed if a conversational agent is able to fool human judges into believing that it is in fact a human being."]}
{"question_id": "a0d45b71feb74774cfdc0d5c6e23cd41bc6bc1f2", "predicted_answer": "The work that pioneered the field of conversational agents is not explicitly mentioned in the given context. However, it can be inferred that the early rule-based methods that depended on hand-engineered features laid the groundwork for the current models. These early methods were likely the pioneering works in the field of conversational agents.\n\nIt's worth noting that the sequence-to-sequence model created in 2015 is mentioned as a significant advancement that revolutionized the domain of conversational agents. However, it is not considered the pioneering work in the field, but rather a key milestone in the development of conversational agents.", "predicted_evidence": ["Next came the era of using machine learning methods in the area of conversation agents which totally revolutionized this field.", "The goal of capturing emotions and having consistent personalities for a conversational agent is an important one. The field is still nascent but advances in the domain will have far reaching consequences for conversational models in general. People tend to trust agents that are emotionally consistent, and in the long term trust is what will decide the fate of large scale adoption of conversational agents.", "In this survey paper we explored the exciting and rapidly changing field of conversational agents. We talked about the early rule-based methods that depended on hand-engineered features. These methods laid the ground work for the current models. However these models were expensive to create and the features depended on the domain that the conversational agent was created for. It was hard to modify these models for a new domain. As computation power increased, and we developed neural networks that were able to capture long range dependencies (RNNs,GRUs,LSTMs) the field moved towards neural models for building these agents. Sequence to sequence model created in 2015 was capable of handling utterances of variable lengths, the application of sequence to sequence to conversation agents truly revolutionized the domain. After this advancement the field has literally exploded with numerous application in the last couple of years. The results have been impressive enough to find their way into commercial applications such that these agents have become truly ubiquitous.", "In this survey paper we explored the exciting and rapidly changing field of conversational agents. We talked about the early rule-based methods that depended on hand-engineered features. These methods laid the ground work for the current models. However these models were expensive to create and the features depended on the domain that the conversational agent was created for. It was hard to modify these models for a new domain. As computation power increased, and we developed neural networks that were able to capture long range dependencies (RNNs,GRUs,LSTMs) the field moved towards neural models for building these agents. Sequence to sequence model created in 2015 was capable of handling utterances of variable lengths, the application of sequence to sequence to conversation agents truly revolutionized the domain. After this advancement the field has literally exploded with numerous application in the last couple of years.", "Another feature that has been traditionally lacking in conversation agents is a personality. O Vinayal et al BIBREF2 hypothesis that not having a consistent personality is one of the main reasons that is stopping us from passing the turing test. Conversational agents also lack emotional consistency in their responses. These features are vital if we want humans to trust conversational agents. In section SECREF7 we discuss state of the art approaches to overcome these problems."]}
{"question_id": "89414ef7fcb2709c47827f30a556f543b9a9e6e0", "predicted_answer": "The research in Croatia was lagging only a couple of years behind the research in the US and USSR, despite being poorly funded. The Croatian research was able to keep pace with the superpowers, which had invested heavily in machine translation research, and was only a few years behind in terms of progress. The Croatian researchers were aware of the work being done in the US and USSR, and were able to build upon and learn from it, despite their limited resources.", "predicted_evidence": ["Laszlo and Petrovi\u0107 BIBREF11 also commented on the state of the art of the time, noting the USA prototype efforts from 1954 and the publication of a collection of research papers in 1955 as well as the USSR efforts starting from 1955 and the UK prototype from 1956. They do not detail or cite the articles they mention. However, the fact that they referred to them in a text published in 1959 (probably prepared for publishing in 1958, based on BIBREF11, where Laszlo and Petrovi\u0107 described that the group had started its work in 1958) leads us to the conclusion that the poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers (which invested heavily in this effort).", "Laszlo and Petrovi\u0107 BIBREF11 also commented on the state of the art of the time, noting the USA prototype efforts from 1954 and the publication of a collection of research papers in 1955 as well as the USSR efforts starting from 1955 and the UK prototype from 1956. They do not detail or cite the articles they mention. However, the fact that they referred to them in a text published in 1959 (probably prepared for publishing in 1958, based on BIBREF11, where Laszlo and Petrovi\u0107 described that the group had started its work in 1958) leads us to the conclusion that the poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers (which invested heavily in this effort). Another interesting moment, which they delineated in BIBREF11, is that the group soon discovered that some experimental work had already been done in 1957 at the Institute of Telecommunications (today a part of the Faculty of Electrical Engineering and Computing at the University of Zagreb) by Vladimir Matkovi\u0107.", "There were many other centers for research in machine translation: Gorkovsky University (Omsk), 1st Moscow Institute for Foreign Languages, Computing Centre of the Armenian SSR and at the Institute for Automatics and Telemechanics of the Georgian SSR BIBREF7. It is worthwhile to note that both the USA and the USSR had access to state-of-the-art computers, and the political support for the production of such systems meant that computers were made available to researchers in machine translation. However, the results were poor in the late 1950s, and a working system was yet to be shown. All work was therefore theoretical work implemented on a computer, which proved to be sub-optimal.", "They do not detail or cite the articles they mention. However, the fact that they referred to them in a text published in 1959 (probably prepared for publishing in 1958, based on BIBREF11, where Laszlo and Petrovi\u0107 described that the group had started its work in 1958) leads us to the conclusion that the poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers (which invested heavily in this effort). Another interesting moment, which they delineated in BIBREF11, is that the group soon discovered that some experimental work had already been done in 1957 at the Institute of Telecommunications (today a part of the Faculty of Electrical Engineering and Computing at the University of Zagreb) by Vladimir Matkovi\u0107. Because of this, they decided to include him in the research group of the Faculty of Humanities and Social Sciences at the University of Zagreb.", "They do not detail or cite the articles they mention. However, the fact that they referred to them in a text published in 1959 (probably prepared for publishing in 1958, based on BIBREF11, where Laszlo and Petrovi\u0107 described that the group had started its work in 1958) leads us to the conclusion that the poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers (which invested heavily in this effort). Another interesting moment, which they delineated in BIBREF11, is that the group soon discovered that some experimental work had already been done in 1957 at the Institute of Telecommunications (today a part of the Faculty of Electrical Engineering and Computing at the University of Zagreb) by Vladimir Matkovi\u0107. Because of this, they decided to include him in the research group of the Faculty of Humanities and Social Sciences at the University of Zagreb. The work done by Matkovi\u0107 was documented in his doctoral dissertation but remained unpublished until 1959."]}
{"question_id": "faffcc6ef27c1441e6528f924e320368430d8da3", "predicted_answer": "The reason this research was not adopted in the 1960s is not explicitly stated in the given context. However, it can be inferred that the poorly funded Croatian research was lagging behind the research of the superpowers, which invested heavily in this effort. This lack of funding and investment may have contributed to the research not being adopted in the 1960s.", "predicted_evidence": ["Laszlo and Petrovi\u0107 BIBREF11 also commented on the state of the art of the time, noting the USA prototype efforts from 1954 and the publication of a collection of research papers in 1955 as well as the USSR efforts starting from 1955 and the UK prototype from 1956. They do not detail or cite the articles they mention. However, the fact that they referred to them in a text published in 1959 (probably prepared for publishing in 1958, based on BIBREF11, where Laszlo and Petrovi\u0107 described that the group had started its work in 1958) leads us to the conclusion that the poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers (which invested heavily in this effort).", "Laszlo and Petrovi\u0107 BIBREF11 also commented on the state of the art of the time, noting the USA prototype efforts from 1954 and the publication of a collection of research papers in 1955 as well as the USSR efforts starting from 1955 and the UK prototype from 1956. They do not detail or cite the articles they mention. However, the fact that they referred to them in a text published in 1959 (probably prepared for publishing in 1958, based on BIBREF11, where Laszlo and Petrovi\u0107 described that the group had started its work in 1958) leads us to the conclusion that the poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers (which invested heavily in this effort). Another interesting moment, which they delineated in BIBREF11, is that the group soon discovered that some experimental work had already been done in 1957 at the Institute of Telecommunications (today a part of the Faculty of Electrical Engineering and Computing at the University of Zagreb) by Vladimir Matkovi\u0107.", "They do not detail or cite the articles they mention. However, the fact that they referred to them in a text published in 1959 (probably prepared for publishing in 1958, based on BIBREF11, where Laszlo and Petrovi\u0107 described that the group had started its work in 1958) leads us to the conclusion that the poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers (which invested heavily in this effort). Another interesting moment, which they delineated in BIBREF11, is that the group soon discovered that some experimental work had already been done in 1957 at the Institute of Telecommunications (today a part of the Faculty of Electrical Engineering and Computing at the University of Zagreb) by Vladimir Matkovi\u0107. Because of this, they decided to include him in the research group of the Faculty of Humanities and Social Sciences at the University of Zagreb.", "They do not detail or cite the articles they mention. However, the fact that they referred to them in a text published in 1959 (probably prepared for publishing in 1958, based on BIBREF11, where Laszlo and Petrovi\u0107 described that the group had started its work in 1958) leads us to the conclusion that the poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers (which invested heavily in this effort). Another interesting moment, which they delineated in BIBREF11, is that the group soon discovered that some experimental work had already been done in 1957 at the Institute of Telecommunications (today a part of the Faculty of Electrical Engineering and Computing at the University of Zagreb) by Vladimir Matkovi\u0107. Because of this, they decided to include him in the research group of the Faculty of Humanities and Social Sciences at the University of Zagreb. The work done by Matkovi\u0107 was documented in his doctoral dissertation but remained unpublished until 1959.", "The idea of machine translation was a tempting idea in the 1950s. The main military interest in machine translation as an intelligence gathering tool (translation of scientific papers, daily press, technical reports, and everything the intelligence services could get their hands on) was sparked by the Soviet advance in nuclear technology, and would later be compounded by the success of Vostok 1 (termed by the USA as a \u201cstrategic surprise\u201d). In the nuclear age, being able to read and understand what the other side was working on was of crucial importance BIBREF4. Machine translation was quickly absorbed in the program of the Dartmouth Summer Research Project on Artificial Intelligence in 1956 (where Artificial Intelligence as a field was born), as one of the five core fields of artificial intelligence (later to be known as natural language processing). One other field was included here, the \u201cnerve nets\u201d as they were known back then, today commonly known as artificial neural networks."]}
{"question_id": "afad388a0141bdda5ca9586803ac53d5f10f41f6", "predicted_answer": "Based on the context, the cybernetic methods mentioned likely include artificial neural networks, as well as the study of analogies between machines and living organisms, specifically the functioning of machines and the human nervous system. This involves modeling analogical processes, rather than just focusing on the theory of electronic computers.", "predicted_evidence": ["This suggests that the strategy would be as follows: first make a thesaurus, and pick a representative for each meaning, then stem or lemmatize the words, then remove the needless letters from words (i.e. letters that carry little information, such as vowels, but being careful not to equate two different words), and then encode the words in binary strings, using the letter frequencies. After that, the texts are ready for translation, but unfortunately, the translation method is never explicated. Nevertheless, it is hinted that it should be \"cybernetic\", which, along with what we have presented earlier, would most probably mean artificial neural networks. This is highlighted by the following passage (BIBREF11, p. 117):", "Laszlo and Petrovi\u0107 BIBREF11 considered cybernetics (as described in BIBREF13 by Wiener, who invented the term \u201ccybernetics\u201d) to be the best approach for machine translation in the long run. The question is whether Laszlo's idea of cybernetics would drive the research of the group towards artificial neural networks. Laszlo and his group do not go into neural network details (bear in mind that this is 1959, the time of Rosenblatt), but the following passage offers a strong suggestion about the idea they had (bearing in mind that Wiener relates McCulloch and Pitts' ideas in his book): \"Cybernetics is the scientific discipline which studies analogies between machines and living organisms\" (BIBREF11, p. 107). They fully commit to the idea two pages later (BIBREF11, p. 109): \"An important analogy is the one between the functioning of the machine and that of the human nervous system\".", "Laszlo and Petrovi\u0107 BIBREF11 considered cybernetics (as described in BIBREF13 by Wiener, who invented the term \u201ccybernetics\u201d) to be the best approach for machine translation in the long run. The question is whether Laszlo's idea of cybernetics would drive the research of the group towards artificial neural networks. Laszlo and his group do not go into neural network details (bear in mind that this is 1959, the time of Rosenblatt), but the following passage offers a strong suggestion about the idea they had (bearing in mind that Wiener relates McCulloch and Pitts' ideas in his book): \"Cybernetics is the scientific discipline which studies analogies between machines and living organisms\" (BIBREF11, p. 107). They fully commit to the idea two pages later (BIBREF11, p.", "107). They fully commit to the idea two pages later (BIBREF11, p. 109): \"An important analogy is the one between the functioning of the machine and that of the human nervous system\". This could be taken to mean a simple computer brain analogy in the spirit of BIBREF14 and later BIBREF15, but Laszlo and Petrovi\u0107 specifically said that thinking of cybernetics as the \"theory of electronic computers\" (as they are made) is wrong BIBREF11, since the emphasis should be on modelling analogical processes. There is a very interesting quote from BIBREF11, where Laszlo and Petrovi\u0107 note that \"today, there is a significant effort in the world to make fully automated machine translation possible; to achieve this, logicians and linguists are making efforts on ever more sophisticated problems\". This seems to suggest that they were aware of the efforts of logicians (such as Bar Hillel, and to some degree Pitts, since Wiener specifically mentions logicians-turned-cyberneticists in his book BIBREF13), but still concluded that a cybernetic approach would probably be a better choice.", "107). They fully commit to the idea two pages later (BIBREF11, p. 109): \"An important analogy is the one between the functioning of the machine and that of the human nervous system\". This could be taken to mean a simple computer brain analogy in the spirit of BIBREF14 and later BIBREF15, but Laszlo and Petrovi\u0107 specifically said that thinking of cybernetics as the \"theory of electronic computers\" (as they are made) is wrong BIBREF11, since the emphasis should be on modelling analogical processes. There is a very interesting quote from BIBREF11, where Laszlo and Petrovi\u0107 note that \"today, there is a significant effort in the world to make fully automated machine translation possible; to achieve this, logicians and linguists are making efforts on ever more sophisticated problems\"."]}
{"question_id": "baaa6ad7148b785429a20f38786cd03ab9a2646e", "predicted_answer": "The usual logical approaches of the time period were:\n\n1. Logical and knowledge-heavy processing, which involved the use of a logical intermediate language, called \"Interlingua\", to model common-sense human knowledge and connect natural languages.\n2. Extension of theoretical ideas of Bar-Hillel to build algorithms for machine translation, which was a mainly logical approach.\n3. Information-theoretic approach, which was considered cybernetic at that time, and also involved trying logic-based approaches.\n\nThese approaches were championed by different teams and researchers, including Andreev, Kulagina, Melchuk, Reformatsky, Lyapunov, and Dobrushin.", "predicted_evidence": ["The source does not specify how this first system worked, but it does delineate that the major figures of machine translation of the time were N. Andreev of the Leningrad State University, O. Kulagina and I. Melchuk of the Steklov Mathematical Institute. There is information on an Indonesian-to-Russian machine translation system by Andreev, Kulagina and Melchuk from the early 1960s, but it is reported that the system was ultimately a failure, in the same way early USA systems were. The system had statistical elements set forth by Andreev, but the bulk was logical and knowledge-heavy processing put forth by Kulagina and Melchuk. The idea was to have a logical intermediate language, under the working name \u201cInterlingua\u201d, which was the connector of both natural languages, and was used to model common-sense human knowledge. For more details, see BIBREF6.", "The second approach, as noted by Muli\u0107 BIBREF7, was championed by the team at the Steklov Mathematical Institute of the USSR Academy of Sciences led by A. A. Reformatsky. Their approach was mainly logical, and they extended the theoretical ideas of Bar-Hillel BIBREF2 to build three algorithms: French-Russian, English-Russian and Hungarian-Russian. The third and perhaps the most successful approach was the one by A. A. Lyapunov, O. S. Kulagina and R. L. Dobrushin. Their efforts resulted in the formation of the Mathematical Linguistics Seminar at the Faculty of Philology in Moscow in 1956 and in Leningrad in 1957. Their approach was mainly information-theoretic (but they also tried logic-based approaches BIBREF7), which was considered cybernetic at that time. This was the main role model for the Croatian efforts from 1957 onwards.", "Their approach was mainly logical, and they extended the theoretical ideas of Bar-Hillel BIBREF2 to build three algorithms: French-Russian, English-Russian and Hungarian-Russian. The third and perhaps the most successful approach was the one by A. A. Lyapunov, O. S. Kulagina and R. L. Dobrushin. Their efforts resulted in the formation of the Mathematical Linguistics Seminar at the Faculty of Philology in Moscow in 1956 and in Leningrad in 1957. Their approach was mainly information-theoretic (but they also tried logic-based approaches BIBREF7), which was considered cybernetic at that time. This was the main role model for the Croatian efforts from 1957 onwards. The fourth, and perhaps most influential, was the approach at the Experimental Laboratory of the Leningrad University championed by N. D. Andreev BIBREF7.", "One of the first recorded attempts of producing a machine translation system in the USSR was in 1954 BIBREF6, and the attempt was applauded by the Communist party of the Soviet Union, by the USSR Committee for Science and Technology and the USSR Academy of Sciences. The source does not specify how this first system worked, but it does delineate that the major figures of machine translation of the time were N. Andreev of the Leningrad State University, O. Kulagina and I. Melchuk of the Steklov Mathematical Institute. There is information on an Indonesian-to-Russian machine translation system by Andreev, Kulagina and Melchuk from the early 1960s, but it is reported that the system was ultimately a failure, in the same way early USA systems were. The system had statistical elements set forth by Andreev, but the bulk was logical and knowledge-heavy processing put forth by Kulagina and Melchuk. The idea was to have a logical intermediate language, under the working name \u201cInterlingua\u201d, which was the connector of both natural languages, and was used to model common-sense human knowledge.", "Andreev's approach was in a sense \"external\". The modelling would be statistical, but its purpose would not be to mimic the stochasticity of the human thought process, but rather to produce a working machine translation system. Kulagina and Melchuk disagreed with this approach as they thought that more of what is presently called \"philosophical logic\" was needed to model the human thought process at the symbolic level, and according to them, the formalization of the human thought process was a prerequisite for developing a machine translation system (cf. BIBREF6). We could speculate that sub-symbolic processing would have been acceptable too, since that approach is also rooted in philosophical logic as a way of formalizing human cognitive functions and is also \"internal\" in the same sense symbolic approaches are."]}
{"question_id": "de346decb1fbca8746b72c78ea9d1208902f5e0a", "predicted_answer": "The language of the research publication is not explicitly stated in the given context. However, based on the content and the references to Russian researchers and institutions, it is likely that the research was published in English, as it appears to be a historical review of machine translation research in the 1950s, including efforts in Yugoslavia and the Soviet Union.", "predicted_evidence": ["In this paper, we are exploring the historical significance of Croatian machine translation research group. The group was active in 1950s, and it was conducted by Bulcsu Laszlo, Croatian linguist, who was a pioneer in machine translation during the 1950s in Yugoslavia.", "There were many other centers for research in machine translation: Gorkovsky University (Omsk), 1st Moscow Institute for Foreign Languages, Computing Centre of the Armenian SSR and at the Institute for Automatics and Telemechanics of the Georgian SSR BIBREF7. It is worthwhile to note that both the USA and the USSR had access to state-of-the-art computers, and the political support for the production of such systems meant that computers were made available to researchers in machine translation. However, the results were poor in the late 1950s, and a working system was yet to be shown. All work was therefore theoretical work implemented on a computer, which proved to be sub-optimal.", "The third and perhaps the most successful approach was the one by A. A. Lyapunov, O. S. Kulagina and R. L. Dobrushin. Their efforts resulted in the formation of the Mathematical Linguistics Seminar at the Faculty of Philology in Moscow in 1956 and in Leningrad in 1957. Their approach was mainly information-theoretic (but they also tried logic-based approaches BIBREF7), which was considered cybernetic at that time. This was the main role model for the Croatian efforts from 1957 onwards. The fourth, and perhaps most influential, was the approach at the Experimental Laboratory of the Leningrad University championed by N. D. Andreev BIBREF7. Here, the algorithms for Indonesian-Russian, Arabic-Russian, Hindu-Russian, Japanese-Russian, Burmese-Russian, Norwegian-Russian, English-Russian, Spanish-Russian and Turkish-Russian were being built. The main approach of Andreev's group was to use an intermediary language, which would capture the meanings BIBREF7.", "Their efforts resulted in the formation of the Mathematical Linguistics Seminar at the Faculty of Philology in Moscow in 1956 and in Leningrad in 1957. Their approach was mainly information-theoretic (but they also tried logic-based approaches BIBREF7), which was considered cybernetic at that time. This was the main role model for the Croatian efforts from 1957 onwards. The fourth, and perhaps most influential, was the approach at the Experimental Laboratory of the Leningrad University championed by N. D. Andreev BIBREF7. Here, the algorithms for Indonesian-Russian, Arabic-Russian, Hindu-Russian, Japanese-Russian, Burmese-Russian, Norwegian-Russian, English-Russian, Spanish-Russian and Turkish-Russian were being built. The main approach of Andreev's group was to use an intermediary language, which would capture the meanings BIBREF7. It was an approach similar to KL-ONE, which would be introduced in the West much later (in 1985) by Brachman and Schmolze BIBREF8.", "Machine translation was quickly absorbed in the program of the Dartmouth Summer Research Project on Artificial Intelligence in 1956 (where Artificial Intelligence as a field was born), as one of the five core fields of artificial intelligence (later to be known as natural language processing). One other field was included here, the \u201cnerve nets\u201d as they were known back then, today commonly known as artificial neural networks. What is also essential for our discussion is that the earliest programming language for artificial intelligence, Lisp, was invented in 1958 by John McCarthy BIBREF5. But let us take a closer look at the history of machine translation. In the USA, the first major wave of government and military funding for machine translation came in 1954, and the period of abundancy lasted until 1964, when the National Research Council established the Automatic Language Processing Advisory Committee (ALPAC), which was to assess the results of the ten years of intense funding. The findings were very negative, and funding was almost gone BIBREF4, hence the ALPAC report became the catalyst for the first \u201cAI Winter\u201d."]}
{"question_id": "0bde3ecfdd7c4a9af23f53da2cda6cd7a8398220", "predicted_answer": "The language of the data is English. This is evident from the mention of \"English Wikipedia (EW)\" and \"SEW\" (Simple English Wikipedia) in the text. Additionally, the datasets mentioned, such as Newsela, are described as containing news articles and Wikipedia sentences, which are likely to be in English.", "predicted_evidence": ["We used the split of the data in BIBREF15 , i.e., 94,208/1,129/1,077 pairs for train/dev/test. (2) WikiSmall BIBREF10 , which contains aligned complex-simple sentence pairs from English Wikipedia (EW) and SEW. The dataset has 88,837/205/100 pairs for train/dev/test. (3) WikiLarge BIBREF15 , a larger corpus in which the training set is a mixture of three Wikipedia datasets in BIBREF10 , BIBREF11 , BIBREF23 , and the development and test sests are complex sentences taken from WikiSmall, each has 8 simplifications written by Amazon Mechanical Turk workers BIBREF12 . The dataset has 296,402/2,000/359 pairs for train/dev/test. Table TABREF7 provides statistics on the training sets.", "Following BIBREF15 , we experiment on three simplification datasets, namely: (1) Newsela BIBREF22 , a high-quality simplification corpus of news articles composed by Newsela professional editors for children at multiple grade levels. We used the split of the data in BIBREF15 , i.e., 94,208/1,129/1,077 pairs for train/dev/test. (2) WikiSmall BIBREF10 , which contains aligned complex-simple sentence pairs from English Wikipedia (EW) and SEW. The dataset has 88,837/205/100 pairs for train/dev/test.", "Following BIBREF15 , we experiment on three simplification datasets, namely: (1) Newsela BIBREF22 , a high-quality simplification corpus of news articles composed by Newsela professional editors for children at multiple grade levels. We used the split of the data in BIBREF15 , i.e., 94,208/1,129/1,077 pairs for train/dev/test. (2) WikiSmall BIBREF10 , which contains aligned complex-simple sentence pairs from English Wikipedia (EW) and SEW. The dataset has 88,837/205/100 pairs for train/dev/test. (3) WikiLarge BIBREF15 , a larger corpus in which the training set is a mixture of three Wikipedia datasets in BIBREF10 , BIBREF11 , BIBREF23 , and the development and test sests are complex sentences taken from WikiSmall, each has 8 simplifications written by Amazon Mechanical Turk workers BIBREF12 .", "The results of human judgments are displayed in Table TABREF16 . On Newsela, NseLstm-B scored highest on Fluency. Pbmt-R was significantly better than all other systems on Adequacy while LstmLstm-S performed best on Simplicity. NseLstm-B did very well on both Adequacy and Simplicity, and was best in terms of Average. Example model outputs on Newsela are provided in Table TABREF18 .", "where INLINEFORM0 is a non-linear activation function (e.g., LSTM), INLINEFORM1 is the hidden state at time INLINEFORM2 . Each time the model generates a target word INLINEFORM3 , the decoder looks at a set of positions in the source sentence where the most relevant information is located. Specifically, another non-linear activation function INLINEFORM4 is used for the decoder where the hidden state INLINEFORM5 at time INLINEFORM6 is computed by:"]}
{"question_id": "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b", "predicted_answer": "The baseline is not explicitly stated in the given context. However, it can be inferred that the \"Reference\" is being used as a baseline for human judgments, as its performance is compared to that of the other models.", "predicted_evidence": ["On WikiSmall, Hybrid \u2013 the current state-of-the-art \u2013 achieved best BLEU (53.94) and SARI (30.46) scores. Among neural models, NseLstm-B yielded the highest BLEU score (53.42), while NseLstm-S performed best on SARI (29.75). On WikiLarge, again, NseLstm-B had the highest BLEU score of 92.02. Sbmt-Sari \u2013 that was trained on a huge corpus of 106M sentence pairs and 2B words \u2013 scored highest on SARI with 39.96, followed by Dress-Ls (37.27), Dress (37.08), and NseLstm-S (36.88).", "The results of human judgments are displayed in Table TABREF16 . On Newsela, NseLstm-B scored highest on Fluency. Pbmt-R was significantly better than all other systems on Adequacy while LstmLstm-S performed best on Simplicity. NseLstm-B did very well on both Adequacy and Simplicity, and was best in terms of Average. Example model outputs on Newsela are provided in Table TABREF18 .", "On WikiSmall, NseLstm-B performed best on both Fluency and Adequacy. On WikiLarge, LstmLstm-B achieved the highest Fluency score while NseLstm-B received the highest Adequacy score. In terms of Simplicity and Average, NseLstm-S outperformed all other systems on both WikiSmall and WikiLarge.", "Finally, as can be seen in Table TABREF16 , Reference scored lower on Adequacy compared to Fluency and Simplicity on Newsela. On Wikipedia-based datasets, Reference obtained high Adequacy scores but much lower Simplicity scores compared to Newsela. This supports the assertion by previous work BIBREF22 that SEW has a large proportion of inadequate simplifications.", "where INLINEFORM0 is some non-linear function. The training objective is to minimize the cross-entropy loss of the training source-target pairs."]}
{"question_id": "051034cc94f2c02d3041575c53f969b3311c9ea1", "predicted_answer": "The automatic metrics used in the evaluation are:\n\n1. BLEU (Bilingual Evaluation Understudy)\n2. SARI (Simplification Automatic Evaluation metric using Ranked Intersection)", "predicted_evidence": ["We measured BLEU, and SARI at corpus-level following BIBREF15 . In addition, we also evaluated system output by eliciting human judgments. Specifically, we randomly selected 40 sentences from each test set, and included human reference simplifications and corresponding simplifications from the systems above. We then asked three volunteers to rate simplifications with respect to Fluency (the extent to which the output is grammatical English), Adequacy (the extent to which the output has the same meaning as the input sentence), and Simplicity (the extent to which the output is simpler than the input sentence) using a five point Likert scale.", "The results of human judgments are displayed in Table TABREF16 . On Newsela, NseLstm-B scored highest on Fluency. Pbmt-R was significantly better than all other systems on Adequacy while LstmLstm-S performed best on Simplicity. NseLstm-B did very well on both Adequacy and Simplicity, and was best in terms of Average. Example model outputs on Newsela are provided in Table TABREF18 .", "On WikiSmall, NseLstm-B performed best on both Fluency and Adequacy. On WikiLarge, LstmLstm-B achieved the highest Fluency score while NseLstm-B received the highest Adequacy score. In terms of Simplicity and Average, NseLstm-S outperformed all other systems on both WikiSmall and WikiLarge.", "Table TABREF20 shows the correlations between the scores assigned by humans and the automatic evaluation measures. There is a positive significant correlation between Fluency and Adequacy (0.69), but a negative significant correlation between Adequacy and Simplicity (-0.64). BLEU correlates well with Fluency (0.63) and Adequacy (0.90) while SARI correlates well with Simplicity (0.73). BLEU and SARI show a negative significant correlation (-0.54). The results reflect the challenge of managing the trade-off between Fluency, Adequacy and Simplicity in sentence simplification.", "On WikiSmall, Hybrid \u2013 the current state-of-the-art \u2013 achieved best BLEU (53.94) and SARI (30.46) scores. Among neural models, NseLstm-B yielded the highest BLEU score (53.42), while NseLstm-S performed best on SARI (29.75). On WikiLarge, again, NseLstm-B had the highest BLEU score of 92.02. Sbmt-Sari \u2013 that was trained on a huge corpus of 106M sentence pairs and 2B words \u2013 scored highest on SARI with 39.96, followed by Dress-Ls (37.27), Dress (37.08), and NseLstm-S (36.88)."]}
{"question_id": "511e46b5aa8e1ee9e7dc890f47fa15ef94d4a0af", "predicted_answer": "According to the text, humans judge the simplified sentences based on three criteria:\n\n1. **Fluency**: the extent to which the output is grammatical English.\n2. **Adequacy**: the extent to which the output has the same meaning as the input sentence.\n3. **Simplicity**: the extent to which the output is simpler than the input sentence.\n\nThese judgments are made using a five-point Likert scale.", "predicted_evidence": ["We measured BLEU, and SARI at corpus-level following BIBREF15 . In addition, we also evaluated system output by eliciting human judgments. Specifically, we randomly selected 40 sentences from each test set, and included human reference simplifications and corresponding simplifications from the systems above. We then asked three volunteers to rate simplifications with respect to Fluency (the extent to which the output is grammatical English), Adequacy (the extent to which the output has the same meaning as the input sentence), and Simplicity (the extent to which the output is simpler than the input sentence) using a five point Likert scale.", "Table TABREF20 shows the correlations between the scores assigned by humans and the automatic evaluation measures. There is a positive significant correlation between Fluency and Adequacy (0.69), but a negative significant correlation between Adequacy and Simplicity (-0.64). BLEU correlates well with Fluency (0.63) and Adequacy (0.90) while SARI correlates well with Simplicity (0.73). BLEU and SARI show a negative significant correlation (-0.54). The results reflect the challenge of managing the trade-off between Fluency, Adequacy and Simplicity in sentence simplification.", "The goal of sentence simplification is to compose complex sentences into simpler ones so that they are more comprehensible and accessible, while still retaining the original information content and meaning. Sentence simplification has a number of practical applications. On one hand, it provides reading aids for people with limited language proficiency BIBREF1 , BIBREF2 , or for patients with linguistic and cognitive disabilities BIBREF3 . On the other hand, it can improve the performance of other NLP tasks BIBREF4 , BIBREF5 , BIBREF6 .", "We differ from the approach of Zhang et al. Zhang:17 in the sense that we implement both a greedy strategy and a beam-search strategy to generate the target sentence. Whereas the greedy decoder always chooses the simplification candidate with the highest log-probability, the beam-search decoder keeps a fixed number (beam) of the highest scoring candidates at each time step. We report the best simplification among the outputs based on automatic evaluation measures.", "In this paper, we explore neural Seq2seq models for sentence simplification. We propose to use an architecture with augmented memory capacities which we believe is suitable for the task, where one is confronted with long and complex sentences. Results of both automatic and human evaluation on different datasets show that our model is capable of significantly reducing the reading difficulty of the input, while performing well in terms of grammaticality and meaning preservation."]}
{"question_id": "6b4006a90aeaaff8914052d72d28851a9c0c0146", "predicted_answer": "The datasets used are:\n\n1. Newsela (BIBREF22)\n2. WikiSmall (BIBREF10)\n3. WikiLarge (BIBREF15)", "predicted_evidence": ["We used the split of the data in BIBREF15 , i.e., 94,208/1,129/1,077 pairs for train/dev/test. (2) WikiSmall BIBREF10 , which contains aligned complex-simple sentence pairs from English Wikipedia (EW) and SEW. The dataset has 88,837/205/100 pairs for train/dev/test. (3) WikiLarge BIBREF15 , a larger corpus in which the training set is a mixture of three Wikipedia datasets in BIBREF10 , BIBREF11 , BIBREF23 , and the development and test sests are complex sentences taken from WikiSmall, each has 8 simplifications written by Amazon Mechanical Turk workers BIBREF12 . The dataset has 296,402/2,000/359 pairs for train/dev/test. Table TABREF7 provides statistics on the training sets.", "Following BIBREF15 , we experiment on three simplification datasets, namely: (1) Newsela BIBREF22 , a high-quality simplification corpus of news articles composed by Newsela professional editors for children at multiple grade levels. We used the split of the data in BIBREF15 , i.e., 94,208/1,129/1,077 pairs for train/dev/test. (2) WikiSmall BIBREF10 , which contains aligned complex-simple sentence pairs from English Wikipedia (EW) and SEW. The dataset has 88,837/205/100 pairs for train/dev/test. (3) WikiLarge BIBREF15 , a larger corpus in which the training set is a mixture of three Wikipedia datasets in BIBREF10 , BIBREF11 , BIBREF23 , and the development and test sests are complex sentences taken from WikiSmall, each has 8 simplifications written by Amazon Mechanical Turk workers BIBREF12 .", "The results of human judgments are displayed in Table TABREF16 . On Newsela, NseLstm-B scored highest on Fluency. Pbmt-R was significantly better than all other systems on Adequacy while LstmLstm-S performed best on Simplicity. NseLstm-B did very well on both Adequacy and Simplicity, and was best in terms of Average. Example model outputs on Newsela are provided in Table TABREF18 .", "Following BIBREF15 , we experiment on three simplification datasets, namely: (1) Newsela BIBREF22 , a high-quality simplification corpus of news articles composed by Newsela professional editors for children at multiple grade levels. We used the split of the data in BIBREF15 , i.e., 94,208/1,129/1,077 pairs for train/dev/test. (2) WikiSmall BIBREF10 , which contains aligned complex-simple sentence pairs from English Wikipedia (EW) and SEW. The dataset has 88,837/205/100 pairs for train/dev/test.", "On WikiSmall, Hybrid \u2013 the current state-of-the-art \u2013 achieved best BLEU (53.94) and SARI (30.46) scores. Among neural models, NseLstm-B yielded the highest BLEU score (53.42), while NseLstm-S performed best on SARI (29.75). On WikiLarge, again, NseLstm-B had the highest BLEU score of 92.02. Sbmt-Sari \u2013 that was trained on a huge corpus of 106M sentence pairs and 2B words \u2013 scored highest on SARI with 39.96, followed by Dress-Ls (37.27), Dress (37.08), and NseLstm-S (36.88)."]}
{"question_id": "eccbbe3684d0cf6b794cb4eef379bb1c8bcc33bf", "predicted_answer": "The previous approaches presented for comparison are:\n\n1. SMT (Statistical Machine Translation) approach\n2. NMT (Neural Machine Translation) approach", "predicted_evidence": ["Additionally, to evaluate the quality of the modernization and the difficulty of each task, we made use of the following well-known metrics:", "ta:quality presents the quality of the modernization. Both SMT and NMT approaches were able to significantly improved the baseline. That is, the modernized documents are easier to comprehend by a contemporary reader than the original documents. An exception to this is El Conde Lucanor. The SMT approach yielded significant improvements in terms of TER, but was worse in terms of BLEU. Moreover, the NMT approach yielded worst results in terms of both BLEU and TER. Most likely, this results are due to having used the systems trained with El Quijote for modernizing El Conde Lucanor (see se:corp).", "We used sacreBLEU BIBREF40 for ensuring consistent BLEU scores. For determining whether two systems presented statistically significant differences, we applied approximate randomization tests BIBREF41, with $10,000$ repetitions and using a $p$-value of $0.05$.", "In order to measure the gains in human effort reduction, we made use of the following metrics:", "In this section, we present our experimental conditions, including translation systems, corpora and evaluation metrics."]}
{"question_id": "a3705b53c6710b41154c65327b7bbec175bdfae7", "predicted_answer": "The data used to train the model is a large parallel corpora.", "predicted_evidence": ["This neural network usually follows an encoder-decoder architecture, featuring recurrent networks BIBREF23, BIBREF24, convolutional networks BIBREF25 or attention mechanisms BIBREF26. Model parameters are jointly estimated on large parallel corpora, using stochastic gradient descent BIBREF27, BIBREF28. At decoding time, the system obtains the most likely translation using a beam search method.", "SMT systems were trained with Moses BIBREF29, following the standard procedure: we estimated a 5-gram language model\u2014smoothed with the improved KneserNey method\u2014using SRILM BIBREF30, and optimized the weights of the log-linear model with MERT BIBREF31.", "ta:corp presents the corpora statistics.", "We built our NMT systems using NMT-Keras BIBREF32. We used long short-term memory units BIBREF33, with all model dimensions set to 512. We trained the system using Adam BIBREF34 with a fixed learning rate of $0.0002$ and a batch size of 60. We applied label smoothing of $0.1$ BIBREF35. At inference time, we used beam search with a beam size of 6. We applied joint byte pair encoding to all corpora BIBREF36, using $32,000$ merge operations.", "As a future work, we want to further research the behavior of the neural systems. For that, we would like to explore techniques for enriching the training corpus with additional data, and the incorrect generation of words due to subwords. We would also like to develop new protocols based on successful IMT approaches. Finally, we should test our proposal with real users to obtain actual measures of the effort reduction."]}
{"question_id": "b62b7ec5128219f04be41854247d5af992797937", "predicted_answer": "Yes, the proposed approach uses neural networks.", "predicted_evidence": ["As a future work, we want to further research the behavior of the neural systems. For that, we would like to explore techniques for enriching the training corpus with additional data, and the incorrect generation of words due to subwords. We would also like to develop new protocols based on successful IMT approaches. Finally, we should test our proposal with real users to obtain actual measures of the effort reduction.", "While the lack of a spelling convention has been extensively researched for years BIBREF6, BIBREF7, BIBREF8, modernization of historical documents is a younger field. BIBREF1 organized a shared task in order to translate historical text to contemporary language. The main goal of this shared task was to tackle the spelling problem. However, they also approached document modernization using a set of rules. BIBREF9 proposed a modernization approach based on statistical machine translation (SMT). A neural machine translation (NMT) approach was proposed by BIBREF2. Finally, BIBREF10 extracted parallel phrases from an original parallel corpus and used them as an additional training data for their NMT approach.", "Statistical IMT systems were implemented following the procedure of word graph exploration and generation of a best suffix for a given prefix described by BIBREF5. Neural IMT systems were built using the interactive branch of NMT-Keras.", "This neural network usually follows an encoder-decoder architecture, featuring recurrent networks BIBREF23, BIBREF24, convolutional networks BIBREF25 or attention mechanisms BIBREF26. Model parameters are jointly estimated on large parallel corpora, using stochastic gradient descent BIBREF27, BIBREF28. At decoding time, the system obtains the most likely translation using a beam search method.", "Regarding the performance of both approaches, SMT achieved the highest effort reduction. This was reasonably expected since its modernization quality was better. However, in past neural IMT works BIBREF15, the neural IMT approach was able to yield further improvements despite having a lower translation quality than its SMT counterpart. Most likely, the reason of this is that, due to the small training corpora, the neural model was not able to reach its best performance, Nonetheless, we should address this in a future work."]}
{"question_id": "e8fa4303b36a47a5c87f862458442941bbdff7d9", "predicted_answer": "The machine learning techniques used in the model architecture are:\n\n1. Recurrent networks\n2. Convolutional networks\n3. Attention mechanisms\n4. Stochastic gradient descent\n5. Beam search\n6. Long short-term memory (LSTM) units\n7. Adam optimization algorithm\n8. Label smoothing\n\nThese techniques are used in the neural machine translation (NMT) systems, specifically in the encoder-decoder architecture.", "predicted_evidence": ["This neural network usually follows an encoder-decoder architecture, featuring recurrent networks BIBREF23, BIBREF24, convolutional networks BIBREF25 or attention mechanisms BIBREF26. Model parameters are jointly estimated on large parallel corpora, using stochastic gradient descent BIBREF27, BIBREF28. At decoding time, the system obtains the most likely translation using a beam search method.", "SMT systems were trained with Moses BIBREF29, following the standard procedure: we estimated a 5-gram language model\u2014smoothed with the improved KneserNey method\u2014using SRILM BIBREF30, and optimized the weights of the log-linear model with MERT BIBREF31.", "As a future work, we want to further research the behavior of the neural systems. For that, we would like to explore techniques for enriching the training corpus with additional data, and the incorrect generation of words due to subwords. We would also like to develop new protocols based on successful IMT approaches. Finally, we should test our proposal with real users to obtain actual measures of the effort reduction.", "We built our NMT systems using NMT-Keras BIBREF32. We used long short-term memory units BIBREF33, with all model dimensions set to 512. We trained the system using Adam BIBREF34 with a fixed learning rate of $0.0002$ and a batch size of 60. We applied label smoothing of $0.1$ BIBREF35. At inference time, we used beam search with a beam size of 6. We applied joint byte pair encoding to all corpora BIBREF36, using $32,000$ merge operations.", "Statistical IMT systems were implemented following the procedure of word graph exploration and generation of a best suffix for a given prefix described by BIBREF5. Neural IMT systems were built using the interactive branch of NMT-Keras."]}
{"question_id": "51e9f446d987219bc069222731dfc1081957ce1f", "predicted_answer": "The language(s) the model is tested on is not explicitly mentioned in the given context. The text only discusses the training and evaluation of Statistical Machine Translation (SMT) systems and neural network-based models, but it does not specify the language(s) used for testing.", "predicted_evidence": ["SMT systems were trained with Moses BIBREF29, following the standard procedure: we estimated a 5-gram language model\u2014smoothed with the improved KneserNey method\u2014using SRILM BIBREF30, and optimized the weights of the log-linear model with MERT BIBREF31.", "In this section, we present our experimental conditions, including translation systems, corpora and evaluation metrics.", "This neural network usually follows an encoder-decoder architecture, featuring recurrent networks BIBREF23, BIBREF24, convolutional networks BIBREF25 or attention mechanisms BIBREF26. Model parameters are jointly estimated on large parallel corpora, using stochastic gradient descent BIBREF27, BIBREF28. At decoding time, the system obtains the most likely translation using a beam search method.", "BiLingual Evaluation Understudy (BLEU) BIBREF38: computes the geometric average of the modified n-gram precision, multiplied by a brevity factor that penalizes short sentences.", "Translation Error Rate (TER) BIBREF39: computes the number of word edit operations (insertion, substitution, deletion and swapping), normalized by the number of words in the final translation."]}
{"question_id": "13fb28e8b7f34fe600b29fb842deef75608c1478", "predicted_answer": "The text does not provide a specific number by which their model outperformed the baselines. It only mentions that their initial submits (RUN 4 and 5) outperformed the memorization baseline on every metric on every task, but it does not provide a quantitative measure of the improvement.", "predicted_evidence": ["All of the tasks were evaluated using the standard metrics of precision(P), recall(R) and INLINEFORM0 : DISPLAYFORM0", "Table TABREF28 shows results on the event expression tasks. Our initial submits RUN 4 and 5 outperformed the memorization baseline on every metric on every task. The precision of event span identification is close to the max report. However, our system got lower recall. One of the main reason is that our training objective function is accuracy-oriented. Table TABREF29 shows results on the phase 2 subtask.", "where INLINEFORM0 is the set of items predicted by the system and INLINEFORM1 is the set of items manually annotated by the humans. Applying these metrics of the tasks only requires a definition of what is considered an \"item\" for each task. For evaluating the spans of event expressions, items were tuples of character offsets. Thus, system only received credit for identifying events with exactly the same character offsets as the manually annotated ones. For evaluating the attributes of event expression types, items were tuples of (begin, end, value) where begin and end are character offsets and value is the value that was given to the relevant attribute. Thus, systems only received credit for an event attribute if they both found an event with correct character offsets and then assigned the correct value for that attribute BIBREF10 .", "where INLINEFORM0 is the number of training examples and the superscript INLINEFORM1 indicates the INLINEFORM2 th example.", "We use Lasagne deep learning framework. We first initialize our word representations using publicly available 300-dimensional Glove word vectors . We deploy CNN model with kernel width of 2, a filter size of 300, sequence length is INLINEFORM0 , number filters is INLINEFORM1 , stride is 1, pool size is INLINEFORM2 , cnn activation function is tangent, MLP activation function is sigmoid. MLP hidden dimension is 50. We initialize CNN weights using a uniform distribution. Finally, by stacking a softmax function on top, we can get normalized log-probabilities. Training is done through stochastic gradient descent over shuffled mini-batches with the AdaGrad update rule BIBREF11 . The learning rate is set to 0.05. The mini-batch size is 100. The model parameters were regularized with a per-minibatch L2 regularization strength of INLINEFORM3 ."]}
{"question_id": "d5bce5da746a075421c80abe10c97ad11a96c6cd", "predicted_answer": "According to the text, they compared their results against a \"memorization baseline\".", "predicted_evidence": ["All of the tasks were evaluated using the standard metrics of precision(P), recall(R) and INLINEFORM0 : DISPLAYFORM0", "where INLINEFORM0 is the set of items predicted by the system and INLINEFORM1 is the set of items manually annotated by the humans. Applying these metrics of the tasks only requires a definition of what is considered an \"item\" for each task. For evaluating the spans of event expressions, items were tuples of character offsets. Thus, system only received credit for identifying events with exactly the same character offsets as the manually annotated ones. For evaluating the attributes of event expression types, items were tuples of (begin, end, value) where begin and end are character offsets and value is the value that was given to the relevant attribute. Thus, systems only received credit for an event attribute if they both found an event with correct character offsets and then assigned the correct value for that attribute BIBREF10 .", "Table TABREF28 shows results on the event expression tasks. Our initial submits RUN 4 and 5 outperformed the memorization baseline on every metric on every task. The precision of event span identification is close to the max report. However, our system got lower recall. One of the main reason is that our training objective function is accuracy-oriented. Table TABREF29 shows results on the phase 2 subtask.", "We use the Clinical TempEval corpus as the evaluation dataset. This corpus was based on a set of 600 clinical notes and pathology reports from cancer patients at the Mayo Clinic. These notes were manually de-identified by the Mayo Clinic to replace names, locations, etc. with generic placeholders, but time expression were not altered. The notes were then manually annotated with times, events and temporal relations in clinical notes. These annotations include time expression types, event attributes and an increased focus on temporal relations. The event, time and temporal relation annotations were distributed separately from the text using the Anafora standoff format. Table TABREF19 shows the number of documents, event expressions in the training, development and testing portions of the 2016 THYME data.", "where INLINEFORM0 is the number of training examples and the superscript INLINEFORM1 indicates the INLINEFORM2 th example."]}
{"question_id": "930733efb3b97e1634b4dcd77123d4d5731e8807", "predicted_answer": "According to the text, the performance of the system on the event expression tasks is as follows:\n\n* Precision of event span identification is close to the maximum reported.\n* Recall is lower, which is attributed to the accuracy-oriented training objective function.\n\nAdditionally, the system outperformed the memorization baseline on every metric on every task in the initial submits (RUN 4 and 5).", "predicted_evidence": ["All of the tasks were evaluated using the standard metrics of precision(P), recall(R) and INLINEFORM0 : DISPLAYFORM0", "Table TABREF28 shows results on the event expression tasks. Our initial submits RUN 4 and 5 outperformed the memorization baseline on every metric on every task. The precision of event span identification is close to the max report. However, our system got lower recall. One of the main reason is that our training objective function is accuracy-oriented. Table TABREF29 shows results on the phase 2 subtask.", "where INLINEFORM0 is the set of items predicted by the system and INLINEFORM1 is the set of items manually annotated by the humans. Applying these metrics of the tasks only requires a definition of what is considered an \"item\" for each task. For evaluating the spans of event expressions, items were tuples of character offsets. Thus, system only received credit for identifying events with exactly the same character offsets as the manually annotated ones. For evaluating the attributes of event expression types, items were tuples of (begin, end, value) where begin and end are character offsets and value is the value that was given to the relevant attribute. Thus, systems only received credit for an event attribute if they both found an event with correct character offsets and then assigned the correct value for that attribute BIBREF10 .", "In the past few years, there has been much interest in applying neural network based deep learning techniques to solve all kinds of natural language processing (NLP) tasks. From low level tasks such as language modeling, POS tagging, named entity recognition, and semantic role labeling BIBREF0 , BIBREF1 , to high level tasks such as machine translation, information retrieval, semantic analysis BIBREF2 , BIBREF3 , BIBREF4 and sentence relation modeling tasks such as paraphrase identification and question answering BIBREF5 , BIBREF6 , BIBREF7 . Deep representation learning has demonstrated its importance for these tasks. All the tasks get performance improvement via learning either word level representations or sentence level representations.", "We use Lasagne deep learning framework. We first initialize our word representations using publicly available 300-dimensional Glove word vectors . We deploy CNN model with kernel width of 2, a filter size of 300, sequence length is INLINEFORM0 , number filters is INLINEFORM1 , stride is 1, pool size is INLINEFORM2 , cnn activation function is tangent, MLP activation function is sigmoid. MLP hidden dimension is 50. We initialize CNN weights using a uniform distribution. Finally, by stacking a softmax function on top, we can get normalized log-probabilities. Training is done through stochastic gradient descent over shuffled mini-batches with the AdaGrad update rule BIBREF11 . The learning rate is set to 0.05. The mini-batch size is 100. The model parameters were regularized with a per-minibatch L2 regularization strength of INLINEFORM3 ."]}
{"question_id": "11f9c207476af75a9272105e646df02594059c3f", "predicted_answer": "The dataset used to evaluate the system is the Clinical TempEval corpus, which is based on 600 clinical notes and pathology reports from cancer patients at the Mayo Clinic.", "predicted_evidence": ["All of the tasks were evaluated using the standard metrics of precision(P), recall(R) and INLINEFORM0 : DISPLAYFORM0", "We use the Clinical TempEval corpus as the evaluation dataset. This corpus was based on a set of 600 clinical notes and pathology reports from cancer patients at the Mayo Clinic. These notes were manually de-identified by the Mayo Clinic to replace names, locations, etc. with generic placeholders, but time expression were not altered. The notes were then manually annotated with times, events and temporal relations in clinical notes. These annotations include time expression types, event attributes and an increased focus on temporal relations. The event, time and temporal relation annotations were distributed separately from the text using the Anafora standoff format. Table TABREF19 shows the number of documents, event expressions in the training, development and testing portions of the 2016 THYME data.", "where INLINEFORM0 is the set of items predicted by the system and INLINEFORM1 is the set of items manually annotated by the humans. Applying these metrics of the tasks only requires a definition of what is considered an \"item\" for each task. For evaluating the spans of event expressions, items were tuples of character offsets. Thus, system only received credit for identifying events with exactly the same character offsets as the manually annotated ones. For evaluating the attributes of event expression types, items were tuples of (begin, end, value) where begin and end are character offsets and value is the value that was given to the relevant attribute. Thus, systems only received credit for an event attribute if they both found an event with correct character offsets and then assigned the correct value for that attribute BIBREF10 .", "And output annotations over the text that capture the key information such as event mentions and attributes. Table TABREF7 illustrates the output of clinical information extraction in details.", "The input of our system consists of raw clinical notes or pathology reports like below:"]}
{"question_id": "b32de10d84b808886d7a91ab0c423d4fc751384c", "predicted_answer": "They obtained part-of-speech tags using a POS tagger from the NLTK (Natural Language Toolkit) library.", "predicted_evidence": ["To solve this task, the major challenge is how to precisely identify the spans (character offsets) of the event expressions from raw clinical notes. Traditional machine learning approaches usually build a supervised classifier with features generated by the Apache clinical Text Analysis and Knowledge Extraction System (cTAKES) . For example, BluLab system BIBREF8 extracted morphological(lemma), lexical(token), and syntactic(part-of-speech) features encoded from cTAKES. Although using the domain specific information extraction tools can improve the performance, learning how to use it well for clinical domain feature engineering is still very time-consuming. In short, a simple and effective method that only leverage basic NLP modules and achieves high extraction performance is desired to save costs.", "In this paper, we introduced a new clinical information extraction system that only leverage deep neural networks to identify event spans and their attributes from raw clinical notes. We trained deep neural networks based classifiers to extract clinical event spans. Our method attached each word to their part-of-speech tag and shape information as extra features. We then hire temporal convolution neural network to learn hidden feature representations. The entire experimental results demonstrate that our approach consistently outperforms the existing baseline methods on standard evaluation datasets.", "The major advantage of our system is that we only leverage NLTK tokenization and a POS tagger to preprocess our training dataset. When implementing our neural network based clinical information extraction system, we found it is not easy to construct high quality training data due to the noisy format of clinical notes. Choosing the proper tokenizer is quite important for span identification. After several experiments, we found \"RegexpTokenizer\" can match our needs. This tokenizer can generate spans for each token via sophisticated regular expression like below,", "Our research proved that we can get competitive results without the help of a domain specific feature extraction toolkit, such as cTAKES. Also we only leverage basic natural language processing modules such as tokenization and part-of-speech tagging. With the help of deep representation learning, we can dramatically reduce the cost of clinical information extraction system development.", "To address this challenge, we propose a deep neural networks based method, especially convolution neural network BIBREF0 , to learn hidden feature representations directly from raw clinical notes. More specifically, one method first extract a window of surrounding words for the candidate word. Then, we attach each word with their part-of-speech tag and shape information as extra features. Then our system deploys a temporal convolution neural network to learn hidden feature representations. Finally, our system uses Multilayer Perceptron (MLP) to predict event spans. Note that we use the same model to predict event attributes."]}
{"question_id": "9ea3669528c2b295f21770cb7f70d0c4b4389223", "predicted_answer": "The F1 score is not explicitly mentioned in the given context. However, it is mentioned that the F-measure of RB+CB is higher than RB, with an improvement of 1.27%.", "predicted_evidence": ["Table 2 shows the evaluation results. The rule based RB gives fairly high precision but with low recall. CB, the common-sense based method, achieves the highest recall. Yet, its precision is the worst. RB+CB, the combination of RB and CB gives higher the F-measure But, the improvement of 1.27% is only marginal compared to RB.", "The final prediction is an output from a softmax function, denoted as INLINEFORM0 : DISPLAYFORM0", "RB (Rule based method): The rule based method proposed in BIBREF33 .", "We compare with the following baseline methods:", "We first presents the experimental settings and then report the results in this section."]}
{"question_id": "9863f5765ba70f7ff336a580346ef70205abbbd8", "predicted_answer": "The baselines mentioned are:\n\n1. Rule-based RB (RB)\n2. Common-sense based method (CB)\n3. Combination of RB and CB (RB+CB)", "predicted_evidence": ["We compare with the following baseline methods:", "In the experiments, we randomly select 90% of the dataset as training data and 10% as testing data. In order to obtain statistically credible results, we evaluate our method and baseline methods 25 times with different train/test splits.", "Table 2 shows the evaluation results. The rule based RB gives fairly high precision but with low recall. CB, the common-sense based method, achieves the highest recall. Yet, its precision is the worst. RB+CB, the combination of RB and CB gives higher the F-measure But, the improvement of 1.27% is only marginal compared to RB.", "In our model, the training epochs are set to 20. In this section, we examine the testing error using a case study. Due to the page length limit, we only choose one example from the corpus. The text below has four clauses:", "We first presents the experimental settings and then report the results in this section."]}
{"question_id": "ced63053eb631c78a4ddd8c85ec0f3323a631a54", "predicted_answer": "The emotion cause dataset used is a simplified Chinese emotion cause corpus, which contains 2,105 documents from SINA city news.", "predicted_evidence": ["We conduct experiments on a simplified Chinese emotion cause corpus BIBREF31 , the only publicly available dataset on this task to the best of our knowledge. The corpus contains 2,105 documents from SINA city news. Each document has only one emotion word and one or more emotion causes. The documents are segmented into clauses manually. The main task is to identify which clause contains the emotion cause.", "In an example shown above, \u201csad\u201d is an emotion word, and the cause of \u201csad\u201d is \u201cI lost my phone\u201d. The emotion cause extraction task aims to identify the reason behind an emotion expression. It is a more difficult task compared to emotion classification since it requires a deep understanding of the text that conveys an emotions.", "The rest of the paper is organized as follows. Section SECREF2 gives a review of related works on emotion analysis. Section SECREF3 presents our proposed deep memory network based model for emotion cause extraction. Section SECREF4 discusses evaluation results. Finally, Section SECREF5 concludes the work and outlines the future directions.", "In this section, we will first define our task. [id=lq]Then, a brief introduction of memory network will be given, including its basic learning structure of memory network and deep architecture. Last, our modified deep memory network for emotion cause extraction will be presented.", "Other than rule based methods, russo2011emocause proposed a crowdsourcing method to construct a common-sense knowledge base which is related to emotion causes. But it is challenging to extend the common-sense knowledge base automatically. ghazi2015detecting used Conditional Random Fields (CRFs) to extract emotion causes. However, it requires emotion cause and emotion keywords to be in the same sentence. More recently, gui2016event proposed a multi-kernel based method to extract emotion causes through learning from a manually annotated emotion cause dataset."]}
{"question_id": "f13a5b6a67a9b10fde68e8b33792879b8146102c", "predicted_answer": "According to the context, the lexical features extracted are:\n\n* Word representations learned by Word2vec (in the Word2vec method)\n* N-gram features (in the SVM method)\n* Word vectors (mapped into a low-dimensional embedding space)", "predicted_evidence": ["Word2vec: This is a SVM classifier using word representations learned by Word2vec BIBREF32 as features.", "For machine learning methods, RB+CB+ML uses both rules and common-sense knowledge as features to train a machine learning classifier. It achieves F-measure of 0.5597, outperforming RB+CB. Both SVM and word2vec are word feature based methods and they have similar performance. For word2vec, even though word representations are obtained from the SINA news raw corpus, it still performs worse than SVM trained using n-gram features only. The multi-kernel method BIBREF31 is the best performer among the baselines because it considers context information in a structured way. It models text by its syntactic tree and also considers an emotion lexicon. Their work shows that the structure information is important for the emotion cause extraction task.", "Identifying emotion categories in text is one of the key tasks in NLP BIBREF6 . Going one step further, emotion cause extraction can reveal important information about what causes a certain emotion and why there is an emotion change. In this section, we introduce related work on emotion analysis including emotion cause extraction.", "The formal definition of emotion cause extraction is given in BIBREF31 . In this task, a given document, which [id=lq]is a passage about an emotion event, contains an emotion word INLINEFORM0 and the cause of the event. The document is manually segmented in the clause level. For each clause INLINEFORM1 consisting of INLINEFORM2 words, the goal [id=lq]is to identify which clause contains the emotion cause. [id=lq]For data representation, we can map each word into a low dimensional embedding space, a.k.a word vector BIBREF32 . All the word vectors are stacked in a word embedding matrix INLINEFORM3 , where INLINEFORM4 is the dimension of word vector and INLINEFORM5 is the vocabulary size.", "We conduct experiments on a simplified Chinese emotion cause corpus BIBREF31 , the only publicly available dataset on this task to the best of our knowledge. The corpus contains 2,105 documents from SINA city news. Each document has only one emotion word and one or more emotion causes. The documents are segmented into clauses manually. The main task is to identify which clause contains the emotion cause."]}
{"question_id": "67c16ba64fe27838b1034d15194c07a9c98cdebe", "predicted_answer": "According to the text, the word level sequence features extracted are:\n\n* Word vectors (learned by Word2vec)\n* N-gram features", "predicted_evidence": ["The formal definition of emotion cause extraction is given in BIBREF31 . In this task, a given document, which [id=lq]is a passage about an emotion event, contains an emotion word INLINEFORM0 and the cause of the event. The document is manually segmented in the clause level. For each clause INLINEFORM1 consisting of INLINEFORM2 words, the goal [id=lq]is to identify which clause contains the emotion cause. [id=lq]For data representation, we can map each word into a low dimensional embedding space, a.k.a word vector BIBREF32 . All the word vectors are stacked in a word embedding matrix INLINEFORM3 , where INLINEFORM4 is the dimension of word vector and INLINEFORM5 is the vocabulary size.", "Word2vec: This is a SVM classifier using word representations learned by Word2vec BIBREF32 as features.", "For machine learning methods, RB+CB+ML uses both rules and common-sense knowledge as features to train a machine learning classifier. It achieves F-measure of 0.5597, outperforming RB+CB. Both SVM and word2vec are word feature based methods and they have similar performance. For word2vec, even though word representations are obtained from the SINA news raw corpus, it still performs worse than SVM trained using n-gram features only. The multi-kernel method BIBREF31 is the best performer among the baselines because it considers context information in a structured way. It models text by its syntactic tree and also considers an emotion lexicon. Their work shows that the structure information is important for the emotion cause extraction task.", "Identifying emotion categories in text is one of the key tasks in NLP BIBREF6 . Going one step further, emotion cause extraction can reveal important information about what causes a certain emotion and why there is an emotion change. In this section, we introduce related work on emotion analysis including emotion cause extraction.", "We conduct experiments on a simplified Chinese emotion cause corpus BIBREF31 , the only publicly available dataset on this task to the best of our knowledge. The corpus contains 2,105 documents from SINA city news. Each document has only one emotion word and one or more emotion causes. The documents are segmented into clauses manually. The main task is to identify which clause contains the emotion cause."]}
{"question_id": "58a3cfbbf209174fcffe44ce99840c758b448364", "predicted_answer": "The recent models they compare with are:\n\n1. LSTM-based models\n2. NAS (Neural Architecture Search) models\n3. RHN (Recurrent Highway Networks) models, including the original publication's 24M parameter model and their own 24M version.", "predicted_evidence": ["Notably, in our experiments even the RHN with only 10M parameters has better perplexity than the 24M one in the original publication. Our 24M version improves on that further. However, a shallow LSTM-based model with only 10M parameters enjoys a very comfortable margin over that, with deeper models following near the estimated noise range. At 24M, all depths obtain very similar results, reaching exp(4.065) [fixed,zerofill,precision=1] at depth 4. Unsurprisingly, NAS whose architecture was chosen based on its performance on this dataset does almost equally well, even better than in BIBREF1 .", "On two of the three datasets, we improved previous results substantially by careful model specification and hyperparameter optimisation, but the improvement for RHNs is much smaller compared to that for LSTMs. While it cannot be ruled out that our particular setup somehow favours LSTMs, we believe it is more likely that this effect arises due to the original RHN experimental condition having been tuned more extensively (this is nearly unavoidable during model development).", "We have not explicitly dealt with the unknown uncertainty remaining in the Gaussian Process that may affect model comparisons, apart from running it until apparent convergence. All in all, our findings suggest that a gap in perplexity of 1.0 is a statistically robust difference between models trained in this way on these datasets. The distribution of results was approximately normal with roughly the same variance for all models, so we still report numbers in a tabular form instead of plotting the distribution of results, for example in a violin plot BIBREF26 .", "In contrast to the previous datasets, our numbers on this task (reported in BPC, following convetion) are slightly off the state of the art. This is most likely due to optimisation being limited to 14 epochs which is about a tenth of what the model of BIBREF0 was trained for. Nevertheless, we match their smaller RHN with our models which are very close to each other. NAS lags the other models by a surprising margin at this task.", "We further measured the contribution of other features of the models in a series of experiments. See Table TABREF22 . To limit the number of resource used, in these experiments only individual features were evaluated (not their combinations) on Penn Treebank at the best depth for each architecture (LSTM or RHN) and parameter budget (10M or 24M) as determined above."]}
{"question_id": "6c6e06f7bfb6d30003fd3801fdaf34649ef1b8f4", "predicted_answer": "The text does not explicitly report the results on the Hutter Prize dataset (Enwik8). It mentions that the dataset is used, but the results are not provided.", "predicted_evidence": ["Wikitext-2 is not much larger than Penn Treebank, so it is not surprising that even models tuned for Penn Treebank perform reasonably on this dataset, and this is in fact how results in previous works were produced. For a fairer comparison, we also tune hyperparameters on the same dataset. In Table TABREF14 , we report numbers for both approaches. All our results are well below the previous state of the are for models without dynamic evaluation or caching. That said, our best result, exp(4.188) [fixed,zerofill,precision=1] compares favourably even to the Neural Cache BIBREF6 whose innovations are fairly orthogonal to the base model.", "Because we treat our corpora as a single sequence, test set contents are not i.i.d., and we cannot apply techniques such as the bootstrap to assess UID21 . Instead, we looked at the gap between validation and test scores as a proxy and observed that it is very stable, contributing variance of 0.12\u20130.3 perplexity to the final results on Penn Treebank and Wikitext-2, respectively.", "We further measured the contribution of other features of the models in a series of experiments. See Table TABREF22 . To limit the number of resource used, in these experiments only individual features were evaluated (not their combinations) on Penn Treebank at the best depth for each architecture (LSTM or RHN) and parameter budget (10M or 24M) as determined above.", "We compare models on three datasets. The smallest of them is the Penn Treebank corpus by BIBREF13 with preprocessing from BIBREF14 . We also include another word level corpus: Wikitext-2 by BIBREF15 . It is about twice the size of Penn Treebank with a larger vocabulary and much lighter preprocessing. The third corpus is Enwik8 from the Hutter Prize dataset BIBREF16 . Following common practice, we use the first 90 million characters for training, and the remaining 10 million evenly split between validation and test.", "To assess the severity of these issues, we conducted the following experiment: models with the best hyperparameter settings for Penn Treebank and Wikitext-2 were retrained from scratch with various initialisation seeds and the validation and test scores were recorded. If during tuning, a model just got a lucky run due to a combination of UID19 and UID20 , then retraining with the same hyperparameters but with different seeds would fail to reproduce the same good results."]}
{"question_id": "b6e97d1b1565732b1b3f1d74e6d2800dd21be37a", "predicted_answer": "The question is not explicitly stated in the given context, but based on the text, I assume the question is asking about the results of the experiment conducted to assess the severity of the issues related to hyperparameter tuning and initialization.\n\nThe answer is:\n\n* The variance induced by UID19 and UID20 together is roughly equivalent to an absolute difference of 0.4 in perplexity on Penn Treebank and 0.5 on Wikitext-2.\n* The validation perplexities of the best checkpoints are about one standard deviation lower than the sample mean of the reruns, so the tuner could fit the noise only to a limited degree.\n* The effect of UID19 is almost as large as that of UID19 and UID20 combined in the experiment's environment (Tensorflow with a single GPU).", "predicted_evidence": ["During the transitional period when deep neural language models began to supplant their shallower predecessors, effect sizes tended to be large, and robust conclusions about the value of the modelling innovations could be made, even in the presence of poorly controlled \u201chyperparameter noise.\u201d However, now that the neural revolution is in full swing, researchers must often compare competing deep architectures. In this regime, effect sizes tend to be much smaller, and more methodological care is required to produce reliable results. Furthermore, with so much work carried out in parallel by a growing research community, the costs of faulty conclusions are increased.", "Because we treat our corpora as a single sequence, test set contents are not i.i.d., and we cannot apply techniques such as the bootstrap to assess UID21 . Instead, we looked at the gap between validation and test scores as a proxy and observed that it is very stable, contributing variance of 0.12\u20130.3 perplexity to the final results on Penn Treebank and Wikitext-2, respectively.", "To assess the severity of these issues, we conducted the following experiment: models with the best hyperparameter settings for Penn Treebank and Wikitext-2 were retrained from scratch with various initialisation seeds and the validation and test scores were recorded. If during tuning, a model just got a lucky run due to a combination of UID19 and UID20 , then retraining with the same hyperparameters but with different seeds would fail to reproduce the same good results.", "We further measured the contribution of other features of the models in a series of experiments. See Table TABREF22 . To limit the number of resource used, in these experiments only individual features were evaluated (not their combinations) on Penn Treebank at the best depth for each architecture (LSTM or RHN) and parameter budget (10M or 24M) as determined above.", "There are a few notable things about the results. First, in our environment (Tensorflow with a single GPU) even with the same seed as the one used by the tuner, the effect of UID19 is almost as large as that of UID19 and UID20 combined. Second, the variance induced by UID19 and UID20 together is roughly equivalent to an absolute difference of 0.4 in perplexity on Penn Treebank and 0.5 on Wikitext-2. Third, the validation perplexities of the best checkpoints are about one standard deviation lower than the sample mean of the reruns, so the tuner could fit the noise only to a limited degree."]}
{"question_id": "4f8b078b9f60be30520fd32a3d8601ab3babb5c0", "predicted_answer": "According to the text, the regularization methods mentioned are:\n\n1. Down-projection\n2. Grid search (mentioned as a suboptimal method)\n\nNote that the text also mentions that the authors of BIBREF1 used similar regularization methods, but it does not specify what those methods are.", "predicted_evidence": ["Naturally, NAS benefitted only to a limited degree from our tuning, since the numbers of BIBREF1 were already produced by employing similar regularisation methods and a grid search. The small edge can be attributed to the suboptimality of grid search (see Section SECREF23 ).", "Optimisation is performed by Adam BIBREF17 with INLINEFORM0 but otherwise default parameters ( INLINEFORM1 , INLINEFORM2 ). Setting INLINEFORM3 so turns off the exponential moving average for the estimates of the means of the gradients and brings Adam very close to RMSProp without momentum, but due to Adam's bias correction, larger learning rates can be used.", "Down-projection was found to be very beneficial by the tuner for some depth/budget combinations. On Penn Treebank, it improved results by about 2\u20135 perplexity points at depths 1 and 2 at 10M, and depth 1 at 24M, possibly by equipping the recurrent cells with more capacity. The very same models benefited from down-projection on Wikitext-2, but even more so with gaps of about 10\u201318 points which is readily explained by the larger vocabulary size.", "In summary, the three recurrent cell architectures are closely matched on all three datasets, with minuscule differences on Enwik8 where regularisation matters the least. These results support the claims of BIBREF21 , that capacities of various cells are very similar and their apparent differences result from trainability and regularisation. While comparing three similar architectures cannot prove this point, the inclusion of NAS certainly gives it more credence. This way we have two of the best human designed and one machine optimised cell that was the top performer among thousands of candidates.", "First, we untied input and output embeddings which made perplexities worse by about 6 points across the board which is consistent with the results of BIBREF9 ."]}
{"question_id": "54517cded8267ea6c9a3f3cf9c37a8d24b3f7c2c", "predicted_answer": "The three recurrent architectures that were reevaluated are:\n\n1. LSTM (Long Short-Term Memory)\n2. RHN (Recurrent Highway Network)\n3. NAS (Neural Architecture Search)", "predicted_evidence": ["Our focus is on three recurrent architectures:", "We further measured the contribution of other features of the models in a series of experiments. See Table TABREF22 . To limit the number of resource used, in these experiments only individual features were evaluated (not their combinations) on Penn Treebank at the best depth for each architecture (LSTM or RHN) and parameter budget (10M or 24M) as determined above.", "Our aim is strictly to do better model comparisons for these architectures and we thus refrain from including techniques that are known to push perplexities even lower, but which are believed to be largely orthogonal to the question of the relative merits of these recurrent cells. In parallel work with a remarkable overlap with ours, BIBREF5 demonstrate the utility of adding a Neural Cache BIBREF6 . Building on their work, BIBREF7 show that Dynamic Evaluation BIBREF8 contributes similarly to the final perplexity.", "For Enwik8 there are relatively few parameters in the embeddings since the vocabulary size is only 205. Here we choose not to share embeddings and to omit the down-projection unconditionally.", "In summary, the three recurrent cell architectures are closely matched on all three datasets, with minuscule differences on Enwik8 where regularisation matters the least. These results support the claims of BIBREF21 , that capacities of various cells are very similar and their apparent differences result from trainability and regularisation. While comparing three similar architectures cannot prove this point, the inclusion of NAS certainly gives it more credence. This way we have two of the best human designed and one machine optimised cell that was the top performer among thousands of candidates."]}
{"question_id": "803babb71e1bdaf507847d6c712585f4128e9f47", "predicted_answer": "The baseline NMT systems.", "predicted_evidence": ["We select 4 MT models for evaluation (see Table TABREF63) and show their 4 outputs at once, sentence-by-sentence, to human judges, who are asked to rank them given the French source sentence in context (with the full review). For each pair of models, we count the number of wins, ties and losses, and apply the Wilcoxon signed-rank test.", "For fine-tuning, we use a fixed learning rate, and a total batch size of 3500 tokens (training on a single GPU without delayed updates). To avoid overfitting on Foursquare-PE, we do early stopping according to perplexity on Foursquare-valid. For each fine-tuned model we test all 16 combinations of dropout in $\\lbrace 0.1,0.2,0.3,0.4\\rbrace $ and learning rate in $\\lbrace 1, 2, 5, 10\\rbrace \\times 10^{-5}$. We keep the model with the best perplexity on Foursquare-valid.", "We sampled 11.5k French reviews from Foursquare, mostly in the food category, split them into 18k sentences, and grouped them into train, valid and test sets (see Table TABREF6). The French reviews contain on average 1.5 sentences and 17.9 words. Then, we hired eight professional translators to translate them to English. Two of them created the training set by post-editing (PE) the outputs of baseline NMT systems. The other six translated the valid and test sets from scratch. They were asked to translate (or post-edit) the reviews sentence-by-sentence (to avoid any alignment problem), but they could see the full context. We manually filtered the test set to remove translations that were not satisfactory. The full reviews and additional metadata (e.g., location and type of the restaurant) are also available as part of this resource, to encourage research on contextual machine translation.", "This in-domain data is concatenated to the out-of-domain parallel data and used for training.", "When small amounts of in-domain parallel data are available, fine-tuning (FT) is often the preferred solution for domain adaptation BIBREF9, BIBREF10. It consists in training a model on out-of-domain data, and then continuing its training for a few epochs on the in-domain data only."]}
{"question_id": "5fd112980d0dd7f7ce30e6273fe6e7b230b13225", "predicted_answer": "The datasets used are:\n\n1. newstest[2012, 2013]\n2. Foursquare-valid\n3. WMT 2014\n4. UGC (User Generated Content) which is a combination of:\n   - Multi UN\n   - OpenSubtitles\n   - Wikipedia\n   - Books\n   - Tatoeba\n   - TED talks\n   - ParaCrawl\n   - Gourmet", "predicted_evidence": ["This in-domain data is concatenated to the out-of-domain parallel data and used for training.", "We conduct a human evaluation to confirm the observations with BLEU and to overcome some of the limitations of this metric.", "During our work, we used BLEU BIBREF28 on newstest[2012, 2013] to ensure that our models stayed good on a more general domain, and on Foursquare-valid to measure performance on the Foursquare domain.", "We obtain a noisy parallel corpus (which we use instead of the \u201cclean\u201d training data), where about 30% of all source sentences have been modified, as shown below:", "After some initial work with the WMT 2014 data, we built a new training corpus named UGC (User Generated Content), closer to our domain, by combining: Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet (See Table TABREF31). UGC does not include Common Crawl (which contains many misaligned sentences and caused hallucinations), but it includes OpenSubtitles BIBREF24 (spoken-language, possibly closer to Foursquare). We observed an improvement of more than 1 BLEU on newstest2014 when switching to UGC, and almost 6 BLEU on Foursquare-valid."]}
{"question_id": "eaae11ffd4ff955de2cd6389b888f5fd2c660a32", "predicted_answer": "The human evaluation metrics mentioned in the text are:\n\n1. Ranking: Human judges rank the output of 4 models by their translation quality.\n2. Wins, ties, and losses: For each pair of models, the number of wins, ties, and losses are counted.\n3. Wilcoxon signed-rank test: This test is applied to the wins, ties, and losses to determine the significance of the differences between models.\n4. Inter-judge Kappa coefficient: This measures the agreement between judges, ranging from 0.29 to 0.63 in this case, with an average of 0.47.\n\nThese metrics are used to evaluate the quality of machine translation (MT) models and to overcome the limitations of automatic metrics like BLEU.", "predicted_evidence": ["We conduct a human evaluation to confirm the observations with BLEU and to overcome some of the limitations of this metric.", "We also did a larger-scale monolingual evaluation using Amazon Mechanical Turk (see Appendix), which lead to similar conclusions.", "We select 4 MT models for evaluation (see Table TABREF63) and show their 4 outputs at once, sentence-by-sentence, to human judges, who are asked to rank them given the French source sentence in context (with the full review). For each pair of models, we count the number of wins, ties and losses, and apply the Wilcoxon signed-rank test.", "We took the first 300 test sentences to create 6 tasks of 50 sentences each. Then we asked bilingual colleagues to rank the output of 4 models by their translation quality. They were asked to do one or more of these tasks. The judge did not know about the list of models, nor the model that produced any given translation. We got 12 answers. The inter-judge Kappa coefficient ranged from 0.29 to 0.63, with an average of 0.47, which is a good value given the difficulty of the task. Table TABREF63 gives the results of the evaluation, which confirm our observations with BLEU.", "Addressing the technical issues of robustness and adaptation of an NMT system is decisive for real-world deployment, but evaluation is also critical. This aspect is stressed by BIBREF13 (NMT of curated hotel descriptions), who point out that automatic metrics like BLEU tend to neglect semantic differences that have a small textual footprint, but may be seriously misleading in practice, for instance by interpreting available parking as if it meant free parking. To mitigate this, we conduct additional evaluations of our models: human evaluation, translation accuracy of polysemous words, and indirect evaluation with sentiment analysis."]}
{"question_id": "290ebf0d1c49b67a6d1858366be751d89086a78b", "predicted_answer": "The automatic evaluation performed is the BLEU (Bilingual Evaluation Understudy) metric.", "predicted_evidence": ["We select 4 MT models for evaluation (see Table TABREF63) and show their 4 outputs at once, sentence-by-sentence, to human judges, who are asked to rank them given the French source sentence in context (with the full review). For each pair of models, we count the number of wins, ties and losses, and apply the Wilcoxon signed-rank test.", "We took the first 300 test sentences to create 6 tasks of 50 sentences each. Then we asked bilingual colleagues to rank the output of 4 models by their translation quality. They were asked to do one or more of these tasks. The judge did not know about the list of models, nor the model that produced any given translation. We got 12 answers. The inter-judge Kappa coefficient ranged from 0.29 to 0.63, with an average of 0.47, which is a good value given the difficulty of the task. Table TABREF63 gives the results of the evaluation, which confirm our observations with BLEU.", "We conduct a human evaluation to confirm the observations with BLEU and to overcome some of the limitations of this metric.", "In addition to BLEU, we do an indirect evaluation on an Aspect-Based Sentiment Analysis (ABSA) task, a human evaluation, and a task-related evaluation based on polysemous words.", "Addressing the technical issues of robustness and adaptation of an NMT system is decisive for real-world deployment, but evaluation is also critical. This aspect is stressed by BIBREF13 (NMT of curated hotel descriptions), who point out that automatic metrics like BLEU tend to neglect semantic differences that have a small textual footprint, but may be seriously misleading in practice, for instance by interpreting available parking as if it meant free parking. To mitigate this, we conduct additional evaluations of our models: human evaluation, translation accuracy of polysemous words, and indirect evaluation with sentiment analysis."]}
{"question_id": "806fefe0e331ddb3c17245d6a9fa7433798e367f", "predicted_answer": "The existing online systems mentioned in the text are:\n\n1. WMT (Workshop on Machine Translation) 2014 data\n2. Multi UN\n3. OpenSubtitles\n4. Wikipedia\n5. Books\n6. Tatoeba\n7. TED talks\n8. ParaCrawl\n9. Gourmet\n10. Common Crawl", "predicted_evidence": ["During our work, we used BLEU BIBREF28 on newstest[2012, 2013] to ensure that our models stayed good on a more general domain, and on Foursquare-valid to measure performance on the Foursquare domain.", "We asked professionals to translate 11.5k French Foursquare reviews (18k sentences) to English. We believe that this resource will be valuable to the community for training and evaluating MT systems addressing challenges posed by user-generated content, which we discuss in detail in this paper.", "To adapt our models to the restaurant review domain we apply the following types of techniques: back-translation of in-domain English data, fine-tuning with small amounts of in-domain parallel data, and domain tags.", "This in-domain data is concatenated to the out-of-domain parallel data and used for training.", "After some initial work with the WMT 2014 data, we built a new training corpus named UGC (User Generated Content), closer to our domain, by combining: Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet (See Table TABREF31). UGC does not include Common Crawl (which contains many misaligned sentences and caused hallucinations), but it includes OpenSubtitles BIBREF24 (spoken-language, possibly closer to Foursquare). We observed an improvement of more than 1 BLEU on newstest2014 when switching to UGC, and almost 6 BLEU on Foursquare-valid."]}
{"question_id": "458e5ed506883bfec6623102ec9f43c071f0616f", "predicted_answer": "The baselines mentioned in the text are:\n\n1. Simple keyword-matching baseline system: This system has the lowest performance for both languages, but obtains high recall for English.\n2. Unoptimised word n-gram baseline system: Feature group and hyperparameter optimisation provides a considerable performance increase over this system.", "predicted_evidence": ["A total of 31 feature type combinations, each with 28 different hyperparameter sets have been tested. Table TABREF45 shows the results for the three best scoring systems by included feature types with optimised hyperparameters. The maximum attained F INLINEFORM0 -score in cross-validation is 64.26% for English and 61.20% for Dutch and shows that the classifier benefits from a variety of feature types. The results on the holdout test set show that the trained systems generalise well on unseen data, indicating little under- or overfitting. The simple keyword-matching baseline system has the lowest performance for both languages even though it obtains high recall for English, suggesting that profane language characterises many cyberbullying-related posts. Feature group and hyperparameter optimisation provides a considerable performance increase over the unoptimised word INLINEFORM1 -gram baseline system. The top-scoring systems for each language do not differ a lot in performance, except the best system for Dutch, which trades recall for precision when compared to the runner-ups.", "Table TABREF45 gives us an indication of which feature type combinations score best and hence contribute most to this task. A total of 31 feature type combinations, each with 28 different hyperparameter sets have been tested. Table TABREF45 shows the results for the three best scoring systems by included feature types with optimised hyperparameters. The maximum attained F INLINEFORM0 -score in cross-validation is 64.26% for English and 61.20% for Dutch and shows that the classifier benefits from a variety of feature types. The results on the holdout test set show that the trained systems generalise well on unseen data, indicating little under- or overfitting. The simple keyword-matching baseline system has the lowest performance for both languages even though it obtains high recall for English, suggesting that profane language characterises many cyberbullying-related posts. Feature group and hyperparameter optimisation provides a considerable performance increase over the unoptimised word INLINEFORM1 -gram baseline system.", "When applied to the training data, this resulted in INLINEFORM0 and INLINEFORM1 features for English and Dutch, respectively.", "font=footnotesize,sc,justification=centering,labelsep=period", "font=footnotesize,sc,justification=centering,labelsep=period"]}
{"question_id": "85ab5f773b297bcf48a274634d402a35e1d57446", "predicted_answer": "Yes, they report the annotation agreement. Specifically, they report inter-annotator agreement scores using Kappa on a subset of each corpus, with scores of 0.69 for Dutch and 0.59 for English.", "predicted_evidence": ["Essentially, the annotation scheme describes two levels of annotation. Firstly, the annotators were asked to indicate, at the post level, whether the post under investigation was related to cyberbullying. If the post was considered a signal of cyberbullying, annotators identified the author's role. Secondly, at the subsentence level, the annotators were tasked with the identification of a number of fine-grained text categories related to cyberbullying. More concretely, they identified all text spans corresponding to one of the categories described in the annotation scheme. To provide the annotators with some context, all posts were presented within their original conversation when possible. All annotations were done using the Brat rapid annotation tool BIBREF52 , some examples of which are presented in Table TABREF33 .", "As shown in Table TABREF35 , inter-annotator agreement for the identification of the more fine-grained categories for English varies from fair to substantial BIBREF55 , except for defamation, which appears to be more difficult to recognise. No encouragements to the harasser were present in this subset of the corpus. For Dutch, the inter-annotator agreement is fair to substantial, except for curse and defamation. Analysis revealed that one of both annotators often annotated the latter as an insult, and in some cases even did not consider it as cyberbullying-related.", "The English and Dutch corpora were independently annotated for cyberbullying by trained linguists. All were Dutch native speakers and English second-language speakers. To demonstrate the validity of our guidelines, inter-annotator agreement scores were calculated using Kappa on a subset of each corpus. Inter-rater agreement for Dutch (2 raters) is calculated using Cohen's Kappa BIBREF53 . Fleiss' Kappa BIBREF54 is used for the English corpus ( INLINEFORM0 2 raters). Kappa scores for the identification of cyberbullying are INLINEFORM1 = 0.69 (Dutch) and INLINEFORM2 = 0.59 (English).", "The present annotation scheme describes some specific textual categories related to cyberbullying, including threats, insults, defensive statements from a victim, encouragements to the harasser, etc. (see Section SECREF15 for a complete overview). All of these forms were inspired by social studies on cyberbullying BIBREF7 , BIBREF19 and manual inspection of cyberbullying examples.", "To operationalise the task of automatic cyberbullying detection, we developed and tested a fine-grained annotation scheme and applied it to our corpora. While a detailed overview of the guidelines is presented in our technical report BIBREF30 , we briefly present the categories and main annotation steps below."]}
{"question_id": "5154f63c50729b8ac04939588c2f5ffeb916e3df", "predicted_answer": "The answer is not explicitly stated in the given context. The text mentions the size of the training corpora for English and Dutch, but it does not provide the size of the test dataset for Dutch. However, it does mention that the results on the holdout test set show that the trained systems generalise well on unseen data, indicating little under- or overfitting. This suggests that a test dataset was used, but its size is not specified.", "predicted_evidence": ["When applied to the training data, this resulted in INLINEFORM0 and INLINEFORM1 features for English and Dutch, respectively.", "Two corpora were constructed by collecting data from the social networking site ASKfm, where users can create profiles and ask or answer questions, with the option of doing so anonymously. ASKfm data typically consists of question-answer pairs published on a user's profile. The data were retrieved by crawling a number of seed profiles using the GNU Wget software in April and October, 2013. After language filtering (i.e., non-English or non-Dutch content was removed), the experimental corpora comprised 113,698 and 78,387 posts for English and Dutch, respectively.", "The English and Dutch corpora were independently annotated for cyberbullying by trained linguists. All were Dutch native speakers and English second-language speakers. To demonstrate the validity of our guidelines, inter-annotator agreement scores were calculated using Kappa on a subset of each corpus. Inter-rater agreement for Dutch (2 raters) is calculated using Cohen's Kappa BIBREF53 . Fleiss' Kappa BIBREF54 is used for the English corpus ( INLINEFORM0 2 raters). Kappa scores for the identification of cyberbullying are INLINEFORM1 = 0.69 (Dutch) and INLINEFORM2 = 0.59 (English).", "A total of 31 feature type combinations, each with 28 different hyperparameter sets have been tested. Table TABREF45 shows the results for the three best scoring systems by included feature types with optimised hyperparameters. The maximum attained F INLINEFORM0 -score in cross-validation is 64.26% for English and 61.20% for Dutch and shows that the classifier benefits from a variety of feature types. The results on the holdout test set show that the trained systems generalise well on unseen data, indicating little under- or overfitting. The simple keyword-matching baseline system has the lowest performance for both languages even though it obtains high recall for English, suggesting that profane language characterises many cyberbullying-related posts. Feature group and hyperparameter optimisation provides a considerable performance increase over the unoptimised word INLINEFORM1 -gram baseline system. The top-scoring systems for each language do not differ a lot in performance, except the best system for Dutch, which trades recall for precision when compared to the runner-ups.", "Table TABREF45 gives us an indication of which feature type combinations score best and hence contribute most to this task. A total of 31 feature type combinations, each with 28 different hyperparameter sets have been tested. Table TABREF45 shows the results for the three best scoring systems by included feature types with optimised hyperparameters. The maximum attained F INLINEFORM0 -score in cross-validation is 64.26% for English and 61.20% for Dutch and shows that the classifier benefits from a variety of feature types. The results on the holdout test set show that the trained systems generalise well on unseen data, indicating little under- or overfitting. The simple keyword-matching baseline system has the lowest performance for both languages even though it obtains high recall for English, suggesting that profane language characterises many cyberbullying-related posts. Feature group and hyperparameter optimisation provides a considerable performance increase over the unoptimised word INLINEFORM1 -gram baseline system."]}
{"question_id": "2aeabec8a734a6e8ca9e7a308dd8c9a1011b3d6e", "predicted_answer": "The training dataset for English consists of 113,698 posts.", "predicted_evidence": ["When applied to the training data, this resulted in INLINEFORM0 and INLINEFORM1 features for English and Dutch, respectively.", "Two corpora were constructed by collecting data from the social networking site ASKfm, where users can create profiles and ask or answer questions, with the option of doing so anonymously. ASKfm data typically consists of question-answer pairs published on a user's profile. The data were retrieved by crawling a number of seed profiles using the GNU Wget software in April and October, 2013. After language filtering (i.e., non-English or non-Dutch content was removed), the experimental corpora comprised 113,698 and 78,387 posts for English and Dutch, respectively.", "Table TABREF45 gives us an indication of which feature type combinations score best and hence contribute most to this task. A total of 31 feature type combinations, each with 28 different hyperparameter sets have been tested. Table TABREF45 shows the results for the three best scoring systems by included feature types with optimised hyperparameters. The maximum attained F INLINEFORM0 -score in cross-validation is 64.26% for English and 61.20% for Dutch and shows that the classifier benefits from a variety of feature types. The results on the holdout test set show that the trained systems generalise well on unseen data, indicating little under- or overfitting. The simple keyword-matching baseline system has the lowest performance for both languages even though it obtains high recall for English, suggesting that profane language characterises many cyberbullying-related posts. Feature group and hyperparameter optimisation provides a considerable performance increase over the unoptimised word INLINEFORM1 -gram baseline system.", "A total of 31 feature type combinations, each with 28 different hyperparameter sets have been tested. Table TABREF45 shows the results for the three best scoring systems by included feature types with optimised hyperparameters. The maximum attained F INLINEFORM0 -score in cross-validation is 64.26% for English and 61.20% for Dutch and shows that the classifier benefits from a variety of feature types. The results on the holdout test set show that the trained systems generalise well on unseen data, indicating little under- or overfitting. The simple keyword-matching baseline system has the lowest performance for both languages even though it obtains high recall for English, suggesting that profane language characterises many cyberbullying-related posts. Feature group and hyperparameter optimisation provides a considerable performance increase over the unoptimised word INLINEFORM1 -gram baseline system. The top-scoring systems for each language do not differ a lot in performance, except the best system for Dutch, which trades recall for precision when compared to the runner-ups.", "Table TABREF45 gives us an indication of which feature type combinations score best and hence contribute most to this task. A total of 31 feature type combinations, each with 28 different hyperparameter sets have been tested. Table TABREF45 shows the results for the three best scoring systems by included feature types with optimised hyperparameters. The maximum attained F INLINEFORM0 -score in cross-validation is 64.26% for English and 61.20% for Dutch and shows that the classifier benefits from a variety of feature types. The results on the holdout test set show that the trained systems generalise well on unseen data, indicating little under- or overfitting. The simple keyword-matching baseline system has the lowest performance for both languages even though it obtains high recall for English, suggesting that profane language characterises many cyberbullying-related posts."]}
{"question_id": "f2b8a2ed5916d75cf568a931829a5a3cde2fc345", "predicted_answer": "The features used are:\n\n1. Term lists:\n\t* Binary features for each of the following lists:\n\t\t+ Proper names\n\t\t+ \"Allness\" indicators (e.g. always, everybody)\n\t\t+ Diminishers (e.g. slightly, relatively)\n\t\t+ Intensifiers (e.g. absolutely, amazingly)\n\t\t+ Negation words\n\t\t+ Aggressive language and profanity words\n2. Person alternation:\n\t* A binary feature indicating whether the combination of a first and second person pronoun occurs in order to capture interpersonal intent\n3. Character n-grams:\n\t* Binary features indicating the presence of character bigrams, trigrams, and fourgrams (without crossing word boundaries)", "predicted_evidence": ["Feature selection over all individual features was not performed because of the large feature space (NL: 795,072 and EN: 871,296 individual features). BIBREF59 , among other researchers, demonstrated the importance of joint optimisation, where feature selection and hyperparameter optimisation are performed simultaneously, since the techniques mutually influence each other.", "After pre-processing of the corpus, the following feature types were extracted:", "Term lists: one binary feature derived for each one out of six lists, indicating the presence of an item from the list in a post: proper names, `allness' indicators (e.g. always, everybody), diminishers (e.g. slightly, relatively), intensifiers (e.g. absolutely, amazingly), negation words and aggressive language and profanity words. Person alternation is a binary feature indicating whether the combination of a first and second person pronoun occurs in order to capture interpersonal intent.", "When applied to the training data, this resulted in INLINEFORM0 and INLINEFORM1 features for English and Dutch, respectively.", "Character INLINEFORM0 -gram bag-of-words: binary features indicating the presence of character bigrams, trigrams and fourgrams (without crossing word boundaries). Character INLINEFORM1 -grams provide some abstraction from the word level and provide robustness to the spelling variation that characterises social media data."]}
{"question_id": "c0af44ebd7cd81270d9b5b54d4a40feed162fa54", "predicted_answer": "The source of the data is the social networking site ASKfm, where users can create profiles and ask or answer questions, with the option of doing so anonymously. The data was retrieved by crawling a number of seed profiles using the GNU Wget software in April and October, 2013.", "predicted_evidence": ["Two corpora were constructed by collecting data from the social networking site ASKfm, where users can create profiles and ask or answer questions, with the option of doing so anonymously. ASKfm data typically consists of question-answer pairs published on a user's profile. The data were retrieved by crawling a number of seed profiles using the GNU Wget software in April and October, 2013. After language filtering (i.e., non-English or non-Dutch content was removed), the experimental corpora comprised 113,698 and 78,387 posts for English and Dutch, respectively.", "A key challenge in cyberbullying research is the availability of suitable data, which is necessary to develop models that characterise cyberbullying. In recent years, only a few datasets have become publicly available for this particular task, such as the training sets provided in the context of the CAW 2.0 workshop and more recently, the Twitter Bullying Traces dataset BIBREF36 . As a result, several studies have worked with the former or have constructed their own corpus from social media websites that are prone to bullying content, such as YouTube BIBREF32 , BIBREF33 , Formspring BIBREF33 , and ASKfm BIBREF37 (the latter two are social networking sites where users can send each other questions or respond to them). Despite the bottleneck of data availability, existing approaches to cyberbullying detection have shown its potential, and the relevance of automatic text analysis techniques to ensure child safety online has been recognised BIBREF38 , BIBREF39 .", "Essentially, the annotation scheme describes two levels of annotation. Firstly, the annotators were asked to indicate, at the post level, whether the post under investigation was related to cyberbullying. If the post was considered a signal of cyberbullying, annotators identified the author's role. Secondly, at the subsentence level, the annotators were tasked with the identification of a number of fine-grained text categories related to cyberbullying. More concretely, they identified all text spans corresponding to one of the categories described in the annotation scheme. To provide the annotators with some context, all posts were presented within their original conversation when possible. All annotations were done using the Brat rapid annotation tool BIBREF52 , some examples of which are presented in Table TABREF33 .", "When applied to the training data, this resulted in INLINEFORM0 and INLINEFORM1 features for English and Dutch, respectively.", "After pre-processing of the corpus, the following feature types were extracted:"]}
{"question_id": "a4a9971799c8860b50f219c93f050ebf6a627b3d", "predicted_answer": "The languages featured in the dataset are English and possibly Arabic (since the dataset includes articles from Dabiq extremist magazines, which are known to be published in Arabic).", "predicted_evidence": ["Feature engineering is the process of exploring large spaces of heterogeneous features with the aim of discovering meaningful features that may aid in modeling the problem at hand. We explore three categories of information to identify relevant features to detect radical content. Some features are user-based while others are message-based. The three categories are: 1) Radical language (Textual features INLINEFORM0 ); 2) Psychological signals (Psychological features INLINEFORM1 ); and 3) Behavioural features ( INLINEFORM2 ). In the following, we detail each of these categories.", "Exp 1: The classification results using the known-bad and random-good datasets are reported in Table TABREF16 . The table shows the average accuracy, precision, recall and f-measure scores obtained from each feature category ( INLINEFORM0 , INLINEFORM1 , INLINEFORM2 ) and their combination ( INLINEFORM3 ). We also compared the two textual models, and find that results obtained from using word embedding outperforms the use of n-grams tf-idf scores. This confirms that contextual information is important in detecting radicalization activities. Furthermore, our model performed best using the INLINEFORM4 features across all metrics. This means that the model is able to distinguish between both radical and non-radical with high confidence using only INLINEFORM5 .", "As illustrated in Fig. FIGREF1 , our approach consists of two main phases: Phase 1:Radical Properties Extraction, where articles from Dabiq extremist magazines are input into this step to perform two parallel tasks. In the first task, we build a language model using (i) Term-Frequency Inverse-Document-Frequency (TF-IDF) scores of uni-, bi-, and tri-grams, and (ii) Word embeddings generated from a word2vec model BIBREF15 . The output of this task is a radical corpus of top k-grams, and a word embedding model giving a vector representation for each word in the corpus. The second task seeks to create a psychological profile based on the language used in the extremist propaganda articles, consisting of a set of emotional and topical categories using LIWC dictionary-based tool. Phase 2: Tweet classification involves the use of the models generated from Phase 1 to engineer features related to radical activities.", "Moreover, among the top contributing features are behavioural features related to the number of mentions a single user makes, and their HITS hub and authority rank among their interaction network. This relates to how active the user is in interacting with other users and how much attention they receive from their community. This links to the objectives of those radical users in spreading their ideologies and reaching out to potential like-minded people. As for the INLINEFORM0 category, we find that the use of word2vec embedding improves the performance in comparison with using the tf-idf features. Additionally, all bi-grams and tri-grams features did not contribute much to the classification; only uni-grams did. This can be related to the differences in the writing styles when constructing sentences and phrases in articles and in the social media context (especially given the limitation of the number of words allowed by the Twitter platform).", "Ashcroft et al. BIBREF6 make an attempt to automatically detect Jihadist messages on Twitter. They adopt a machine-learning method to classify tweets as ISIS supporters or not. In the article, the authors focus on English tweets that contain a reference to a set of predefined English hashtags related to ISIS. Three different classes of features are used, including stylometric features, temporal features and sentiment features. However, one of the main limitations of their approach is that it is highly dependent on the data. Rowe and Saif BIBREF7 focused on studying Europe-based Twitter accounts in order to understand what happens before, during, and after they exhibit pro-ISIS behaviour. They define such behaviour as sharing of pro-ISIS content and/or using pro-ISIS terms. To achieve this, they use a term-based approach such that a user is considered to exhibit a radicalization behaviour if he/she uses more pro-ISIS terms than anti-ISIS terms. While such an approach seems effective in distinguishing radicalised users, it is unable to properly deal with lexical ambiguity (i.e., polysemy)."]}
{"question_id": "778c6a27182349dc5275282c3e9577bda2555c3d", "predicted_answer": "Based on the provided context, the following textual, psychological, and behavioral patterns are observed in radical users:\n\n**Textual patterns:**\n\n* Use of radical language, which is characterized by a distinct linguistic style that is similar to the language used in propaganda material published by extremist groups.\n* Use of pronouns that emphasize the separation between the radical group and the rest of the world, such as \"us\" vs. \"them\".\n\n**Psychological patterns:**\n\n* Social isolation, which is often self-imposed, and spending a long time engaging with radical content.\n* Homophily, or the tendency to interact and associate with similar others, which leads to a strengthening of the individual's mindset and progression to more extreme levels.\n* Group polarization, where individuals feel part of a group with a strong group identity, leading to the adoption of more extreme actions.\n* A distinct psychological profile that is similar to the average psychological profile calculated from propaganda magazine articles.\n\n**Behavioral patterns:**\n\n* Engaging with radical content for extended periods, which contributes to social isolation and homophily.\n* Interacting with like-minded individuals, which reinforces the individual's radical beliefs and leads to group polarization.\n* Adopting more extreme actions and behaviors as a result of group polarization.\n\nThese patterns are observed in radical users, particularly those who support extremist groups like ISIS, and are used to detect online radicalization.", "predicted_evidence": ["We investigated which features contribute most to the classification task to distinguish between radical and non-radical tweets. We used the mean decrease impurity method of random forests BIBREF27 to identify the most important features in each feature category. The ten most important features are shown in Table TABREF22 . We found that the most important feature for distinguishing radical tweets is the psychological feature distance measure. This measures how similar the Twitter user is to the average psychological profile calculated from the propaganda magazine articles. Following this is the Us-them dichotomy which looks at the total number of pronouns used (I,they, we, you). This finding is in line with the tactics reported in the radicalization literature with regards to emphasizing the separation between the radical group and the world.", "Building on the findings of previous research efforts, this paper aims to study the effects of using new textual and psycholinguistic signals to detect extremist content online. These signals are developed based on insights gathered from analyzing propaganda material published by known extremist groups. In this study, we focus mainly on the ISIS group as they are one of the leading terrorist groups that utilise social media to share their propaganda and recruit individuals. We analyze the propaganda material they publish in their online English magazine called Dabiq, and use data-mining techniques to computationally uncover contextual text and psychological properties associated with these groups. From our analysis of these texts, we are able to extract a set of signals that provide some insight into the mindset of the radical group. This allows us to create a general radical profile that we apply as a signal to detect pro-ISIS supporters on Twitter. Our results show that these identified signals are indeed critical to help improve existing efforts to detect online radicalization.", "Feature engineering is the process of exploring large spaces of heterogeneous features with the aim of discovering meaningful features that may aid in modeling the problem at hand. We explore three categories of information to identify relevant features to detect radical content. Some features are user-based while others are message-based. The three categories are: 1) Radical language (Textual features INLINEFORM0 ); 2) Psychological signals (Psychological features INLINEFORM1 ); and 3) Behavioural features ( INLINEFORM2 ). In the following, we detail each of these categories.", "Radicalization literature also looked at psychological factors involved with adopting such behaviour. Torok BIBREF12 used a grounded theory approach to develop an explanatory model for the radicalization process utilizing concepts of psychiatric power. Their findings show that the process typically starts with the social isolation of individuals. This isolation seems to be self-imposed as individuals tend to spend a long time engaging with radical content. This leads to the concept of homophily, the tendency to interact and associate with similar others. Through constant interaction with like-minded people, an individual gradually strengthens their mindset and progresses to more extreme levels. Similarly, they start to feel as being part of a group with a strong group identity which leads to group polarization. In psychology, group polarization occurs when discussion leads the group to adopt actions that are more extreme than the initial actions of the individual group members BIBREF13 . Moreover, the National Police Service Agency of the Netherlands developed a model to describe the phases a Jihadist may pass through before committing an act of terrorism BIBREF14 .", "Radicalization literature also looked at psychological factors involved with adopting such behaviour. Torok BIBREF12 used a grounded theory approach to develop an explanatory model for the radicalization process utilizing concepts of psychiatric power. Their findings show that the process typically starts with the social isolation of individuals. This isolation seems to be self-imposed as individuals tend to spend a long time engaging with radical content. This leads to the concept of homophily, the tendency to interact and associate with similar others. Through constant interaction with like-minded people, an individual gradually strengthens their mindset and progresses to more extreme levels. Similarly, they start to feel as being part of a group with a strong group identity which leads to group polarization. In psychology, group polarization occurs when discussion leads the group to adopt actions that are more extreme than the initial actions of the individual group members BIBREF13 ."]}
{"question_id": "42dcf1bb19b8470993c05e55413eed487b0f2559", "predicted_answer": "The propaganda material is sourced from Dabiq, an online magazine published by ISIS terrorist groups.", "predicted_evidence": ["In order to understand how radical messages are constructed and used, as mentioned earlier, we analyze content of ISIS propaganda material published in Dabiq magazine. Dabiq is an online magazine published by ISIS terrorist groups with the purpose of recruiting people and promoting their propaganda and ideology. Using this data source, we investigate what topics, textual properties, and linguistic cues exist in these magazines. Our intuition is that utilising these linguistic cues from the extremist propaganda would allow us to detect supporters of ISIS group who are influenced by their propaganda.", "Building on the findings of previous research efforts, this paper aims to study the effects of using new textual and psycholinguistic signals to detect extremist content online. These signals are developed based on insights gathered from analyzing propaganda material published by known extremist groups. In this study, we focus mainly on the ISIS group as they are one of the leading terrorist groups that utilise social media to share their propaganda and recruit individuals. We analyze the propaganda material they publish in their online English magazine called Dabiq, and use data-mining techniques to computationally uncover contextual text and psychological properties associated with these groups. From our analysis of these texts, we are able to extract a set of signals that provide some insight into the mindset of the radical group. This allows us to create a general radical profile that we apply as a signal to detect pro-ISIS supporters on Twitter. Our results show that these identified signals are indeed critical to help improve existing efforts to detect online radicalization.", "We conducted two experiments using the datasets described in Section SECREF11 . Our hypothesis is that supporters of groups such as ISIS may exhibit similar textual and psychological properties when communicating in social media to the properties seen in the propaganda magazines. A tweet is considered radical if it promotes violence, racism, or supports violent behaviour. In Exp 1 we use the first two datasets, i.e., the known-bad and the random-good datasets to classify tweets to radical and normal classes. For Exp 2 we examine if our classifier can also distinguish between tweets that are discussing similar topics (ISIS related) by using the known-bad and the counterpoise datasets.", "In this paper, we identified different signals that can be utilized to detect evidence of online radicalization. We derived linguistic and psychological properties from propaganda published by ISIS for recruitment purposes. We utilize these properties to detect pro-ISIS tweets that are influenced by their ideology. Unlike previous efforts, these properties do not only focus on lexical keyword analysis of the messages, but also add a contextual and psychological dimension. We validated our approach in different experiments and the results show that this method is robust across multiple datasets. This system can aid law enforcement and OSN companies to better address such threats and help solve a challenging real-world problem. In future work, we aim to investigate if the model is resilient to different evasion techniques that users may adopt. We will also expand the analysis to other languages.", "We use two methods to extract the radical language from the propaganda corpus. First we calculate tf-idf scores for each gram in the propaganda corpus. We use uni-grams, bi-grams, and tri-grams to capture phrases and context in which words are being used. We then select the top scoring grams to be used as features for the language model. N-grams and words frequency have been used in the literature to classify similar problems, such as hate-speech and extremist text and have proven successful BIBREF16 . The second method we use is word embeddings to capture semantic meanings. Research in NLP has compared the effectiveness of word embedding methods for encoding semantic meaning and found that semantic relationships between words are best captured by word vectors within word embedding models BIBREF17 . Therefore, we train word2vec model on our propaganda corpus to build the lexical semantic aspects of the text using vector space models."]}
{"question_id": "2ecd12069388fd58ad5f8f4ae7ac1bb4f56497b9", "predicted_answer": "The behavioural features used are:\n\n1. Frequency of tweets posted (to measure user activity)\n2. Followers/following ratio\n3. Users' interactions with others through:\n\t* Using hashtags\n\t* Engagement in discussions using mention action\n4. Centrality measures of the mention interaction graph, including:\n\t* Degree centrality\n\t* Betweenness centrality\n\t* HITS-Hub", "predicted_evidence": ["Feature engineering is the process of exploring large spaces of heterogeneous features with the aim of discovering meaningful features that may aid in modeling the problem at hand. We explore three categories of information to identify relevant features to detect radical content. Some features are user-based while others are message-based. The three categories are: 1) Radical language (Textual features INLINEFORM0 ); 2) Psychological signals (Psychological features INLINEFORM1 ); and 3) Behavioural features ( INLINEFORM2 ). In the following, we detail each of these categories.", "This category consists of measuring behavioural features to capture different properties related to the user and their behaviour. This includes how active the user is (frequency of tweets posted) and the followers/following ratio. Additionally, we use features to capture users' interactions with others through using hashtags, and engagement in discussions using mention action. To capture this, we construct the mention interaction graph ( INLINEFORM0 ) from our dataset, such that INLINEFORM1 = INLINEFORM2 , where INLINEFORM3 represents the user nodes and INLINEFORM4 represents the set of edges. The graph INLINEFORM5 is a directed graph, where an edge INLINEFORM6 exists between two user nodes INLINEFORM7 and INLINEFORM8 , if user INLINEFORM9 mentions user INLINEFORM10 . After constructing the graph, we measure the degree of influence each user has over their network using different centrality measures, such as degree centrality, betweenness centrality, and HITS-Hub.", "This category consists of measuring behavioural features to capture different properties related to the user and their behaviour. This includes how active the user is (frequency of tweets posted) and the followers/following ratio. Additionally, we use features to capture users' interactions with others through using hashtags, and engagement in discussions using mention action. To capture this, we construct the mention interaction graph ( INLINEFORM0 ) from our dataset, such that INLINEFORM1 = INLINEFORM2 , where INLINEFORM3 represents the user nodes and INLINEFORM4 represents the set of edges. The graph INLINEFORM5 is a directed graph, where an edge INLINEFORM6 exists between two user nodes INLINEFORM7 and INLINEFORM8 , if user INLINEFORM9 mentions user INLINEFORM10 . After constructing the graph, we measure the degree of influence each user has over their network using different centrality measures, such as degree centrality, betweenness centrality, and HITS-Hub. Such properties have been adopted in the research literature to study properties of cyber-criminal networks and their behaviour BIBREF22 , BIBREF23 .", "We investigated which features contribute most to the classification task to distinguish between radical and non-radical tweets. We used the mean decrease impurity method of random forests BIBREF27 to identify the most important features in each feature category. The ten most important features are shown in Table TABREF22 . We found that the most important feature for distinguishing radical tweets is the psychological feature distance measure. This measures how similar the Twitter user is to the average psychological profile calculated from the propaganda magazine articles. Following this is the Us-them dichotomy which looks at the total number of pronouns used (I,they, we, you). This finding is in line with the tactics reported in the radicalization literature with regards to emphasizing the separation between the radical group and the world.", "Tone which reflects positive emotions (high value) versus more negative emotions such as anxiety, sadness, or anger (low value). Authentic which reflects whether the text is conveying honesty and disclosing (high value) versus more guarded, and distanced (low value). (2) Big five: Measures the five psychological properties (OCEAN), namely Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. (3) Emotional Analysis: Measures the positive emotions conveyed in the text, and the negative emotions (including anger, sadness, anxiety). (4) Personal Drives: Focuses on five personal drives, namely power, reward, risk, achievement, and affiliation. (5) Personal Pronouns: Counts the number of 1st, 2nd, and 3rd personal pronouns used. For each Twitter user, we calculate their psychological profiles across these categories."]}
{"question_id": "824629b36a75753b1500d9dcaee0fc3c758297b1", "predicted_answer": "The psychological features used are:\n\n1. Clout (reflecting expertise and confidence levels)\n2. Tone (reflecting positive or negative emotions)\n3. Authentic (reflecting honesty and disclosure)\n4. Big five (measuring Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism)\n5. Emotional Analysis (measuring positive and negative emotions, including anger, sadness, and anxiety)\n6. Personal Drives (focusing on power, reward, risk, achievement, and affiliation)\n7. Personal Pronouns (counting 1st, 2nd, and 3rd personal pronouns used)\n8. Analytically thinking (reflecting formal, logical, and hierarchical thinking)", "predicted_evidence": ["Feature engineering is the process of exploring large spaces of heterogeneous features with the aim of discovering meaningful features that may aid in modeling the problem at hand. We explore three categories of information to identify relevant features to detect radical content. Some features are user-based while others are message-based. The three categories are: 1) Radical language (Textual features INLINEFORM0 ); 2) Psychological signals (Psychological features INLINEFORM1 ); and 3) Behavioural features ( INLINEFORM2 ). In the following, we detail each of these categories.", "Clout which reflects high expertise and confidence levels (high value), versus tentative, humble, and anxious levels (low value). Tone which reflects positive emotions (high value) versus more negative emotions such as anxiety, sadness, or anger (low value). Authentic which reflects whether the text is conveying honesty and disclosing (high value) versus more guarded, and distanced (low value). (2) Big five: Measures the five psychological properties (OCEAN), namely Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. (3) Emotional Analysis: Measures the positive emotions conveyed in the text, and the negative emotions (including anger, sadness, anxiety). (4) Personal Drives: Focuses on five personal drives, namely power, reward, risk, achievement, and affiliation. (5) Personal Pronouns: Counts the number of 1st, 2nd, and 3rd personal pronouns used.", "We utilise LIWC dictionaries to assign a score to a set of psychological, personality, and emotional categories. Mainly, we look at the following properties: (1) Summary variables: Analytically thinking which reflects formal, logical, and hierarchical thinking (high value), versus informal, personal, and narrative thinking (low value). Clout which reflects high expertise and confidence levels (high value), versus tentative, humble, and anxious levels (low value). Tone which reflects positive emotions (high value) versus more negative emotions such as anxiety, sadness, or anger (low value). Authentic which reflects whether the text is conveying honesty and disclosing (high value) versus more guarded, and distanced (low value). (2) Big five: Measures the five psychological properties (OCEAN), namely Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism.", "We utilise LIWC dictionaries to assign a score to a set of psychological, personality, and emotional categories. Mainly, we look at the following properties: (1) Summary variables: Analytically thinking which reflects formal, logical, and hierarchical thinking (high value), versus informal, personal, and narrative thinking (low value). Clout which reflects high expertise and confidence levels (high value), versus tentative, humble, and anxious levels (low value). Tone which reflects positive emotions (high value) versus more negative emotions such as anxiety, sadness, or anger (low value). Authentic which reflects whether the text is conveying honesty and disclosing (high value) versus more guarded, and distanced (low value). (2) Big five: Measures the five psychological properties (OCEAN), namely Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. (3) Emotional Analysis: Measures the positive emotions conveyed in the text, and the negative emotions (including anger, sadness, anxiety).", "Tone which reflects positive emotions (high value) versus more negative emotions such as anxiety, sadness, or anger (low value). Authentic which reflects whether the text is conveying honesty and disclosing (high value) versus more guarded, and distanced (low value). (2) Big five: Measures the five psychological properties (OCEAN), namely Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. (3) Emotional Analysis: Measures the positive emotions conveyed in the text, and the negative emotions (including anger, sadness, anxiety). (4) Personal Drives: Focuses on five personal drives, namely power, reward, risk, achievement, and affiliation. (5) Personal Pronouns: Counts the number of 1st, 2nd, and 3rd personal pronouns used. For each Twitter user, we calculate their psychological profiles across these categories. Additionally, using Minkowski distance measure, we calculate the distance between each of these profiles and the average values of the psychological properties created from the ISIS magazines."]}
{"question_id": "31894361833b3e329a1fb9ebf85a78841cff229f", "predicted_answer": "The textual features used in this context are:\n\n1. Uni-grams\n2. Bi-grams\n3. Tri-grams\n4. Word2vec embedding\n5. TF-IDF features\n6. Violent word ratio\n7. LongWords\n8. AllCaps features\n\nThese features are used to analyze and detect radical content in tweets.", "predicted_evidence": ["Feature engineering is the process of exploring large spaces of heterogeneous features with the aim of discovering meaningful features that may aid in modeling the problem at hand. We explore three categories of information to identify relevant features to detect radical content. Some features are user-based while others are message-based. The three categories are: 1) Radical language (Textual features INLINEFORM0 ); 2) Psychological signals (Psychological features INLINEFORM1 ); and 3) Behavioural features ( INLINEFORM2 ). In the following, we detail each of these categories.", "Moreover, among the top contributing features are behavioural features related to the number of mentions a single user makes, and their HITS hub and authority rank among their interaction network. This relates to how active the user is in interacting with other users and how much attention they receive from their community. This links to the objectives of those radical users in spreading their ideologies and reaching out to potential like-minded people. As for the INLINEFORM0 category, we find that the use of word2vec embedding improves the performance in comparison with using the tf-idf features. Additionally, all bi-grams and tri-grams features did not contribute much to the classification; only uni-grams did. This can be related to the differences in the writing styles when constructing sentences and phrases in articles and in the social media context (especially given the limitation of the number of words allowed by the Twitter platform).", "Moreover, among the top contributing features are behavioural features related to the number of mentions a single user makes, and their HITS hub and authority rank among their interaction network. This relates to how active the user is in interacting with other users and how much attention they receive from their community. This links to the objectives of those radical users in spreading their ideologies and reaching out to potential like-minded people. As for the INLINEFORM0 category, we find that the use of word2vec embedding improves the performance in comparison with using the tf-idf features. Additionally, all bi-grams and tri-grams features did not contribute much to the classification; only uni-grams did. This can be related to the differences in the writing styles when constructing sentences and phrases in articles and in the social media context (especially given the limitation of the number of words allowed by the Twitter platform). Additionally, the violent word ratio, longWords, and allCaps features are among the top contributing features from this category.", "As illustrated in Fig. FIGREF1 , our approach consists of two main phases: Phase 1:Radical Properties Extraction, where articles from Dabiq extremist magazines are input into this step to perform two parallel tasks. In the first task, we build a language model using (i) Term-Frequency Inverse-Document-Frequency (TF-IDF) scores of uni-, bi-, and tri-grams, and (ii) Word embeddings generated from a word2vec model BIBREF15 . The output of this task is a radical corpus of top k-grams, and a word embedding model giving a vector representation for each word in the corpus. The second task seeks to create a psychological profile based on the language used in the extremist propaganda articles, consisting of a set of emotional and topical categories using LIWC dictionary-based tool. Phase 2: Tweet classification involves the use of the models generated from Phase 1 to engineer features related to radical activities.", "As illustrated in Fig. FIGREF1 , our approach consists of two main phases: Phase 1:Radical Properties Extraction, where articles from Dabiq extremist magazines are input into this step to perform two parallel tasks. In the first task, we build a language model using (i) Term-Frequency Inverse-Document-Frequency (TF-IDF) scores of uni-, bi-, and tri-grams, and (ii) Word embeddings generated from a word2vec model BIBREF15 . The output of this task is a radical corpus of top k-grams, and a word embedding model giving a vector representation for each word in the corpus. The second task seeks to create a psychological profile based on the language used in the extremist propaganda articles, consisting of a set of emotional and topical categories using LIWC dictionary-based tool. Phase 2: Tweet classification involves the use of the models generated from Phase 1 to engineer features related to radical activities. We identify three groups of features and then train a binary classifier to detect radical tweets."]}
{"question_id": "cef3a26d8b46cd057bcc2abd3d648dc15336a2bf", "predicted_answer": "The cold-start problem refers to the challenge of dealing with new items (in this case, hotels) that lack sufficient prior user engagement data, making it difficult to learn their embeddings or representations. This problem arises when new items are added to a platform or inventory, and there is no user interaction data available to learn from.", "predicted_evidence": ["We address the cold-start problem by including hotel metadata which are independent of user click-stream interactions and available for all hotels. This helps us to better impute embeddings for sparse items/hotels.", "In addition, we use a simple heuristic for cold-start imputation and compare the results with the enriched model for cold-start hotels. To impute vectors for cold-start hotels, we borrow the idea in BIBREF2 and use price, star rating, geodesic distance, type of the property (e.g., hotel, vacation rental, etc.) size in terms of number of rooms, and the geographic market information. For each imputed property, we collect the most similar properties in the same market based on the above features, considering only those properties that fall within a radius of 5km of the target hotel. Results are in Table TABREF33. The heuristic imputation technique improves the Session-32 model's performance on cold-start hotels, but it remains well below that of the enriched model.", "The structure of the remainder of this paper is as follows. Section 2 gives an overview of some of the recent works on neural embedding. Section 3 provides details of the proposed framework, including the neural network architecture, training methodology, and how the cold-start problem is addressed. In Section 4, we present experimental results on several different tasks and a comparison with previous state-of-the-art work. Section 5 concludes the paper.", "Dealing with such hotels/items and choosing appropriate weights for them is referred to as the \"cold start problem.\" One of the main advantages of the enriched hotel2vec model over session-only approaches is its ability to better handle cold start cases. Although an item might lack sufficient prior user engagement, there are often other attributes available. For example, in our use case, thousands of new properties are added to the lodging platform's inventory each quarter. While we don't have prior user engagement data from which to learn a click embedding, we do have other attributes such as geographical location, star rating, amenities, etc. Hotel2vec can take advantage of this supplemental information to provide a better cold-start embedding.", "An interesting phenomenon is the effect of increasing the number of negative samples on training time and accuracy. Although it takes more time to create a large number of negative samples, as Figure FIGREF36 shows, using more negative samples results in faster training times."]}
{"question_id": "636ac549cf4917c5922cd09a655abf278924c930", "predicted_answer": "The experiment was evaluated through the analysis of learning curves, training progress, and the effect of various hyperparameters (such as the number of negative samples, learning rate, and optimization algorithm) on the performance of the model.", "predicted_evidence": ["We show significant gains over previous work based on click-embedding in several experimental studies.", "We consider two main scenarios: in the first, we are given the current hotel clicked by the user, and we try to predict the next clicked hotel among all approximately 1.1M hotels (raw evaluation). The second scenario is identical except we limit the candidates to hotels within the same market (filtered evaluation).", "In this section, we first look at the learning curves for both the session-32 and enriched-32 models. Then, we analyse the effect of $N$ (number of negative samples), $lr$ (learning rate), and the optimization algorithm on the performance of our model.", "Figure FIGREF35 shows the overall training progress of both the session-32 and enriched-32 models with their respective best hyperparameters. As shown in Figure FIGREF35, our model achieves similar performance with fewer data.", "An interesting phenomenon is the effect of increasing the number of negative samples on training time and accuracy. Although it takes more time to create a large number of negative samples, as Figure FIGREF36 shows, using more negative samples results in faster training times."]}
{"question_id": "c61c0b25f9de4a7ca2013d2e4aba8a5047e14ce4", "predicted_answer": "Based on the given context, it can be inferred that the authors experimented with applications in hotel search and recommendation systems.", "predicted_evidence": ["We show significant gains over previous work based on click-embedding in several experimental studies.", "An interesting phenomenon is the effect of increasing the number of negative samples on training time and accuracy. Although it takes more time to create a large number of negative samples, as Figure FIGREF36 shows, using more negative samples results in faster training times.", "It is also dynamic, meaning it is easy to reflect future changes in attributes such as star-rating or addition of amenities in the embedding vectors without retraining.", "It is well known BIBREF18, BIBREF0, BIBREF19 that using negative sampling, a version of noise contrastive estimation, significantly decreases the amount of time required to train a classifier with a large number of possible classes. In the case of recommendation, there is typically a large inventory of items available to recommend to the user, and thus we train our skip-gram model using negative sampling. However, it is not uncommon that users frequently search exclusively within a particular subdomain. For example, in hotel search, a customer looking to stay in Miami will focus on that market and rarely across different markets. This motivates a more targeted strategy when selecting negative samples: we select half of our negative samples following the schema in BIBREF20, i.e., from the complete set of all hotels, and the other half uniformly at random from the same market as the clicked hotel.", "In the case of recommendation, there is typically a large inventory of items available to recommend to the user, and thus we train our skip-gram model using negative sampling. However, it is not uncommon that users frequently search exclusively within a particular subdomain. For example, in hotel search, a customer looking to stay in Miami will focus on that market and rarely across different markets. This motivates a more targeted strategy when selecting negative samples: we select half of our negative samples following the schema in BIBREF20, i.e., from the complete set of all hotels, and the other half uniformly at random from the same market as the clicked hotel. Throughout this paper, a market is defined as a set of similar hotels in the same geographic region. It's worth noting that there may be multiple markets in the same city or other geo region. In the experimental section, we show that this improves the model's within-market similarities and its predictions."]}
{"question_id": "1d047286ac63e5dca1ab811172b89d7d125679e5", "predicted_answer": "The dataset used for training is not explicitly mentioned in the given context. However, it can be inferred that the dataset is related to hotels, as the model is referred to as \"hotel2vec\" and the text mentions \"item metadata\" and \"attribute groups\", which suggests that the dataset contains information about hotels and their attributes.", "predicted_evidence": ["Figure FIGREF35 shows the overall training progress of both the session-32 and enriched-32 models with their respective best hyperparameters. As shown in Figure FIGREF35, our model achieves similar performance with fewer data.", "In BIBREF13, the item metadata is injected into the model as side information to regularize the item embeddings. In their approach, they only use one feature (singer ID) in the experiments. In addition, their approach does not accommodate learning independent embedding vectors for each attribute group. Most recently, BIBREF14 propose a method where they train separate encoders for text data, click-stream session data, and product image data, and then use a simple weighted average to unify these embeddings. The weights are learned using grid search on the downstream task. While their approach allows for exploring independent embedding vectors, the sub-embeddings of different attribute groups are learned independently rather than jointly. In addition to efforts extending the skip-gram framework, emerging research attempts to extend GloVe BIBREF15 by incorporating various attributes. BIBREF16 incorporate attribute information into GloVe by modifying the loss function such that the representation of a location can be learned by combining both text and structural data.", "In this section, we present several experiments to evaluate the performance of the trained hotel2vec embeddings. Before diving into the details of the experiments, we first describe the dataset and model parameters.", "An interesting phenomenon is the effect of increasing the number of negative samples on training time and accuracy. Although it takes more time to create a large number of negative samples, as Figure FIGREF36 shows, using more negative samples results in faster training times.", "We tune the hyperparameters for all models, including the baseline session-only model, on the validation set. We search for a learning rate from $\\lbrace 0.01, 0.1, 0.5, 1.0, 2.5\\rbrace $ and embedding dimensions from $\\lbrace 32, 128\\rbrace $. To train the model weights, we use stochastic gradient descent (SGD) with exponential decay since it performs better than other optimizers in our case, and a batch size of 4096."]}
{"question_id": "6d17dc00f7e5331128b6b585e78cac0b9082e13d", "predicted_answer": "Yes, the entire annotation process was done manually.", "predicted_evidence": ["The annotation was performed by several student assistants with a background in linguistics and with Norwegian as their native language. 100 documents containing 2065 sentences were annotated doubly and disagreements were resolved before moving on. The remaining documents were annotated by one annotator. The doubly annotated documents were adjudicated by a third annotator different from the two first annotators. In the single annotation phase, all annotators were given the possibility to discuss difficult choices in joint annotator meetings, but were encouraged to take independent decisions based on the guidelines if possible. Annotation was performed using the web-based annotation tool Brat BIBREF22.", "In our current fine-grained annotation effort we annotate both the EVAL and FACT-NP sentences from the NoReC$_\\text{\\textit {eval}}$ corpus. Figure FIGREF4 provides an overview of the annotation scheme and the entities, relations and attributes annotated. Example annotations are provided in Figure FIGREF7, for an EVAL sentence, and Figure FIGREF8 for a FACT-NP. As we can see, positive or negative polarity is expressed by a relation between a polar expression and the target(s) of this expression and is further specified for its strength on a three-point scale, resulting in six polarity values, ranging from strong positive to strong negative. The holder of the opinion is also annotated if it is explicitly mentioned. Some of the annotated entities are further annotated with attributes indicating, for instance, if the opinion is not on topic (in accordance with the topic of the review) or whether the target or holder is implicit.", "We will here discuss some general issues that are relevant for several of the annotated entities and relations in our annotation effort.", "The SemEval 2014 shared task BIBREF4 proposes a different annotation scheme. Given an English tweet, the annotators identify targets, the aspect category they belong to, and the polarity expressed towards the target. They do not annotate holders or polar expressions.", "Table TABREF31 presents some relevant statistics for the resulting NoReC$_\\text{\\textit {fine}}$ dataset, providing the distribution of sentences, as well as holders, targets and polar expressions in the train, dev and test portions of the dataset, as well as the total counts for the dataset as a whole. We also report the average length of the different annotated categories. As we can see, the total of 7451 sentences that are annotated comprise almost 6949 polar expressions, 5289 targets, and 635 holders. In the following we present and discuss some additional core statistics of the annotations."]}
{"question_id": "de0154affd86c608c457bf83d888bbd1f879df93", "predicted_answer": "The results of their experiment are as follows:\n\n* The baseline model achieved modest results, with Proportional F1 scores of .41, .31, and .31 on Holders, Targets, and Polarity Expressions, respectively.\n* The Binary F1 scores were .41, .36, and .56 on Holders, Targets, and Polarity Expressions, respectively.\n* The overlap between Holders, Targets, and Polarity Expressions seen in training and those at test time was lower, with 56%, 28%, and 50% overlap, respectively.", "predicted_evidence": ["Below we review some of the most important principles described in our annotation guidelines relating to the targets of polarity.", "Figure FIGREF32 plots the distribution of polarity labels and their intensity scores. We see that the intensities are clearly dominated by standard strength, while there are also 627 strong labels for positive. Regardless of intensity, we see that positive valence is more prominent than negative, and this reflects a similar skew for the document-level ratings in this data BIBREF0.", "Table TABREF37 shows the results of the proportional and binary Overlap measures for precision, recall, and $\\text{F}_1$. The baseline model achieves modest results when compared to datasets that do not involve multiple domains BIBREF11, BIBREF10, with .41, .31, and .31 Proportional $\\text{F}_1$ on Holders, Targets, and Polarity Expressions, respectively (.41, .36, .56 Binary $\\text{F}_1$). However, this is still better than previous results on cross-domain datasets BIBREF33. The domain variation between documents leads to a lower overlap between Holders, Targets, and Polar Expressions seen in training and those at test time (56%, 28%, and 50%, respectively).", "The slight intensity is infrequent, with 213 positive and 329 negative polar expressions with this label. This relative difference can be explained by the tendency to hedge negative statements more than positive ones BIBREF24. Strong negative is the minority class, with only 144 examples. Overall, the distribution of intensity scores in NoReC$_\\text{\\textit {fine}}$ is very similar to what is reported for other fine-grained sentiment datasets for English and Dutch BIBREF25.", "Table TABREF37 shows the results of the proportional and binary Overlap measures for precision, recall, and $\\text{F}_1$. The baseline model achieves modest results when compared to datasets that do not involve multiple domains BIBREF11, BIBREF10, with .41, .31, and .31 Proportional $\\text{F}_1$ on Holders, Targets, and Polarity Expressions, respectively (.41, .36, .56 Binary $\\text{F}_1$). However, this is still better than previous results on cross-domain datasets BIBREF33. The domain variation between documents leads to a lower overlap between Holders, Targets, and Polar Expressions seen in training and those at test time (56%, 28%, and 50%, respectively). We argue, however, that this is a more realistic situation regarding available data, and that it is important to move away from simplifications where training and test data are taken from the same distribution."]}
{"question_id": "9887ca3d25e2109f41d1da80eeea05c465053fbc", "predicted_answer": "The dataset contains 7451 sentences.", "predicted_evidence": ["Table TABREF31 presents some relevant statistics for the resulting NoReC$_\\text{\\textit {fine}}$ dataset, providing the distribution of sentences, as well as holders, targets and polar expressions in the train, dev and test portions of the dataset, as well as the total counts for the dataset as a whole. We also report the average length of the different annotated categories. As we can see, the total of 7451 sentences that are annotated comprise almost 6949 polar expressions, 5289 targets, and 635 holders. In the following we present and discuss some additional core statistics of the annotations.", "Table TABREF37 shows the results of the proportional and binary Overlap measures for precision, recall, and $\\text{F}_1$. The baseline model achieves modest results when compared to datasets that do not involve multiple domains BIBREF11, BIBREF10, with .41, .31, and .31 Proportional $\\text{F}_1$ on Holders, Targets, and Polarity Expressions, respectively (.41, .36, .56 Binary $\\text{F}_1$). However, this is still better than previous results on cross-domain datasets BIBREF33. The domain variation between documents leads to a lower overlap between Holders, Targets, and Polar Expressions seen in training and those at test time (56%, 28%, and 50%, respectively). We argue, however, that this is a more realistic situation regarding available data, and that it is important to move away from simplifications where training and test data are taken from the same distribution.", "While most fine-grained sentiment datasets are in English, there are datasets available in several languages, such as German BIBREF5, Czech BIBREF6, Arabic, Chinese, Dutch, French, Russian, Spanish, Turkish BIBREF7, Hungarian BIBREF8, and Hindi BIBREF9. Additionally, there has been an increased effort to create fine-grained resources for low-resource languages, such as Basque and Catalan BIBREF10. No datasets for fine-grained SA have previously been created for Norwegian, however.", "Our model uses a single BiLSTM layer (100 dim.) to extract features and then a CRF layer to make predictions. We train the model using Adam BIBREF29 for 40 epochs with a patience of 5, and use dropout to regularize both the BiLSTM (0.5) and CRF (0.3) layers. The word embeddings are 100 dimensional fastText SkipGram BIBREF30 vectors trained on the NoWaC corpus BIBREF31 and made available from the NLPL vector repository BIBREF32. The pre-trained embeddings are further fine-tuned during training. We report held-out test results for the model that achieves the best performance on the development set and use the standard train/development/test split provided with the dataset (shown in Table TABREF31). All results are reported using the Proportional and Binary precision, recall and $\\text{F}_1$ scores, computed as described in Section SECREF27 above.", "Table TABREF37 shows the results of the proportional and binary Overlap measures for precision, recall, and $\\text{F}_1$. The baseline model achieves modest results when compared to datasets that do not involve multiple domains BIBREF11, BIBREF10, with .41, .31, and .31 Proportional $\\text{F}_1$ on Holders, Targets, and Polarity Expressions, respectively (.41, .36, .56 Binary $\\text{F}_1$). However, this is still better than previous results on cross-domain datasets BIBREF33. The domain variation between documents leads to a lower overlap between Holders, Targets, and Polar Expressions seen in training and those at test time (56%, 28%, and 50%, respectively)."]}
{"question_id": "87b65b538d79e1218fa19aaac71e32e9b49208df", "predicted_answer": "The domains that the corpus came from are:\n\n* Reviews (multiple domains within this genre)\n* News (from the MPQA corpus)\n* Editorials (planned for future addition)\n\nNote that the corpus currently only includes reviews, but the authors plan to add annotations for other genres and text types, such as editorials, in the future.", "predicted_evidence": ["Table TABREF31 presents some relevant statistics for the resulting NoReC$_\\text{\\textit {fine}}$ dataset, providing the distribution of sentences, as well as holders, targets and polar expressions in the train, dev and test portions of the dataset, as well as the total counts for the dataset as a whole. We also report the average length of the different annotated categories. As we can see, the total of 7451 sentences that are annotated comprise almost 6949 polar expressions, 5289 targets, and 635 holders. In the following we present and discuss some additional core statistics of the annotations.", "The SemEval 2014 shared task BIBREF4 proposes a different annotation scheme. Given an English tweet, the annotators identify targets, the aspect category they belong to, and the polarity expressed towards the target. They do not annotate holders or polar expressions.", "The inter annotator agreement scores obtained in the first rounds of (double) annotation are reported in Table TABREF28. We find that even though annotators tend to agree on certain parts of the expressions, they agree less when it comes to exact spans. This reflects the annotators subjective experiences, and although an attempt has been made to follow the guidelines strictly, it seems to be difficult to reach high agreement scores. The binary polar expression score is the highest score (96% Binary $\\text{F}_1$). This is unsurprising, as we noted during annotation that there was strong agreement on the most central elements, even though there were certain disagreements when it comes to the exact span of a polar expression. As holder expressions tend to be short, the relatively low binary agreement might reflect the tendency of holder expressions to occur multiple times in the same sentence, creating some confusion over which of these expressions to choose.", "In follow-up work we plan to further enrich the annotations with additional compositional information relevant to sentiment, most importantly negation but also other forms of valence shifters. Although our data already contains multiple domains, it is still all within the genre of reviews, and while we plan to test cross-domain effects within the existing data we would also like to add annotations for other different genres and text types, like editorials.", "One of the earliest datasets for fine-grained opinion mining is the MPQA corpus BIBREF1, which contains annotations of private states in English-language texts taken from the news domain. The authors propose a detailed annotation scheme in which annotators identify subjective expressions, as well as their targets and holders."]}
{"question_id": "075d6ab5dd132666e85d0b6ad238118271dfc147", "predicted_answer": "The benefit of the editing approach compared to generating entire SQL from scratch is not explicitly quantified in the given context. However, it is mentioned that \"editing the gold query consistently improves both question match and interaction match accuracy\", which suggests that the editing approach is helpful in improving the generation quality. Additionally, the paper claims that the approach is more robust to error propagation than copying segments, and its performance increases when the basic text-to-SQL generation quality (without editing) is better. This implies that the editing approach has a significant benefit compared to generating entire SQL from scratch, but the exact magnitude of the benefit is not specified.", "predicted_evidence": ["As shown in Table , editing the gold query consistently improves both question match and interaction match accuracy. This shows the editing approach is indeed helpful to improve the generation quality when the previous query is the oracle.", "In this paper, we propose an editing-based encoder-decoder model to address the problem of context-dependent cross-domain text-to-SQL generation. While being simple, empirical results demonstrate the benefits of our editing mechanism. The approach is more robust to error propagation than copying segments, and its performance increases when the basic text-to-SQL generation quality (without editing) is better.", "To empirically verify the usefulness of leveraging the previous query, we consider the process of generating the current query by applying copy and insert operations to the previous query. Figure FIGREF18 shows the SQL query length and the number of copy and insert operations at different turns. As the interaction proceeds, the user question becomes more complicated as it requires longer SQL query to answer. However, more query tokens overlap with the previous query, and thus the number of new tokens remains small at the third turn and beyond.", "We further investigate the effect of our query editing mechanism. To this end, we apply editing from both the gold query and the predicted query on our model with or without the utterance-table BERT embedding. We also perform an ablation study to validate the contribution of query attention and sequence editing separately.", "While the copy mechanism has been introduced by gu2016incorporating and see2017get, they focus on summarization or response generation applications by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas."]}
{"question_id": "f2b1e87f61c65aaa99bcf9825de11ae237260270", "predicted_answer": "The state-of-the-art baselines mentioned in the text are:\n\n1. IRNet with BERT\n2. Context-dependent Seq2Seq (CD-Seq2Seq) model released by yu2019sparc, which is adapted from suhr2018learning.\n\nThese models are considered state-of-the-art because they have achieved high performance on the SParC dataset, which is a challenging context-dependent text-to-SQL task.", "predicted_evidence": ["Furthermore, adding the utterance-table BERT embedding gives significant improvement, achieving 57.6% on dev set and 53.4% on test set, which is comparable to the state-of-the-art results from IRNet with BERT. We attribute our BERT model's high performance to (1) the empirically powerful text understanding ability of pretrained BERT model and (2) the early interaction between utterances and column headers when they are concatenated in a single sequence as the BERT input.", "To better understand how models perform as the interaction proceeds, Figure FIGREF30 (Left) shows the performance split by turns on the dev set. The questions asked in later turns are more difficult to answer given longer context history. While the baselines have lower performance as the turn number increases, our model still maintains 38%-48% accuracy for turn 2 and 3, and 20% at turn 4 or beyond. Similarly, Figure FIGREF30 (Right) shows the performance split by hardness levels with the frequency of examples. This also demonstrates our model is more competitive in answering hard and extra hard questions.", "SParC. We compare with the two baseline models released by yu2019sparc. (1) Context-dependent Seq2Seq (CD-Seq2Seq): This model is adapted from suhr2018learning. The original model was developed for ATIS and does not take the database schema as input hence cannot generalize well across domains. yu2019sparc adapt it to perform context-dependent SQL generation in multiple domains by adding a bi-LSTM database schema encoder which takes bag-of-words representations of column headers as input. They also modify the decoder to select between a SQL keyword or a column header.", "We summarize and compare the data statistics in Table and Table . While the ATIS dataset has been extensively studied, it is limited to a particular domain. By contrast, SParC is both context-dependent and cross-domain. Each interaction in SParC is constructed using a question in Spider as the interaction goal, where the annotator asks inter-related questions to obtain information that completes the goal. SParC contains interactions over 200 databases and it follows the same database split as Spider where each database appears only in one of train, dev and test sets. In summary, SParC introduces new challenges to context-dependent text-to-SQL because it (1) contains more complex context dependencies, (2) has greater semantic coverage, and (3) adopts a cross-domain task setting.", "Therefore, in the context-dependent scenario, the contextual history is crucial to understand the follow-up questions from users, and the system often needs to reproduce partial sequences generated in previous turns. Recently, suhr2018learning proposes a context-dependent text-to-SQL model including an interaction-level encoder and an attention mechanism over previous utterances. To reuse what has been generated, they propose to copy complete segments from the previous query. While their model is successful to reason about explicit and implicit references, it does not need to explicitly address different database schemas because the ATIS contains only the flight-booking domain. Furthermore, the model is confined to copy whole segments which are extracted by a rule-based procedure, limiting its capacity to utilize the previous query when only one or a few tokens are changed in the segment."]}
{"question_id": "78c7318b2218b906a67d8854f3e511034075f79a", "predicted_answer": "They use human-human logs as the \"gold\" dialogue history to evaluate on.", "predicted_evidence": ["Currently the standard approach in chitchat dialogue is to perform human evaluations BIBREF2, BIBREF20, BIBREF21, BIBREF4, BIBREF5, BIBREF7, typically reporting a judgment such as conversation quality or appropriateness via a Likert scale or pairwise comparison. While conversations are naturally multi-turn, pairwise setups typically consider single turn evaluations, taking the \u201cgold\u201d dialogue history from human-human logs, and only consider altering a single utterance. A more complete multi-turn evaluation is typically measured with a Likert scale (usually 1-4 or 1-5) after the conversation takes place. Some works such as BIBREF6 ask a series of questions relating to different aspects of conversational ability. There are some notable variants from these standard setups. BIBREF22 provide a method that combines continuous scales and relative assessments, but in single-turn, rather than multi-turn evaluation. BIBREF19 compare human evaluations to automatic metrics computed on self-chats.", "Currently the standard approach in chitchat dialogue is to perform human evaluations BIBREF2, BIBREF20, BIBREF21, BIBREF4, BIBREF5, BIBREF7, typically reporting a judgment such as conversation quality or appropriateness via a Likert scale or pairwise comparison. While conversations are naturally multi-turn, pairwise setups typically consider single turn evaluations, taking the \u201cgold\u201d dialogue history from human-human logs, and only consider altering a single utterance. A more complete multi-turn evaluation is typically measured with a Likert scale (usually 1-4 or 1-5) after the conversation takes place. Some works such as BIBREF6 ask a series of questions relating to different aspects of conversational ability. There are some notable variants from these standard setups. BIBREF22 provide a method that combines continuous scales and relative assessments, but in single-turn, rather than multi-turn evaluation.", "While conversations are naturally multi-turn, pairwise setups typically consider single turn evaluations, taking the \u201cgold\u201d dialogue history from human-human logs, and only consider altering a single utterance. A more complete multi-turn evaluation is typically measured with a Likert scale (usually 1-4 or 1-5) after the conversation takes place. Some works such as BIBREF6 ask a series of questions relating to different aspects of conversational ability. There are some notable variants from these standard setups. BIBREF22 provide a method that combines continuous scales and relative assessments, but in single-turn, rather than multi-turn evaluation. BIBREF19 compare human evaluations to automatic metrics computed on self-chats. Note that we also use self-chats in this work, but we evaluate these with humans, rather than automatic metrics.", "Dialogue tasks have traditionally been separated into two areas: goal-oriented and chitchat. Goal-oriented tasks typically have a clearer evaluation, e.g. task completion can be measured if the correct actions are taken BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14. Chitchat tasks are more open ended, and instead feature conversations without a precise goal that can be automatically evaluated. For example, conversations where two speaking partners are discussing interests BIBREF5 or topics BIBREF7. We study the latter in this work.", "In this work we introduce Acute-eval, a method that combines the benefits, and attempts to mitigate the deficiencies, of the above two approaches by introducing a pairwise relative comparison setup for multi-turn dialogues. In each trial, we show the annotator two whole conversations, with the second speaker in each conversation highlighted, as the judgment should be independent of the quality of the first speaker, see Figure FIGREF1. We then show a carefully worded question with two choices: speaker A or B, where the question measures a desired quality such as which speaker is more engaging, interesting or knowledgeable. Our experiments show that annotators perform well in this setup, and that our method can reveal subtle but significant differences between conversational models that other approaches, such as multi-turn Likert, cannot."]}
{"question_id": "697c5d2ba7e019ddb91a1de5031a90fe741f2468", "predicted_answer": "The text does not provide a direct answer to the question of how much faster pairwise annotations are compared to other annotations. The text focuses on the importance of question phrasing, inter-annotator agreement, and quality control measures in pairwise annotation tasks, but it does not provide a comparison with other types of annotations in terms of speed.", "predicted_evidence": ["We use crowdworkers for our annotations. We recommend limiting the number of annotations a single worker may complete to be only a few pairs (in our experiments, if we are making $N$ model comparisons then we allow $N$ annotations). In preliminary trials, we found that limiting the influence of any one worker was important for replicability, but that results were highly consistent across multiple runs with this limitation.", "We calculate the inter-annotator agreement for each question. The question achieving the highest inter-annotator agreement is selected for use in the rest of our experiments. The specific question phrasing and the texts accompanying the option for Speaker 1 (i.e. the left-hand conversation) are listed in Table TABREF11 along with inter-annotator agreements. As can be seen, the phrasing of the question is important, with poor phrasing choices leading to much lower agreement levels, e.g. 86.7% agreement in the best case for interestingness, and 69.6% in the worst case.", "The annotator is posed a question phrasing (e.g. \u201cwhich speaker is more knowledgeable\u201d or \u201cwhich speaker sounds more human?\u201d), and asked to make a binary choice between model $A$ and model $B$. They are strongly encouraged to provide a short text justification for their choice. We collect $N$ trials of such pairwise judgments, and use them to decide which model wins. Statistical significance can be computed using a binomial test.", "Additionally, the first comparison any worker is asked to annotate consists of a conversation between a weak baseline model and human, and a human-human conversation. If a worker fails to rate the human-human conversation as better, we remove their annotations from the results, in order to remove poor quality annotators. We additionally remove workers who never give a reason for their choice. Note that adding such worker quality tests to pairwise annotation tasks is straightforward where the gold annotation is known, while it is harder for Likert tests which have integer scores. One may also increase the number of quality-control annotations to decrease the likelihood of fraudulent workers, but we found using a single control question had a reasonable cost-noise ratio.", "So far, we have not detailed the actual question(s) asked of the annotators. The framing and phrasing of questions in surveys is known to greatly affect the direction of responses, and therefore, in the case of evaluation, inter-annotator agreement. Though this has been noted in prior work BIBREF1, we have found no systematic experimentation on question formulation or task presentation. We therefore aim to propose and evaluate multiple potential question wordings to achieve higher agreement."]}
{"question_id": "e25b73f700e8c958b64951f14a71bc60d225125c", "predicted_answer": "The question asks about the improvement in the BLEU score, but the provided context does not mention a specific improvement in the BLEU score. The context discusses the relationship between the cross-domain BLEU scores and the cluster proximity in the embedding space, and how the closer the domain centroids are, the higher the cross-domain BLEU score is. It also mentions a strong correlation of 0.81 between the cosine similarity of the centroids and the cross-domain BLEU score. However, it does not provide a specific improvement in the BLEU score.", "predicted_evidence": ["Cross-Domain BLEU vs. Cluster Proximity An interesting observation can be made with respect to the visual analysis of the domain clusters as depicted in Figure FIGREF15: as the Medical cluster (in Yellow), Law cluster (in Purple) and IT cluster (in Red) are close to each other in the embedding space, their cross-domain BLEU scores are also higher. For example, note how in the results for the Medical domain-specific model (first row in Table TABREF28), the BLEU scores on the Law and IT test sets are much higher in comparison to those on the Koran and Subtitles test sets, which clusters are farther away in the visualized embedding space. Similarly, as the Subtitles cluster (Blue) is closer to the Koran cluster (Green), the highest cross-domain BLEU score on the Koran test set is from the Subtitles model.", "For example, note how in the results for the Medical domain-specific model (first row in Table TABREF28), the BLEU scores on the Law and IT test sets are much higher in comparison to those on the Koran and Subtitles test sets, which clusters are farther away in the visualized embedding space. Similarly, as the Subtitles cluster (Blue) is closer to the Koran cluster (Green), the highest cross-domain BLEU score on the Koran test set is from the Subtitles model. To further quantify this phenomenon, we plot and measure Pearson's correlation between the cosine similarity of the centroids for the English BERT-based dev sentence representations for each domain pair, and the cross-domain BLEU score for this domain pair. This is shown in Figure FIGREF29. We can see the general trend where the closer the domain centroids are (with a similarity of 1 for training and evaluating on the same domain), the higher the cross-domain BLEU is between those domains, resulting in a Pearson's correlation of 0.81 (strong correlation).", "For example, note how in the results for the Medical domain-specific model (first row in Table TABREF28), the BLEU scores on the Law and IT test sets are much higher in comparison to those on the Koran and Subtitles test sets, which clusters are farther away in the visualized embedding space. Similarly, as the Subtitles cluster (Blue) is closer to the Koran cluster (Green), the highest cross-domain BLEU score on the Koran test set is from the Subtitles model. To further quantify this phenomenon, we plot and measure Pearson's correlation between the cosine similarity of the centroids for the English BERT-based dev sentence representations for each domain pair, and the cross-domain BLEU score for this domain pair. This is shown in Figure FIGREF29.", "Results The results for the cross-domain evaluation are available in Table TABREF28. In most cases, the best results for each domain are obtained by training on the in-domain data. Training on all the available data helped mostly for the Koran test set. This is expected as the training data for this domain is considerably smaller than the training data for rest of the domains (Table TABREF24). We can also see that more data is not necessarily better BIBREF37: while the subtitles corpus is the largest of all 5 and includes 500,000 sentence pairs, it is second to last in performance as measured by the average BLEU across all test sets.", "Cross-Domain BLEU vs. Cluster Proximity An interesting observation can be made with respect to the visual analysis of the domain clusters as depicted in Figure FIGREF15: as the Medical cluster (in Yellow), Law cluster (in Purple) and IT cluster (in Red) are close to each other in the embedding space, their cross-domain BLEU scores are also higher. For example, note how in the results for the Medical domain-specific model (first row in Table TABREF28), the BLEU scores on the Law and IT test sets are much higher in comparison to those on the Koran and Subtitles test sets, which clusters are farther away in the visualized embedding space. Similarly, as the Subtitles cluster (Blue) is closer to the Koran cluster (Green), the highest cross-domain BLEU score on the Koran test set is from the Subtitles model. To further quantify this phenomenon, we plot and measure Pearson's correlation between the cosine similarity of the centroids for the English BERT-based dev sentence representations for each domain pair, and the cross-domain BLEU score for this domain pair."]}
{"question_id": "908ba58d26d15c14600623498d4e86c9b73b14b2", "predicted_answer": "The established approach used for comparison is Pearson's correlation.", "predicted_evidence": ["Cross-Domain BLEU vs. Cluster Proximity An interesting observation can be made with respect to the visual analysis of the domain clusters as depicted in Figure FIGREF15: as the Medical cluster (in Yellow), Law cluster (in Purple) and IT cluster (in Red) are close to each other in the embedding space, their cross-domain BLEU scores are also higher. For example, note how in the results for the Medical domain-specific model (first row in Table TABREF28), the BLEU scores on the Law and IT test sets are much higher in comparison to those on the Koran and Subtitles test sets, which clusters are farther away in the visualized embedding space. Similarly, as the Subtitles cluster (Blue) is closer to the Koran cluster (Green), the highest cross-domain BLEU score on the Koran test set is from the Subtitles model.", "Cross-Domain BLEU vs. Cluster Proximity An interesting observation can be made with respect to the visual analysis of the domain clusters as depicted in Figure FIGREF15: as the Medical cluster (in Yellow), Law cluster (in Purple) and IT cluster (in Red) are close to each other in the embedding space, their cross-domain BLEU scores are also higher. For example, note how in the results for the Medical domain-specific model (first row in Table TABREF28), the BLEU scores on the Law and IT test sets are much higher in comparison to those on the Koran and Subtitles test sets, which clusters are farther away in the visualized embedding space. Similarly, as the Subtitles cluster (Blue) is closer to the Koran cluster (Green), the highest cross-domain BLEU score on the Koran test set is from the Subtitles model. To further quantify this phenomenon, we plot and measure Pearson's correlation between the cosine similarity of the centroids for the English BERT-based dev sentence representations for each domain pair, and the cross-domain BLEU score for this domain pair.", "The results are available in Table TABREF32. We can see that all selection methods performed much better in terms of BLEU than random selection. It is also nice to see that all selection methods performed better than using all the available data or the oracle-selected data when averaged across all domains, showing again that more data is not necessarily better in multi-domain scenarios and that data selection is a useful approach. Regarding a comparison of the data selection methods, Moore-Lewis performed better than Domain-Cosine, while Domain-Finetune performed best, showing the benefit of fine-tuning large pretrained models for the data selection task. Using the positively-labeled examples alone (Domain-Finetune-Positive) performed worse than using the top 500k examples but better than Domain-Cosine, while not requiring to determine the number of selected sentences.", "For example, note how in the results for the Medical domain-specific model (first row in Table TABREF28), the BLEU scores on the Law and IT test sets are much higher in comparison to those on the Koran and Subtitles test sets, which clusters are farther away in the visualized embedding space. Similarly, as the Subtitles cluster (Blue) is closer to the Koran cluster (Green), the highest cross-domain BLEU score on the Koran test set is from the Subtitles model. To further quantify this phenomenon, we plot and measure Pearson's correlation between the cosine similarity of the centroids for the English BERT-based dev sentence representations for each domain pair, and the cross-domain BLEU score for this domain pair. This is shown in Figure FIGREF29. We can see the general trend where the closer the domain centroids are (with a similarity of 1 for training and evaluating on the same domain), the higher the cross-domain BLEU is between those domains, resulting in a Pearson's correlation of 0.81 (strong correlation).", "For example, note how in the results for the Medical domain-specific model (first row in Table TABREF28), the BLEU scores on the Law and IT test sets are much higher in comparison to those on the Koran and Subtitles test sets, which clusters are farther away in the visualized embedding space. Similarly, as the Subtitles cluster (Blue) is closer to the Koran cluster (Green), the highest cross-domain BLEU score on the Koran test set is from the Subtitles model. To further quantify this phenomenon, we plot and measure Pearson's correlation between the cosine similarity of the centroids for the English BERT-based dev sentence representations for each domain pair, and the cross-domain BLEU score for this domain pair. This is shown in Figure FIGREF29."]}
{"question_id": "3e0fd1a3944e207edbbe7c7108239dbaf3bccd4f", "predicted_answer": "The five domains mentioned in the text are:\n\n1. Medical\n2. Law\n3. Koran\n4. IT\n5. Subtitles", "predicted_evidence": ["It is common knowledge in modern NLP that using large amounts of high-quality training data is a key aspect in building successful machine-learning based systems. For this reason, a major challenge when building such systems is obtaining data in the domain of interest. But what defines a domain? Natural language varies greatly across topics, styles, levels of formality, genres and many other linguistic nuances BIBREF0, BIBREF1, BIBREF2. This overwhelming diversity of language makes it hard to find the right data for the task, as it is nearly impossible to well-define the exact requirements from such data with respect to all the aforementioned aspects. On top of that, domain labels are usually unavailable \u2013 e.g. in large-scale web-crawled data like Common Crawl which was recently used to train state-of-the-art pretrained language models for various tasks BIBREF3.", "Regarding domain clustering for MT, BIBREF47 discovered topics using LDA instead of using domain labels. BIBREF48 induced latent subdomains from the training data using a dedicated probabilistic model.", "We propose two methods for domain data selection with pretrained language models.", "We perform data selection experiments for each domain in the multi-domain dataset. As the small set of monolingual in-domain data we take the 2000 development sentences from each domain. For the general-domain corpus we concatenate the training data from all domains, resulting in 1,456,317 sentences. To enable faster experimentation we used DistilBERT BIBREF18 for the Domain-Cosine and Domain-Finetune methods. More technical details are available in the supplementary material. We compare our methods to four approches: (1) The established method by BIBREF4, (2) a random selection baseline, (3) an oracle which is trained on all the available in-domain data, and (4) the model we train on all the domains concatenated. We select the top 500k examples to cover the size of every specific in-domain dataset. We train Transformer NMT models on the selected data with a similar configuration to the ones trained in the cross-domain evaluation.", "To simulate a diverse multi-domain setting we use the dataset proposed in BIBREF8, as it was recently adopted for domain adaptation research in NMT BIBREF28, BIBREF29, BIBREF30, BIBREF31. The dataset includes parallel text in German and English from five diverse domains (Medical, Law, Koran, IT, Subtitles; as discussed in Section SECREF2), available via OPUS BIBREF32, BIBREF33."]}
{"question_id": "c0847af3958d791beaa14c4040ada2d364251c4d", "predicted_answer": "The pre-trained language models used are not explicitly mentioned, but based on the mention of \"BERT-base model\" in Figure FIGREF15, it can be inferred that BERT is one of the models used. Additionally, the text mentions exploring both masked-language-models (MLMs) and auto-regressive LMs, which suggests that other models like RoBERTa, XLNet, or Transformers may also be used, but this is not explicitly confirmed.", "predicted_evidence": ["By fine-tuning the pretrained language model for binary classification, where positive examples are from the domain of interest.", "We hypothesize that massive pretrained LMs can learn representations that cluster to domains, as texts from similar domains will appear in similar contexts. We test this hypothesis across several large, publicly-available pretrained LMs; we explore both masked-language-models (MLMs) and auto-regressive LMs.", "As can be seen in Table TABREF7, pre-trained language models are indeed highly capable of generating sentence representations that cluster by domains, resulting in up to 87.66%, 89.04% and 89.94% accuracy when using k=5, k=10 and k=15 clusters, respectively, across 10,000 sentences in 5 domains. We find these scores remarkably high given our straight-forward average-pooling strategy and that no domain-supervision was involved in the process of learning the pre-trained representations. Figure FIGREF15 also demonstrates the quality of the obtained clusters in 2D using the BERT-base model, where the ellipses describe the mean and variance parameters learned for each cluster by the GMM with $k=5$.", "We propose two methods for domain data selection with pretrained language models.", "While previous work made important contributions to domain data selection, our work is the first to explore massive pretrained language models for both unsupervised domain clustering and for data selection in NMT."]}
{"question_id": "2f142cd11731d29d0c3fa426e26ef80d997862e0", "predicted_answer": "No, they report results on two datasets: RumourEval and PHEME.", "predicted_evidence": ["Table TABREF30 provides the experimental results of these methods on RumourEval and PHEME datasets. We have the following observations:", "SVM A Support Vector Machines model in BIBREF36 detects misinformation relying on manually extracted features.", "We perform experiments on RumourEval and PHEME datasets to evaluate the performance of our method and the baselines. The experimental results are shown in Table TABREF27. We gain the following observations:", "Experiments on two public, widely used fake news datasets demonstrate that our method significantly outperforms previous state-of-the-art methods.", "Effectiveness of multi-task learning. MT-trans boosts about 9% and 15% performance improvements in accuracy on both datasets compared with Single-task, which indicates that the multi-task learning method is effective to detect fake news."]}
{"question_id": "ce23849e9e9a22626965f1ca8ca948a5c87280e9", "predicted_answer": "The hyperparameter settings of the MTL (Multi-Task Learning) model are:\n\n* Word embeddings size: 200\n* Position embeddings size: 100\n* Number of attention heads: 6\n* Number of blocks in transformer encoder: 2\n* Dropout rate of multi-head attention: 0.7\n* Minibatch size: 64\n* Initial learning rate: 0.001\n* Dropout rate: 0.3\n* \u03bb (lambda) for fake news detection: 0.6\n* Number of heads (h) for focused shared features: 2", "predicted_evidence": ["Parameters - hyper-parameters configurations of our model: for each task, we strictly turn all the hyper-parameters on the validation dataset, and we achieve the best performance via a small grid search. The sizes of word embeddings and position embeddings are set to 200 and 100. In transformer encoder, attention heads and blocks are set to 6 and 2 respectively, and the dropout of multi-head attention is set to 0.7. Moreover, the minibatch size is 64; the initial learning rate is set to 0.001, the dropout rate to 0.3, and $\\lambda $ to 0.6 for fake news detection.", "where ${\\rm \\textbf {H}}_{shared}\\! \\in \\! \\mathbb {R}^{1 \\times l(d_p+d_w)}$ denotes the outputs of shared layer upstream, ${\\rm \\textbf {W}}_{fake} \\in \\mathbb {R}^{l(d_p+d_w) \\times l(d_p+d_w)}$ and ${\\rm \\textbf {b}}_{fake} \\in \\mathbb {R}^{1 \\times l(d_p+d_w)}$ are trainable parameters. $\\sigma $ is a non-linear activation - sigmoid, which makes final choices for retaining and discarding features in shared layer.", "We perform experiments on RumourEval and PHEME datasets to evaluate the performance of our method and the baselines. The experimental results are shown in Table TABREF27. We gain the following observations:", "Effectiveness of the selected sharing layer. Analysis of the results of the comparison with MT-trans, MT-trans-G, MT-Trans-A, and MT-trans-G-A shows that MT-trans-G-A ensures optimal performance with the help of the selected sharing layer of the model, which confirms the reasonability of selectively sharing different features for different tasks.", "where the dimensions of ${\\rm \\textbf {E}}_{fake}$, ${\\rm \\textbf {K}}_{shared}$, and ${\\rm \\textbf {V}}_{shared}$ are all $\\mathbb {R}^{l\\times (d_p+d_w)}$. The dimensions of remaining parameters in Eqs.(DISPLAY_FORM16, DISPLAY_FORM17) are the same as in Eqs.(DISPLAY_FORM10, DISPLAY_FORM11). Moreover, in order to guarantee the diversity of focused shared features, the number of heads $h$ should not be set too large. Experiments show that our method performs the best performance when $h$ is equal to 2."]}
{"question_id": "d9a45fea8539aac01dec01f29b7d04b44b9c2ca6", "predicted_answer": "The architecture of the rest of the multi-task learning setup is not explicitly stated in the given context. However, it can be inferred that the setup uses a deep neural network architecture, as it is mentioned that the limited number of training examples in RumourEval is not enough to train deep neural networks. Additionally, the use of softmax functions in the last layer suggests a typical neural network architecture with a softmax output layer for classification tasks.", "predicted_evidence": ["In order to obtain deeper insights and detailed interpretability about the effectiveness of the selected shared layer of the sifted multi-task learning method, we devise experiments to explore some ideas in depth: 1) Aiming at different tasks, what effective features can the selected sharing layer in our method obtain? 2) In the selected sharing layer, what features are learned from different cells?", "In terms of recall (R), our method and MTL-LSTM, both based on multi-task learning, achieve more competitive performances than other baselines, which presents that sufficient features are shared for each other among multiple tasks. Furthermore, our method reflects a more noticeable performance boost than MTL-LSTM on both datasets, which extrapolates that our method earns more valuable shared features.", "Although the sifted multi-task learning method outperforms previous state-of-the-art methods on two datasets (From Table TABREF27), we observe that the proposed method achieves more remarkable performance boosts on PHEME than on RumourEval. There are two reasons for our analysis according to Table TABREF24 and Table TABREF27. One is that the number of training examples in RumourEval (including 5,568 tweets) is relatively limited as compared with PHEME (including 105,354 tweets), which is not enough to train deep neural networks. Another is that PHEME includes more threads (6,425 threads) than RumourEval (325 threads) so that PHEME can offer more rich credibility features to our proposed method.", "Effectiveness of multi-task learning. MT-trans boosts about 9% and 15% performance improvements in accuracy on both datasets compared with Single-task, which indicates that the multi-task learning method is effective to detect fake news.", "As the last layer, softmax functions are applied to achieve the classification of different tasks, which emits the prediction of probability distribution for the specific task $i$."]}
{"question_id": "246e924017c48fa1f069361c44133fdf4f0386e1", "predicted_answer": "The selected sharing layer is trained as part of the overall model, specifically as part of the MT-trans-G-A model. The training process is not explicitly described in the given context, but it can be inferred that the selected sharing layer is trained along with the other components of the model, such as the shared layer and the private features, to optimize the predictive results $\\hat{{\\rm \\textbf {y}}}_i$ for each task $i$. The trainable parameters ${\\rm \\textbf {W}}_i$ and ${\\rm \\textbf {b}}_i$ are likely updated during the training process to minimize a loss function.", "predicted_evidence": ["In order to obtain deeper insights and detailed interpretability about the effectiveness of the selected shared layer of the sifted multi-task learning method, we devise experiments to explore some ideas in depth: 1) Aiming at different tasks, what effective features can the selected sharing layer in our method obtain? 2) In the selected sharing layer, what features are learned from different cells?", "In order to select valuable and appropriate shared features for different tasks, we design a selected sharing layer following the shared layer. The selected sharing layer consists of two cells: gated sharing cell for filtering useless features and attention sharing cell for focusing on valuable shared features for specific tasks. The description of this layer is depicted in Figure FIGREF6 and Figure FIGREF15. In the following, we introduce two cells in details.", "Effectiveness of the selected sharing layer. Analysis of the results of the comparison with MT-trans, MT-trans-G, MT-Trans-A, and MT-trans-G-A shows that MT-trans-G-A ensures optimal performance with the help of the selected sharing layer of the model, which confirms the reasonability of selectively sharing different features for different tasks.", "where $\\hat{{\\rm \\textbf {y}}}_i$ is the predictive result, ${\\rm \\textbf {F}}_i$ is the concatenation of private features ${\\rm \\textbf {H}}_i$ of task $i$ and the outputs ${\\rm \\textbf {SSL}}_i$ of selected sharing layer for task $i$. ${\\rm \\textbf {W}}_i$ and ${\\rm \\textbf {b}}_i$ are trainable parameters.", "MT-trans-G-A Gated sharing cell and attention sharing cell are organically combined as selected sharing layer behind the shared layer of MT-trans, called MT-trans-G-A."]}
{"question_id": "96459b02efa82993a0b413530ed0b517c6633eea", "predicted_answer": "The length constraints they set were:\n\n1. Constrained to the length of the best Beam-10 hypothesis.\n2. Constrained to the reference length.\n3. Constrained to translations longer than 0.25 times the source sentence length.\n4. Constrained to a certain range (e.g. zero to 1.2 times the source sentence length).", "predicted_evidence": ["Exact search under length normalization does not suffer from the length deficiency anymore (last row in Tab. TABREF19 ), but it is not able to match our best BLEU score under Beam-10 search. This suggests that while length normalization biases search towards translations of roughly the correct length, it does not fix the fundamental modelling problem.", "We then constrained exact search to either the length of the best Beam-10 hypothesis or the reference length. Tab. TABREF18 shows that exact search constrained to the Beam-10 hypothesis length does not improve over beam search, suggesting that any search errors between beam search score and global best score for that length are insignificant enough so as not to affect the BLEU score. The oracle experiment in which we constrained exact search to the correct reference length (last row in Tab. TABREF18 ) improved the BLEU score by 0.9 points.", "To find out more about the length deficiency we constrained exact search to certain translation lengths. Constraining search that way increases the run time as the INLINEFORM0 -bounds are lower. Therefore, all results in this section are conducted on only a subset of the test set to keep the runtime under control. We first constrained search to translations longer than 0.25 times the source sentence length and thus excluded the empty translation from the search space. Although this mitigates the problem slightly (Fig. FIGREF16 ), it still results in a peak in the INLINEFORM1 cluster. This suggests that the problem of empty translations is the consequence of an inherent model bias towards shorter hypotheses and cannot be fixed with a length constraint.", "A popular method to counter the length bias in NMT is length normalization BIBREF6 , BIBREF7 which simply divides the sentence score by the sentence length. We can find the global best translations under length normalization by generalizing our exact inference scheme to length dependent lower bounds INLINEFORM0 . The generalized scheme finds the best model scores for each translation length INLINEFORM1 in a certain range (e.g. zero to 1.2 times the source sentence length). The initial lower bounds are derived from the Beam-10 hypothesis INLINEFORM2 as follows: DISPLAYFORM0", "We have presented an exact inference scheme for NMT. Exact search may not be practical, but it allowed us to discover deficiencies in widely used NMT models. We linked deteriorating BLEU scores of large beams with the reduction of search errors and showed that the model often prefers the empty translation \u2013 an evidence of NMT's failure to properly model adequacy. Our investigations into length constrained exact search suggested that simple heuristics like length normalization are unlikely to remedy the problem satisfactorily."]}
{"question_id": "6c1614991647705265fb348d28ba60dd3b63b799", "predicted_answer": "The test set size is not explicitly stated in the given context. However, it is mentioned that \"all results in this section are conducted on only a subset of the test set to keep the runtime under control\", which implies that the test set size is larger than the subset used in the experiments, but the exact size is not provided.", "predicted_evidence": ["We then constrained exact search to either the length of the best Beam-10 hypothesis or the reference length. Tab. TABREF18 shows that exact search constrained to the Beam-10 hypothesis length does not improve over beam search, suggesting that any search errors between beam search score and global best score for that length are insignificant enough so as not to affect the BLEU score. The oracle experiment in which we constrained exact search to the correct reference length (last row in Tab. TABREF18 ) improved the BLEU score by 0.9 points.", "To find out more about the length deficiency we constrained exact search to certain translation lengths. Constraining search that way increases the run time as the INLINEFORM0 -bounds are lower. Therefore, all results in this section are conducted on only a subset of the test set to keep the runtime under control. We first constrained search to translations longer than 0.25 times the source sentence length and thus excluded the empty translation from the search space. Although this mitigates the problem slightly (Fig. FIGREF16 ), it still results in a peak in the INLINEFORM1 cluster. This suggests that the problem of empty translations is the consequence of an inherent model bias towards shorter hypotheses and cannot be fixed with a length constraint.", "To the best of our knowledge, this is the first work that reports the exact number of search errors in NMT as prior work often relied on approximations, e.g. via INLINEFORM0 -best lists BIBREF3 or constraints BIBREF4 .", "A popular method to counter the length bias in NMT is length normalization BIBREF6 , BIBREF7 which simply divides the sentence score by the sentence length. We can find the global best translations under length normalization by generalizing our exact inference scheme to length dependent lower bounds INLINEFORM0 . The generalized scheme finds the best model scores for each translation length INLINEFORM1 in a certain range (e.g. zero to 1.2 times the source sentence length). The initial lower bounds are derived from the Beam-10 hypothesis INLINEFORM2 as follows: DISPLAYFORM0", "We have presented an exact inference scheme for NMT. Exact search may not be practical, but it allowed us to discover deficiencies in widely used NMT models. We linked deteriorating BLEU scores of large beams with the reduction of search errors and showed that the model often prefers the empty translation \u2013 an evidence of NMT's failure to properly model adequacy. Our investigations into length constrained exact search suggested that simple heuristics like length normalization are unlikely to remedy the problem satisfactorily."]}
{"question_id": "b948bb86855b2c0bfc8fad88ff1e29cd94bb6ada", "predicted_answer": "The evaluation metrics used in this context are:\n\n1. BLEU-4 score\n2. Slot error rate (ERR)\n3. Informativeness\n4. Naturalness\n5. Pairwise preferences\n6. Fleiss' kappa (to measure inter-rater consistency)\n\nAdditionally, statistical tests such as two-tailed t-test are used to determine the significance of the results.", "predicted_evidence": ["To better evaluate the quality of the generated utterances, we performed manual evaluation.", "As in earlier NLG researches, we use the BLEU-4 score BIBREF26 and the slot error rate (ERR) as evaluation metrics. ERR is computed by the ratio of the sum of the number of missing and redundant slots in a generated utterance divided by the total number of slots in the DA. We randomly sampled target low-resource task five times for each experiment and reported the average score.", "Metric-based: The idea is to learn a metric space and then use it to compare low-resource testing samples to rich training samples. The representative works in this category include Siamese Network BIBREF12 , Matching Network BIBREF13 , Memory-augmented Neural Network (MANN BIBREF14 ), Prototype Net BIBREF15 , and Relation Network BIBREF16 .", "Annotation Statistics: Cases with identical utterances generated by two models were filtered out. We obtained in total 600 annotations on each individual metric for each target domain. We calculated the Fleiss\u2019 kappa BIBREF27 to measure inter-rater consistency. The overall Fleiss\u2019 kappa values for informativeness and naturalness are 0.475 and 0.562, indicating \u201cModerate Agreement\u201d, and 0.637 for pairwise preferences, indicating \u201cSubstantial Agreement\u201d.", "Results: Scores of informativeness and naturalness are presented in Table TABREF36 . Meta-NLG outscores MTL-NLG in terms of both metrics on all four domains. Overall, Meta-NLG received significantly (two-tailed t-test, INLINEFORM0 ) higher scores than MTL-NLG. Results for pairwise preferences are summarized in Table TABREF37 . Even though there are certain amount of cases where the utterances generated by different models are nearly the same (Tie) to annotators, Meta-NLG is overall significantly preferred over MTL-NLG (two-tailed t-test, INLINEFORM1 ) across different target domains."]}
{"question_id": "157284acedf13377cbc6d58c8f3648d3a62f5db5", "predicted_answer": "According to the given context, the other training procedures explored are:\n\n1. Multi-task learning (MTL): This method learns through individual DA-utterance pairs from different high-resource NLG tasks and does not explicitly learn to adapt to new low-resource NLG tasks.\n2. Meta-NLG: This method repeatedly constructs auxiliary Meta NLG tasks from source tasks and directly learns from them, resulting in a substantial gain in the final performance.", "predicted_evidence": ["Meta-learning or learning-to-learn, which can date back to some early works BIBREF11 , has recently attracted extensive attentions. A fundamental problem is \u201cfast adaptation to new and limited observation data\u201d. In pursuing this problem, there are three categories of meta-learning methods:", "Zero-NLG: Train INLINEFORM0 using multi-task learning (MTL) with source task data, then directly test on a target task without a fine-tuning step. This corresponds to a zero-shot learning scenario.", "To better understand the meta objective, we include a standard multi-task learning (MTL) objective in Eq.( EQREF17 ). MTL learns through individual DA-utterance pairs from different high-resource NLG tasks INLINEFORM0 , and it does not explicitly learn to adapt to new low-resource NLG tasks. Figure FIGREF16 visually illustrates the differences with three high-resource source tasks INLINEFORM1 with optimal parameters INLINEFORM2 for each task. INLINEFORM3 is learned from individual DA-utterance pairs in INLINEFORM4 , while Meta-NLG repeatedly constructs auxiliary Meta NLG tasks INLINEFORM5 from INLINEFORM6 and directly learns INLINEFORM7 from them. As a result, INLINEFORM8 is closer to INLINEFORM9 and INLINEFORM10 (the optimal parameters of some new low-resource tasks, e.g, INLINEFORM11 and INLINEFORM12 ) than INLINEFORM13 . As we will see soon later, our meta optimization scheme results in a substantial gain in the final performance.", "The main contribution of this paper is two-fold:", "To better evaluate the quality of the generated utterances, we performed manual evaluation."]}
{"question_id": "e4ea0569b637d5f56f63e933b8f269695fe1a926", "predicted_answer": "The baseline used is not explicitly mentioned in the given context. The text only discusses the performance of the Fact Checking system and the question generation subtask, but does not compare it to a specific baseline or existing system.", "predicted_evidence": ["where $n_c$ indicates the number of correct questions, and $N$ is the total number of questions generated for the given claim. The label is assigned based on the correctness score ($s$) and the derived threshold ($\\phi $) as:", "In this stage, we compute the final label based on the correctness score of the predictions that we received from the previous stage. The correctness score ($s$) is computed as:", "Table TABREF16 shows the performance of our Fact Checking system on the \u201cSUPPORTS\u201d label, the output of our system. We compare the results against two different classification thresholds. Table TABREF3 shows that on an average there are 3 questions generated per claim. Here, $\\phi $ = 0.76 suggests that at least 3 out of the 4 questions have to be answered correctly while $\\phi $ = 0.67 suggests that at least 2 out of the 3 questions has to be answered correctly for the claim to be classified as \u201cSUPPORTS\u201d.", "For the subtask of question generation, the results in Table TABREF3 show that the system is able to generate questions given a claim with considerably good accuracy. The conversion accuracy is defined as the ratio of the number of claims in which the named entities are extracted to the number of claims. The results also support our assumption that the claims generally feature information about one or more entities.", "Trainable parameters: 110M"]}
{"question_id": "e3c44964eb6ddc554901244eb6595f26a9bae47e", "predicted_answer": "The threshold is $\\phi$ = 0.76.", "predicted_evidence": ["Here, the classification threshold ($\\phi $) is derived empirically based on the precision-recall curve.", "In contrast to the results reported in Table TABREF16, here we consider $\\phi $ = 0.76 to be a better classification threshold as it improvises over False Positives considerably over the entire dataset.", "If only 1 question is generated, then it has to be answered correctly for the claim to be classified as \u201cSUPPORTS\u201d in case of both the thresholds.", "where $n_c$ indicates the number of correct questions, and $N$ is the total number of questions generated for the given claim. The label is assigned based on the correctness score ($s$) and the derived threshold ($\\phi $) as:", "Hidden Units: 768"]}
{"question_id": "905a8d775973882227549e960c7028e4a3561752", "predicted_answer": "The masking was done by masking the named entities in the claims with a blank, similar to a Cloze-task or masked language modeling task.", "predicted_evidence": ["Layers: 12", "We fine-tune our model (BERT) on the masked language modeling task on the wiki-text provided along with the FEVER dataset for 2 epochs.", "Originally inspired by the Cloze-task and developed to learn to predict the masked entities as well as the next sentence, BERT creates a deep bidirectional transformer model for the predictions. Since the FEVER claims are masked to generate the questions, we use BERT to tokenize the claims. We observed that the BERT tokenizer sometimes fails to tokenize the named entities correctly (e.g. Named entity \u201cTaran\u201d was tokenized as \u201cTara\u201d, \u201c##n\u201d). This is due to the insufficient vocabulary used while training the WordPiece tokenizer.", "This process not only transforms the dataset but also transforms the task into a Cloze-task or masked language modeling task. Although the original masked language modeling task masks some of the tokens randomly, here we mask the named entities for generating the questions.", "In this section, we explain the design and all the underlying methods that our system has adopted. Our system is a pipeline consisting of three stages: (1) Question Generation, (2) Question Answering, (3) Label Classification. The question generation stage attempts to convert the claims into appropriate questions and answers. It generates questions similar to a Cloze-task or masked language modeling task where the named entities are masked with a blank. Question Answering stage predicts the masked blanks in an unsupervised manner. The respective predictions are then compared with the original answers and exported into a file for label classification. The label classifier calculates the predicted label based on a threshold."]}
{"question_id": "76f90c88926256e7f90d2104a88acfdd7fc5475e", "predicted_answer": "The FEVER dataset contains around 185k claims from a corpus of 5.4M Wikipedia articles.", "predicted_evidence": ["In our case, FEVER claims are derived from Wikipedia. We first collect all the claims from the FEVER dataset along with \u201cid\u201d, \u201clabel\u201d and \u201cverifiable\u201d fields. We don't perform any normalization on the claims such as lowercasing, transforming the spaces to underscore or parenthesis to special characters as it may decrease the accuracy of the NER tagger. These claims are then processed by the NER tagger to identify the named entities and their type. The named entities are then used to generate the questions by masking the entities for the subsequent stage.", "We fine-tune our model (BERT) on the masked language modeling task on the wiki-text provided along with the FEVER dataset for 2 epochs.", "Wikipedia manages to verify all this new information with a number of human reviewers. Manual review processes introduce delays in publishing and is not a well scalable approach. To address this issue, researchers have launched relevant challenges, such as the Fake News Challenge (BIBREF0), Fact Extraction and VERification (FEVER) (BIBREF1) challenge along with the datasets. Moreover, Thorne and Vlachos (BIBREF2) released a survey on the current models for automated fact-checking. FEVER is the largest dataset and contains around 185k claims from the corpus of 5.4M Wikipedia articles. The claims are labeled as \u201cSUPPORTS\u201d, \u201cREFUTES\u201d, or \u201cNOT ENOUGH INFO\u201d, based on the evidence set.", "In contrast to the results reported in Table TABREF16, here we consider $\\phi $ = 0.76 to be a better classification threshold as it improvises over False Positives considerably over the entire dataset.", "To transform the FEVER dataset into the above-mentioned task, we first generate the questions from the claims. In literature, there are majorly two types of Question Generation systems: Rule-based and Neural Question Generation (NQG) model based. Ali et al. (BIBREF3) proposed a rule-based pipeline to automate the question generation using POS (Part-of-speech) tagging and Named Entity Recognition (NER) tagging from the sentences. Recently, many NQG models have been introduced to generate questions in natural language. Serban et al. (BIBREF4) achieved better performance for question generation utilizing (passage, question, answer) triplets as training data and an encoder-decoder based architecture as their learning model."]}
{"question_id": "182eb91090017a7c8ea38a88b219b641842664e4", "predicted_answer": "They obtain structured data by splitting each game report into individual sentences and finding the corresponding data in the box-score table as the content record. Each record can contain a varying number of tuples, with each tuple containing three fields: a data type, a value, and an associated player or team.", "predicted_evidence": ["Generating natural language text to describe structured content, such as a database record or a table, is of ubiquitous use in real-life applications including data report generation BIBREF0 , article writing BIBREF1 , BIBREF2 , dialog systems BIBREF3 , BIBREF4 , and many others. Recent efforts have developed many techniques to improve fidelity to the source content, such as new powerful neural architectures BIBREF5 , BIBREF6 , hybrid generation and retrieval BIBREF7 , BIBREF8 , and so forth, most of which are applied in supervised context.", "Beyond generating text from scratch, there is another line of work that first retrieves a similar sentence and then rewrites it to express desired information BIBREF8 , BIBREF7 , BIBREF13 , BIBREF14 . For example, BIBREF8 used the framework to generate response in dialogues, while BIBREF7 studied programming code generation. The goal of the work is to manifest useful information from neighbors, usually in a supervised context, without aiming at controlling writing characteristics, and thus has fundamentally different assumptions to ours.", "As the original corpus is already largely clean, we found some simple rules are sufficient to obtain high-quality results in this step. Please see the supplementary materials for more details. Each of the resulting record-sentence pairs is treated as a pair of INLINEFORM0 , namely (content record, auxiliary sentence). The next step is to find a suitable reference sentence INLINEFORM1 for each content record INLINEFORM2 . As defined above, the reference sentence should cover similar but not the same content as in record INLINEFORM3 . We achieve this by retrieving from the data another record-sentence pair using INLINEFORM4 , where the retrieved record is designated to have a slightly different structure than that of INLINEFORM5 by having less or more tuples and different data types. More details of the retrieval method are deferred to supplements. The retrieved record-sentence pair thus plays the role of INLINEFORM6 and is paired with INLINEFORM7 to form an instance.", "We may also have access to other side information at training time. For example, in the dataset developed below, each content record INLINEFORM0 is associated with an auxiliary sentence INLINEFORM1 that was originally written to describe INLINEFORM2 without following the reference INLINEFORM3 . Each reference sentence INLINEFORM4 also has its corresponding record INLINEFORM5 containing the content information. The side information can provide valuable clues for models to understand the content structure and text semantics at training time. For example, the auxiliary sentence provides a hint on how the desired content can be presented in natural language, though it is stylistically irrelevant to the reference sentence. Note that, at test time, a solution to the task should only rely on the inputs INLINEFORM6 without using the side information.", "To obtain our data, we first split each game report into individual sentences, and, for each sentence, find its corresponding data in the box-score table as the content record. A record can contain a varying number of tuples, with each tuple containing three fields, namely a data type, a value, and an associated player or team, e.g., (team_points, 106, Lakers). As the original corpus is already largely clean, we found some simple rules are sufficient to obtain high-quality results in this step. Please see the supplementary materials for more details. Each of the resulting record-sentence pairs is treated as a pair of INLINEFORM0 , namely (content record, auxiliary sentence). The next step is to find a suitable reference sentence INLINEFORM1 for each content record INLINEFORM2 . As defined above, the reference sentence should cover similar but not the same content as in record INLINEFORM3 . We achieve this by retrieving from the data another record-sentence pair using INLINEFORM4 , where the retrieved record is designated to have a slightly different structure than that of INLINEFORM5 by having less or more tuples and different data types."]}
{"question_id": "0ef114d24a7a32821967e912dff23c016c4eab41", "predicted_answer": "The prior approaches for style transfer that they test with are:\n\n1. Rule-based baseline\n2. Multi-Attribute Style Transfer (MAST)\n3. AdvST (Adversarial Style Transfer)\n4. AttnCopy-S2S", "predicted_evidence": ["For empirical study, we devise automatic metrics to measure content fidelity and style preservation, respectively. We also perform human evaluations to compare different approaches. Results demonstrate the proposed method significantly improves over others, including a strong rule-based baseline and the recent style transfer models.", "We conduct both automatic and human evaluations to assess the model performance. For automatic evaluation, we use two metrics to measure content fidelity and style preservation, respectively. Results show our model balances well between the two goals, and outperforms a variety of comparison methods. All code will be released soon.", "We can see that the rule-based method achieves reasonably good precision and recall, setting a strong baseline for content fidelity. As discussed above, the rule-based method can reach the maximum BLEU (100) after masking out content tokens. To improve over the strong rule-based baseline, we would expect a method that provides significantly higher precision/recall, while keeping a high BLEU score. The two style transfer methods (MAST and AdvST) fail the expectation, as their content fidelity performance is greatly inferior or merely comparable to the rule-based method. This is partially because these models are built on a different task assumption (i.e., modifying independent textual attributes) and cannot manipulate content well. In comparison, our proposed model achieves better content precision/recall, substantially improving over other methods (e.g., with a 15-point precision boost in comparison with the rule-based baseline) except for AttnCopy-S2S which has failed in style control. Our method also manages to preserve a high BLEU score of over 80.", "In theory, rule-based method sets the best possible style preservation performance, as it only replaces content related tokens (particularly numbers) without modifying other parts of the reference sentence. The output, however, tends to miss or contain extra content compared to the content record of interest.", "Multi-Attribute Style Transfer (MAST) BIBREF11 . We compare with the most recent style transfer approach that models multiple attributes. To apply to our setting, we treat content record INLINEFORM0 as the attributes. The method is based on back-translation BIBREF23 that first generates a target sentence INLINEFORM1 conditioning on INLINEFORM2 , and then treat it as the reference to reconstruct INLINEFORM3 conditioning on INLINEFORM4 . Auxiliary sentence INLINEFORM5 is used in an extra auto-encoding loss."]}
{"question_id": "67672648e7ebcbef18921006e2c8787966f8cdf2", "predicted_answer": "The competing objectives for their unsupervised method are content fidelity and style preservation.", "predicted_evidence": ["We compare with a diverse set of approaches:", "We now compare the performance of different methods in terms of the above metrics. Table TABREF29 shows the results.", "We use separate metrics to evaluate in terms of the two primary goals of the task, namely content fidelity and style preservation, respectively. A desired solution should balance and excel on both metrics.", "Ours w/o Coverage. For ablation study, we compare with a model variant that omits the content coverage constraint. That is, the model is trained by maximizing only Eq.( EQREF13 ).", "We can see that, in general, the proposed full model can manipulate the reference sentence more accurately to express the new content. For example, in the first case, the rule-based method was confused between the winning and losing teams, due to its incapacity of understanding the semantics of text such as \u201cheld off\u201d. The style transfer model AdvST failed to comprehend the content record well and generated irrelevant data \u201c100 - 100\u201d. The simplified variant without explicit coverage constraint copied the content of Bulls twice. In contrast, the full model successfully generates the desired sentence. Similarly, in the second and third cases, other methods tend to keep irrelevant content originally in the reference sentence (e.g., \u201cand 5 rebounds\u201d in the second case), or miss necessary information in the record (e.g., one of the player names was missed in the third case). The proposed model performs better in properly adding or deleting text portions for accurate content descriptions, though sometimes it can yield sentences of lower language quality (e.g., in the third case)."]}
{"question_id": "c32fc488f0527f330273263fa8956788bd071efc", "predicted_answer": "They design a content coverage constraint, which encourages the model to mention each of the data tuples exactly once, similar to a common pattern of human-written descriptions.", "predicted_evidence": ["Ours w/o Coverage. For ablation study, we compare with a model variant that omits the content coverage constraint. That is, the model is trained by maximizing only Eq.( EQREF13 ).", "As discussed above, the rule-based method can reach the maximum BLEU (100) after masking out content tokens. To improve over the strong rule-based baseline, we would expect a method that provides significantly higher precision/recall, while keeping a high BLEU score. The two style transfer methods (MAST and AdvST) fail the expectation, as their content fidelity performance is greatly inferior or merely comparable to the rule-based method. This is partially because these models are built on a different task assumption (i.e., modifying independent textual attributes) and cannot manipulate content well. In comparison, our proposed model achieves better content precision/recall, substantially improving over other methods (e.g., with a 15-point precision boost in comparison with the rule-based baseline) except for AttnCopy-S2S which has failed in style control. Our method also manages to preserve a high BLEU score of over 80. The superior performance of the full model compared to the variant Ours-w/o-Coverage demonstrates the usefulness of the content coverage constraint (Eq.", "To improve over the strong rule-based baseline, we would expect a method that provides significantly higher precision/recall, while keeping a high BLEU score. The two style transfer methods (MAST and AdvST) fail the expectation, as their content fidelity performance is greatly inferior or merely comparable to the rule-based method. This is partially because these models are built on a different task assumption (i.e., modifying independent textual attributes) and cannot manipulate content well. In comparison, our proposed model achieves better content precision/recall, substantially improving over other methods (e.g., with a 15-point precision boost in comparison with the rule-based baseline) except for AttnCopy-S2S which has failed in style control. Our method also manages to preserve a high BLEU score of over 80. The superior performance of the full model compared to the variant Ours-w/o-Coverage demonstrates the usefulness of the content coverage constraint (Eq. EQREF15 ). By explicitly encouraging the model to mention each of the data tuples exactly once\u2014a common pattern of human-written descriptions\u2014the model achieves higher content fidelity with less style-preservation ability \u201csacrificed\u201d.", "To improve over the strong rule-based baseline, we would expect a method that provides significantly higher precision/recall, while keeping a high BLEU score. The two style transfer methods (MAST and AdvST) fail the expectation, as their content fidelity performance is greatly inferior or merely comparable to the rule-based method. This is partially because these models are built on a different task assumption (i.e., modifying independent textual attributes) and cannot manipulate content well. In comparison, our proposed model achieves better content precision/recall, substantially improving over other methods (e.g., with a 15-point precision boost in comparison with the rule-based baseline) except for AttnCopy-S2S which has failed in style control. Our method also manages to preserve a high BLEU score of over 80. The superior performance of the full model compared to the variant Ours-w/o-Coverage demonstrates the usefulness of the content coverage constraint (Eq. EQREF15 ).", "We can see that, in general, the proposed full model can manipulate the reference sentence more accurately to express the new content. For example, in the first case, the rule-based method was confused between the winning and losing teams, due to its incapacity of understanding the semantics of text such as \u201cheld off\u201d. The style transfer model AdvST failed to comprehend the content record well and generated irrelevant data \u201c100 - 100\u201d. The simplified variant without explicit coverage constraint copied the content of Bulls twice. In contrast, the full model successfully generates the desired sentence. Similarly, in the second and third cases, other methods tend to keep irrelevant content originally in the reference sentence (e.g., \u201cand 5 rebounds\u201d in the second case), or miss necessary information in the record (e.g., one of the player names was missed in the third case). The proposed model performs better in properly adding or deleting text portions for accurate content descriptions, though sometimes it can yield sentences of lower language quality (e.g., in the third case)."]}
{"question_id": "8908d1b865137bc309dde10a93735ec76037e5f9", "predicted_answer": "The evaluation metric used is macro-average recall, which is considered the official measure for system ranking. Additionally, the average recall score is used to select the best-performing networks during training.", "predicted_evidence": ["Official ranking: Our system is ranked fourth over 38 systems in terms of macro-average recall. Table 4 shows the results of our system on the test set of 2016 and 2017.", "For training the network, we used about 30000 English tweets provided by SemEval organisers and the test set of 2016 which contains 12000 tweets as development set. The test set of 2017 is used to evaluate the system in SemEval-2017 competition. For implementing our system we used python and Keras.", "We create ten instances of this network, we randomly initialize them using the uniform distribution, we repeat the random initialization for each instance 100 times, then we pick the networks which gives the highest average recall score as it is considered the official measure for system ranking. If the top network of each instance gives more than 95% of its results identical to another chosen network, we choose the next top networks to make sure that the ten networks are enough different.", "Thus, we have ten classifiers, we count the number of classifiers which give the positive, negative and neutral sentiment label to each tweet and select the sentiment label which have the highest number of votes. For each new tweet from the test set, we convert it to 2-dim matrix, if the tweet is longer than maxl, it will be truncated. We then feed it into the ten networks and pass the results to the voting system.", "Polarity classification is the basic task of sentiment analysis in which the polarity of a given text should be classified into three categories: positive, negative or neutral. In Twitter where the tweet is short and written in informal language, this task needs more attention. SemEval has proposed the task of Message Polarity Classification in Twitter since 2013, the objective is to classify a tweet into one of the three polarity labels BIBREF0 ."]}
{"question_id": "d207f78beb6cd754268881bf575c8f98000667ea", "predicted_answer": "According to the context, they explore 3 sentiment labels: positive, negative, and neutral.", "predicted_evidence": ["Polarity classification is the basic task of sentiment analysis in which the polarity of a given text should be classified into three categories: positive, negative or neutral. In Twitter where the tweet is short and written in informal language, this task needs more attention. SemEval has proposed the task of Message Polarity Classification in Twitter since 2013, the objective is to classify a tweet into one of the three polarity labels BIBREF0 .", "Thus, we have ten classifiers, we count the number of classifiers which give the positive, negative and neutral sentiment label to each tweet and select the sentiment label which have the highest number of votes. For each new tweet from the test set, we convert it to 2-dim matrix, if the tweet is longer than maxl, it will be truncated. We then feed it into the ten networks and pass the results to the voting system.", "We presented our deep learning approach to Twitter sentiment analysis. We used ten convolutional neural network voters to get the polarity of a tweet, each voter has been trained on the same training data using the same word embeddings but different initial weights. The results demonstrate that our system is competitive as it is ranked forth in SemEval-2017 task 4-A.", "In 2016, we remark that the number of participations which use feature extraction systems were degraded, and the first four systems used Deep Learning, the majority used a convolutional network except the fourth one BIBREF4 . Despite of that, using Deep Learning for sentiment analysis in Twitter has not yet shown a big improvement in comparison to feature extraction, the fifth and sixth systems BIBREF5 in 2016 which were built upon feature extraction process were only (3 and 3.5% respectively) less than the first system. But We think that Deep Learning is a promising direction in sentiment analysis. Therefore, we proposed to use convolutional networks for Twitter polarity classification.", "In our work, we propose to vary the neural network weights instead of tweet representation which can get the same effect of varying the word embeddings, therefore we vary the initial weights of the network to produce ten different nets, a voting system over the these ten voters will decide the sentiment label for a tweet."]}
{"question_id": "35c01dc0b50b73ee5ca7491d7d373f6e853933d2", "predicted_answer": "The datasets used for text-altering attributes matching to image parts are CUB and COCO.", "predicted_evidence": ["All the above works mainly focus on specific problems, and few studies BIBREF8, BIBREF9 concentrate on more general and user-friendly image manipulation by using natural language descriptions. More precisely, the task aims to semantically edit parts of an image according to the given text provided by a user, while preserving other contents that are not described in the text. However, current state-of-the-art text-guided image manipulation methods are only able to produce low-quality images (see Fig. FIGREF1: first row), far from satisfactory, and even fail to effectively manipulate complex scenes (see Fig. FIGREF1: second row).", "Finally, we suggest a new metric to assess image manipulation results. The metric can appropriately reflect the performance of image manipulation, in terms of both the generation of new visual attributes corresponding to the given text, and the reconstruction of text-irrelevant contents of the original image. Extensive experiments on the CUB BIBREF10 and COCO BIBREF11 datasets demonstrate the superiority of our model, where our model outperforms existing state-of-the-art methods both qualitatively and quantitatively.", "As shown in Table TABREF11, our method has the highest IS and MP values on both the CUB and COCO datasets compared with the state-of-the-art approaches, which demonstrates that (1) our method can produce high-quality manipulated results, and (2) our method can better generate new attributes matching the given text, and also effectively reconstruct text-irrelevant contents of the original image.", "Given an input image $I$, and a text description ${S}^{\\prime }$ provided by a user, the model aims to generate a manipulated image $I^{\\prime }$ that is semantically aligned with ${S}^{\\prime }$ while preserving text-irrelevant contents existing in $I$. To achieve this, we propose two novel components: (1) a text-image affine combination module (ACM), and (2) a detail correction module (DCM). We elaborate our model as follows.", "To achieve effective image manipulation guided by text descriptions, the key is to exploit both text and image cross-modality information, generating new attributes matching the given text and also preserving text-irrelevant contents of the original image. To fuse text and image information, existing methods BIBREF8, BIBREF9 typically choose to directly concatenate image and global sentence features along the channel direction. Albeit simple, the above heuristic may suffer from some potential issues. Firstly, the model cannot precisely correlate fine-grained words with corresponding visual attributes that need to be modified, leading to inaccurate and coarse modification. For instance, shown in the first row of Fig. FIGREF1, both models cannot generate detailed visual attributes like black eye rings and a black bill. Secondly, the model cannot effectively identify text-irrelevant contents and thus fails to reconstruct them, resulting in undesirable modification of text-irrelevant parts in the image. For example, in Fig."]}
{"question_id": "c077519ea42c9649fb78da34485de2262a0df779", "predicted_answer": "Yes, the DCM module is designed to correct text-relevant content, specifically to refine the manipulated results by enhancing details and completing missing contents. It does this by exploiting word-level text information and fine-grained image features to rectify inappropriate attributes and complete missing details in the synthetic image.", "predicted_evidence": ["Given an input image $I$, and a text description ${S}^{\\prime }$ provided by a user, the model aims to generate a manipulated image $I^{\\prime }$ that is semantically aligned with ${S}^{\\prime }$ while preserving text-irrelevant contents existing in $I$. To achieve this, we propose two novel components: (1) a text-image affine combination module (ACM), and (2) a detail correction module (DCM). We elaborate our model as follows.", "For each stage, the text features are refined with several convolutional layers to produce hidden features $h$. The proposed ACM further combines $h$ with the original image features $v$ in order to effectively select image regions corresponding to the given text, and then correlate those regions with text information for accurate manipulation. Meanwhile, it also encodes the original image representations for stable reconstruction. The output features from the ACM module are fed into the corresponding generator to produce an edited image, and are also upsampled serving as input to the next stage for image manipulation at a higher resolution. The whole framework gradually generates new visual attributes matching the given text description at a higher resolution with higher quality, and also reconstructs text-irrelevant contents existing in the input image at a finer scale. Finally, the proposed detail correction module (DCM) is used to rectify inappropriate attributes, and to complete missing details.", "To further enhance the details and complete missing contents in the synthetic image, we propose a detail correction module (DCM), exploiting word-level text information and fine-grained image features.", "Effectiveness of the detail correction module and main module. As shown in Fig. FIGREF16 (f), our model without DCM misses some attributes (e.g., the bird missing the tail in the second row, the zebra missing the mouth in the third row), or generates new contents (e.g., new background in the first row, different appearance of the bus in the fourth row), which indicates that our DCM can correct inappropriate attributes and reconstruct text-irrelevant contents. Fig. FIGREF16 (e) shows that without main module, our model fails to do image manipulation on both datasets, which just achieves an identity mapping. This is mainly because the model fails to correlate words with corresponding attributes, which has been done in the main module. Table TABREF11 also illustrates the identity mapping, as our model without main module gets the lowest $L_{1}$ pixel difference value.", "Why does the detail correction module work? This module aims to refine the manipulated results by enhancing details and completing missing contents. On the one hand, the word-level spatial and channel-wise attentions closely correlate fine-grained word-level information with the intermediate feature maps, enhancing the detailed attribute modification. On the other hand, the shallow neural network layer is adopted to derive visual representations, which contain more detailed colour, texture, and edge information, contributing to missing detail construction. Finally, further benefiting from our ACM, the above fine-grained text-image representations collaborate to enhance the quality."]}
{"question_id": "a51c680a63ee393792d885f66de75484dc6bc9bc", "predicted_answer": "No, an ablation test is not explicitly mentioned as being performed in the given context. However, the fourth set of experiments examines the effect of memory size on the performance of DUPMN, which can be considered a form of ablation test, as it involves evaluating the model's performance with different memory sizes.", "predicted_evidence": ["Best results are marked in bold; second best are underlined in the table", "In order to make a systematic comparison, three groups of baselines are used in the evaluation. Group 1 includes all commonly used feature sets mentioned in Chen et al. chen2016neural including Majority, Trigram, Text features (TextFeatures), and AveWordvec. All feature sets in Group 1 except Majority use the SVM classifier.", "Performance evaluations are conducted on three datasets and DUPMN is compared with a set of commonly used baseline methods including the state-of-the-art LSTM based method BIBREF5 , BIBREF21 .", "Four sets of experiments are conducted. The first experiment compares DUPMN with other sentiment analysis methods. The second experiment evaluates the effectiveness of different hop size INLINEFORM0 of memory network. The third experiment evaluates the effectiveness of UMN and PMN in different datasets. The fourth set of experiment examines the effect of memory size INLINEFORM1 on the performance of DUPMN. Performance measures include Accuracy (ACC), Root-Mean-Square-Error (RMSE), and Mean Absolute Error (MAE) for our model. For other baseline methods in Group 2 and Group 3, their reported results are used. We also show the p-value by comparing the result of 10 random tests for both our model and the state-of-the-art model in the t-test .", "When comparing Group 2 and Group 3 methods, we can see that user profiles and product information can improve performance as most of the methods in Group 3 perform better than methods in Group 2. This is more obvious in the IMDB dataset which naturally contains more subjectivity. In the IMDB dataset, almost all models with user and product information outperform the text-only models in Group 2 except LSTM+CBA BIBREF13 . However, the two LSTM models in Group 2 which include local attention mechanism do show that attention base methods can outperform methods using user profile and product information. In fact, the LSTM+CBA model using attention mechanism based on cognition grounded eye-tracking data in Group 2 outperforms quite a number of methods in Group 3. LSTM+CBA in Group 2 is only inferior to LSTM+UPA in Group 3 because of the additional user profile and production information used in LSTM+UPA."]}
{"question_id": "e752dc4d721a2cf081108b6bd71e3d10b4644354", "predicted_answer": "The statistical test performed is a t-test. This is mentioned in the sentence: \"We also show the p-value by comparing the result of 10 random tests for both our model and the state-of-the-art model in the t-test.\"", "predicted_evidence": ["In order to make a systematic comparison, three groups of baselines are used in the evaluation. Group 1 includes all commonly used feature sets mentioned in Chen et al. chen2016neural including Majority, Trigram, Text features (TextFeatures), and AveWordvec. All feature sets in Group 1 except Majority use the SVM classifier.", "Best results are marked in bold; second best are underlined in the table", "Four sets of experiments are conducted. The first experiment compares DUPMN with other sentiment analysis methods. The second experiment evaluates the effectiveness of different hop size INLINEFORM0 of memory network. The third experiment evaluates the effectiveness of UMN and PMN in different datasets. The fourth set of experiment examines the effect of memory size INLINEFORM1 on the performance of DUPMN. Performance measures include Accuracy (ACC), Root-Mean-Square-Error (RMSE), and Mean Absolute Error (MAE) for our model. For other baseline methods in Group 2 and Group 3, their reported results are used. We also show the p-value by comparing the result of 10 random tests for both our model and the state-of-the-art model in the t-test .", "The three benchmarking datasets include movie reviews from IMDB, restaurant reviews from Yelp13 and Yelp14 developed by Tang tang2015document. All datasets are tokenized using the Stanford NLP tool BIBREF22 . Table TABREF11 lists statistics of the datasets including the number of classes, number of documents, average length of sentences, the average number of documents per user, and the average number of documents per product. Since postings in social networks by both users and products follow the long tail distribution BIBREF23 , we only show the distribution of total number of posts for different products. For example, #p(0-50) means the number of products which have reviews between the size of 0 to 50. We split train/development/test sets at the rate of 8:1:1 following the same setting in BIBREF3 , BIBREF5 . The best configuration by the development dataset is used for the test set to obtain the final result.", "Performance evaluations are conducted on three datasets and DUPMN is compared with a set of commonly used baseline methods including the state-of-the-art LSTM based method BIBREF5 , BIBREF21 ."]}
{"question_id": "c79f168503a60d1b08bb2c9aac124199d210b06d", "predicted_answer": "The downstream tasks used for evaluation in this paper are:\n\n1. NER (Named Entity Recognition)\n2. SST (Stanford Sentiment Treebank)\n3. Parsing\n4. SNLI (Stanford Natural Language Inference)", "predicted_evidence": ["Individual Layers: Only a single layer is used for the downstream task.", "In this paper we show that 1) the weighting scheme can have a significant impact on downstream NLP tasks, 2) that the learned weighted average proposed by Peters et al. does not yield the optimal performance for all datasets, and 3) that the second layer of the biLM yields in many cases a better performance than the third (last) layer.", "The output of the three layers is integrated into task-specific neural architectures. However, the integration of ELMo into neural architectures is not straightforward. For example, Peters et al. describe two methods for the integration: Either the output of the last layer is used for downstream tasks, or a task-specific weighting of the three layer outputs is learned: $\\text{ELMo}_{\\text{weighted\\_average}} = \\gamma \\sum _{j=0}^{2}s_j h_j$", "It is unclear what the impact of these different weighting schemes is for downstream tasks. Is the (rather complicated) learned weighted average proposed by Peters et al. needed to achieve optimal performance? Will a simpler method, like computing a fixed average, decrease the performance?", "We notice that computing a weighted average of the first two layers yields the highest performance in 4 out of 5 datasets. It appears that computing the third layer of the biLM does not necessarily add value for downstream tasks. Removing it increases training as well as inference speed, as computing the bidirectional LSTM of the third layer is rather slow. By not computing the third layer of the biLM, we observe a training speed-up of 38% for the NER model, 44% for the SST model, 19% for the parsing model, and 27% for the SNLI model. The training of the SNLI model requires on a Tesla P100 GPU about one day."]}
{"question_id": "9dd8ce48a2a59a63ae6366ab8b2b8828e5ae7f35", "predicted_answer": "The datasets used for evaluation are:\n\n1. POS (Part-of-Speech tags from Universal Dependencies v. 1.3 for English)\n2. Chunking (CoNLL 2000 shared task dataset on chunking)\n3. NER (CoNLL 2003 shared task on named entity recognition)\n4. GENIA NER (The Bio-Entity Recognition Task at JNLPBA annotated Medline abstracts with information on bio-entities)\n5. WNUT16 (Named Entity Recognition over Twitter)\n6. Arguments (Argument component detection in 402 persuasive essays)\n7. ACE Entities/Events (ACE 2005 dataset, detecting events or entities in documents)\n8. Stanford Sentiment Treebank (SST-5)", "predicted_evidence": ["To our surprise, using the second layer of the biLM yields in most cases a better performance than using the third (last) layer of the biLM. For 7 out of 8 datasets it outperforms even the learned weighted average method proposed by Peters et al. Only for the GENIA dataset achieved the learned weighted average a significantly better performance than using the output of the second layer. However, for this dataset, it appears that context information is less critical, as the system achieves a high performance by using only the characters of a word (1. Layer).", "To our knowledge, only Peters2018 evaluated different weighting schemes. They evaluated to use either the output of the last layer or to learn a task-specific weighted average of all three layer outputs. They compare these two options in their paper and show a slight advantage for learning a weighted average. However, the evaluation is in our opinion insufficient. First, they evaluate both options on the development set, so it remains unclear if there are changes for unseen data (test set). Further, they evaluate it only with a single random seed. As shown in BIBREF3 , the performance of a neural network can change significantly with a different random seed. For example, we observe test score differences of up to 1.5 percentage points when the same model is trained with a different random seed with the AllenNLP model for the Stanford Sentiment Treebank (SST-5). The differences Peters et al.", "POS: We use the part-of-speech tags from Universal Dependencies v. 1.3 for English with the provided data splits. We reduced the training set to the first 500 sentences to increase the difficulty for the network. The development and test set were kept unchanged. Chunking: CoNLL 2000 shared task dataset on chunking. NER: CoNLL 2003 shared task on named entity recognition. GENIA NER: The Bio-Entity Recognition Task at JNLPBA BIBREF9 annotated Medline abstracts with information on bio-entities (like protein or DNA-names). The dataset consists of 2000 abstracts for training (we used 400 of those as development set) and the test set contains 404 abstracts. WNUT16: WNUT16 was a shared task on Named Entity Recognition over Twitter BIBREF10 . Training data are 2,394 annotated tweets, development data are 1,000 tweets, and test data are 3,856 tweets.", "We trained this architecture for the following datasets: Arguments: Argument component detection (major claim, claim, premise) in 402 persuasive essays BIBREF7 . Development and test set were 80 randomly selected essays each. ACE Entities/Events: ACE 2005 dataset BIBREF8 consists of 599 annotated documents from six different domains (newswire, broadcast news, broadcast conversations, blogs, forums, and speeches). We train the architecture to either detect events or to detect entities in these documents. We used 90 randomly selected documents each for the development and test set. POS: We use the part-of-speech tags from Universal Dependencies v. 1.3 for English with the provided data splits. We reduced the training set to the first 500 sentences to increase the difficulty for the network. The development and test set were kept unchanged. Chunking: CoNLL 2000 shared task dataset on chunking.", "For both experiments, we use the pre-trained ELMo 5.5B model, which was trained on a dataset of 5.5 billion tokens. We trained each setup with ten different random seed and report average test scores."]}
{"question_id": "5cc5e2db82f5d40a5244224dad94da50b4f673db", "predicted_answer": "According to the context, the human-in-the-loop helps the system by using the \"meaningful clues or pieces of evidence\" provided by the system to \"appropriately modify the text to create an unbiased version\" of the original text.", "predicted_evidence": ["Such de-biasing systems may be of two types 1) an end-to-end system that takes in a biased text and returns an unbiased version of it or 2) a system with a human-in-the-loop that takes a text, analyzes it and returns meaningful clues or pieces of evidence to the human who can appropriately modify the text to create an unbiased version. Since multiple types of biases may exist in the given text, the former de-biasing system requires identifying which biases to focus on and how to paraphrase or modify the sentence to de-bias it. These notions can often be subjective and it might be desirable to have a human-in-the-loop. This is the focus of the latter de-biasing system as well as the approach taken by us in the paper.", "To further explain it more, given a sentence -", "patients well. One day, he fell", "Our system is represented in figure FIGREF7 . We have the following components in our system -", "patients well. One day, she fell"]}
{"question_id": "ab975efc916c34f55e1144b1d28e7dfdc257e371", "predicted_answer": "According to the given context, the model is trained on the \"Names Dataset\" which consists of 5453 male names and 6990 female names sourced from the CMU repository of names, as well as the NLTK dataset and records of baby names and their genders.", "predicted_evidence": ["Our dataset comprises of the following - 1) Occupation Data 2) Names Data. We will iterate over each of this one by one.", "Person Tagging - We use a dictionary based tagging for annotating person names in the text using the Names Dataset described in the previous section.", "Gender Tagging - We further use the names dataset to resolve the genders of the persons identified in the previous person tagging step.", "Names Data: We created a corpus of 5453 male and 6990 female names sourced from [ref: CMU repository of names]. For the dataset to map names to a gender, we referred to the NLTK data set and the records of baby names and their genders.", "Analysis of gender bias in machine learning in recent years has not only revealed the prevalence of such biases but also motivated much of the recent interest and work in de-biasing of ML models. BIBREF2 have pointed to the presence of gender bias in structured prediction from images. BIBREF3 , BIBREF0 notice these biases in movies while BIBREF4 , BIBREF5 notice the same in children books and music lyrics."]}
{"question_id": "e7ce612f53e9be705cdb8daa775eae51778825ef", "predicted_answer": "Yes, the approach can be extended to eliminate racial or ethnic biases. The concept of occupation de-biasing can be applied to other dimensions of biases, including racial and ethnic biases. The system can be designed to identify and generate evidence of racial or ethnic biases in text, and then provide suggestions to remove or modify the biased language.\n\nThe two types of de-biasing systems mentioned, end-to-end and human-in-the-loop, can also be applied to racial and ethnic biases. The end-to-end system can be trained to identify and remove racial and ethnic biases, while the human-in-the-loop system can provide clues and evidence to a human reviewer to modify the text and remove biases.\n\nAdditionally, the techniques mentioned, such as training paradigms to ensure fair predictions, latent-variable based approaches, and adversarial training, can also be applied to remove racial and ethnic biases from machine learning models.\n\nThe story-writer scenario can also be extended to include racial and ethnic biases, where the system can help identify and remove biases in stories to make them more inclusive and appealing to a diverse audience.", "predicted_evidence": ["Occupation De-biasing is a first-of-a-kind tool to identify possibility of gender bias from occupation point of view, and to generate pieces of evidences by responding to different cultural contexts. Our future work would involve exploring other dimensions of biases and have a more sophisticated definition of bias in text.", "Such de-biasing systems may be of two types 1) an end-to-end system that takes in a biased text and returns an unbiased version of it or 2) a system with a human-in-the-loop that takes a text, analyzes it and returns meaningful clues or pieces of evidence to the human who can appropriately modify the text to create an unbiased version. Since multiple types of biases may exist in the given text, the former de-biasing system requires identifying which biases to focus on and how to paraphrase or modify the sentence to de-bias it. These notions can often be subjective and it might be desirable to have a human-in-the-loop. This is the focus of the latter de-biasing system as well as the approach taken by us in the paper.", "De-biasing the training algorithm as a way to remove the biases focusses on training paradigms that would result in fair predictions by an ML model. In the Bayesian network setting, Kushner et al. have proposed a latent-variable based approach to ensure counter-factual fairness in ML predictions. Another interesting technique ( BIBREF6 and BIBREF7 ) is to train a primary classifier while simultaneously trying to \"deceive\" an adversarial classifier that tries to predict gender from the predictions of the primary classifier.", "Consider a story-writer as a user of our system. The task is to be able to write bias free stories which are liked by viewers and earns high revenue in the BOX office. Here are few scenarios where this system can be used to identify bias.", "De-biasing the model after training as a way to remove bias focuses on \"fixing\" the model after training is complete. BIBREF8 in their famous work on gender bias in word embeddings take this approach to \"fix\" the embeddings after training."]}
{"question_id": "6c5a64b5150305c584326882d37af5b0e58de2fd", "predicted_answer": "The evaluation of their de-biasing approach is not explicitly mentioned in the given context. However, it can be inferred that the evaluation will be discussed in Section 4, which lays out the implementation of their de-biasing system, or possibly in Section 5, which provides a walk-through of their system.", "predicted_evidence": ["Such de-biasing systems may be of two types 1) an end-to-end system that takes in a biased text and returns an unbiased version of it or 2) a system with a human-in-the-loop that takes a text, analyzes it and returns meaningful clues or pieces of evidence to the human who can appropriately modify the text to create an unbiased version. Since multiple types of biases may exist in the given text, the former de-biasing system requires identifying which biases to focus on and how to paraphrase or modify the sentence to de-bias it. These notions can often be subjective and it might be desirable to have a human-in-the-loop. This is the focus of the latter de-biasing system as well as the approach taken by us in the paper.", "Occupation De-biasing is a first-of-a-kind tool to identify possibility of gender bias from occupation point of view, and to generate pieces of evidences by responding to different cultural contexts. Our future work would involve exploring other dimensions of biases and have a more sophisticated definition of bias in text.", "The rest of the paper is organized as follows - Section 2 describes the related work, Section 3 discusses about the design and Section 4 lays out the implementation of our de-biasing system. In Section 5 we describe a walk-through of our system and in Section 6 we conclude our paper.", "Consider a story-writer as a user of our system. The task is to be able to write bias free stories which are liked by viewers and earns high revenue in the BOX office. Here are few scenarios where this system can be used to identify bias.", "De-biasing the model after training as a way to remove bias focuses on \"fixing\" the model after training is complete. BIBREF8 in their famous work on gender bias in word embeddings take this approach to \"fix\" the embeddings after training."]}
{"question_id": "f7a27de3eb6447377eb48ef6d2201205ff943751", "predicted_answer": "The metric that rewards good stylistic response is the style expression score, which measures how well the generated responses express the desired style. This score is evaluated by human annotators on a 5-point scale, where 5 means very strong style, 3 means no obvious style, and 1 means very conflicted style.", "predicted_evidence": ["The response prototype is constructed from the retrieved response by masking the stylistic words. To determine whether a word is stylistic, we use the pointwise mutual information (PMI) BIBREF29 metric. The relevance between the word $x$ and the style $s$ is measured as", "In practice, a satisfactory stylistic dialogue system should express the desired style on the premise of the response quality. Based on the criterion of human evaluation metric, 3 is the marginal score of acceptance. So we deem a response as marginally acceptable by actual users when both quality and style expression scores are greater or equal to 3. On the other hand, 4 is the score that well satisfies the users, so responses with both scores greater or equal to 4 are deemed as satisfying to actual users.", "We propose to use a new style-aware learning objective to train the stylistic response generator. Consider a training instance consists of the input query ${\\bf X} = (x_1, ..., x_N)$, the reference response ${\\bf Y} = (y_1, ..., y_T)$, the reference language style $s$ and the response prototype ${\\bf C} = (c_1, ..., c_T)$, the proposed objective is defined as", "This metric measures how well the generated responses express the desired style. The annotators give a score ranging from 1 to 5 to this metric, where 5 means very strong style, 3 means no obvious style and 1 means very conflicted style. The style conflict means the generated style is conflicted to the desired one (e.g. female to male, positive to negative emotion).", "This metric evaluates the content quality of the generated responses. The annotators are asked to give a score within 5-point scale where 5 means perfectly human-like response (relevant, fluent and informative), 3 means marginally acceptable and 1 means unreadable and impossible to understand."]}
{"question_id": "2df3cd12937591481e85cf78c96a24190ad69e50", "predicted_answer": "The existing baseline models mentioned in the text are:\n\n1. GPT2-FT\n2. PS w/o R\n3. PS\n4. RST\n5. RRe", "predicted_evidence": ["To fully evaluate the proposed approach, we conduct extensive experiments on three benchmark datasets. Results of both human and automatic evaluation show that the proposed approach significantly outperforms several strong baselines. In addition, we also conduct an extensive cross-domain experiment to demonstrate that the proposed approach is more robust than such baselines.", "We compare the proposed approach with several competitive baselines that can be categorized into two classes: generative approaches and retrieval-based approaches.", "As shown in Figure FIGREF55, some of the strong baselines exhibit a drastic drop in response quality after domain variation such as GPT2-FT and PS w/o R. In contrast, the PS model successfully maintains high response quality in spite of domain variation. The model seems to benefit from leveraging retrieved results to bridge the gap between the two different domains. This can also be observed in the results of RST and RRe which also use the retrieved results and get a even higher performance when facing domain variation.", "Both human and automatic evaluation results on the three benchmark datasets are shown in Table TABREF25, TABREF26 and TABREF27. For each dataset, we present results on individual styles as well as the overall results. We observe that the proposed model achieves the top performance results on most of the metrics. It generates responses with both intense style and high response quality. In addition, we also measure the diversity of the generated responses with two automatic metrics: Distinct-1 and Distinct-2 BIBREF16. The results show that the proposed model achieves the closest performance to that of the RRe approach whose responses are all written by human. On the ranking metric which jointly evaluates the content quality and the style expression, the proposed model outperforms other approaches by a substantial margin.", "To optimize the model, we use the Adam optimizer BIBREF37 with a batch size of 64 and learning rate of 2e-5. During inference, the retrieval system is built from the training corpus, and the retrieved responses are selected using the Jaccard similarity BIBREF38 between queries."]}
{"question_id": "fcb0ac1934e2fd9f58f4b459e6853999a27844f9", "predicted_answer": "The text does not explicitly mention the two languages experimented on. It appears to be discussing a language generation model that can produce responses in different styles, but it does not specify the languages being used.", "predicted_evidence": ["The aforementioned approaches explicitly incorporate the language style information into the model configuration either via embeddings or memory modules to control the process of response generation. In our replication experiments, we found that these approaches tend to overemphasise the importance of the language style. As a result, the generated responses tend to be generic and non-informative BIBREF17, but they do express a distinct style; e.g., they generate a generic response: \u201cI am happy to hear that.\" that conveys a `happy' emotion to different queries.", "In the ablated model, we examine how the retrieved prototype effects our model's performance. To this end, we remove the response prototype from the input representation.", "Model proposed by BIBREF11 which uses memory modules to control the stylistic expressions in the generated responses.", "where $p(x, s)$ is the frequency that the word $x$ appears in a response with style $s$ in the training corpus. And a word $x$ is stylistic given the style $s$ if $\\textup {PMI}(x,s)\\ge t_s$. In our experiments, we empirically set $t_s$ as $t_s = \\frac{3}{4}\\times \\max _{v\\in \\mathcal {V}}\\textup {PMI}(v; s)$, where $\\mathcal {V}$ is the vocabulary set of the training corpus. Given the set of all possible language styles $\\mathcal {S}$, the stylistic vocabulary $\\mathcal {SV}$ is defined as all words that express any style $s\\in \\mathcal {S}$. An example is provided in Figure FIGREF2 where the prototype: \u201cThat's _ .", "where $p(x, s)$ is the frequency that the word $x$ appears in a response with style $s$ in the training corpus. And a word $x$ is stylistic given the style $s$ if $\\textup {PMI}(x,s)\\ge t_s$. In our experiments, we empirically set $t_s$ as $t_s = \\frac{3}{4}\\times \\max _{v\\in \\mathcal {V}}\\textup {PMI}(v; s)$, where $\\mathcal {V}$ is the vocabulary set of the training corpus. Given the set of all possible language styles $\\mathcal {S}$, the stylistic vocabulary $\\mathcal {SV}$ is defined as all words that express any style $s\\in \\mathcal {S}$. An example is provided in Figure FIGREF2 where the prototype: \u201cThat's _ . I will go with my _ together !\u201d is extracted from the retrieved response by masking the stylistic words great, bro and buddies."]}
{"question_id": "fc9aa04de4018b7d55e19a39663a2e9837328de7", "predicted_answer": "The three benchmark datasets used are:\n\n1. A gender-specific dataset\n2. An emotion-specific dataset\n3. (Not explicitly mentioned, but implied to be a third dataset, possibly a general or mixed dataset)\n\nNote that the text does not provide explicit names for the three datasets, but based on the context, we can infer that there are three datasets being used for evaluation.", "predicted_evidence": ["To fully evaluate the proposed approach, we conduct extensive experiments on three benchmark datasets. Results of both human and automatic evaluation show that the proposed approach significantly outperforms several strong baselines. In addition, we also conduct an extensive cross-domain experiment to demonstrate that the proposed approach is more robust than such baselines.", "Both human and automatic evaluation results on the three benchmark datasets are shown in Table TABREF25, TABREF26 and TABREF27. For each dataset, we present results on individual styles as well as the overall results. We observe that the proposed model achieves the top performance results on most of the metrics. It generates responses with both intense style and high response quality. In addition, we also measure the diversity of the generated responses with two automatic metrics: Distinct-1 and Distinct-2 BIBREF16. The results show that the proposed model achieves the closest performance to that of the RRe approach whose responses are all written by human. On the ranking metric which jointly evaluates the content quality and the style expression, the proposed model outperforms other approaches by a substantial margin.", "We compare the proposed approach with several competitive baselines that can be categorized into two classes: generative approaches and retrieval-based approaches.", "To optimize the model, we use the Adam optimizer BIBREF37 with a batch size of 64 and learning rate of 2e-5. During inference, the retrieval system is built from the training corpus, and the retrieved responses are selected using the Jaccard similarity BIBREF38 between queries.", "To evaluate the robustness of different approaches, we further analyze their performances when there is a notable difference between the data distribution of the training and testing set. Specifically, we use the models trained on gender-specific dataset to conduct inference on the test set of emotion-specific dataset and vise versa, which is regarded as domain variation. In Figure FIGREF50, we show the data distributions of these two datasets from which we can observe a notable distribution discrepancy. For evaluation, all results are evaluated with the same metrics as in the previous experiments. The averages response quality scores before and after domain variation are shown in Figure FIGREF55. For a direct comparison, the in-domain performance of each model can be found in Table TABREF25 and TABREF26."]}
{"question_id": "044cb5ef850c0a2073682bb31d919d504667f907", "predicted_answer": "Based on the context, versification refers to the study of the rhythmic and metrical patterns in poetry or drama, including features such as:\n\n* Frequencies of enjambment (the running on of a sentence or phrase from one line to the next without a pause)\n* Frequencies of particular types of unstressed line endings\n* Frequencies of contractions\n* Vocabulary richness\n* Phrase length measured by the number of words\n* Complex versification analysis\n\nIn other words, versification is the analysis of the sound, rhythm, and meter of language in written works, particularly in poetry and drama. It is used in this context to help identify the authorship of a play, along with linguistic features, by analyzing the patterns and characteristics of the language used.", "predicted_evidence": ["Since then many scholars have brought new evidence supporting Spedding\u2019s division of the play based both on versification and linguistic features. This includes e.g. frequencies of enjambment BIBREF4, frequencies of particular types of unstressed line endings BIBREF5, BIBREF6, frequencies of contractions BIBREF7, vocabulary richness BIBREF8, phrase length measured by the number of words BIBREF9, or complex versification analysis BIBREF10, BIBREF11. From the very beginning, beside advocates of Shakespeare\u2019s sole authorship (e.g.", "As shown in Table TABREF14, the versification-based models yield a very high accuracy with the recognition of Shakespeare and Fletcher (0.97 to 1 with the exception of Valentinian), yet slightly lower accuracy with the recognition of Massinger (0.81 to 0.88). The accuracy of words-based models remains very high across all three authors (0.95 to 1); in three cases it is nevertheless outperformed by the combined model. We thus may conclude that combined models provide a reliable discriminator between Shakespeare\u2019s, Fletcher\u2019s and Massinger\u2019s styles.", "Combined versification-based and word-based models trained on 17th century English drama yield a high accuracy of authorship recognition. We can thus state with high reliability that H8 is a result of collaboration between William Shakespeare and John Fletcher, while the participation of Philip Massinger is rather unlikely.", "For the sake of comparison of the attribution power of both feature subsets, cross-validations are performed not only of the combined models (500 words $\\cup $ 500 rhythmic types), but also of the words-based models (500 words) and versification-based models (500 rhythmic types) alone.", "Since then many scholars have brought new evidence supporting Spedding\u2019s division of the play based both on versification and linguistic features. This includes e.g. frequencies of enjambment BIBREF4, frequencies of particular types of unstressed line endings BIBREF5, BIBREF6, frequencies of contractions BIBREF7, vocabulary richness BIBREF8, phrase length measured by the number of words BIBREF9, or complex versification analysis BIBREF10, BIBREF11. From the very beginning, beside advocates of Shakespeare\u2019s sole authorship (e.g. BIBREF13, BIBREF14), there were also those who supported alternative hypotheses concerning mixed authorship of either Shakespeare, Fletcher, and Philip Massinger BIBREF15, BIBREF16, BIBREF17, Fletcher and Massinger only BIBREF18, BIBREF19, Shakespeare and an unknown author BIBREF20, Shakespeare, Fletcher, Massinger, and an unknown author BIBREF21, BIBREF22 or Shakespeare and Fletcher with different shares than those proposed by Spedding BIBREF23."]}
{"question_id": "c845110efee2f633d47f5682573bc6091e8f5023", "predicted_answer": "Based on the text, the conclusion about Shakespeare vs Fletcher is very confident, with an estimated accuracy of 0.9977 when distinguishing between the two authors. This means that out of 4412 groups of five lines, only 10 were misattributed, suggesting a high degree of reliability in the rolling attribution method used.", "predicted_evidence": ["Fig. FIGREF21 gives the results for each of the eight plays. Each data point corresponds to a group of five lines and gives the mean probability of Shakespeare\u2019s and Fletcher\u2019s authorship. For the sake of clarity, the values for Fletcher are displayed as negative. The distance between Shakespeare\u2019s data point and Fletcher\u2019s data point thus always equals 1. The black curve gives the average of both values. The results suggest the rolling attribution method with combined versification and lexical features to be very reliable: (1) Probability of Fletcher\u2019s authorship is very low for vast majority of Shakespeare\u2019s work. The only place where Fletcher is assigned higher probability than Shakespeare is the sequence of 10 five-line groups in the second act of scene 2 of the Tempest. (2) Probability of Shakespeare\u2019s authorship is very low for vast majority of Fletcher\u2019s work. The only place where Shakespeare comes closer to Fletcher\u2019s values is the first scene of act 5 of Bonduca.", "More recent articles usually fall in the last mentioned category and attribute the play to Shakespeare and Fletcher (although the shares proposed by them differ). Thomas Horton BIBREF24 employed discriminant analysis of three sets of function words and on this basis attributed most of the scenes to Shakespeare or left them undecided. Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare\u2019s and Fletcher\u2019s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27.", "Each data point corresponds to a group of five lines and gives the mean probability of Shakespeare\u2019s and Fletcher\u2019s authorship. For the sake of clarity, the values for Fletcher are displayed as negative. The distance between Shakespeare\u2019s data point and Fletcher\u2019s data point thus always equals 1. The black curve gives the average of both values. The results suggest the rolling attribution method with combined versification and lexical features to be very reliable: (1) Probability of Fletcher\u2019s authorship is very low for vast majority of Shakespeare\u2019s work. The only place where Fletcher is assigned higher probability than Shakespeare is the sequence of 10 five-line groups in the second act of scene 2 of the Tempest. (2) Probability of Shakespeare\u2019s authorship is very low for vast majority of Fletcher\u2019s work. The only place where Shakespeare comes closer to Fletcher\u2019s values is the first scene of act 5 of Bonduca. Having only 10 groups misattributed out of 4412 we may estimate the accuracy of rolling attribution to be as high as 0.9977 when distinguishing between Shakespeare and Fletcher.", "Fig. FIGREF21 gives the results for each of the eight plays. Each data point corresponds to a group of five lines and gives the mean probability of Shakespeare\u2019s and Fletcher\u2019s authorship. For the sake of clarity, the values for Fletcher are displayed as negative. The distance between Shakespeare\u2019s data point and Fletcher\u2019s data point thus always equals 1. The black curve gives the average of both values. The results suggest the rolling attribution method with combined versification and lexical features to be very reliable: (1) Probability of Fletcher\u2019s authorship is very low for vast majority of Shakespeare\u2019s work. The only place where Fletcher is assigned higher probability than Shakespeare is the sequence of 10 five-line groups in the second act of scene 2 of the Tempest. (2) Probability of Shakespeare\u2019s authorship is very low for vast majority of Fletcher\u2019s work.", "In the first collection of William Shakespeare\u2019s works published in 1623 (the so-called First Folio) a play appears entitled The Famous History of the Life of King Henry the Eight for the very first time. Nowadays it is widely recognized that along with Shakespeare, other authors were involved in the writing of this play, yet there are different opinions as to who these authors were and what the precise shares were of their authorial contributions. This article aims to contribute to the question of the play\u2019s authorship using combined analysis of vocabulary and versification and modern machine learning techniques (as proposed in BIBREF0, BIBREF1)."]}
{"question_id": "2301424672cb79297cf7ad95f23b58515e4acce8", "predicted_answer": "No, Henry VIII is not reflective of Shakespeare in general. The play's stylistic dissimilarity to Shakespeare's other plays has been pointed out, and many scholars have questioned Shakespeare's sole authorship of the play. The analysis of vocabulary, versification, and machine learning techniques suggests that the play is a result of collaboration between William Shakespeare and John Fletcher, with possible mixed authorship in some scenes. This implies that Henry VIII is an outlier in Shakespeare's oeuvre, and its characteristics are not representative of his typical style.", "predicted_evidence": ["In the first collection of William Shakespeare\u2019s works published in 1623 (the so-called First Folio) a play appears entitled The Famous History of the Life of King Henry the Eight for the very first time. Nowadays it is widely recognized that along with Shakespeare, other authors were involved in the writing of this play, yet there are different opinions as to who these authors were and what the precise shares were of their authorial contributions. This article aims to contribute to the question of the play\u2019s authorship using combined analysis of vocabulary and versification and modern machine learning techniques (as proposed in BIBREF0, BIBREF1).", "While the stylistic dissimilarity of Henry VIII (henceforth H8) to Shakespeare\u2019s other plays had been pointed out before BIBREF2, it was not until the mid-nineteenth century that Shakespeare\u2019s sole authorship was called into question. In 1850 British scholar James Spedding published an article BIBREF3 attributing several scenes to John Fletcher. Spedding supported this with data from the domain of versification, namely the ratios of iambic lines ending with a stressed syllable (\u201cThe view of earthly glory: men might say\u201d) to lines ending with an extra unstressed one (\u201cTill this time pomp was single, but now married\u201d), pointing out that the distribution of values across scenes is strongly bimodal.", "Combined versification-based and word-based models trained on 17th century English drama yield a high accuracy of authorship recognition. We can thus state with high reliability that H8 is a result of collaboration between William Shakespeare and John Fletcher, while the participation of Philip Massinger is rather unlikely.", "Scene 3.2 is usually attributed to both Shakespeare and Fletcher. All three sets of models support this. While Spedding and other authors locate the shift to line 2081, all our sets of models indicate that it occurred later. Combined models locate it precisely at line 2200 (in agreement with earlier studies by Merriam BIBREF25, BIBREF26. A certain decrease in the probability of Shakespeare\u2019s authorship found in the neighborhood of line 2081 in word-based models and combined models may support Merriam\u2019s later attributions BIBREF27, i.e. mixed authorship even after the line 2081.", "Since then many scholars have brought new evidence supporting Spedding\u2019s division of the play based both on versification and linguistic features. This includes e.g. frequencies of enjambment BIBREF4, frequencies of particular types of unstressed line endings BIBREF5, BIBREF6, frequencies of contractions BIBREF7, vocabulary richness BIBREF8, phrase length measured by the number of words BIBREF9, or complex versification analysis BIBREF10, BIBREF11. From the very beginning, beside advocates of Shakespeare\u2019s sole authorship (e.g."]}
{"question_id": "6c05376cd0f011e00d1ada0254f6db808f33c3b7", "predicted_answer": "Based on the provided context, it appears that both vocabulary and versification are important for the analysis, but versification-based models seem to yield higher accuracy in recognizing the styles of Shakespeare, Fletcher, and Massinger. The text states that \"the versification-based models yield a very high accuracy with the recognition of Shakespeare and Fletcher (0.97 to 1 with the exception of Valentinian)\" and that \"combined models provide a reliable discriminator between Shakespeare\u2019s, Fletcher\u2019s and Massinger\u2019s styles.\" This suggests that versification may be more important for the analysis, but vocabulary is still a significant factor, as the accuracy of words-based models remains high across all three authors.", "predicted_evidence": ["Since then many scholars have brought new evidence supporting Spedding\u2019s division of the play based both on versification and linguistic features. This includes e.g. frequencies of enjambment BIBREF4, frequencies of particular types of unstressed line endings BIBREF5, BIBREF6, frequencies of contractions BIBREF7, vocabulary richness BIBREF8, phrase length measured by the number of words BIBREF9, or complex versification analysis BIBREF10, BIBREF11. From the very beginning, beside advocates of Shakespeare\u2019s sole authorship (e.g.", "Thomas Horton BIBREF24 employed discriminant analysis of three sets of function words and on this basis attributed most of the scenes to Shakespeare or left them undecided. Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare\u2019s and Fletcher\u2019s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27. Eisen, Riberio, Segarra, and Egan BIBREF28 used Word adjacency networks BIBREF29 to analyze the frequencies of collocations of selected function words in particular scenes of the play. In contrast to Spedding, they reattribute several scenes back to Shakespeare.", "As shown in Table TABREF14, the versification-based models yield a very high accuracy with the recognition of Shakespeare and Fletcher (0.97 to 1 with the exception of Valentinian), yet slightly lower accuracy with the recognition of Massinger (0.81 to 0.88). The accuracy of words-based models remains very high across all three authors (0.95 to 1); in three cases it is nevertheless outperformed by the combined model. We thus may conclude that combined models provide a reliable discriminator between Shakespeare\u2019s, Fletcher\u2019s and Massinger\u2019s styles.", "More recent articles usually fall in the last mentioned category and attribute the play to Shakespeare and Fletcher (although the shares proposed by them differ). Thomas Horton BIBREF24 employed discriminant analysis of three sets of function words and on this basis attributed most of the scenes to Shakespeare or left them undecided. Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare\u2019s and Fletcher\u2019s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27.", "More recent articles usually fall in the last mentioned category and attribute the play to Shakespeare and Fletcher (although the shares proposed by them differ). Thomas Horton BIBREF24 employed discriminant analysis of three sets of function words and on this basis attributed most of the scenes to Shakespeare or left them undecided. Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare\u2019s and Fletcher\u2019s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27. Eisen, Riberio, Segarra, and Egan BIBREF28 used Word adjacency networks BIBREF29 to analyze the frequencies of collocations of selected function words in particular scenes of the play."]}
{"question_id": "9925e7d8757e8fd7411bcb5250bc08158a244fb3", "predicted_answer": "Thomas Merriam proposed a modification to Spedding's original attribution concerning re-attribution of several parts of supposedly Fletcher's scenes back to Shakespeare and vice versa. This modification was based on:\n\n1. Measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare's and Fletcher's plays.\n2. Controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable.\n3. Principal component analysis of 64 most frequent words.", "predicted_evidence": ["In the present study, with regard to the aforementioned studies, Shakespeare, Fletcher, and Massinger are considered as candidates to the authorship of H8.", "While the stylistic dissimilarity of Henry VIII (henceforth H8) to Shakespeare\u2019s other plays had been pointed out before BIBREF2, it was not until the mid-nineteenth century that Shakespeare\u2019s sole authorship was called into question. In 1850 British scholar James Spedding published an article BIBREF3 attributing several scenes to John Fletcher. Spedding supported this with data from the domain of versification, namely the ratios of iambic lines ending with a stressed syllable (\u201cThe view of earthly glory: men might say\u201d) to lines ending with an extra unstressed one (\u201cTill this time pomp was single, but now married\u201d), pointing out that the distribution of values across scenes is strongly bimodal.", "For scenes 4.1 and 4.2 the rhythmic types indicate Shakespeare\u2019s authorship of the first (contrary to Spedding) and Fletcher\u2019s authorship of the latter. Location of the shift does not however fully correspond to the scene boundaries. Probabilities extracted from word-based models and combined models are close to 0.5 for both authors which may support Merriam\u2019s attribution (mixed authorship).", "Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare\u2019s and Fletcher\u2019s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27. Eisen, Riberio, Segarra, and Egan BIBREF28 used Word adjacency networks BIBREF29 to analyze the frequencies of collocations of selected function words in particular scenes of the play. In contrast to Spedding, they reattribute several scenes back to Shakespeare. Details on Spedding\u2019s attribution as well as the ones mentioned in this paragraph are given in Table TABREF3.", "In the first collection of William Shakespeare\u2019s works published in 1623 (the so-called First Folio) a play appears entitled The Famous History of the Life of King Henry the Eight for the very first time. Nowadays it is widely recognized that along with Shakespeare, other authors were involved in the writing of this play, yet there are different opinions as to who these authors were and what the precise shares were of their authorial contributions. This article aims to contribute to the question of the play\u2019s authorship using combined analysis of vocabulary and versification and modern machine learning techniques (as proposed in BIBREF0, BIBREF1)."]}
{"question_id": "fa468c31dd0f9095d7cec010f2262eeed565a7d2", "predicted_answer": "In the context of Shakespeare's plays, \"stop words\" are not explicitly mentioned. However, the text refers to \"function words\" and \"most frequent words\" which are related concepts.\n\nIn linguistics, \"stop words\" are common words that do not carry much meaning in a sentence, such as \"the\", \"and\", \"a\", \"is\", etc. They are often ignored in text analysis and information retrieval because they do not provide significant information about the content of the text.\n\nIn the context of the given text, \"function words\" seem to refer to a similar concept. Function words are words that have little lexical meaning but serve a grammatical function, such as prepositions, conjunctions, and auxiliary verbs. They are often used to analyze the style and authorship of texts, as they can be indicative of an author's writing habits.\n\nThe text also mentions \"most frequent words\", which are likely to include both content words (words that carry meaning) and function words. The analysis of these words is used to attribute authorship of scenes in a play to Shakespeare or Fletcher.", "predicted_evidence": ["More recent articles usually fall in the last mentioned category and attribute the play to Shakespeare and Fletcher (although the shares proposed by them differ). Thomas Horton BIBREF24 employed discriminant analysis of three sets of function words and on this basis attributed most of the scenes to Shakespeare or left them undecided. Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare\u2019s and Fletcher\u2019s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27.", "Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare\u2019s and Fletcher\u2019s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27. Eisen, Riberio, Segarra, and Egan BIBREF28 used Word adjacency networks BIBREF29 to analyze the frequencies of collocations of selected function words in particular scenes of the play. In contrast to Spedding, they reattribute several scenes back to Shakespeare. Details on Spedding\u2019s attribution as well as the ones mentioned in this paragraph are given in Table TABREF3.", "More recent articles usually fall in the last mentioned category and attribute the play to Shakespeare and Fletcher (although the shares proposed by them differ). Thomas Horton BIBREF24 employed discriminant analysis of three sets of function words and on this basis attributed most of the scenes to Shakespeare or left them undecided. Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare\u2019s and Fletcher\u2019s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27. Eisen, Riberio, Segarra, and Egan BIBREF28 used Word adjacency networks BIBREF29 to analyze the frequencies of collocations of selected function words in particular scenes of the play.", "Thomas Horton BIBREF24 employed discriminant analysis of three sets of function words and on this basis attributed most of the scenes to Shakespeare or left them undecided. Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare\u2019s and Fletcher\u2019s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27. Eisen, Riberio, Segarra, and Egan BIBREF28 used Word adjacency networks BIBREF29 to analyze the frequencies of collocations of selected function words in particular scenes of the play. In contrast to Spedding, they reattribute several scenes back to Shakespeare.", "Scene 5.1 is according to all sets of models authored by Shakespeare. Rhythmic types and combined models locate the shift at its end; word-based models locate it a little later on."]}
{"question_id": "8c89f1d1b3c2a45c0254c4c8d6e700ab9a4b4ffb", "predicted_answer": "According to the text, the following sources of less sensitive data are available:\n\n1. Data from deceased persons, which can be disclosed under certain conditions without the need for IRB agreement and sanitization, as per the HIPAA privacy rule.\n2. Data made publicly available by individuals through open consent initiatives, such as the Personal Genome Project.\n3. Sanitized data, which is processed in accordance with the GDPR's principles of data minimization and transparency.\n\nThese sources of less sensitive data can provide alternatives to using sensitive clinical data, which are subject to stricter regulations and require more stringent measures to ensure confidentiality and privacy.", "predicted_evidence": ["paragraph4 0.9ex plus1ex minus.2ex-1em Are there less sensitive data? One criterion which may have influence on data accessibility is whether the data is about living subjects or not. The HIPAA privacy rule under certain conditions allows disclosure of personal health information of deceased persons, without the need to seek IRB agreement and without the need for sanitization BIBREF39 . It is not entirely clear though how often this possibility has been used in clinical NLP research or broader.", "The GDPR lists general principles relating to processing of personal data, including that processing must be lawful (e.g. by means of consent), fair and transparent; it must be done for explicit and legitimate purposes; and the data should be kept limited to what is necessary and as long as necessary. This is known as data minimization, and it includes sanitization. The scientific usage of health data concerns \u201cspecial categories of personal data\". Their processing is only allowed when the data subject gives explicit consent, or the personal data is made public by the data subject. Scientific usage is defined broadly and includes technological development, fundamental and applied research, as well as privately funded research.", "Yet another possibility is open consent, in which individuals make their data publicly available. Initiatives like Personal Genome Project may have an exemplary role, however, they can only provide limited data and they represent a biased population sample BIBREF33 .", "paragraph4 0.9ex plus1ex minus.2ex-1em Secure access Since withholding data from researchers would be a dubious way of ensuring confidentiality BIBREF21 , the research has long been active on secure access and storage of sensitive clinical data, and the balance between the degree of privacy loss and the degree of utility. This is a broad topic that is outside the scope of this article. The interested reader can find the relevant information in Dwork and Pottenger DworkAndPottenger2013, Malin et al. MalinEtAL2013 and Rindfleisch Rindfleisch1997.", "Clinical notes contain detailed information about patient-clinician encounters in which patients confide not only their health complaints, but also their lifestyle choices and possibly stigmatizing conditions. This confidential relationship is legally protected in US by the HIPAA privacy rule in the case of individuals' medical data. In EU, the conditions for scientific usage of health data are set out in the General Data Protection Regulation (GDPR). Sanitization of sensitive data categories and individuals' informed consent are in the forefront of those legislative acts and bear immediate consequences for the NLP research."]}
{"question_id": "f5bc07df5c61dcb589a848bd36f4ce9c22abd46a", "predicted_answer": "According to the text, the other major ethical challenges in clinical data, aside from privacy, are:\n\n1. **Social impact**: The text mentions that clinical notes and their processing can have social implications, although it does not elaborate on what these implications might be.\n2. **Biases**: The text notes that clinical notes and their processing can also lead to biases, although it does not specify what kind of biases or how they might arise.\n\nThese ethical challenges are not unique to clinical NLP and are also relevant to general data science.", "predicted_evidence": ["Clinical notes contain detailed information about patient-clinician encounters in which patients confide not only their health complaints, but also their lifestyle choices and possibly stigmatizing conditions. This confidential relationship is legally protected in US by the HIPAA privacy rule in the case of individuals' medical data. In EU, the conditions for scientific usage of health data are set out in the General Data Protection Regulation (GDPR). Sanitization of sensitive data categories and individuals' informed consent are in the forefront of those legislative acts and bear immediate consequences for the NLP research.", "The alternative might be a default opt-in policy with a right to withdraw (opt-out). Here, consent can be presumed either in a broad manner\u2014allowing unspecified future research, subject to ethical restrictions\u2014or a tiered manner\u2014allowing certain areas of research but not others BIBREF33 , BIBREF36 . Since the information about the intended use is no longer uniquely tied to each research case but is more general, this could facilitate the reuse of datasets by several research teams, without the need to ask for consent each time. The success of implementing this approach in practice is likely to depend on public trust and awareness about possible risks and opportunities. We also believe that a distinction between academic research and commercial use of clinical data should be implemented, as the public is more willing to allow research than commercial exploitation BIBREF37 , BIBREF38 .", "The ethics discussion is gaining momentum in general NLP BIBREF8 . We aim in this paper to gather the ethical challenges that are especially relevant for clinical NLP, and to stimulate discussion about those in the broader NLP community. Although enhancing privacy through restricted data access has been the norm, we do not only discuss the right to privacy, but also draw attention to the social impact and biases emanating from clinical notes and their processing. The challenges we describe here are in large part not unique to clinical NLP, and are applicable to general data science as well.", "paragraph4 0.9ex plus1ex minus.2ex-1em Are there less sensitive data? One criterion which may have influence on data accessibility is whether the data is about living subjects or not. The HIPAA privacy rule under certain conditions allows disclosure of personal health information of deceased persons, without the need to seek IRB agreement and without the need for sanitization BIBREF39 . It is not entirely clear though how often this possibility has been used in clinical NLP research or broader.", "The GDPR lists general principles relating to processing of personal data, including that processing must be lawful (e.g. by means of consent), fair and transparent; it must be done for explicit and legitimate purposes; and the data should be kept limited to what is necessary and as long as necessary. This is known as data minimization, and it includes sanitization. The scientific usage of health data concerns \u201cspecial categories of personal data\". Their processing is only allowed when the data subject gives explicit consent, or the personal data is made public by the data subject. Scientific usage is defined broadly and includes technological development, fundamental and applied research, as well as privately funded research."]}
{"question_id": "8126c6b8a0cab3e22661d3d71d96aa57360da65c", "predicted_answer": "The evaluation metrics used are:\n\n1. Rouge-1 (uni-gram recall on reference headings)\n2. EM (Exact Matching, measures overall accuracy of generated outline based on exact matching of predicted section boundaries and generated section headings with ground-truth)", "predicted_evidence": ["To measure the quality of outline generated by our model and the baselines, we employ three automatic metrics, namely", "In this section, we conduct experiments to verify the effectiveness of our proposed model.", "Rouge INLINEFORM0 evaluates the similarities between generated headings and referenced headings only for the correctly predicted sections. Specifically, we employ Rouge-1 BIBREF39 to measure the uni-gram recall on the reference headings.", "EM INLINEFORM0 : evaluates the overall accuracy of the generated outline based on exact matching. That is, if both the predicted section boundaries and the generated section headings in a document exactly match with the ground-truth, we treat the document as a positive sample. Otherwise the document is a negative sample.", "The overall performance comparisons between our HiStGen and the step-wise baselines are shown in Table TABREF61 . We have the following observations: (1) The INLINEFORM0 process (i.e., INLINEFORM1 , INLINEFORM2 , INLINEFORM3 and INLINEFORM4 ) performs very poorly. By looking at the results of the INLINEFORM5 methods, we find that INLINEFORM6 tends to segment the document into too much sections since it usually generates different headings even for paragraphs that should belong to a same section. (2) For the INLINEFORM7 process, the methods based on INLINEFORM8 perform better than that based on INLINEFORM9 . For example, the relative improvement of INLINEFORM10 over INLINEFORM11 is about INLINEFORM12 in terms of EM INLINEFORM13 on the mixture set. We analyze the results and find that using INLINEFORM14 can obtain better section prediction results, showing that the dependency on the context labels is more important than that on all the paragraphs for section identification."]}
{"question_id": "2f01d3e5120d1fef4b01028536cb5fe0abad1968", "predicted_answer": "According to the text, the state-of-the-art models compared with are:\n\n1. INLINEFORM0 (baseline model)\n2. INLINEFORM1 (CRF-based model)\n3. INLINEFORM3 (model that generates repetitive headings and inconsistent styles)\n\nNote that the exact names of these models are not provided in the text, but they are referred to by their INLINEFORM numbers.", "predicted_evidence": ["In this section, we conduct experiments to verify the effectiveness of our proposed model.", "We run our model on a Tesla K80 GPU card, and we run the training for up to 12 epochs, which takes approximately two days. We select the model that achieves the lowest perplexity on the development set, and report results on the test set.", "To better understand how different models perform, we conduct some case studies. We take one Wikipedia article from the \u201ccelebrity\u201d test data as an example. As shown in Figure FIGREF62 , there are 15 paragraphs in this article, which are segmented into 7 sections. We show the identified sections and generated headings from our model as well as that from the baseline model INLINEFORM0 . We can find that: (1) The number of sections predicted by INLINEFORM1 is larger than the ground-truth (i.e., INLINEFORM2 ) and the segmentation is totally wrong. The results show that using current paragraph representation and context label dependency, CRF may not be able to make correct section boundary prediction.", "To better understand how different models perform, we conduct some case studies. We take one Wikipedia article from the \u201ccelebrity\u201d test data as an example. As shown in Figure FIGREF62 , there are 15 paragraphs in this article, which are segmented into 7 sections. We show the identified sections and generated headings from our model as well as that from the baseline model INLINEFORM0 . We can find that: (1) The number of sections predicted by INLINEFORM1 is larger than the ground-truth (i.e., INLINEFORM2 ) and the segmentation is totally wrong. The results show that using current paragraph representation and context label dependency, CRF may not be able to make correct section boundary prediction. (2) Without considering the coherence between context headings, INLINEFORM3 generates repetitive headings (e.g., \u201ccareer\u201d repeats twice) and the heading with inconsistent style (e.g., \u201ccitizen political\u201d is not suitable for the description of a celebrity).", "For evaluation, we compare with several state-of-the-art methods to verify the effectiveness of our model. Empirical results demonstrate that outline generation for capturing the inherent content structure is feasible and our proposed method can outperform all the baselines significantly. We also provide detailed analysis on the proposed model, and conduct case studies to provide better understanding on the learned content structure."]}
{"question_id": "b78bb6fe817c2d4bc69236df998f546e94c3ee21", "predicted_answer": "Yes, the performance improvement (with and without affect attributes) is statistically significant. The p-values for the multivariate results and follow-up ANOVAs are all less than 0.0001, indicating that the differences in performance are statistically significant. Additionally, the post-hoc Tukey tests also revealed significant differences in grammatical correctness at p < 0.05.", "predicted_evidence": ["Positive Emotion Sentences. The multivariate result was significant for positive emotion generated sentences (Pillai's Trace $=$ .327, F(4,437) $=$ 6.44, p $<$ .0001). Follow up ANOVAs revealed significant results for all DVs except angry with p $<$ .0001, indicating that both affective valence and happy DVs were successfully manipulated with $\\beta $ , as seen in Figure 2 (a). Grammatical correctness was also significantly influenced by the affect strength parameter $\\beta $ and results show that the correctness deteriorates with increasing $\\beta $ (see Figure 3 ). However, a post-hoc Tukey test revealed that only the highest $\\beta $ value shows a significant drop in grammatical correctness at p $<$ .05.", "For each target emotion (i.e., intended emotion of generated sentences) we conducted an initial MANOVA, with human ratings of affect categories the DVs (dependent variables) and the affect strength parameter $\\beta $ the IV (independent variable). We then conducted follow-up univariate ANOVAs to identify which DV changes significantly with $\\beta $ . In total we conducted 5 MANOVAs and 30 follow-up ANOVAs, which required us to update the significance level to p $<$ 0.001 following a Bonferroni correction.", "Anxious Sentences. The multivariate result was significant for anxious generated sentences (Pillai's Trace $=$ .289, F(4,421) $=$ 6.44, p $<$ .0001). Follow up ANOVAs revealed significant results for affective valence, happy and anxious DVs with p $<$ .0001, indicating that both affective valence and anxiety DVs were successfully manipulated with $\\beta $ , as seen in Figure 2 (e). Grammatical correctness was also significantly influenced by the affect strength parameter $\\beta $ and results show that the correctness deteriorates with increasing $\\beta $ . Similarly for sad, a post-hoc Tukey test revealed that only the two highest $\\beta $ values show a significant drop in grammatical correctness at p $<$ .05 (see Figure 3 ).", "Anxious Sentences. The multivariate result was significant for anxious generated sentences (Pillai's Trace $=$ .289, F(4,421) $=$ 6.44, p $<$ .0001). Follow up ANOVAs revealed significant results for affective valence, happy and anxious DVs with p $<$ .0001, indicating that both affective valence and anxiety DVs were successfully manipulated with $\\beta $ , as seen in Figure 2 (e). Grammatical correctness was also significantly influenced by the affect strength parameter $\\beta $ and results show that the correctness deteriorates with increasing $\\beta $ . Similarly for sad, a post-hoc Tukey test revealed that only the two highest $\\beta $ values show a significant drop in grammatical correctness at p $<$ .05 (see Figure 3 ). Again, a post-hoc Tukey test for anxious reveals that $\\beta =3$ is optimal for this DV, since it leads to a", "Angry Sentences. The multivariate result was significant for angry generated sentences (Pillai's Trace $=$ .199, F(4,433) $=$ 3.76, p $<$ .0001). Follow up ANOVAs revealed significant results for affective valence, happy, and angry DVs with p $<$ .0001, indicating that both affective valence and angry DVs were successfully manipulated with $\\beta $ , as seen in Figure 2 (c). Grammatical correctness was not significantly influenced by the affect strength parameter $\\beta $ , which indicates that angry sentences are highly stable across a wide range of $\\beta $ (see Figure 3 ). However, it seems that human raters could not successfully distinguish between angry, sad, and anxious affect categories, indicating that the generated sentences likely follow a general negative affect dimension."]}
{"question_id": "1a419468d255d40ae82ed7777618072a48f0091b", "predicted_answer": "To extract affect attributes from a sentence, you can use the following steps:\n\n1. **Identify emotionally colored words**: Look for words that convey emotions, such as \"furious\", \"joy\", \"angry\", \"sad\", \"anxious\", etc.\n2. **Detect affect categories**: Determine the affect category of the sentence based on the emotionally colored words present. The affect categories mentioned in the context are:\n\t* Positive emotion\n\t* Anger\n\t* Sad\n\t* Anxiety\n\t* Negative emotion (superclass of anger, sad, and anxiety)\n3. **Infer affect strength**: Analyze the sentence to determine the degree of dominance of the affect-dependent energy term on the word prediction. This can be done by measuring the frequency of emotionally colored words and their impact on the overall sentence meaning.\n4. **Use LIWC or input emotion descriptor**: If the sentence beginning is emotionally colored, use LIWC (Linguistic Inquiry and Word Count) to infer the affect category. Alternatively, set an input emotion descriptor (a binary vector encoding the desired emotion) to specify the affect category.\n5. **Analyze sentence structure and grammatical correctness**: Evaluate the sentence's grammatical structure and correctness to determine how \"emotionally colored\" the sentence is. A higher affect strength (\u03b2) would result in sentences with more emotionally colored words, but potentially less grammatical structure.\n\nBy following these steps, you can extract affect attributes from a sentence, including the affect category, affect strength, and the degree of emotional coloring.", "predicted_evidence": ["Q1:Can Affect-LM be used to generate affective sentences for a target emotion with varying degrees of affect strength through a customizable model parameter?", "As described in Section \"Conclusions and Future Work\" , the affect strength $\\beta $ defines the degree of dominance of the affect-dependent energy term on the word prediction in the language model, consequently after model training we can change $\\beta $ to control the degree of how \u201cemotionally colored\" a generated utterance is, varying from $\\beta =0$ (neutral; baseline model) to $\\beta =\\infty $ (the generated sentences only consist of emotionally colored words, with no grammatical structure). When Affect-LM is used for generation, the affect categories could be either (1) inferred from the context using LIWC (this occurs when we provide sentence beginnings which are emotionally colored themselves), or (2) set to an input emotion descriptor $\\mathbf {e}$ (this is obtained by setting $\\mathbf {e}$ to a binary vector encoding the desired emotion and works even for neutral sentence beginnings). Given an initial starting set of $M$ words $w_1,w_2,...,w_M$ to complete, affect strength $\\beta $ , and the number of words $\\beta $0 to generate each $\\beta $1 -th generated word is obtained by sampling from $\\beta $2 for $\\beta $3 .", "Affect is a term that subsumes emotion and longer term constructs such as mood and personality and refers to the experience of feeling or emotion BIBREF0 . BIBREF1 picard1997affective provides a detailed discussion of the importance of affect analysis in human communication and interaction. Within this context the analysis of human affect from text is an important topic in natural language understanding, examples of which include sentiment analysis from Twitter BIBREF2 , affect analysis from poetry BIBREF3 and studies of correlation between function words and social/psychological processes BIBREF4 . People exchange verbal messages which not only contain syntactic information, but also information conveying their mental and emotional states. Examples include the use of emotionally colored words (such as furious and joy) and swear words. The automated processing of affect in human verbal communication is of great importance to understanding spoken language systems, particularly for emerging applications such as dialogue systems and conversational agents.", "The parameter $\\beta $ defined in Equation 7 , which we call the affect strength defines the influence of the affect category information (frequency of emotionally colored words) on the overall prediction of the target word $w_t$ given its context. We can consider the formulation as an energy based model (EBM), where the additional energy term captures the degree of correlation between the predicted word and the affective input BIBREF13 .", "Affect-LM can be used to generate sentences conditioned on the input affect category, the affect strength $\\beta $ , and the context words. For our experiments, we have chosen the following affect categories - positive emotion, anger, sad, anxiety, and negative emotion (which is a superclass of anger, sad and anxiety). As described in Section \"Conclusions and Future Work\" , the affect strength $\\beta $ defines the degree of dominance of the affect-dependent energy term on the word prediction in the language model, consequently after model training we can change $\\beta $ to control the degree of how \u201cemotionally colored\" a generated utterance is, varying from $\\beta =0$ (neutral; baseline model) to $\\beta =\\infty $ (the generated sentences only consist of emotionally colored words, with no grammatical structure). When Affect-LM is used for generation, the affect categories could be either (1) inferred from the context using LIWC (this occurs when we provide sentence beginnings which are emotionally colored themselves), or (2) set to an input emotion descriptor $\\mathbf {e}$ (this is obtained by setting $\\mathbf {e}$ to a binary vector encoding the desired emotion and works even for neutral sentence beginnings)."]}
{"question_id": "52f5249a9a2cb7210eeb8e52cb29d18912f6c3aa", "predicted_answer": "The neural network has multiple layers, but the exact number is not specified. However, we can identify the following layers:\n\n1. Contextualized embedding layer (using BERT)\n2. Multiple classification layers $L_{g_k}$, one for each task of increasing granularity (e.g., document-level, paragraph-level, sentence-level, etc.)\n3. Projection layer (part of the gate $f$)\n4. Activation function (part of the gate $f$)\n\nAdditionally, there are multiple granularity tasks, each with its own output layer, but these are not necessarily separate layers in the neural network architecture.", "predicted_evidence": ["For the loss function, we use a cross-entropy loss with sigmoid activation for every layer, except for the highest-granularity layer $L_{g_K}$, which uses a cross-entropy loss with softmax activation. Unlike softmax, which normalizes over all dimensions, the sigmoid allows each output component of layer $L_{g_k}$ to be independent from the rest. Thus, the output of the sigmoid for the positive class increases the degree of freedom by not affecting the negative class, and vice versa. As we have two tasks, we use sigmoid activation for $L_{g_1}$ and softmax activation for $L_{g_2}$. Moreover, we use a weighted sum of losses with a hyper-parameter $\\alpha $:", "More generally, suppose there are $k$ tasks of increasing granularity, e.g., document-level, paragraph-level, sentence-level, word-level, subword-level, character-level. Each task has a separate classification layer $L_{g_k}$ that receives the feature representation of the specific level of granularity $g_k$ and outputs $o_{g_k}$. The dimension of the representation depends on the embedding layer, while the dimension of the output depends on the number of classes in the task. The output $o_{g_k}$ is used to generate a weight for the next granularity task $g_{k+1}$ through a trainable gate $f$:", "Again, we use BERT BIBREF12 for the contextualized embedding layer and we place the multi-granularity network on top of it.", "The gate $f$ consists of a projection layer to one dimension and an activation function. The resulting weight is multiplied by each element of the output of layer $L_{g_{k+1}}$ to produce the output for task $g_{k+1}$:", "Multi-Granularity Network. We propose a model that can drive the higher-granularity task (FLC) on the basis of the lower-granularity information (SLC), rather than simply using low-granularity information directly. Figure FIGREF7-d shows the architecture of this model."]}
{"question_id": "baad4b6f834d5944f61bd12f30908e3cf3739dcd", "predicted_answer": "They compare their model to two BERT-based baselines: \n\n1. BERT-Joint: This model uses the layers for both tasks in the BERT baseline and trains for both FLC and SLC jointly.\n\n2. BERT: This model adds a linear layer on top of BERT and fine-tunes it, as suggested in BIBREF12.", "predicted_evidence": ["We depart from BERT BIBREF12, and we design three baselines.", "We experimented with a number of BERT-based models and devised a novel architecture which outperforms standard BERT-based baselines. Our fine-grained task can complement document-level judgments, both to come out with an aggregated decision and to explain why a document \u2014or an entire news outlet\u2014 has been flagged as potentially propagandistic by an automatic system.", "BERT-Joint. We use the layers for both tasks in the BERT baseline, $L_{g_1}$ and $L_{g_2}$, and we train for both FLC and SLC jointly (cf. Figure FIGREF7-b).", "BERT. We add a linear layer on top of BERT and we fine-tune it, as suggested in BIBREF12. For the FLC task, we feed the final hidden representation for each token to a layer $L_{g_2}$ that makes a 19-way classification: does this token belong to one of the eighteen propaganda techniques or to none of them (cf. Figure FIGREF7-a). For the SLC task, we feed the final hidden representation for the special [CLS] token, which BERT uses to represent the full sentence, to a two-dimensional layer $L_{g_1}$ to make a binary classification.", "The right side of Table TABREF12 shows the results for the SLC task. We apply our multi-granularity network model to the sentence-level classification task to see its effect on low granularity when we train the model with a high granularity task. Interestingly, it yields huge performance improvements on the sentence-level classification result. Compared to the BERT baseline, it increases the recall by 8.42%, resulting in a 3.24% increase of the F$_1$ score. In this case, the result of token-level classification is used as additional information for the sentence-level task, and it helps to find more positive samples. This shows the opposite effect of our model compared to the FLC task."]}
{"question_id": "37b972a3afae04193411dc569f672d802c16ad71", "predicted_answer": "The text does not explicitly list the types of propaganda techniques. However, it mentions that the authors plan to extend their tool to support other propaganda techniques in future work, implying that there are multiple types of propaganda techniques. The authors also propose a novel task of identifying specific instances of propaganda techniques used within an article, suggesting that there are various techniques that can be identified and studied.", "predicted_evidence": ["We have argued for a new way to study propaganda in news media: by focusing on identifying the instances of use of specific propaganda techniques. Going at this fine-grained level can yield more reliable systems and it also makes it possible to explain to the user why an article was judged as propagandistic by an automatic system.", "In future work, we plan to include more media sources, especially from non-English-speaking media and regions. We further want to extend the tool to support other propaganda techniques.", "We argue that in order to study propaganda in a sound and reliable way, we need to rely on high-quality trusted professional annotations and it is best to do so at the fragment level, targeting specific techniques rather than using a label for an entire document or an entire news outlet. Therefore, we propose a novel task: identifying specific instances of propaganda techniques used within an article. In particular, we design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines.", "Journalistic organisations, such as Media Bias/Fact Check, provide reports on news sources highlighting the ones that are propagandistic. Obviously, such analysis is time-consuming and possibly biased and it cannot be applied to the enormous amount of news that flood social media and the Internet. Research on detecting propaganda has focused primarily on classifying entire articles as propagandistic/non-propagandistic BIBREF0, BIBREF1, BIBREF2. Such learning systems are trained using gold labels obtained by transferring the label of the media source, as per Media Bias/Fact Check judgment, to each of its articles. Such distant supervision setting inevitably introduces noise in the learning process BIBREF3 and the resulting systems tend to lack explainability.", "This research is part of the Propaganda Analysis Project, which is framed within the Tanbih project. The Tanbih project aims to limit the effect of \u201cfake news\u201d, propaganda, and media bias by making users aware of what they are reading, thus promoting media literacy and critical thinking. The project is developed in collaboration between the Qatar Computing Research Institute (QCRI), HBKU and the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL)."]}
{"question_id": "a01af34c7f630ba0e79e0a0120d2e1c92d022df5", "predicted_answer": "The answer is no, they do not look at various languages in this research. The text does not mention anything about analyzing multiple languages, and the context suggests that the research is focused on English language articles. However, it is mentioned in the future work section that they plan to include more media sources, especially from non-English-speaking media and regions, which implies that they may consider analyzing other languages in the future.", "predicted_evidence": ["The total number of technique instances found in the articles, after the consolidation phase, is $7,485$, out of a total number of $21,230$ sentences (35.2%). The distribution of the techniques in the corpus is also uneven: while there are $2,547$ occurrences of loaded language, there are only 15 instances of straw man (more statistics about the corpus can be found in BIBREF10). We define two tasks based on the corpus described in Section SECREF2: (i) SLC (Sentence-level Classification), which asks to predict whether a sentence contains at least one propaganda technique, and (ii) FLC (Fragment-level classification), which asks to identify both the spans and the type of propaganda technique. Note that these two tasks are of different granularity, $g_1$ and $g_2$, namely tokens for FLC and sentences for SLC.", "The total number of technique instances found in the articles, after the consolidation phase, is $7,485$, out of a total number of $21,230$ sentences (35.2%). The distribution of the techniques in the corpus is also uneven: while there are $2,547$ occurrences of loaded language, there are only 15 instances of straw man (more statistics about the corpus can be found in BIBREF10). We define two tasks based on the corpus described in Section SECREF2: (i) SLC (Sentence-level Classification), which asks to predict whether a sentence contains at least one propaganda technique, and (ii) FLC (Fragment-level classification), which asks to identify both the spans and the type of propaganda technique. Note that these two tasks are of different granularity, $g_1$ and $g_2$, namely tokens for FLC and sentences for SLC. We split the corpus into training, development and test, each containing 293, 57, 101 articles and 14,857, 2,108, 4,265 sentences, respectively.", "In future work, we plan to include more media sources, especially from non-English-speaking media and regions. We further want to extend the tool to support other propaganda techniques.", "This research is part of the Propaganda Analysis Project, which is framed within the Tanbih project. The Tanbih project aims to limit the effect of \u201cfake news\u201d, propaganda, and media bias by making users aware of what they are reading, thus promoting media literacy and critical thinking. The project is developed in collaboration between the Qatar Computing Research Institute (QCRI), HBKU and the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL).", "BERT. We add a linear layer on top of BERT and we fine-tune it, as suggested in BIBREF12. For the FLC task, we feed the final hidden representation for each token to a layer $L_{g_2}$ that makes a 19-way classification: does this token belong to one of the eighteen propaganda techniques or to none of them (cf. Figure FIGREF7-a). For the SLC task, we feed the final hidden representation for the special [CLS] token, which BERT uses to represent the full sentence, to a two-dimensional layer $L_{g_1}$ to make a binary classification."]}
{"question_id": "0c4e419fe57bf01d58a44f3e263777c22cdd90dc", "predicted_answer": "The dataset used in the experiment is a corpus of articles, which is split into training, development, and test sets. The statistics of the corpus are:\n\n* Total number of technique instances: 7,485\n* Total number of sentences: 21,230\n* Distribution of techniques: 2,547 occurrences of loaded language, 15 instances of straw man, and other techniques (more statistics can be found in BIBREF10)\n* Split into:\n\t+ Training set: 293 articles, 14,857 sentences\n\t+ Development set: 57 articles, 2,108 sentences\n\t+ Test set: 101 articles, 4,265 sentences", "predicted_evidence": ["We depart from BERT BIBREF12, and we design three baselines.", "We now define variants of precision and recall able to account for the imbalance in the corpus:", "The total number of technique instances found in the articles, after the consolidation phase, is $7,485$, out of a total number of $21,230$ sentences (35.2%). The distribution of the techniques in the corpus is also uneven: while there are $2,547$ occurrences of loaded language, there are only 15 instances of straw man (more statistics about the corpus can be found in BIBREF10). We define two tasks based on the corpus described in Section SECREF2: (i) SLC (Sentence-level Classification), which asks to predict whether a sentence contains at least one propaganda technique, and (ii) FLC (Fragment-level classification), which asks to identify both the spans and the type of propaganda technique. Note that these two tasks are of different granularity, $g_1$ and $g_2$, namely tokens for FLC and sentences for SLC. We split the corpus into training, development and test, each containing 293, 57, 101 articles and 14,857, 2,108, 4,265 sentences, respectively.", "BERT. We add a linear layer on top of BERT and we fine-tune it, as suggested in BIBREF12. For the FLC task, we feed the final hidden representation for each token to a layer $L_{g_2}$ that makes a 19-way classification: does this token belong to one of the eighteen propaganda techniques or to none of them (cf. Figure FIGREF7-a). For the SLC task, we feed the final hidden representation for the special [CLS] token, which BERT uses to represent the full sentence, to a two-dimensional layer $L_{g_1}$ to make a binary classification.", "In future work, we plan to include more media sources, especially from non-English-speaking media and regions. We further want to extend the tool to support other propaganda techniques."]}
{"question_id": "7b76b8b69246525a48c0a8ca0c42db3319cd10a5", "predicted_answer": "According to the text, bigram language models performed slightly better than trigram models on the evaluation data.", "predicted_evidence": ["These results suggest that there are only slight differences between bigram and trigram models, and that the type and quantity of corpora used to train the models is what really determines the results.", "We relied on bigram and trigram language models because tweets are short and concise, and often only consist of just a few words.", "In this section we present the results from our development stage (Table 2), the evaluation stage (Table 3), and two post-evaluation results (Table 3). Since we implemented both bigram and trigam language models during the development stage but only results from trigram language models were submitted to the task, we evaluated bigram language models in the post-evaluation stage. Note that the accuracy and distance measurements listed in Table 2 and Table 3 are defined by the task organizers BIBREF6 .", "Training Language Models (LMs) is a straightforward way to collect a set of rules by utilizing the fact that words do not appear in an arbitrary order; we in fact can gain useful information about a word by knowing the company it keeps BIBREF7 . A statistical language model estimates the probability of a sequence of words or an upcoming word. An N-gram is a contiguous sequence of N words: a unigram is a single word, a bigram is a two-word sequence, and a trigram is a three-word sequence. For example, in the tweet", "The performance of our system was not consistent when comparing the development to the evaluation results. During development language models trained on the tweet data performed better. However during the evaluation and post-evaluation stage, language models trained on the news data were significantly more effective. We also observed that bigram language models performed slightly better than trigram models on the evaluation data. This suggests that going forward we should also consider both the use of unigram and character\u2013level language models."]}
{"question_id": "8b1af67e3905244653b4cf66ba0acec8d6bff81f", "predicted_answer": "The N-gram models were used to generate predictions on the data by predicting the next word in a sequence based on the previous N-1 words. Specifically, the trigram language model predicts the conditional probability of the next word using the approximation: P(w_n | w_{n-2}, w_{n-1}).", "predicted_evidence": ["These results suggest that there are only slight differences between bigram and trigram models, and that the type and quantity of corpora used to train the models is what really determines the results.", "Training Language Models (LMs) is a straightforward way to collect a set of rules by utilizing the fact that words do not appear in an arbitrary order; we in fact can gain useful information about a word by knowing the company it keeps BIBREF7 . A statistical language model estimates the probability of a sequence of words or an upcoming word. An N-gram is a contiguous sequence of N words: a unigram is a single word, a bigram is a two-word sequence, and a trigram is a three-word sequence. For example, in the tweet", "An N-gram model can predict the next word from a sequence of N-1 previous words. A trigram Language Model (LM) predicts the conditional probability of the next word using the following approximation: DISPLAYFORM0", "We relied on bigram and trigram language models because tweets are short and concise, and often only consist of just a few words.", "The performance of our system was not consistent when comparing the development to the evaluation results. During development language models trained on the tweet data performed better. However during the evaluation and post-evaluation stage, language models trained on the news data were significantly more effective. We also observed that bigram language models performed slightly better than trigram models on the evaluation data. This suggests that going forward we should also consider both the use of unigram and character\u2013level language models."]}
{"question_id": "9a7aeecbecf5e30ffa595c233fca31719c9b429f", "predicted_answer": "The package used to build the ngram language models is the KenLM Toolkit.", "predicted_evidence": ["In this section we present the results from our development stage (Table 2), the evaluation stage (Table 3), and two post-evaluation results (Table 3). Since we implemented both bigram and trigam language models during the development stage but only results from trigram language models were submitted to the task, we evaluated bigram language models in the post-evaluation stage. Note that the accuracy and distance measurements listed in Table 2 and Table 3 are defined by the task organizers BIBREF6 .", "The performance of our system was not consistent when comparing the development to the evaluation results. During development language models trained on the tweet data performed better. However during the evaluation and post-evaluation stage, language models trained on the news data were significantly more effective. We also observed that bigram language models performed slightly better than trigram models on the evaluation data. This suggests that going forward we should also consider both the use of unigram and character\u2013level language models.", "We relied on bigram and trigram language models because tweets are short and concise, and often only consist of just a few words.", "Training Language Models (LMs) is a straightforward way to collect a set of rules by utilizing the fact that words do not appear in an arbitrary order; we in fact can gain useful information about a word by knowing the company it keeps BIBREF7 . A statistical language model estimates the probability of a sequence of words or an upcoming word. An N-gram is a contiguous sequence of N words: a unigram is a single word, a bigram is a two-word sequence, and a trigram is a three-word sequence. For example, in the tweet", "Once we had the corpora ready, we used the KenLM Toolkit to train the N-gram language models on each corpus. We trained using both bigrams and trigrams on the tweet and news data. Our language models accounted for unknown words and were built both with and without considering sentence or tweet boundaries."]}
{"question_id": "3605ea281e72e9085a0ac0a7270cef25fc23063f", "predicted_answer": "The language model system achieved a rank of 1 on the hashtag #BreakUpIn5Words in both Subtask A (with an accuracy of 0.913) and Subtask B (with a distance score of 0.636) in the evaluation stage.", "predicted_evidence": ["In this section we present the results from our development stage (Table 2), the evaluation stage (Table 3), and two post-evaluation results (Table 3). Since we implemented both bigram and trigam language models during the development stage but only results from trigram language models were submitted to the task, we evaluated bigram language models in the post-evaluation stage. Note that the accuracy and distance measurements listed in Table 2 and Table 3 are defined by the task organizers BIBREF6 .", "Table 3 shows the results of our system during the task evaluation. We submitted two runs, one with a trigram language model trained on the tweet data, and another with a trigram language model trained on the news data. In addition, after the evaluation was concluded we also decided to run the bigram language models as well. Contrary to what we observed in the development data, the bigram language model actually performed somewhat better than the trigram language model. In addition, and also contrary to what we observed with the development data, the news data proved generally more effective in the post\u2013evaluation runs than the tweet data.", "The performance of our system was not consistent when comparing the development to the evaluation results. During development language models trained on the tweet data performed better. However during the evaluation and post-evaluation stage, language models trained on the news data were significantly more effective. We also observed that bigram language models performed slightly better than trigram models on the evaluation data. This suggests that going forward we should also consider both the use of unigram and character\u2013level language models.", "The task description paper BIBREF6 reported system by system results for each hashtag. We were surprised to find that our performance on the hashtag file #BreakUpIn5Words in the evaluation stage was significantly better than any other system on both Subtask A (with accuracy of 0.913) and Subtask B (with distance score of 0.636). While we still do not fully understand the cause of these results, there is clearly something about the language used in this hashtag that is distinct from the other hashtags, and is somehow better represented or captured by a language model. Reaching a better understanding of this result is a high priority for future work.", "Our system estimated tweet probability using N-gram LMs. Specifically, it solved the comparison (Subtask A) and semi-ranking (Subtask B) subtasks in four steps:"]}
{"question_id": "21f6cb3819c85312364dd17dd4091df946591ef0", "predicted_answer": "Subtask A involves comparing each pair of tweets in a hashtag file to determine which one is funnier, and outputting the tweet IDs followed by a \"1\" if the first tweet is funnier, or a \"0\" if the second tweet is funnier.\n\nSubtask B involves outputting all the tweet IDs for a hashtag file in order of their funniness, starting from the funniest.", "predicted_evidence": ["For Subtask A, the system goes through the sorted list of tweets in a hashtag file and compares each pair of tweets. For each pair, if the first tweet was funnier than the second, the system would output the tweet_ids for the pair followed by a \u201c1\u201d. If the second tweet is funnier it outputs the tweet_ids followed by a \u201c0\u201d. For Subtask B, the system outputs all the tweet_ids for a hashtag file starting from the funniest.", "Our system estimated tweet probability using N-gram LMs. Specifically, it solved the comparison (Subtask A) and semi-ranking (Subtask B) subtasks in four steps:", "The task description paper BIBREF6 reported system by system results for each hashtag. We were surprised to find that our performance on the hashtag file #BreakUpIn5Words in the evaluation stage was significantly better than any other system on both Subtask A (with accuracy of 0.913) and Subtask B (with distance score of 0.636). While we still do not fully understand the cause of these results, there is clearly something about the language used in this hashtag that is distinct from the other hashtags, and is somehow better represented or captured by a language model. Reaching a better understanding of this result is a high priority for future work.", "Table 2 shows results from the development stage. These results show that for the tweet data the best setting is to keep the # and @, omit sentence boundaries, be case sensitive, and ignore tokenization. While using these settings the trigram language model performed better on Subtask B (.887) and the bigram language model performed better on Subtask A (.548). We decided to rely on trigram language models for the task evaluation since the advantage of bigrams on Subtask A was very slight (.548 versus .543). For the news data, we found that the best setting was to perform tokenization, omit sentence boundaries, and to be case sensitive. Given that trigrams performed most effectively in the development stage, we decided to use those during the evaluation.", "SemEval-2017 Task 6 BIBREF6 also focuses on humor detection by asking participants to develop systems that learn a sense of humor from the Comedy Central TV show, @midnight with Chris Hardwick. Our system ranks tweets according to how funny they are by training N-gram language models on two different corpora. One consisting of funny tweets provided by the task organizers, and the other on a freely available research corpus of news data. The funny tweet data is made up of tweets that are intended to be humorous responses to a hashtag given by host Chris Hardwick during the program."]}
{"question_id": "fd8a8eb69f07c584a76633f8802c2746f7236d64", "predicted_answer": "Yes, the authors report only on English. This is evident from the following phrases:\n\n* \"English language predictions\"\n* \"We only accepted workers from England, the US or Australia\"\n* There is no mention of any other language being studied or evaluated.", "predicted_evidence": ["The study of gender bias in NLP is still nascent; gender bias has not been studied in many NLP tasks. Typically, prior work first observes the gender bias, then attempts to mitigate it BIBREF15. In this paper, we undertake that first step of observation for the task of RE.", "That female articles mention the females' spouses more often than male articles indicates gender bias in Wikipedia's composition; authors do not write about the two genders equally.", "We provide the first evaluation of social bias in NRE models; specifically, we evaluate gender bias in English language predictions of a collection of popularly used and open source NRE models BIBREF2, BIBREF4, BIBREF3, BIBREF5. We evaluate OpenNRE on two fronts: (1) examining Equality of Opportunity BIBREF7 when OpenNRE is trained on an unmodified dataset and (2) examining the effect that various debiasing options BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12 have on both absolute F1 score and the difference in F1-scores on male and female datapoints.", "To find the ground truth, we collected annotations from AMT workers. We asked these workers to determine whether or not a given sentence expressed a given relation. If the majority answer was no, then we labeled that sentence as expressing no relation. (We denote no relation as NA in WikiGenderBias.) Each sentence was annotated by three different workers. Each worker was paid 15 cents per annotation. We only accepted workers from England, the US or Australia and with HIT Approval Rate greater than $95\\%$ and Number of HITs greater than 100. We found the pairwise inter-annotator agreement as measured by Fleiss' Kappa BIBREF26 $\\kappa $ to be 0.44, which is consistent across both genders and signals moderate agreement. We note that our $\\kappa $ value is affected by asking workers to make binary classifications, which limits the degree of agreement that is attainable above chance. We also found the pairwise inter-annotator agreement to be 84%.", "To find the ground truth, we collected annotations from AMT workers. We asked these workers to determine whether or not a given sentence expressed a given relation. If the majority answer was no, then we labeled that sentence as expressing no relation. (We denote no relation as NA in WikiGenderBias.) Each sentence was annotated by three different workers. Each worker was paid 15 cents per annotation. We only accepted workers from England, the US or Australia and with HIT Approval Rate greater than $95\\%$ and Number of HITs greater than 100. We found the pairwise inter-annotator agreement as measured by Fleiss' Kappa BIBREF26 $\\kappa $ to be 0.44, which is consistent across both genders and signals moderate agreement. We note that our $\\kappa $ value is affected by asking workers to make binary classifications, which limits the degree of agreement that is attainable above chance."]}
{"question_id": "452e978bd597411b65be757bf47dc6a78f3c67c9", "predicted_answer": "According to the text, Counterfactual Data Augmentation (CDA) mitigates contextual biases in the training data, which can lead to gender bias in predictions. By replacing masculine words with their corresponding feminine words and vice versa, CDA equalizes the contexts for feminine and masculine words, reducing the bias in the training data. The study demonstrates that using CDA, along with debiased embeddings, effectively mitigates bias in the model's predictions and improves the model's performance when the training data contains contextual biases.", "predicted_evidence": ["After discovering gender bias exists, prior work has developed methods to mitigate that bias. Debiasing methods can debias the training set, word embeddings, or the prediction or training algorithms. In the case of training set or training algorithm debiasing, the model must be retrained. We use two training set debiasing methods (Counterfactual Data Augmentation BIBREF10 and Name Anonymization BIBREF10) and a word embedding debiasing method (Hard Debiasing BIBREF8) and analyze their affect on bias in predictions of NRE models.", "While these findings will help future work avoid gender biases, this study is preliminary. We only consider binary gender, but future work should consider non-binary genders. Additionally, future work should further probe the source of gender bias in the model's predictions, perhaps by visualizing attention or looking more closely at the model's outputs.", "The contexts in which males and females are written about can differ; for instance, on Wikipedia women are more often written about with words related to sexuality than men BIBREF25. Counterfactual Data Augmentation (CDA) mitigates these contextual biases. CDA consists of replacing masculine words in a sentence with their corresponding feminine words and vice versa for all sentences in a corpus, then training on the union of the original and augmented corpora . This equalizes the contexts for feminine and masculine words; if previously 100 doctors were referred to as he and 50 as she, in the new training set he and she will refer to doctor 150 times each.", "We demonstrate that using both gender-swapping and debiased embeddings effectively mitigates bias in the model's predictions and that using genderswapping improves the model's performance when the training data contains contextual biases.", "Since a form of measurement is required for observation, prior work has created methods for measuring gender bias BIBREF16, BIBREF9, BIBREF10, BIBREF17, BIBREF11, BIBREF12, BIBREF18. Gender bias has been measured mainly in training sets and in predictions. Measuring the latter is simple: measure the difference in performance of the model on male and female datapoints, with the definition of the gender of a datapoint being domain-dependent BIBREF11, BIBREF12. Other metrics have been proposed to evaluate fairness of predictors and allocative bias BIBREF19, BIBREF7, such as Equality of Opportunity. We use both of these methods to evaluate NRE models."]}
{"question_id": "159025c44c0115ab4cdc253885384f72e592e83a", "predicted_answer": "According to the text, hard debiasing (a word embedding debiasing method) effectively mitigates bias in the model's predictions, along with gender-swapping. This suggests that hard debiasing helps to reduce gender bias in the predictions made by the NRE model.", "predicted_evidence": ["While these findings will help future work avoid gender biases, this study is preliminary. We only consider binary gender, but future work should consider non-binary genders. Additionally, future work should further probe the source of gender bias in the model's predictions, perhaps by visualizing attention or looking more closely at the model's outputs.", "After discovering gender bias exists, prior work has developed methods to mitigate that bias. Debiasing methods can debias the training set, word embeddings, or the prediction or training algorithms. In the case of training set or training algorithm debiasing, the model must be retrained. We use two training set debiasing methods (Counterfactual Data Augmentation BIBREF10 and Name Anonymization BIBREF10) and a word embedding debiasing method (Hard Debiasing BIBREF8) and analyze their affect on bias in predictions of NRE models.", "We provide the first evaluation of NRE systems for gender bias and find that it exhibits gender bias.", "We demonstrate that using both gender-swapping and debiased embeddings effectively mitigates bias in the model's predictions and that using genderswapping improves the model's performance when the training data contains contextual biases.", "In this paper, we take the first step at understanding and evaluating gender bias in NRE systems. We analyze gender bias by measuring the differences in model performance when extracting relations from sentences written about females versus sentences written about males. Significant discrepancies in performance between genders could diminish the fairness of systems and distort outcomes in applications that use them. For example, if a model predicts the occupation relation for with higher recall for male entities, this could lead to KBs having more occupation information for males. Downstream search tasks using that KB could produce biased predictions, such as ranking articles about female computer scientists below articles about their male peers."]}
{"question_id": "6590055fb033cb32826f2afecb3d7f607dd97d57", "predicted_answer": "According to the text, name anonymization:\n\n* Substantially increases the F1 score gap for the hypernym relation, but slightly decreases the F1 score gap for all other relations.\n* Appears to be effective at debiasing all relations aside from hypernym, though not as effective as either gender-swapping or using debiased embeddings.\n* Mitigates entity bias, which is the tendency of models to use entity names as a proxy for gender.\n\nIn summary, name anonymization has a mixed effect on gender bias in predictions and performance, depending on the specific relation being evaluated. It is effective at debiasing most relations, but not as effective as other methods, and it actually increases the bias in the hypernym relation.", "predicted_evidence": ["Name Anonymization surprisingly substantially increases F1 score gap for the hypernym relation, but slightly decreases F1 score gap for all other relations. Name Anonymization appears to be effective at debiasing all relations aside from hypernym, though not as effective as either Gender-Swapping or using Debiased Embeddings. These results indicate that entity bias likely does not contribute very much the gender bias in the models' original predictions.", "Combinations Combining debiased embeddings and gender swapping turned out to have the highest relative difference in F1 score between male and female sentences for spouse while also reducing bias in other relations (see Figure FIGREF19). All models which use name anonymization (Models 1-4) have significantly higher F1 score gaps for the hypernym relation. While all combinations reduced gender bias to varying extents, gender bias in the spouse relation was mitigated to a similar extent by all combinations. Surprisingly, applying gender-swapping on its own reduces gender bias about as well or better as any combination of methods.", "After discovering gender bias exists, prior work has developed methods to mitigate that bias. Debiasing methods can debias the training set, word embeddings, or the prediction or training algorithms. In the case of training set or training algorithm debiasing, the model must be retrained. We use two training set debiasing methods (Counterfactual Data Augmentation BIBREF10 and Name Anonymization BIBREF10) and a word embedding debiasing method (Hard Debiasing BIBREF8) and analyze their affect on bias in predictions of NRE models.", "Sometimes, models use entity names as a proxy for gender; if a model associates females with politician and John with males, then it might be less likely to predict that John is a politician expresses (John, hypernym, politican) than it would if it associated John with females. Name Anonymization (NA) mitigates this. NA consists of finding all person entities with a Named Entity Recognition system BIBREF30 then replacing the names of these entities with corresponding anonymizations. For instance, the earlier example might become E1 is a politcian, thereby preventing the model from using names as a proxy for gender.", "WikiGenderBias's gender splits are given in Table TABREF6. We first train OpenNRE on the raw, gender-imbalanced training data to reflect model performance without modification. We then introduce bias mitigation methods such as gender-swapping, name anonymzation, and hard debiasing from prior work to evaluate the tradeoff between model performance and gender parity."]}
{"question_id": "3435e365adf7866e45670c865dc33bb7d2a6a0c6", "predicted_answer": "The sentences in WikiGenderBias are curated based on a variant of the Distant Supervision assumption, which assumes that any sentence from a Wikipedia article about one entity that mentions another entity expresses the relation between them. For example, if the relation (Barack, spouse, Michelle) is known, and a sentence in Barack's Wikipedia article mentions Michelle, it is assumed that the sentence expresses the spouse relation.", "predicted_evidence": ["To generate WikiGenderBias, we use a variant of the Distant Supervision assumption: for a given relation between two entities, assume that any sentence from an article written about one of those entities that mentions the other entity expresses the relation. For instance, if we know (Barack, spouse, Michelle) is a relation and we find the sentence He and Michelle were married in Barack's Wikipedia article, then we assume that sentence expresses the (Barack, spouse, Michelle) relation. This assumption is similar to that made by BIBREF20 and allows us to scalably create the dataset.", "We use Wikpedia because many entities on Wikipedia have gender information and because Wikipedia contains articles written about these entities. This combined with relation information about these entities obtainable from DBPedia, Wikipedia's KB, allowed us to create WikiGenderBias using our variant of the Distant Supervision assumption.", "In our creation of WikiGenderBias, we performed some statistical analysis on the Wikipedia data we obtained. We build on the work of BIBREF25, who discover that a higher proportion of Wikipedia Infoboxes on Wikipedia pages of female entities have spouse information than Wikipedia Infoboxes on Wikipedia pages of male entities. However, Figure FIGREF9 demonstrates a further discrepancy: that amongst articles for females and males which contain spouse information, articles written about females mention females' spouses far more often than articles written about men. Additionally, we show that amongst female and male articles we sampled, hypernyms are mentioned far more often in male than female articles.", "WikiGenderBias is the first dataset aimed at training and evaluating NRE systems for gender bias. It contains ground truth labels for the test set and about 45,000 sentences in total.", "We train every encoder-selector combination on the WikiGenderBias training set and use Word2Vec embeddings BIBREF29 also trained on WikiGenderBias and test each combination on the WikiGenderBias test set."]}
{"question_id": "cd82bdaa0c94330f8cccfb1c59b4e6761a5a4f4d", "predicted_answer": "The crowdsourcing platform used is not explicitly mentioned in the given context. However, based on the text, it can be inferred that the platform is likely Amazon Mechanical Turk (MTurk), as the authors mention using \"crowd workers\" and \"MACE\" (Multi-Annotator Computing Ensemble), which is a method commonly used on MTurk for aggregating annotations from multiple workers.", "predicted_evidence": ["Stance annotation. Every ETS was annotated by at least six crowd workers. We evaluate the inter-annotator agreement between groups of workers as proposed by BIBREF12, i.e. by randomly dividing the workers into two equal groups and determining the aggregate annotation for each group using MACE BIBREF13. The final inter-annotator agreement score is obtained by comparing the aggregate annotation of the two groups. Using this procedure, we obtain a Cohen's Kappa of $\\kappa = 0.7$ BIBREF14, indicating a substantial agreement between the crowd workers BIBREF15. The gold annotations of the ETS stances were computed with MACE, using the annotations of all crowd workers. We have further assessed the quality of the annotations performed by crowd workers by comparing them to expert annotations. Two experts labeled 200 ETSs, reaching the same agreement as the crowd workers, i.e.", "Stance annotation. Every ETS was annotated by at least six crowd workers. We evaluate the inter-annotator agreement between groups of workers as proposed by BIBREF12, i.e. by randomly dividing the workers into two equal groups and determining the aggregate annotation for each group using MACE BIBREF13. The final inter-annotator agreement score is obtained by comparing the aggregate annotation of the two groups. Using this procedure, we obtain a Cohen's Kappa of $\\kappa = 0.7$ BIBREF14, indicating a substantial agreement between the crowd workers BIBREF15. The gold annotations of the ETS stances were computed with MACE, using the annotations of all crowd workers. We have further assessed the quality of the annotations performed by crowd workers by comparing them to expert annotations. Two experts labeled 200 ETSs, reaching the same agreement as the crowd workers, i.e. $\\kappa = 0.7$.", "Moreover, in this paper, we consider the three tasks independent of each other rather than as a pipeline. In other words, we always take the gold standard from the preceding task instead of the output of the preceding model in the pipeline. For the three independent tasks, we use recently suggested models that achieved high performance in similar problem settings. In addition, we provide the human agreement bound, which is determined by comparing expert annotations for 200 ETSs to the gold standard derived from crowd worker annotations (Section SECREF19).", "We evaluate the inter-annotator agreement between groups of workers as proposed by BIBREF12, i.e. by randomly dividing the workers into two equal groups and determining the aggregate annotation for each group using MACE BIBREF13. The final inter-annotator agreement score is obtained by comparing the aggregate annotation of the two groups. Using this procedure, we obtain a Cohen's Kappa of $\\kappa = 0.7$ BIBREF14, indicating a substantial agreement between the crowd workers BIBREF15. The gold annotations of the ETS stances were computed with MACE, using the annotations of all crowd workers. We have further assessed the quality of the annotations performed by crowd workers by comparing them to expert annotations. Two experts labeled 200 ETSs, reaching the same agreement as the crowd workers, i.e. $\\kappa = 0.7$. The agreement between the experts' annotations and the computed gold annotations from the crowd workers is also substantial, $\\kappa = 0.683$.", "FGE Annotation. Similar to the stance annotation, we used the approach of BIBREF12 to compute the agreement. The inter-annotator agreement between the crowd workers in this case is $\\kappa = 0.55$ Cohen's Kappa. We compared the annotations of FGE in 200 ETSs by experts with the annotations by crowd workers, reaching an agreement of $\\kappa = 0.56$. This is considered as moderate inter-annotator agreement BIBREF15."]}
{"question_id": "753a187c1dd8d96353187fbb193b5f86293a796c", "predicted_answer": "Yes, they crowdsourced annotations. They used Amazon Mechanical Turk to ask crowd workers to annotate the ETSs, with every ETS being annotated by at least six crowd workers.", "predicted_evidence": ["We evaluate the inter-annotator agreement between groups of workers as proposed by BIBREF12, i.e. by randomly dividing the workers into two equal groups and determining the aggregate annotation for each group using MACE BIBREF13. The final inter-annotator agreement score is obtained by comparing the aggregate annotation of the two groups. Using this procedure, we obtain a Cohen's Kappa of $\\kappa = 0.7$ BIBREF14, indicating a substantial agreement between the crowd workers BIBREF15. The gold annotations of the ETS stances were computed with MACE, using the annotations of all crowd workers. We have further assessed the quality of the annotations performed by crowd workers by comparing them to expert annotations. Two experts labeled 200 ETSs, reaching the same agreement as the crowd workers, i.e. $\\kappa = 0.7$. The agreement between the experts' annotations and the computed gold annotations from the crowd workers is also substantial, $\\kappa = 0.683$.", "Stance annotation. Every ETS was annotated by at least six crowd workers. We evaluate the inter-annotator agreement between groups of workers as proposed by BIBREF12, i.e. by randomly dividing the workers into two equal groups and determining the aggregate annotation for each group using MACE BIBREF13. The final inter-annotator agreement score is obtained by comparing the aggregate annotation of the two groups. Using this procedure, we obtain a Cohen's Kappa of $\\kappa = 0.7$ BIBREF14, indicating a substantial agreement between the crowd workers BIBREF15. The gold annotations of the ETS stances were computed with MACE, using the annotations of all crowd workers. We have further assessed the quality of the annotations performed by crowd workers by comparing them to expert annotations. Two experts labeled 200 ETSs, reaching the same agreement as the crowd workers, i.e.", "Stance annotation. Every ETS was annotated by at least six crowd workers. We evaluate the inter-annotator agreement between groups of workers as proposed by BIBREF12, i.e. by randomly dividing the workers into two equal groups and determining the aggregate annotation for each group using MACE BIBREF13. The final inter-annotator agreement score is obtained by comparing the aggregate annotation of the two groups. Using this procedure, we obtain a Cohen's Kappa of $\\kappa = 0.7$ BIBREF14, indicating a substantial agreement between the crowd workers BIBREF15. The gold annotations of the ETS stances were computed with MACE, using the annotations of all crowd workers. We have further assessed the quality of the annotations performed by crowd workers by comparing them to expert annotations. Two experts labeled 200 ETSs, reaching the same agreement as the crowd workers, i.e. $\\kappa = 0.7$.", "FGE Annotation. Similar to the stance annotation, we used the approach of BIBREF12 to compute the agreement. The inter-annotator agreement between the crowd workers in this case is $\\kappa = 0.55$ Cohen's Kappa. We compared the annotations of FGE in 200 ETSs by experts with the annotations by crowd workers, reaching an agreement of $\\kappa = 0.56$. This is considered as moderate inter-annotator agreement BIBREF15.", "Stance annotation. We asked crowd workers on Amazon Mechanical Turk to annotate whether an ETS agrees with the claim, refutes it, or has no stance towards the claim. An ETS was only considered to express a stance if it explicitly referred to the claim and either expressed support for it or refuted it. In all other cases, the ETS was considered as having no stance."]}
{"question_id": "29794bda61665a1fbe736111e107fd181eacba1b", "predicted_answer": "The data from Snopes comes from various sources, including:\n\n1. News websites (38%): mainstream news, tabloid press, and partisan news.\n2. False news and satirical articles (30%): primarily from websites like thelastlineofdefense.org and worldnewsdailyreport.com.\n3. Social media (11%): Facebook and Twitter.\n4. Diverse sources (21%): debate blogs, governmental domains, online retail, or entertainment websites.\n\nAdditionally, Snopes fact-checkers provide URLs for original documents (ODCs) from which evidence text snippets (ETSs) have been extracted or which provide additional information.", "predicted_evidence": ["We further investigated the sources of the collected documents (ODCs) and grouped them into a number of classes. We found that 38% of the articles are from different news websites ranging from mainstream news like CNN to tabloid press and partisan news. The second largest group of documents are false news and satirical articles with 30%. Here, the majority of articles are from the two websites thelastlineofdefense.org and worldnewsdailyreport.com. The third class of documents, with a share of 11%, are from social media like Facebook and Twitter. The remaining 21% of documents come from diverse sources, such as debate blogs, governmental domains, online retail, or entertainment websites.", "This section describes the original data from the Snopes platform, followed by a detailed report on our corpus annotation methodology.", "Snopes is a large-scale fact-checking platform that employs human fact-checkers to validate claims. A simple fact-checking instance from the Snopes website is shown in Figure FIGREF14. At the top of the page, the claim and the verdict (rating) are given. The fact-checkers additionally provide a resolution (origin), which backs up the verdict. Evidence in the resolution, which we call evidence text snippets (ETSs), is marked with a yellow bar. As additional validation support, Snopes fact-checkers provide URLs for original documents (ODCs) from which the ETSs have been extracted or which provide additional information.", "A corpus with significantly more validated claims was introduced by BIBREF2. Nevertheless, for each claim, the corpus provides 30 documents which are retrieved from the web using the Google search engine instead of a document collection aggregated by fact-checkers. Thus, many of the documents are unrelated to the claim and important information for the validation may be missing.", "Snopes17 A corpus featuring a substantially larger number of validated claims was introduced by BIBREF2. It contains 4,956 claims annotated with verdicts which have been extracted from the Snopes website as well as the Wikipedia collections of proven hoaxes and fictitious people. For each claim, the authors extracted about 30 associated documents using the Google search engine, resulting in a collection of 136,085 documents. However, since the documents were not annotated by fact-checkers, irrelevant information is present and important information for the claim validation might be missing."]}
{"question_id": "dd80a38e578443496d3720d883ad194ce82c5f39", "predicted_answer": "The authors compare their new corpus with existing fact-checking corpora, which are summarized in Table TABREF7. One of the specific corpora mentioned is the one introduced by BIBREF2.", "predicted_evidence": ["Below, we give a comprehensive overview of existing fact-checking corpora, summarized in Table TABREF7. We focus on their key parameters: fact-checking sub-task coverage, annotation quality, corpus size, and domain. It must be acknowledged that a fair comparison between the datasets is difficult to accomplish since the length of evidence and documents, as well as the annotation quality, significantly varies between the corpora.", "2) To support the creation of further fact-checking corpora, we present our methodology for data collection and annotation, which allows for the efficient construction of large-scale corpora with a substantial inter-annotator agreement.", "Automated fact-checking has received significant attention in the NLP community in the past years. Multiple corpora have been created to assist the development of fact-checking models, varying in quality, size, domain, and range of annotated phenomena. Importantly, the successful development of a full-fledged fact-checking system requires that the underlying corpus satisfies certain characteristics. First, training data needs to contain a large number of instances with high-quality annotations for the different fact-checking sub-tasks. Second, the training data should not be limited to a particular domain, since potentially wrong information sources can range from official statements to blog and Twitter posts.", "A corpus with significantly more validated claims was introduced by BIBREF2. Nevertheless, for each claim, the corpus provides 30 documents which are retrieved from the web using the Google search engine instead of a document collection aggregated by fact-checkers. Thus, many of the documents are unrelated to the claim and important information for the validation may be missing.", "As our analysis shows, while multiple fact-checking corpora are already available, no single existing resource provides full fact-checking sub-task coverage backed by a substantially-sized and validated dataset spanning across multiple domains. To eliminate this gap, we have created a new corpus as detailed in the following sections."]}
{"question_id": "9a9774eacb8f75bcfa07a4e60ed5eb02646467e3", "predicted_answer": "The size of the FEVER corpus is 185,445 validated claims.", "predicted_evidence": ["Table TABREF21 displays the main statistics of the corpus. In the table, FGE sets denotes groups of FGE extracted from the same ETS. Many of the ETSs have been annotated as no stance (see Table TABREF23) and, following our annotation study setup, are not used for FGE extraction. Therefore, the number of FGE sets is much lower than that of ETSs. We have found that, on average, an ETS consists of 6.5 sentences. For those ETSs that have support/refute stance, on average, 2.3 sentences are selected as FGE. For many of the ETSs, no original documents (ODCs) have been provided (documents from which they have been extracted). On the other hand, in many instances, links to ODCs are given that provide additional information, but from which no ETSs have been extracted.", "A corpus with significantly more validated claims was introduced by BIBREF2. Nevertheless, for each claim, the corpus provides 30 documents which are retrieved from the web using the Google search engine instead of a document collection aggregated by fact-checkers. Thus, many of the documents are unrelated to the claim and important information for the validation may be missing.", "This section describes the original data from the Snopes platform, followed by a detailed report on our corpus annotation methodology.", "We have described the structure and statistics of the corpus, as well as our methodology for the annotation of evidence and the stance of the evidence. We have also presented experiments for stance detection, evidence extraction, and claim validation with models that achieve high performance in similar problem settings. In order to support the development of machine learning approaches that go beyond the presented models, we provided an error analysis for each of the three tasks, identifying difficulties with each.", "FEVER18 The FEVER corpus introduced by BIBREF1 is the largest available fact-checking corpus, consisting of 185,445 validated claims. The corpus is based on about 50k popular Wikipedia articles. Annotators modified sentences in these articles to create the claims and labeled other sentences in the articles, which support or refute the claim, as evidence. The corpus is large enough to train deep learning systems able to retrieve evidence from Wikipedia. Nevertheless, since the corpus only covers Wikipedia and the claims are created synthetically, the trained systems are unlikely to be able to extract evidence from heterogeneous web-sources and validate claims on the basis of evidence found on the Internet."]}
{"question_id": "4ed58d828cd6bb9beca1471a9fa9f5e77488b1d1", "predicted_answer": "The question asks about the architectures experimented with, but the given context does not mention any specific architectures. It appears to discuss experiments and results related to identifying and analyzing Evidence, Topic, and Stance (ETS) in documents, but it does not mention any specific architectures. Therefore, the answer is \"None\" or \"Not mentioned\".", "predicted_evidence": ["The contributions of our work are as follows:", "There are a number of experiments beyond the scope of this paper, which are left for future work: (1) retrieval of the original documents (ODCs) given a claim, (2) identification of ETSs in ODCs, and (3) prediction of a claim's verdict on the basis of FGE, the stance of FGE, and their sources.", "For supporting and refuting ETSs annotators identified FGE sets for 8,291 out of 8,998 ETSs. ETSs with a stance but without FGE sets often miss a clear connection to the claim, so the annotators did not annotate any sentences in these cases. The class distribution of the FGE sets in Table TABREF23 shows that supporting ETSs are more dominant.", "Example:", "Example:"]}
{"question_id": "de580e43614ee38d2d9fc6263ff96e6ca2b54eb5", "predicted_answer": "The domains present in the corpus are not explicitly stated, but it can be inferred that the corpus is not limited to a particular domain, as it is mentioned that \"potentially wrong information sources can range from official statements to blog and Twitter posts.\" This suggests that the corpus may include domains such as:\n\n* News articles\n* Official statements\n* Blogs\n* Social media (e.g. Twitter)\n* Websites (as mentioned in the context of Snopes)\n\nHowever, the exact domains present in the corpus are not specified.", "predicted_evidence": ["Table TABREF21 displays the main statistics of the corpus. In the table, FGE sets denotes groups of FGE extracted from the same ETS. Many of the ETSs have been annotated as no stance (see Table TABREF23) and, following our annotation study setup, are not used for FGE extraction. Therefore, the number of FGE sets is much lower than that of ETSs. We have found that, on average, an ETS consists of 6.5 sentences. For those ETSs that have support/refute stance, on average, 2.3 sentences are selected as FGE. For many of the ETSs, no original documents (ODCs) have been provided (documents from which they have been extracted). On the other hand, in many instances, links to ODCs are given that provide additional information, but from which no ETSs have been extracted.", "1) We provide a substantially sized mixed-domain corpus of natural claims with annotations for different fact-checking tasks. We publish a web crawler that reconstructs our dataset including all annotations. For research purposes, we are allowed to share the original corpus.", "As our analysis shows, while multiple fact-checking corpora are already available, no single existing resource provides full fact-checking sub-task coverage backed by a substantially-sized and validated dataset spanning across multiple domains. To eliminate this gap, we have created a new corpus as detailed in the following sections.", "Our crawler extracts the claims, verdicts, ETSs, the resolution, as well as ODCs along with their URLs, thereby enriching the ETSs with useful contextual information. Snopes is almost entirely focused on claims made on English speaking websites. Our corpus therefore only features English fact-checking instances.", "Automated fact-checking has received significant attention in the NLP community in the past years. Multiple corpora have been created to assist the development of fact-checking models, varying in quality, size, domain, and range of annotated phenomena. Importantly, the successful development of a full-fledged fact-checking system requires that the underlying corpus satisfies certain characteristics. First, training data needs to contain a large number of instances with high-quality annotations for the different fact-checking sub-tasks. Second, the training data should not be limited to a particular domain, since potentially wrong information sources can range from official statements to blog and Twitter posts."]}
{"question_id": "ae89eed483c11ccd70a34795e9fe416af8a35da2", "predicted_answer": "The inter-annotator agreement was:\n\n* For FGE annotation: \u03ba = 0.55 (moderate agreement) between crowd workers, and \u03ba = 0.56 (moderate agreement) between experts and crowd workers.\n* For stance annotation: \u03ba = 0.7 (substantial agreement) between crowd workers, and \u03ba = 0.7 (substantial agreement) between experts and crowd workers.", "predicted_evidence": ["FGE Annotation. Similar to the stance annotation, we used the approach of BIBREF12 to compute the agreement. The inter-annotator agreement between the crowd workers in this case is $\\kappa = 0.55$ Cohen's Kappa. We compared the annotations of FGE in 200 ETSs by experts with the annotations by crowd workers, reaching an agreement of $\\kappa = 0.56$. This is considered as moderate inter-annotator agreement BIBREF15.", "As the example illustrates, there is a gradual transition between sentences that can be considered as essential for the validation of the claim and those which just provide minor negligible details or unrelated information. Nevertheless, even though the inter-annotator agreement for the annotation of FGE is lower than for the annotation of ETS stance, compared to other annotation problems BIBREF16, BIBREF17, BIBREF18 that are similar to the annotation of FGE, our framework leads to a better agreement.", "Stance annotation. Every ETS was annotated by at least six crowd workers. We evaluate the inter-annotator agreement between groups of workers as proposed by BIBREF12, i.e. by randomly dividing the workers into two equal groups and determining the aggregate annotation for each group using MACE BIBREF13. The final inter-annotator agreement score is obtained by comparing the aggregate annotation of the two groups. Using this procedure, we obtain a Cohen's Kappa of $\\kappa = 0.7$ BIBREF14, indicating a substantial agreement between the crowd workers BIBREF15. The gold annotations of the ETS stances were computed with MACE, using the annotations of all crowd workers. We have further assessed the quality of the annotations performed by crowd workers by comparing them to expert annotations. Two experts labeled 200 ETSs, reaching the same agreement as the crowd workers, i.e.", "Stance annotation. Every ETS was annotated by at least six crowd workers. We evaluate the inter-annotator agreement between groups of workers as proposed by BIBREF12, i.e. by randomly dividing the workers into two equal groups and determining the aggregate annotation for each group using MACE BIBREF13. The final inter-annotator agreement score is obtained by comparing the aggregate annotation of the two groups. Using this procedure, we obtain a Cohen's Kappa of $\\kappa = 0.7$ BIBREF14, indicating a substantial agreement between the crowd workers BIBREF15. The gold annotations of the ETS stances were computed with MACE, using the annotations of all crowd workers. We have further assessed the quality of the annotations performed by crowd workers by comparing them to expert annotations. Two experts labeled 200 ETSs, reaching the same agreement as the crowd workers, i.e. $\\kappa = 0.7$.", "We evaluate the inter-annotator agreement between groups of workers as proposed by BIBREF12, i.e. by randomly dividing the workers into two equal groups and determining the aggregate annotation for each group using MACE BIBREF13. The final inter-annotator agreement score is obtained by comparing the aggregate annotation of the two groups. Using this procedure, we obtain a Cohen's Kappa of $\\kappa = 0.7$ BIBREF14, indicating a substantial agreement between the crowd workers BIBREF15. The gold annotations of the ETS stances were computed with MACE, using the annotations of all crowd workers. We have further assessed the quality of the annotations performed by crowd workers by comparing them to expert annotations. Two experts labeled 200 ETSs, reaching the same agreement as the crowd workers, i.e. $\\kappa = 0.7$. The agreement between the experts' annotations and the computed gold annotations from the crowd workers is also substantial, $\\kappa = 0.683$."]}
{"question_id": "fc62549a8f0922c09996a119b2b6a8b5e829e989", "predicted_answer": "The metrics used for quantitative analysis mentioned in the text are:\n\n1. Cross entropy\n2. BLEU score\n3. Perplexity\n\nThese metrics are used to measure the similarity between the output of a personalized language model and the target language style, as well as to evaluate the quality of the model output.", "predicted_evidence": ["The characteristics of a user speech can mainly be distinguished by the word dictionary. Thus, this metric tries to measure the differences of the word dictionary among the comparing set. Table 1 shows the quantitative measure results from the dialogue set of the main characters in drama data from \u201cFriends,\" a famous American television sitcom. In the figures, \u201ccharacter_1\" to \u201ccharacter_6\" are the main characters of the drama (Chandler, Joey, Monica, Phoebe, Rachel, and Ross, respectively). The dialogues were measured against one another by using the cross entropy metric. As shown in the table, the lower cross entropy value among the same character's dialogue was calculated, and the higher value was calculated among the different character's dialogues as expected. This result demonstrates that the cross entropy metric can be used to measure the similarities among the members of the set.", "An output of a personalized language model can be measured by calculating the cross entropy between the word distribution of the model output and that of the target data. Word distribution can be acquired by normalizing a word histogram which is calculated based on word counts in the target corpus. Equation (3) shows the metric formulation.", "Another measure, the BLEU score algorithm BIBREF4 , has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring a quality of the personalized model output because it considers the similarity between one language and the target language. Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen's left-over novel with partially completed BIBREF5 . This research counted the occurrence of several words in the literature, compared their relative frequencies with those of the words in the target literature, and concluded that the target literature was a forgery. This approach could be applied to a text evaluation where a large amount of data is available and certain words are used more frequently. In spoken language, such as in the message-reply case, however, whole word distribution must be considered instead of considering the occurrence of several words, because the data is usually not enough than the literature case. So, we use a simple and efficient metric to measure the similarity between the user style and the output of the personalized model.", "It measures how well the language model predicts a sample. However, it is not good at measuring how well the output of the language model matches a target language style. Another measure, the BLEU score algorithm BIBREF4 , has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring a quality of the personalized model output because it considers the similarity between one language and the target language. Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen's left-over novel with partially completed BIBREF5 . This research counted the occurrence of several words in the literature, compared their relative frequencies with those of the words in the target literature, and concluded that the target literature was a forgery. This approach could be applied to a text evaluation where a large amount of data is available and certain words are used more frequently. In spoken language, such as in the message-reply case, however, whole word distribution must be considered instead of considering the occurrence of several words, because the data is usually not enough than the literature case.", "The perplexity is one of the popular measures for a language model. It measures how well the language model predicts a sample. However, it is not good at measuring how well the output of the language model matches a target language style. Another measure, the BLEU score algorithm BIBREF4 , has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring a quality of the personalized model output because it considers the similarity between one language and the target language. Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen's left-over novel with partially completed BIBREF5 . This research counted the occurrence of several words in the literature, compared their relative frequencies with those of the words in the target literature, and concluded that the target literature was a forgery. This approach could be applied to a text evaluation where a large amount of data is available and certain words are used more frequently."]}
{"question_id": "e2a507749a4a3201edd6413c77ad0d4c23e9c6ce", "predicted_answer": "The answer is: No, their data is not open-sourced. The text does not mention anything about open-sourcing the data, and it appears that the authors are using proprietary data, such as the WMT'14 corpus and English bible data, for their research.", "predicted_evidence": ["The perplexity is one of the popular measures for a language model. It measures how well the language model predicts a sample. However, it is not good at measuring how well the output of the language model matches a target language style. Another measure, the BLEU score algorithm BIBREF4 , has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring a quality of the personalized model output because it considers the similarity between one language and the target language. Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen's left-over novel with partially completed BIBREF5 . This research counted the occurrence of several words in the literature, compared their relative frequencies with those of the words in the target literature, and concluded that the target literature was a forgery. This approach could be applied to a text evaluation where a large amount of data is available and certain words are used more frequently.", "It measures how well the language model predicts a sample. However, it is not good at measuring how well the output of the language model matches a target language style. Another measure, the BLEU score algorithm BIBREF4 , has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring a quality of the personalized model output because it considers the similarity between one language and the target language. Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen's left-over novel with partially completed BIBREF5 . This research counted the occurrence of several words in the literature, compared their relative frequencies with those of the words in the target literature, and concluded that the target literature was a forgery. This approach could be applied to a text evaluation where a large amount of data is available and certain words are used more frequently. In spoken language, such as in the message-reply case, however, whole word distribution must be considered instead of considering the occurrence of several words, because the data is usually not enough than the literature case.", "In the commercial sphere, Google recently released a smart-reply service that could generate a response to a given email by using a sequence-to-sequence learning model BIBREF12 . There was another trial on the generation of responses in technical troubleshooting discourses BIBREF13 . This research also required complete data in one place and did not provide a personalized model.", "The perplexity is one of the popular measures for a language model. It measures how well the language model predicts a sample. However, it is not good at measuring how well the output of the language model matches a target language style. Another measure, the BLEU score algorithm BIBREF4 , has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring a quality of the personalized model output because it considers the similarity between one language and the target language. Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen's left-over novel with partially completed BIBREF5 . This research counted the occurrence of several words in the literature, compared their relative frequencies with those of the words in the target literature, and concluded that the target literature was a forgery.", "We also apply the transfer learning schemes with some of the English bible data. The same general language model, which involved previously training with the WMT'14 corpus for 10 days, is used. English bible data is added and employed in training for another 4 hours using proposed transfer learning schemes."]}
{"question_id": "a3a867f7b3557c168d05c517c468ff6c7337bff9", "predicted_answer": "The datasets used in this study are:\n\n1. \"Friends\" drama corpus\n2. \"Chandler\" corpus\n3. \"Bible\" corpus\n4. WMT'14 corpus", "predicted_evidence": ["Table 5 indicates the cross entropy measure between the output of \u201cscheme_1\" to \u201cscheme_3\" model and that of the target corpus, the \u201cfriends\" drama corpus, the \u201cchandler\" corpus, and the \u201cbible\" corpus. It shows the similarity between the personalized model output and the target corpus as the number of training epoch increasing. The general model was pre-trained with the \u201cFriends\u201d corpus and the \u201cChandler\u201d corpus was used training personalized model. Each Model is selected from various training epoch (0, 10, 20 and 40) and schemes, and test messages of 753 are used for the reply generation with the selected model used. As the table shows, the cross entropy measure has the highest value when the target corpus is the \u201cbible\u201d as expected because it is written in different style than dialogues in drama script. For the drama script case, the cross entropy measured with the \u201cchandler\" corpus shows the lowest value among schemes.", "Table 5 indicates the cross entropy measure between the output of \u201cscheme_1\" to \u201cscheme_3\" model and that of the target corpus, the \u201cfriends\" drama corpus, the \u201cchandler\" corpus, and the \u201cbible\" corpus. It shows the similarity between the personalized model output and the target corpus as the number of training epoch increasing. The general model was pre-trained with the \u201cFriends\u201d corpus and the \u201cChandler\u201d corpus was used training personalized model. Each Model is selected from various training epoch (0, 10, 20 and 40) and schemes, and test messages of 753 are used for the reply generation with the selected model used. As the table shows, the cross entropy measure has the highest value when the target corpus is the \u201cbible\u201d as expected because it is written in different style than dialogues in drama script.", "It is certain that our proposed method reveals the applicability of the RNN-based language model in a user device with the preservation of privacy. Furthermore, with our method the personalized language model can be generated with a smaller amount of user data than the huge amount of training data that is usually required in the traditional deep neural network discipline. In the future work, we aim to visualize the deep neural network and to investigate the specific relationship among users' language styles and the LSTM cells in the network. This approach seems likely to uncover enhanced learning schemes that require less data than was previously necessary.", "To adapt a neural network model to an embedded system with limited resources, BIBREF17 BIBREF18 reduced the size of the model by pruning the unnecessary connections within it. It repeatedly tried to reduce the model size without accuracy degradation. This research inspired us to a considerable extent. It applied a neural model to mobile devices. However, the research focused on reducing the model size using a powerful machine and releasing the final model to an embedded system, whereas ours investigated how to train a model within mobile devices so that private user data could be kept.", "We train a general language model of literary-style with the WMT'14 corpus. We then apply a transfer learning scheme with \u201cFriends\" drama data for the model to learn the spoken-style language. Training the general language model took about 10 days then we spent another 4 hours training the personalized language model in each scheme. A \u201ctitan-X GPU\" and a \u201cGeForce GT 730 GPU\" were used for these experiments. The latter GPU is one of the low-end GPU series of which computing power was similar to that of latest mobile GPUs such as \u201cQualcomm Adreno 530\" in \u201cSamsung Galaxy S7\" or \u201cNVIDIA Tegra K1\" in \u201cGoogle Nexus 9\". For a vocabulary setting, we construct our dictionary as 50,002 words, including \u201c $<eos>$ \" to mark ends of sentence and \u201c**unknown**\" to replace unconsidered vocabulary in the data."]}
{"question_id": "8bb2280483af8013a32e0d294e97d44444f08ab0", "predicted_answer": "The metric used for qualitative evaluation is not explicitly mentioned in the given context. However, it is mentioned that the cross entropy metric is used to measure the similarity between the personalized model output and the target corpus, and the BLEU score algorithm is mentioned as another measure that has been widely used for automatic evaluation of model output. But it is not clear which one is used for qualitative evaluation.", "predicted_evidence": ["The characteristics of a user speech can mainly be distinguished by the word dictionary. Thus, this metric tries to measure the differences of the word dictionary among the comparing set. Table 1 shows the quantitative measure results from the dialogue set of the main characters in drama data from \u201cFriends,\" a famous American television sitcom. In the figures, \u201ccharacter_1\" to \u201ccharacter_6\" are the main characters of the drama (Chandler, Joey, Monica, Phoebe, Rachel, and Ross, respectively). The dialogues were measured against one another by using the cross entropy metric. As shown in the table, the lower cross entropy value among the same character's dialogue was calculated, and the higher value was calculated among the different character's dialogues as expected. This result demonstrates that the cross entropy metric can be used to measure the similarities among the members of the set.", "It measures how well the language model predicts a sample. However, it is not good at measuring how well the output of the language model matches a target language style. Another measure, the BLEU score algorithm BIBREF4 , has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring a quality of the personalized model output because it considers the similarity between one language and the target language. Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen's left-over novel with partially completed BIBREF5 . This research counted the occurrence of several words in the literature, compared their relative frequencies with those of the words in the target literature, and concluded that the target literature was a forgery. This approach could be applied to a text evaluation where a large amount of data is available and certain words are used more frequently. In spoken language, such as in the message-reply case, however, whole word distribution must be considered instead of considering the occurrence of several words, because the data is usually not enough than the literature case.", "Table 5 indicates the cross entropy measure between the output of \u201cscheme_1\" to \u201cscheme_3\" model and that of the target corpus, the \u201cfriends\" drama corpus, the \u201cchandler\" corpus, and the \u201cbible\" corpus. It shows the similarity between the personalized model output and the target corpus as the number of training epoch increasing. The general model was pre-trained with the \u201cFriends\u201d corpus and the \u201cChandler\u201d corpus was used training personalized model. Each Model is selected from various training epoch (0, 10, 20 and 40) and schemes, and test messages of 753 are used for the reply generation with the selected model used. As the table shows, the cross entropy measure has the highest value when the target corpus is the \u201cbible\u201d as expected because it is written in different style than dialogues in drama script. For the drama script case, the cross entropy measured with the \u201cchandler\" corpus shows the lowest value among schemes.", "An output of a personalized language model can be measured by calculating the cross entropy between the word distribution of the model output and that of the target data. Word distribution can be acquired by normalizing a word histogram which is calculated based on word counts in the target corpus. Equation (3) shows the metric formulation.", "Table 5 indicates the cross entropy measure between the output of \u201cscheme_1\" to \u201cscheme_3\" model and that of the target corpus, the \u201cfriends\" drama corpus, the \u201cchandler\" corpus, and the \u201cbible\" corpus. It shows the similarity between the personalized model output and the target corpus as the number of training epoch increasing. The general model was pre-trained with the \u201cFriends\u201d corpus and the \u201cChandler\u201d corpus was used training personalized model. Each Model is selected from various training epoch (0, 10, 20 and 40) and schemes, and test messages of 753 are used for the reply generation with the selected model used. As the table shows, the cross entropy measure has the highest value when the target corpus is the \u201cbible\u201d as expected because it is written in different style than dialogues in drama script."]}
{"question_id": "a68acd8364764d5601dc12e4b31d9102fb7d5f7e", "predicted_answer": "The metric used for quantitative evaluation is the cross entropy metric, which measures the difference between the word dictionary of the model output and that of the target data.", "predicted_evidence": ["It measures how well the language model predicts a sample. However, it is not good at measuring how well the output of the language model matches a target language style. Another measure, the BLEU score algorithm BIBREF4 , has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring a quality of the personalized model output because it considers the similarity between one language and the target language. Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen's left-over novel with partially completed BIBREF5 . This research counted the occurrence of several words in the literature, compared their relative frequencies with those of the words in the target literature, and concluded that the target literature was a forgery. This approach could be applied to a text evaluation where a large amount of data is available and certain words are used more frequently. In spoken language, such as in the message-reply case, however, whole word distribution must be considered instead of considering the occurrence of several words, because the data is usually not enough than the literature case.", "The characteristics of a user speech can mainly be distinguished by the word dictionary. Thus, this metric tries to measure the differences of the word dictionary among the comparing set. Table 1 shows the quantitative measure results from the dialogue set of the main characters in drama data from \u201cFriends,\" a famous American television sitcom. In the figures, \u201ccharacter_1\" to \u201ccharacter_6\" are the main characters of the drama (Chandler, Joey, Monica, Phoebe, Rachel, and Ross, respectively). The dialogues were measured against one another by using the cross entropy metric. As shown in the table, the lower cross entropy value among the same character's dialogue was calculated, and the higher value was calculated among the different character's dialogues as expected. This result demonstrates that the cross entropy metric can be used to measure the similarities among the members of the set.", "Table 5 indicates the cross entropy measure between the output of \u201cscheme_1\" to \u201cscheme_3\" model and that of the target corpus, the \u201cfriends\" drama corpus, the \u201cchandler\" corpus, and the \u201cbible\" corpus. It shows the similarity between the personalized model output and the target corpus as the number of training epoch increasing. The general model was pre-trained with the \u201cFriends\u201d corpus and the \u201cChandler\u201d corpus was used training personalized model. Each Model is selected from various training epoch (0, 10, 20 and 40) and schemes, and test messages of 753 are used for the reply generation with the selected model used. As the table shows, the cross entropy measure has the highest value when the target corpus is the \u201cbible\u201d as expected because it is written in different style than dialogues in drama script. For the drama script case, the cross entropy measured with the \u201cchandler\" corpus shows the lowest value among schemes.", "The perplexity is one of the popular measures for a language model. It measures how well the language model predicts a sample. However, it is not good at measuring how well the output of the language model matches a target language style. Another measure, the BLEU score algorithm BIBREF4 , has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring a quality of the personalized model output because it considers the similarity between one language and the target language. Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen's left-over novel with partially completed BIBREF5 . This research counted the occurrence of several words in the literature, compared their relative frequencies with those of the words in the target literature, and concluded that the target literature was a forgery.", "An output of a personalized language model can be measured by calculating the cross entropy between the word distribution of the model output and that of the target data. Word distribution can be acquired by normalizing a word histogram which is calculated based on word counts in the target corpus. Equation (3) shows the metric formulation."]}
{"question_id": "6d55e377335815b7ad134d1a2977d231ad34a25b", "predicted_answer": "The similarity metrics used for quantitative analysis mentioned in the text are:\n\n1. Cross entropy\n2. BLEU score\n3. Perplexity", "predicted_evidence": ["The characteristics of a user speech can mainly be distinguished by the word dictionary. Thus, this metric tries to measure the differences of the word dictionary among the comparing set. Table 1 shows the quantitative measure results from the dialogue set of the main characters in drama data from \u201cFriends,\" a famous American television sitcom. In the figures, \u201ccharacter_1\" to \u201ccharacter_6\" are the main characters of the drama (Chandler, Joey, Monica, Phoebe, Rachel, and Ross, respectively). The dialogues were measured against one another by using the cross entropy metric. As shown in the table, the lower cross entropy value among the same character's dialogue was calculated, and the higher value was calculated among the different character's dialogues as expected. This result demonstrates that the cross entropy metric can be used to measure the similarities among the members of the set.", "Another measure, the BLEU score algorithm BIBREF4 , has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring a quality of the personalized model output because it considers the similarity between one language and the target language. Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen's left-over novel with partially completed BIBREF5 . This research counted the occurrence of several words in the literature, compared their relative frequencies with those of the words in the target literature, and concluded that the target literature was a forgery. This approach could be applied to a text evaluation where a large amount of data is available and certain words are used more frequently. In spoken language, such as in the message-reply case, however, whole word distribution must be considered instead of considering the occurrence of several words, because the data is usually not enough than the literature case. So, we use a simple and efficient metric to measure the similarity between the user style and the output of the personalized model.", "An output of a personalized language model can be measured by calculating the cross entropy between the word distribution of the model output and that of the target data. Word distribution can be acquired by normalizing a word histogram which is calculated based on word counts in the target corpus. Equation (3) shows the metric formulation.", "The perplexity is one of the popular measures for a language model. It measures how well the language model predicts a sample. However, it is not good at measuring how well the output of the language model matches a target language style. Another measure, the BLEU score algorithm BIBREF4 , has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring a quality of the personalized model output because it considers the similarity between one language and the target language. Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen's left-over novel with partially completed BIBREF5 . This research counted the occurrence of several words in the literature, compared their relative frequencies with those of the words in the target literature, and concluded that the target literature was a forgery. This approach could be applied to a text evaluation where a large amount of data is available and certain words are used more frequently.", "It measures how well the language model predicts a sample. However, it is not good at measuring how well the output of the language model matches a target language style. Another measure, the BLEU score algorithm BIBREF4 , has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring a quality of the personalized model output because it considers the similarity between one language and the target language. Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen's left-over novel with partially completed BIBREF5 . This research counted the occurrence of several words in the literature, compared their relative frequencies with those of the words in the target literature, and concluded that the target literature was a forgery. This approach could be applied to a text evaluation where a large amount of data is available and certain words are used more frequently. In spoken language, such as in the message-reply case, however, whole word distribution must be considered instead of considering the occurrence of several words, because the data is usually not enough than the literature case."]}
{"question_id": "0035b351df63971ec57e36d4bfc6f7594bed41ae", "predicted_answer": "The data is labeled as positive or negative.", "predicted_evidence": ["These basic features are extracted from the text. They are the following:", "The evaluation metrics used by the task organizers were the macroaveraged recall ( INLINEFORM0 ), the F1 averaged across the positives and the negatives INLINEFORM1 and the accuracy ( INLINEFORM2 ) BIBREF25 .", "This section explains the main steps of the SiTAKA system, the features used to describe a tweet and the classification method.", "where INLINEFORM0 denotes the concatenation operation. The pooling function is an element-wise function, and it converts texts with various lengths into a fixed-length vector allowing to capture the information throughout the entire text.", "Word embeddings are an approach for distributional semantics which represents words as vectors of real numbers. Such representation has useful clustering properties, since the words that are semantically and syntactically related are represented by similar vectors BIBREF22 . For example, the words \"coffee\" and \"tea\" will be very close in the created space."]}
{"question_id": "2b021e1486343d503bab26c2282f56cfdab67248", "predicted_answer": "The question asks about the best performing model, but the provided context does not explicitly mention the performance of different models. It discusses the evaluation metrics used and the features used in the SiTAKA system, but it does not provide a comparison of the performance of different models. Therefore, it is not possible to determine the best performing model based on the provided context.", "predicted_evidence": ["The evaluation metrics used by the task organizers were the macroaveraged recall ( INLINEFORM0 ), the F1 averaged across the positives and the negatives INLINEFORM1 and the accuracy ( INLINEFORM2 ) BIBREF25 .", "With this approach, most studies have focused on designing a set of efficient features to obtain a good classification performance BIBREF1 , BIBREF2 , BIBREF3 . For instance, the authors in BIBREF4 used diverse sentiment lexicons and a variety of hand-crafted features.", "SiTAKA uses five types of features: basic text, syntactic, lexicon, cluster and Word Embeddings. These features are described in the following subsections:", "This section explains the tools and the resources that have been used in the SiTAKA system. Let us denote to its Arabic language and English language versions by Ar-SiTAKA and En-SiTAKA, respectively.", "This section explains the main steps of the SiTAKA system, the features used to describe a tweet and the classification method."]}
{"question_id": "e801b6a6048175d3b1f3440852386adb220bcb36", "predicted_answer": "The dataset sizes mentioned in the text are:\n\n* Arabic-SKIP-G300: 6M words and phrases\n* Word2vec: 3M words and phrases, trained on 100 billion words from the Google News dataset\n* SSWEu: 100K words\n* Ark Tweet NLP tool: 56M English-language tweets\n* Word2vec cluster ngrams: 255,657 words from a Twitter dataset", "predicted_evidence": ["In Ar-SiTAKA we used the model Arabic-SKIP-G300 provided by BIBREF13 . Arabic-SKIP-G300 has been trained on a large corpus of Arabic text collected from different sources such as Arabic Wikipedia, Arabic Gigaword Corpus, Ksucorpus, King Saud University Corpus, Microsoft crawled Arabic Corpus, etc. It contains 300-dimensional vectors for 6M words and phrases.", "We used two pre-trained embedding models in En-SiTAKA. The first one is word2vec which is provided by Google. It is trained on part of the Google News dataset (about 100 billion words) and it contains 300-dimensional vectors for 3M words and phrases BIBREF11 . The second one is SSWEu, which has been trained to capture the sentiment information of sentences as well as the syntactic contexts of words BIBREF12 . The SSWEu model contains 50-dimensional vectors for 100K words.", "The evaluation metrics used by the task organizers were the macroaveraged recall ( INLINEFORM0 ), the F1 averaged across the positives and the negatives INLINEFORM1 and the accuracy ( INLINEFORM2 ) BIBREF25 .", "We used sum, standard-deviation, min and max pooling functions BIBREF23 to obtain the tweet representation in the embedding space. The result is the concatenation of vectors derived from different pooling functions. More formally, let us consider an embedding matrix INLINEFORM0 and a tweet INLINEFORM1 , where INLINEFORM2 is the dimension size, INLINEFORM3 is the length of the vocabulary (i.e. the number of words in the embedding model), INLINEFORM4 is the word INLINEFORM5 in the tweet and INLINEFORM6 is the number of words. First, each word INLINEFORM7 is substituted by the corresponding vector INLINEFORM8 in the matrix INLINEFORM9 where INLINEFORM10 is the index of the word INLINEFORM11 in the vocabulary. This step ends with the matrix INLINEFORM12 . The vector INLINEFORM13 is computed using the following formula: DISPLAYFORM0", "We used two set of clusters in En-SiTAKA to represent the English-language tweets by mapping each tweet to a set of clusters. The first one is the well known set of clusters provided by the Ark Tweet NLP tool which contains 1000 clusters produced with the Brown clustering algorithm from 56M English-language tweets. These 1000 clusters are used to represent each tweet by mapping each word in the tweet to its cluster. The second one is Word2vec cluster ngrams, which is provided by BIBREF21 . They used the word2vec tool to learn 40-dimensional word embeddings of 255,657 words from a Twitter dataset and the K-means algorithm to cluster them into 4960 clusters. We were not able to find publicly available semantic clusters to be used in Ar-SiTAKA."]}
{"question_id": "3699927c6c1146f5057576034d226a99946d52cb", "predicted_answer": "The languages evaluated in this study are:\n\n1. English\n2. German (de)\n3. French (fr)\n4. Chinese", "predicted_evidence": ["Comparing results across languages we observe that scores for English exceed scores for all other languages. At the same time, for almost all models and languages the IAA scores fall under the category of `fair agreement' ( $0.20 < \\kappa < 0.40$ ) indicating that the elicitation task was feasible for crowdworkers. This applies to both evaluations (Tables 5 and 6 ). We observed a similar pattern in the results of Experiment 1 (Table 3 ). We believe there are two reasons for this drop. Firstly, in order to perform cross-linguistic experiments, we translated English categories into other languages. As mentioned in Sections \"Results\" and \"Results\" , such a direct correspondence may not always exist. Consequently, annotators for languages other than English are faced with a noisier (and potentially harder) task.", "We observe a slight drop in performance for languages other than English which is likely due to smaller stimuli sets (see Table 2 ). BCF, nevertheless, achieves purity scores of 0.4 or higher for all languages, meaning that on average at least 40% of the members of a gold standard category are clustered together by BCF (purity rises to 58% for English). This indicates that meaningful categories emerge throughout. Qualitative model output shown in Figures 2 (English) and 21 (all languages) corroborates this result. The categories shown are intuitively meaningful; in particular vegetable and clothing (Figure 21 ) are interpretable, and thematically consistent across languages.", "Comparing results across languages we observe that scores for English exceed scores for all other languages. At the same time, for almost all models and languages the IAA scores fall under the category of `fair agreement' ( $0.20 < \\kappa < 0.40$ ) indicating that the elicitation task was feasible for crowdworkers. This applies to both evaluations (Tables 5 and 6 ). We observed a similar pattern in the results of Experiment 1 (Table 3 ). We believe there are two reasons for this drop. Firstly, in order to perform cross-linguistic experiments, we translated English categories into other languages. As mentioned in Sections \"Results\" and \"Results\" , such a direct correspondence may not always exist. Consequently, annotators for languages other than English are faced with a noisier (and potentially harder) task. Secondly, while it is straightforward to recruit English native speakers on crowd sourcing platforms, it has proven more challenging for the other languages.", "At the same time, for almost all models and languages the IAA scores fall under the category of `fair agreement' ( $0.20 < \\kappa < 0.40$ ) indicating that the elicitation task was feasible for crowdworkers. This applies to both evaluations (Tables 5 and 6 ). We observed a similar pattern in the results of Experiment 1 (Table 3 ). We believe there are two reasons for this drop. Firstly, in order to perform cross-linguistic experiments, we translated English categories into other languages. As mentioned in Sections \"Results\" and \"Results\" , such a direct correspondence may not always exist. Consequently, annotators for languages other than English are faced with a noisier (and potentially harder) task. Secondly, while it is straightforward to recruit English native speakers on crowd sourcing platforms, it has proven more challenging for the other languages. We suspect that our effort to recruit native speakers, might not have been entirely fail-safe for languages other that English, and that the language competence of those crowdworkers might have impacted the quality of their judgments.", "Figures 2 and 21 qualitatively support the claim that BCF learns meaningful features across languages, which are overall coherent and relevant to their associated category. Some interesting cultural differences emerge, for example German is the only language for which a measurement feature type is induced for vegetables (Figure 21 ; de, 4th from left), while for clothing, a fashion industry feature emerges in French (Figure 21 ; fr, 3rd from left). For the same category, a feature type pertaining to colour emerges for all five languages ( 21 , bold margins). In addition, some features in other languages were not straightforwardly translatable into English. For example, the 3rd feature type for vegetables in Chinese (Figure 21 ) includes the word UTF8gbsn \u5206 which refers to the extent to which food is cooked and UTF8gbsn \u70c2 which is the stage when food starts to fall apart after cooking (stewing)."]}
{"question_id": "6606160e210d05b94f7cbd9c5ff91947339f9d02", "predicted_answer": "Yes, the categories mentioned in the text were human-evaluated. The text states that \"human subjects are presented with small sets of often artificial concepts\" and that \"hypotheses and principles of human categorization are established based on the processes and characteristics of the categorizations produced by the participants.\" This indicates that human subjects were involved in evaluating and categorizing the concepts, and that their responses were used to inform theories of human categorization.", "predicted_evidence": ["Given their prevalent function in human cognition, the acquisition and representation of categories has attracted considerable attention in cognitive science, and numerous theories have emerged BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 . Empirical studies of category acquisition and representation, have been predominantly based on small-scale laboratory experiments. In a typical experiment, human subjects are presented with small sets of often artificial concepts, such as binary strings BIBREF14 or colored shapes BIBREF15 , with strictly controlled features BIBREF16 , BIBREF17 , BIBREF18 . Hypotheses and principles of human categorization are established based on the processes and characteristics of the categorizations produced by the participants.", "Even though much empirical research glosses over this observation, there is strong evidence that human conceptual representations are structured (see BIBREF37 for a recent critique and overview of cognitive studies of categorization). Categories mentally represent the complex structure of the environment. They allow to make inferences about concepts or categories that go beyond their perceived similarities capturing abstract and potentially complex properties (for example, the nutritional value of food items, or the emotional benefits of pets). Much research on human categorization is based on laboratory experiments where subjects are presented with artificial stimuli represented by a restricted set of task-relevant features. Observations of natural concepts, however, are often noisy or incomplete so that a notion of systematic relations among features might be more important here than under artificial conditions in the lab BIBREF38 .", "Given their prevalent function in human cognition, the acquisition and representation of categories has attracted considerable attention in cognitive science, and numerous theories have emerged BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 . Empirical studies of category acquisition and representation, have been predominantly based on small-scale laboratory experiments. In a typical experiment, human subjects are presented with small sets of often artificial concepts, such as binary strings BIBREF14 or colored shapes BIBREF15 , with strictly controlled features BIBREF16 , BIBREF17 , BIBREF18 . Hypotheses and principles of human categorization are established based on the processes and characteristics of the categorizations produced by the participants. The distribution of subjects participating in such studies is often skewed towards members of cultural and socioeconomic groups which are prevalent in the environment where the research is conducted, and typically consists to a large proportion of western, educated, wealthy and English-speaking participants often sampled from the even more specific population of college students.", "Empirical studies of category acquisition and representation, have been predominantly based on small-scale laboratory experiments. In a typical experiment, human subjects are presented with small sets of often artificial concepts, such as binary strings BIBREF14 or colored shapes BIBREF15 , with strictly controlled features BIBREF16 , BIBREF17 , BIBREF18 . Hypotheses and principles of human categorization are established based on the processes and characteristics of the categorizations produced by the participants. The distribution of subjects participating in such studies is often skewed towards members of cultural and socioeconomic groups which are prevalent in the environment where the research is conducted, and typically consists to a large proportion of western, educated, wealthy and English-speaking participants often sampled from the even more specific population of college students. The demographic and socioeconomic bias has been long recognized, and the question of how this bias might impact conclusions about human cognition in general BIBREF19 and category learning specifically is under active debate BIBREF9 .", "Categories such as animal or furniture are fundamental cognitive building blocks allowing humans to efficiently represent and communicate the complex world around them. Concepts (e.g., dog, table) are grouped into categories based on shared properties pertaining, for example, to their behavior, appearance, or function. Categorization underlies other cognitive functions such as perception BIBREF0 , BIBREF1 or language BIBREF2 , BIBREF3 , and there is evidence that categories are not only shaped by the world they represent, but also by the language through which they are communicated BIBREF4 , BIBREF5 . Although mental categories exist across communities and cultures, their exact manifestations differ BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 . For example, American English speakers prefer taxonomic categorizations (e.g., mouse,squirrel) while Chinese speakers tend to prefer to categorize objects relationally (e.g., tree, squirrel; BIBREF7 )."]}
{"question_id": "0dc9050c832a6091bc9db3f7fa7be72139f51177", "predicted_answer": "According to the text, the answer is: yes, language share categories, but their exact manifestations differ across languages and cultures.\n\nThe text states that \"mental categories exist across communities and cultures\", and that \"meaningful categories emerge throughout\" in the computational simulations across five languages (English, French, German, Arabic, and Chinese). However, it also notes that the exact manifestations of these categories differ, for example, in the way American English speakers prefer taxonomic categorizations, while Chinese speakers tend to prefer relational categorizations.", "predicted_evidence": ["Categories such as animal or furniture are fundamental cognitive building blocks allowing humans to efficiently represent and communicate the complex world around them. Concepts (e.g., dog, table) are grouped into categories based on shared properties pertaining, for example, to their behavior, appearance, or function. Categorization underlies other cognitive functions such as perception BIBREF0 , BIBREF1 or language BIBREF2 , BIBREF3 , and there is evidence that categories are not only shaped by the world they represent, but also by the language through which they are communicated BIBREF4 , BIBREF5 . Although mental categories exist across communities and cultures, their exact manifestations differ BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 . For example, American English speakers prefer taxonomic categorizations (e.g., mouse,squirrel) while Chinese speakers tend to prefer to categorize objects relationally (e.g., tree, squirrel; BIBREF7 ).", "In this work we leverage large-scale computational simulations to advance our understanding of categories and features across languages and cultures. Our research touches on the representation of categories, concepts, and their features; the mechanisms with which these are learnt; and the use of computational models and large-scale naturalistic data sets to investigate these questions.", "We observe a slight drop in performance for languages other than English which is likely due to smaller stimuli sets (see Table 2 ). BCF, nevertheless, achieves purity scores of 0.4 or higher for all languages, meaning that on average at least 40% of the members of a gold standard category are clustered together by BCF (purity rises to 58% for English). This indicates that meaningful categories emerge throughout. Qualitative model output shown in Figures 2 (English) and 21 (all languages) corroborates this result. The categories shown are intuitively meaningful; in particular vegetable and clothing (Figure 21 ) are interpretable, and thematically consistent across languages.", "In the remainder of this article, we first review related literature, before we present a cognitively motivated model for learning categories and their structured representations from large natural language corpora. We then evaluate the quality of the emerging representations, as well as the generalizability of our model across languages. Note that the primary goal of this work is not to characterize differences in categories and features arising from different languages (even though this would be an interesting avenue for future work). Rather, we aim to demonstrate the utility of large-scale naturalistic datasets for cognitive modeling, and to verify mechanisms of categorization known from laboratory studies at scale and across communities.", "We approach this question by applying BCF to data sets in five languages: English, French, German, Arabic, and Chinese. We train five models in total, one per language, each time using stimuli from the respective language alone. We evaluate induced categories by comparison against a human-created reference categorization; and collect judgments on the coherence of learnt feature types, and their relevance to their associated categories from large crowds of native speakers."]}
{"question_id": "4beb50ba020f624446ff1ef5bf4adca5ed318b98", "predicted_answer": "The languages evaluated are not explicitly mentioned in the given context. However, based on the mention of the \"One Billion Word dataset\" and the NLP tasks mentioned (syntactic dependency parsing, semantic role labeling, implicit discourse relation recognition, and textual entailment), it can be inferred that the languages being evaluated are likely to be English or languages with similar linguistic characteristics.", "predicted_evidence": ["Recently, pre-trained language representation has shown to be useful for improving many NLP tasks BIBREF0, BIBREF1, BIBREF2, BIBREF3. Embeddings from Language Models (ELMo) BIBREF0 is one of the most outstanding works, which uses a character-aware language model to augment word representation.", "The ESuLMo is evaluated in two ways, task independent and task dependent. For the former, we examine the perplexity of the pre-trained language models. For the latter, we examine on four benchmark NLP tasks, syntactic dependency parsing, semantic role labeling, implicity discourse relation recognition, and textual entailment.", "In this section, we examine the pre-trained language models of ESuLMo in terms of PPL. All the models' training and evaluation are done on One Billion Word dataset BIBREF19 . During training, we strictly follow the same hyper-parameter published by ELMo, including the hidden size, embedding size, and the number of LSTM layers. Meanwhile, we train each model on 4 Nvidia P40 GPUs, which takes about three days for each epoch. Table TABREF5 shows that our pre-trained language models can improve the performance of RNN-based language models by a large margin and our subword-aware language models outperform all previous RNN-based language models, including ELMo, in terms of PPL. During the experiment, we find that 500 is the best vocabulary size for both segmentation algorithms, and BPE is better than ULM in our setting.", "We apply ESuLMo to four benchmark NLP tasks. And we choose the fine-tuned model by validation set and report the results in the test set. The comparisons in Table TABREF10 show that ESuLMo outperforms ELMo significantly in all tasks and achieves the new state-of-the-art result in three of four tasks .", "To apply our pre-trained language models to other NLP tasks, we combine the input vector and the last layer's hidden state of the Bi-LSTM to represent each word."]}
{"question_id": "9bf60073fbb69fbf860196513fc6fd2f466535f6", "predicted_answer": "According to the text, the training time of ESuLMo is not significantly longer compared to ELMo. The text states that \"we train each model on 4 Nvidia P40 GPUs, which takes about three days for each epoch.\" This suggests that the training time of ESuLMo is similar to that of ELMo, as the same hyper-parameters are used for both models.", "predicted_evidence": ["While applying our pre-trained ESuLMo to other NLP tasks, we have two different strategies: (1) Fine-tuning our ESuLMo while training other NLP tasks; (2) Fixing our ESuLMo while training other NLP tasks. During the experiment, we find there is no significant difference between these two strategies. However, the first strategy consumes much more resource than the second one. Therefore, we choose the second strategy to conduct all the remaining experiments.", "We apply ESuLMo to four benchmark NLP tasks. And we choose the fine-tuned model by validation set and report the results in the test set. The comparisons in Table TABREF10 show that ESuLMo outperforms ELMo significantly in all tasks and achieves the new state-of-the-art result in three of four tasks .", "Task Independent vs. Task Specific To discover the necessary training progress, we show the accuracy in SNLI and PPL for language model in Figure FIGREF15. The training curves show that our ESuLMo helps ESIM reach stable accuracy for SNLI while the corresponding PPL of the language model is far away from convergence.", "In this section, we examine the pre-trained language models of ESuLMo in terms of PPL. All the models' training and evaluation are done on One Billion Word dataset BIBREF19 . During training, we strictly follow the same hyper-parameter published by ELMo, including the hidden size, embedding size, and the number of LSTM layers. Meanwhile, we train each model on 4 Nvidia P40 GPUs, which takes about three days for each epoch. Table TABREF5 shows that our pre-trained language models can improve the performance of RNN-based language models by a large margin and our subword-aware language models outperform all previous RNN-based language models, including ELMo, in terms of PPL. During the experiment, we find that 500 is the best vocabulary size for both segmentation algorithms, and BPE is better than ULM in our setting.", "Subword Segmentation Algorithms Tables TABREF5 and TABREF10 show that ESuLMo based on both ULM and BPE segmentation with 500 subwords outperform the original ELMo, and BPE is consistently better than ULM on all evaluations under the same settings. We notice that BPE can give static subword segmentation for the same word in different sentences, while ULM cannot. It suggests that ESuLMo is sensitive to segmentation consistency."]}
{"question_id": "7d503b3d4d415cf3e91ab08bd5a1a2474dd1047b", "predicted_answer": "The vocabulary of subwords can be of various sizes, including 500, 1K, and 2K.", "predicted_evidence": ["$\\bullet $ Repeatedly, we calculate the frequencies of all bigrams and merge the bigram with the highest one until we get the desired subword vocabulary.", "$\\bullet $ Iteratively, the probability of each subword is estimated by the expectation maximization (EM) algorithm and the top $\\eta \\%$ subwords with the highest probabilities are kept. Note that we always keep the single character in subword vocabulary to avoid out-of-vocabulary.", "We also analyze the subword vocabularies from two algorithms and find that the overlapping rates for 500, 1K and 2K sizes are 60.2%, 55.1% and 51.9% respectively. This indicates subword mechanism can stably work in different vocabularies.", "Subword Vocabulary Size Tables TABREF5 and TABREF10 show the performance of ESuLMo drops with the vocabulary size increases . We explain the trend that neural network pipeline especially CNN would fail to capture necessary details of building word embeddings as more subwords are introduced.", "Splitting a word into subwords and using them to augment the word representation may recover the latent syntactic or semantic information BIBREF8. For example, uselessness could be split into the following subwords: $<$use, less, ness$>$. Previous work usually considers linguistic knowledge-based methods to tokenize each word into subwords (namely, morphemes) BIBREF9, BIBREF10, BIBREF11. However, such treatment may encounter three main inconveniences. First, the subwords from linguistic knowledge, typically including the morphological suffix, prefix, and stem, may not be suitable for a targeted NLP task BIBREF12 or mislead the representation of some words, like the meaning of understand cannot be formed by under and stand. Second, linguistic knowledge, including related annotated lexicons or corpora, may not even be available for a specific low-resource language."]}
{"question_id": "1c8958ec50976a9b1088c51e8f73a767fb3973fa", "predicted_answer": "According to the given context, the RNN classifiers used were LSTM (Long Short-Term Memory Network) classifiers.", "predicted_evidence": ["To improve classification ability we employ an ensemble of LSTM-based classifiers.", "The power of neural networks comes from their ability to find data representations that are useful for classification. Recurrent Neural Networks (RNN) are a special type of neural network, which can be thought of as the addition of loops to the architecture. RNNs use back propagation in the training process to update the network weights in every layer. In our experimentation we used a powerful type of RNN known as Long Short-Term Memory Network (LSTM). Inspired by the work by BIBREF15 , we experiment with combining various LSTM models enhanced with a number of novel features in an ensemble. More specifically we introduce:", "In total we experimented with 11 different setups of the proposed scheme, each with a different ensemble of classifiers, see Table TABREF17 .", "The results are shown in Table TABREF24 , along with the reported results from state of the art approaches proposed by other researchers in the field. Note that the performance numbers P,R and F of the other state of the art approaches are based on the authors' reported data in the cited works. Additionally, we report the performance of each individual LSTM classifier as if used alone over the same data (that is, without the ensemble logic). The F-score for our proposed approaches shown in the last column, is the weighted average value over the 3 classes (Neutral,Sexism,Racism). Moreover, all the reported values are average values produced for a number of runs of the same tested scheme over the same data. Figure FIGREF23 shows the F-Score as a function of the number of training samples for each ensemble of classifiers. We clearly see that the models converge.", "The results are shown in Table TABREF24 , along with the reported results from state of the art approaches proposed by other researchers in the field. Note that the performance numbers P,R and F of the other state of the art approaches are based on the authors' reported data in the cited works. Additionally, we report the performance of each individual LSTM classifier as if used alone over the same data (that is, without the ensemble logic). The F-score for our proposed approaches shown in the last column, is the weighted average value over the 3 classes (Neutral,Sexism,Racism). Moreover, all the reported values are average values produced for a number of runs of the same tested scheme over the same data. Figure FIGREF23 shows the F-Score as a function of the number of training samples for each ensemble of classifiers. We clearly see that the models converge. For the final run the F-score has standard deviation value not larger than 0.001, for all classifiers."]}
{"question_id": "363d0cb0cd5c9a0b0364d61d95f2eff7347d5a36", "predicted_answer": "The results of their system are not explicitly stated in the given context. The text describes the research question, goals, methodology, and experimental setup, but it does not provide the actual results or performance metrics obtained by their system.", "predicted_evidence": ["The research question we address in this work is:", "To answer this question, our main goals can be summarized as follows:", "Another interesting finding is the observed performance improvement by using an ensemble instead of a single classifier; some ensembles outperform the best single classifier. Furthermore, the NRS classifier, which produces the best score in relation to other single classifiers, is the one included in the best performing ensemble.", "To achieve stability in the results produced, we ran every single classifier for 15 times and the output values were aggregated. In addition, the output from each single classifier run was combined with the output from another two single classifiers to build the input of an ensemble, producing INLINEFORM0 combinations. For the case of the ensemble that incorporates all five classifiers we restricted to using the input by only the first five runs of the single classifiers ( INLINEFORM1 combinations). That was due to the prohibitively very large number of combinations that were required.", "To avoid over-fitting, the model training was allowed to run for a maximum number of 100 epochs, out of which the optimally trained state was chosen for the model evaluation. An optimal epoch was identified so, such that the validation accuracy was maximized, while at the same time the error remained within INLINEFORM0 of the lowest ever figure within the current fold. Throughout the experiment we observed that the optimal epochs typically occurred after between the 30 and 40 epochs."]}
{"question_id": "cf0b7d8a2449d04078f69ec9717a547adfb67d17", "predicted_answer": "The existing approaches to automatic detection of abusive messages include:\n\n1. Manual effort: Relying on users to report offensive comments, which requires a huge effort by human annotators and has the risk of applying discrimination under subjective judgment.\n2. Current solutions deployed by social media services: These solutions have attempted to address the problem with manual effort, but are not scalable and have strong impact on system response times.\n\nNote that these existing approaches are falling short to effectively detect abusive messages, and there is a need for new algorithms that can classify such content more effectively and efficiently.", "predicted_evidence": ["To answer this question, our main goals can be summarized as follows:", "The research question we address in this work is:", "Note that existing solutions for automatic detection are still falling short to effectively detect abusive messages. Therefore there is a need for new algorithms which would do the job of classification of such content more effectively and efficiently. Our work is towards that direction.", "Social media services such as Facebook and Twitter have been criticized for not having done enough to prohibit the use of their services for attacking people belonging to some specific race, minority etc. BIBREF0 . They have announced though that they would seek to battle against racism and xenophobia BIBREF1 . Nevertheless, the current solutions deployed by them have attempted to address the problem with manual effort, relying on users to report offensive comments BIBREF2 . This not only requires a huge effort by human annotators, but it also has the risk of applying discrimination under subjective judgment. Moreover, a non-automated task by human annotators would have strong impact on system response times, since a computer-based solution can accomplish this task much faster than humans. The massive rise in the user-generated content in the above social media services, with manual filtering not being scalable, highlights the need for automating the process of on-line hate-speech detection.", "We believe that deep learning models have a high potential wrt. classifying text or analyzing the sentiment in general. In our opinion there is still space for further improving the classification algorithms."]}
{"question_id": "8de0e1fdcca81b49615a6839076f8d42226bf1fe", "predicted_answer": "The text does not explicitly mention a specific dataset used by the authors. It references various bibliographic sources (BIBREF10, BIBREF11, etc.) that likely correspond to different datasets or research papers, but it does not specify a particular dataset used in their work.", "predicted_evidence": ["The power of some of the best conversational assistants lies in domain-dependent human knowledge. Amazon's Alexa is improving with the user generated data it gathers BIBREF10. Some of the most common human knowledge base structures used in NLP are word lists such as dictionaries for ASR BIBREF11, sentiment lexicons BIBREF12 knowledge graphs such as WordNet BIBREF13, BIBREF14 and ConceptNet BIBREF15. Conceptually, our work is similar to BIBREF16, however, they do not allow for fuzzy search through the lattice.", "The above methods allow the annotators to create hundreds of intents efficiently, with thousands of examples, allowing millions of distinct potential phrases to be matched. When combined with the ability for customers to configure entities and select a subset of intents that are relevant to their business, this approach produces highly customer-specific repositories of domain knowledge.", "Finite state transducers have been widely used in speech recognition BIBREF36, BIBREF37, BIBREF38, named entity recognition BIBREF39, BIBREF40, morpho-syntactic tagging BIBREF41, BIBREF42, BIBREF43 or language generation BIBREF44.", "Domain-adaptation of an ASR system in spite of data scarcity issues. Generic ASR systems tend to be lackluster when confronted with specialized jargon, often very specific to a single domain (e.g., healthcare services). Creating a new ASR model for each domain is often impractical due to limited in-domain data availability or long training times. Our method improves the speech recognition, without the need for any re-training, by improving the recognition recall of the anticipated intents \u2013 the key insight sources in these conversations.", "Finally, speech based interfaces are defined by a set of dimensions, such as domain and vocabulary (retail, finance, entertainment), language (English, Spanish), application (voice search, personal assistant, information retrieval), and environment (mobile, car, home, distant speech recognition). These dimensions make it very challenging to provide a low cost domain adaptation."]}
{"question_id": "909ecf675f874421eecc926a9f7486475aa1423c", "predicted_answer": "The extracted intent is used to rescore by applying a set of heuristics to the annotated lattice $\\mathcal{A}$. The heuristics are applied sequentially to search for the best path, which is then composed with the annotated lattice $\\mathcal{A}$ to produce the annotated lattice $\\mathcal{A^*}$ with the final variant of the transcript. The heuristics are:\n\n(a) Select the path with the longest intent annotation;\n(b) Select the path with the largest number of intent annotations;\n(c) Select the path with the intent annotation with the longest span (i.e., consider also blank words);\n(d) Select the path with the highest original ASR likelihood.\n\nThe output intent annotations are retrieved by traversing every path in $\\mathcal{A^*}$.", "predicted_evidence": ["To perform intent annotation (i.e., to recognize and mark the position of intent instances in the transcript), we first create the FST index $\\mathcal {I}$ of all intent examples. This index is a transducer which maps the alphabet of words (input symbols) onto the alphabet of intents (output symbols). We construct index $\\mathcal {I}$ in such a way that its composition with the lattice results in another transducer $\\mathcal {A} = \\mathcal {L} \\circ \\mathcal {I}$ representing the annotated lattice.", "The proposed algorithm finds 658 549 intents in all conversations, covering 4.1% of all (62 450 768) words, whereas the baseline algorithm finds 526 356 intents, covering 3.3% of all words. Therefore, the increase in intent recognition of the method is 25.1% by rescoring 8.3% of all annotated words (0.34% of all words). Particular intents achieve different improvements ranging from no improvement up to 1062% \u2013 ranked percentile results are presented in Table TABREF23. We see that half of intents gain at least 35.7% of improvement, while 20% of all intents gain at least 83.5%.", "While an in-depth description of tools used in the intent definition process is beyond the scope of this paper, we provide a brief overview to underline the application potential of our algorithm when combined with a sufficient body of domain knowledge. First, let us formalize the notion of intents and intent examples. An intent example is a sequence of words which conveys a particular meaning, e.g., \"let me go over your account\" or \"this is outrageous\". An intent is a collection of intent examples conveying a similar meaning, which can be labeled with an intelligible and short description helpful in understanding the conversation.", "The complete set of heuristics applied sequentially to the annotated lattice $\\mathcal {A}$ in search of the best path is the following: (a) select the path with the longest intent annotation; (b) select the path with the largest number of intent annotations; (c) select the path with the intent annotation with the longest span (i.e. consider also blank words), (d) select the path with the highest original ASR likelihood. The chosen best path is composed with the annotated lattice $\\mathcal {A}$ to produce the annotated lattice $\\mathcal {A^*}$ with the final variant of the transcript. The output intent annotations are retrieved by traversing every path in $\\mathcal {A^*}$.", "To evaluate the effectiveness of the proposed algorithm, we have sampled a dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain. The correctness of the rescoring was judged by two annotators, who labeled 250 examples each. The annotators read the whole conversation transcript and listened to the recording to establish whether the rescoring is meaningful. In cases when a rescored word was technically incorrect (e.g., mistaken tense of a verb), but the rescoring led to the recognition of the correct intent, we labeled the intent annotation as correct. The results are shown in Table TABREF22. Please note that every result above 50% indicates an improvement over the ASR best path recognition, since we correct more ASR errors than we introduce new mistakes."]}
{"question_id": "29477c8e28a703cacb716a272055b49e2439a695", "predicted_answer": "No, they do not evaluate by how much their FST improves ASR compared to state-of-the-art. Instead, they evaluate their method by comparing the correctness of the rescoring of intent annotations in the lattices with the ASR best path recognition, and show that their method improves the recognition recall of anticipated intents.", "predicted_evidence": ["Last but not least, production ASR systems impose strict constraints on the additional computation that can be performed. Since we operate in a near real-time environment, this precludes the use of computationally expensive language models which could compensate for some of the ASR errors.", "Firstly, each ASR engine introduces a mixture of systematic and stochastic errors which are intrinsic to the procedure of transcription of spoken audio. The quality of transcription, as measured by the popular word error rate (WER), attains the level of 5%-15% WER for high quality ASR systems for English BIBREF4, BIBREF5, BIBREF6, BIBREF7. The WER highly depends on the evaluation data difficulty and the speed to accuracy ratio. Importantly, errors in the transcription appear stochastically, both in audio segments which carry important semantic information, as well as in inessential parts of the conversation.", "ASR transcripts lack punctuation, normalization, true-casing of words, and proper segmentation into phrases as these features are not present in the conversational input BIBREF9. These are difficult to correct as the majority of NLP algorithms have been trained and evaluated on text and not on the output of an ASR system. Thus, a simple application of vanilla NLP tools to ASR transcripts seldom produces actionable and useful results.", "To evaluate the effectiveness of the proposed algorithm, we have sampled a dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain. The correctness of the rescoring was judged by two annotators, who labeled 250 examples each. The annotators read the whole conversation transcript and listened to the recording to establish whether the rescoring is meaningful. In cases when a rescored word was technically incorrect (e.g., mistaken tense of a verb), but the rescoring led to the recognition of the correct intent, we labeled the intent annotation as correct. The results are shown in Table TABREF22. Please note that every result above 50% indicates an improvement over the ASR best path recognition, since we correct more ASR errors than we introduce new mistakes.", "Domain-adaptation of an ASR system in spite of data scarcity issues. Generic ASR systems tend to be lackluster when confronted with specialized jargon, often very specific to a single domain (e.g., healthcare services). Creating a new ASR model for each domain is often impractical due to limited in-domain data availability or long training times. Our method improves the speech recognition, without the need for any re-training, by improving the recognition recall of the anticipated intents \u2013 the key insight sources in these conversations."]}
{"question_id": "9186b2c5b7000ab7f15a46a47da73ea45544bace", "predicted_answer": "The model is evaluated against the original recursive training algorithm by using the trained model to segment the training data and comparing the resulting segmentation to a Morfessor Baseline model. The evaluation metrics used are the unweighted prior and likelihood, as well as their \u03b1-weighted sum.", "predicted_evidence": ["In the first phase, a character-level language model is trained. The initial probabilities of the subwords are computed using the language model. The probabilities are refined by EM, followed by hard-EM. During the hard-EM, frequency based pruning of subwords begins.", "When applied to a (hidden) Markov model, EM is called the forward-backward algorithm. Using instead the related Viterbi algorithm BIBREF9 is sometimes referred to as hard-EM. spitkovsky2011lateen present lateen-EM, a hybrid variant in which EM and Viterbi optimization are alternated.", "In the second phase, hard-EM is used for parameter estimation. At the end of each iteration, the least frequent subwords are selected as candidates for pruning. For each candidate subword, the change in likelihood when removing the subword is estimated by resegmenting all words in which the subword occurs. After each pruned subword, the parameters of the model are updated. Pruning ends when the goal lexicon size is reached or the change in likelihood no longer exceeds a given threshold.", "The Expectation Maximization (EM) algorithm BIBREF8 is an iterative algorithm for finding Maximum Likelihood (ML) or Maximum a Posteriori (MAP) estimates for parameters in models with latent variables. The EM algorithm consists of two steps. In the E-step (SECREF5), the expected value of the complete data likelihood including the latent variable is taken, and in the M-step (SECREF5), the parameters are updated to maximize the expected value of the E-step: Q(, (i-1)) = y (, y ) (y , (i-1)) dy", "The ability of the training algorithm to find parameters minimizing the Morfessor cost is evaluated by using the trained model to segment the training data, and loading the resulting segmentation as if it was a Morfessor Baseline model. We observe both unweighted prior and likelihood, and their $\\alpha $-weighted sum."]}
{"question_id": "d30b2fb5b29faf05cf5e04d0c587a7310a908d8c", "predicted_answer": "The improvement in performance compared to the linguistic gold standard is measured by the boundary F1-score (BPR), which is the harmonic mean of precision and recall. The proposed Morfessor EM+Prune configuration significantly outperforms Morfessor Baseline in terms of F-score for all languages except North S\u00e1mi, where there is no significant difference between the methods.", "predicted_evidence": ["Figure shows the Precision\u2013Recall curves for the primary systems, for all four languages. While increasing the Morfessor cost, forced splitting improves BPR. Tables to show test set Boundary Precision, Recall and F$_{1}$-score (BPR) results at the optimal tuning point (selected using a development set) for each model, for English, Finnish, Turkish and North S\u00e1mi, respectively. The default Morfessor EM+Prune configuration (\u201csoft\u201d EM, full prior, forcesplit) significantly outperforms Morfessor Baseline w.r.t. the F-score for all languages except North S\u00e1mi, for which there is no significant difference between the methods.", "English, Finnish and Turkish data are from the Morpho Challenge 2010 data set BIBREF17, BIBREF18. The training sets contain ca 878k, 2.9M and 617k word types, respectively. As test sets we use the union of the 10 official test set samples. For North S\u00e1mi, we use a list of ca 691k word types extracted from Den samiske tekstbanken corpus (Sametinget, 2004) and the 796 word type test set from version 2 of the data set collected by BIBREF19, BIBREF20.", "The closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary $F_{1}$-score BIBREF21. The boundary $F_{1}$-score (F-score for short) equals the harmonic mean of precision (the percentage of correctly assigned boundaries with respect to all assigned boundaries) and recall (the percentage of correctly assigned boundaries with respect to the reference boundaries). Precision and recall are calculated using macro-averages over the word types in the test set. In the case that a word has more than one annotated segmentation, we take the one that gives the highest score.", "Omitting the frequency distribution appears to have little effect on Morfessor cost and BPR. Turning off Bayesian EM (noexp$\\Psi $) results in a less compact lexicon resulting in higher prior cost, but improves BPR for two languages: English and Turkish.", "Tables to show the Morfessor cost of the segmented training data for particular $\\alpha $ values. Again, the proposed Morfessor EM+Prune reaches a lower Morfessor cost than Morfessor Baseline. Using the lateen-EM has only minimal effect to the costs, decreasing the total cost slightly for English and increasing for the other languages. Turkish results include the \u201ckeep-redundant\u201d setting discussed below in more detail."]}
{"question_id": "526dc757a686a1fe41e77f7e3848e3507940bfc4", "predicted_answer": "The improvement in performance brought by lexicon pruning on a simple EM algorithm is not explicitly stated in the given context. However, it can be inferred that lexicon pruning helps to improve the performance of the EM algorithm by reducing the size of the lexicon and removing unnecessary subwords, which can lead to faster convergence and better parameter estimation.\n\nThe use of pruning criteria, such as Viterbi counts and estimated change in likelihood, helps to identify and remove subwords that do not contribute significantly to the model's performance. This can lead to a more efficient and effective use of the EM algorithm, as it can focus on the most important subwords and avoid overfitting.\n\nAdditionally, the iterative pruning process, where a fixed proportion of remaining subwords are pruned each iteration, helps to avoid aggressive pruning and allows for more gradual and controlled reduction of the lexicon size.\n\nOverall, the combination of EM and pruning (EM+Prune) can lead to improved performance and more efficient use of computational resources, especially when compared to simple EM algorithms without pruning.", "predicted_evidence": ["Each iteration begins with 3 sub-iterations of EM. In the pruning phase of each iteration, the subwords in the current lexicon are sorted in ascending order according to the estimated change in the cost function if the subword is removed from the lexicon. Subwords consisting of a single character are always kept, to retain the ability to represent an open vocabulary without OOV issues. The list is then pruned according to one of three available pruning criteria:", "Another solution is to combine EM with pruning (EM+Prune). The methods based on pruning begin with a seed lexicon, which is then iteratively pruned until a stopping condition is reached. Subwords cannot be added to the lexicon after initialization. As a consequence, proper initialization is important, and the methods should not prune too aggressively without reestimating parameters, as pruning decisions cannot be backtracked. For this reason, EM+Prune methods proceed iteratively, only pruning subwords up to a predefined iteration pruning quota, e.g. removing at most 20% of the remaining lexicon at a time.", "Morfessor EM+Prune is less responsive to tuning than Morfessor Baseline. This is visible in the shorter lines in Figures and , although the tuning parameter takes values from the same range. In particular, EM+Prune can not easily be tuned to produce very large lexicons.", "The seed lexicon is simply the e.g. one million most frequent substrings. SentencePiece uses an EM+Prune training algorithm. Each iteration consists of two sub-iterations of EM, after which the lexicon is pruned. Pruning is based on Viterbi counts (EM+Viterbi-prune). First, subwords that do not occur in the Viterbi segmentation are pre-pruned. The cost function is the estimated change in likelihood when the subword is removed, estimated using the assumption that all probability mass of the removed subword goes to its Viterbi segmentation. Subwords are sorted according to the cost, and a fixed proportion of remaining subwords are pruned each iteration. Single character subwords are never pruned. A predetermined lexicon size is used as the stopping condition.", "In the second phase, hard-EM is used for parameter estimation. At the end of each iteration, the least frequent subwords are selected as candidates for pruning. For each candidate subword, the change in likelihood when removing the subword is estimated by resegmenting all words in which the subword occurs. After each pruned subword, the parameters of the model are updated. Pruning ends when the goal lexicon size is reached or the change in likelihood no longer exceeds a given threshold."]}
{"question_id": "2d91554c3f320a4bcfeb00aa466309074a206712", "predicted_answer": "The metrics used to evaluate results are:\n\n1. BLEU (BiLingual Evaluation Understudy)\n2. METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n3. chrF", "predicted_evidence": ["METEOR (Metric for Evaluation of Translation with Explicit ORdering) is based on the harmonic mean of precision and recall, whereby recall is weighted higher than precision. In addition to exact word (or phrase) matching it has additional features, i.e. stemming, paraphrasing and synonymy matching. In contrast to BLEU, the metric produces good correlation with human judgement at the sentence or segment level.", "The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU BIBREF35 , METEOR BIBREF36 and chrF BIBREF37 metrics.", "BLEU (BiLingual Evaluation Understudy) is calculated for individual translated segments (n-grams) by comparing them with a data set of reference translations. The calculated scores, between 0 and 100 (perfect translation), are averaged over the whole evaluation data set to reach an estimate of the translation's overall quality. Considering the short length of the terms in WordNet, while we report scores based on the unigram overlap (BLEU-1), and as this is in most cases only precision, so in addition we also report other metrics.", "The average length of each word in the text are also compared as above.", "The development of a match is performed by a matcher, which defines the process for finding the optimal match between two lists of entities. The most straightforward matcher is called the greedy matcher, which simply compares each pair of entities between the two datasets using a similarity function to compute the similarity between this pair. The similarity calculation is divided into three stages: firstly, the lens examines the entity pair and extracts information that can be more easily compared, for example a pair of labels one for each element. This is then fed into a feature extractor that analyses the facet to produce a numerical value that is assumed to be related to the similarity of this entity pair. Finally, a supervised similarity classifier is used to aggregate a number of features and is trained on existing training data. This then creates a single score between zero (totally dissimilar) and one (identical) for each pair of entities between the two datasets. The greedy matcher then proceeds by adding matches between the dataset that increase a topological constrained score (TCS), which is a function that both checks the validity and computes the score of a matching between two datasets."]}
{"question_id": "53362c2870cf76b7981c27b3520a71eb1e3e7965", "predicted_answer": "The question is not directly answered in the given context, but we can make an educated inference. The performance of the metrics (BLEU, chrF3) is not directly related to the number of languages used. The metrics are used to evaluate the quality of translations, and their performance is more dependent on the quality of the translations and the reference datasets used.\n\nHowever, the use of multiple languages can indirectly impact the performance of the metrics. For example, using multiple languages can provide more reference translations, which can improve the robustness of the metrics. Additionally, the Polylingual WordNet, which is a multilingual resource, can provide a more comprehensive coverage of concepts and senses, which can lead to better performance of the metrics.\n\nIn summary, while the number of languages used may not directly impact the performance of the metrics, it can indirectly influence the quality of the translations and the reference datasets, which can in turn affect the performance of the metrics.", "predicted_evidence": ["chrF3 is a character n-gram metric, which has shown very good correlations with human judgements on the WMT2015 shared metric task BIBREF38 , especially when translating from English into morphologically rich(er) languages. As there are multiple translations available for each sense in the target wordnet we use all translations as multiple references for BLEU, for the other two metrics we compare only to the most frequent member of the synset.", "BLEU (BiLingual Evaluation Understudy) is calculated for individual translated segments (n-grams) by comparing them with a data set of reference translations. The calculated scores, between 0 and 100 (perfect translation), are averaged over the whole evaluation data set to reach an estimate of the translation's overall quality. Considering the short length of the terms in WordNet, while we report scores based on the unigram overlap (BLEU-1), and as this is in most cases only precision, so in addition we also report other metrics.", "To cross the language barrier, huge efforts have been made to extend the Princeton WordNet with multilingual information in projects, such as EuroWordNet BIBREF1 , BalkaNet BIBREF2 and MultiWordNet BIBREF3 , mostly following the extend approach, where the structure of the Princeton WordNet is preserved and only the words in each synset are translated and new synsets are added for concepts. Furthermore, the Princeton WordNet has many fewer concepts than large scale encyclopedias such as Wikipedia and resources derived from it such as DBpedia BIBREF4 and BabelNet BIBREF5 . This problem is even worse for many non-English wordnets, due to the extend approach, as these resources have even fewer synsets than Princeton WordNet. Furthermore, there are still many languages for which a wordnet does not exist or is not available to all potential users due to licensing restrictions.", "Polylingual WordNet is the largest multilingual resource released under an open license and the only resource that has been developed in a fully automatic manner. As such, the resource plays a number of useful roles in covering applications for languages or applications where the manually constructed wordnets do not have sufficient coverage. Moreover, this WordNet is intended to be a basis that can help in the translation of existing wordnets by providing a basis from which lexicographers can work.", "Overall they obtained over 2 million senses for over 100 thousand concepts, linking over 1.4 million words in hundreds of languages. Since using existing lexical resources guarantees a high precision, it may also provide a low recall due to the limitedness of lexical resources in different languages and domains. A different approach to expand English WordNet synsets with lexicalizations in other languages was proposed in BIBREF51 . The authors do not directly match concepts in the two different language resources, but demonstrate an approach that learns how to determine the best translation for English synsets by taking bilingual dictionaries, structural information of the English WordNet and corpus frequency information into account. With the growing amount of parallel data, BIBREF52 show an approach to acquire a set of synsets from parallel corpora. The synsets are obtained by comparing aligned words in parallel corpora in several languages. Similarly, the sloWNet for Slovene BIBREF53 and Wolf for French BIBREF12 are constructed using a multilingual corpus and word alignment techniques in combination with other existing lexical resources."]}
{"question_id": "5138121b9e9bd56962e69bfe49d5df5301cb7745", "predicted_answer": "The answer is not explicitly stated in the given context. The question asks by how much the approach outperforms translating without contextual information, but the text does not provide a direct comparison or a specific metric to answer this question. It describes the approach and its components, but it does not provide a quantitative evaluation of its performance compared to translating without contextual information.", "predicted_evidence": ["BLEU (BiLingual Evaluation Understudy) is calculated for individual translated segments (n-grams) by comparing them with a data set of reference translations. The calculated scores, between 0 and 100 (perfect translation), are averaged over the whole evaluation data set to reach an estimate of the translation's overall quality. Considering the short length of the terms in WordNet, while we report scores based on the unigram overlap (BLEU-1), and as this is in most cases only precision, so in addition we also report other metrics.", "That is, a context is disambiguated in INLINEFORM0 languages for a word, if for each of its translations we have a context in the parallel corpus that contains one of the known synset translations. Furthermore, we assume we have an SMT system that can translate any context in INLINEFORM1 into our target language, INLINEFORM2 , and produces an alignment such that we know which word or phrase in the output corresponds to the input. Within the set of identified disambiguated contexts, the INLINEFORM3 top scoring contexts are used, with ties broken at random. Each of these contexts is given to the SMT system and the most frequent translation across these INLINEFORM4 contexts is used. Furthermore, the SMT system is configured to return the INLINEFORM5 highest scoring translations, according to its model, and we select the translation as the most frequent translation of the context among this INLINEFORM6 -best list.", "That is, a context is disambiguated in INLINEFORM0 languages for a word, if for each of its translations we have a context in the parallel corpus that contains one of the known synset translations. Furthermore, we assume we have an SMT system that can translate any context in INLINEFORM1 into our target language, INLINEFORM2 , and produces an alignment such that we know which word or phrase in the output corresponds to the input. Within the set of identified disambiguated contexts, the INLINEFORM3 top scoring contexts are used, with ties broken at random. Each of these contexts is given to the SMT system and the most frequent translation across these INLINEFORM4 contexts is used. Furthermore, the SMT system is configured to return the INLINEFORM5 highest scoring translations, according to its model, and we select the translation as the most frequent translation of the context among this INLINEFORM6 -best list. In our experiments, we combined this with INLINEFORM7 disambiguations to give INLINEFORM8 candidate translations from which the candidate is chosen.", "The Princeton WordNet is a large, publicly available lexical semantic database of English nouns, verbs, adjectives and adverbs, grouped into synsets ( INLINEFORM0 117,000), which are aligned in terms of semantic and lexical relations. While it has been translated using the expand approach to many other languages, this is an expensive manual process. Therefore it would be beneficial to have a high-quality automatic translation approach that would support NLP techniques, which rely on WordNet in new languages. The translation of wordnets is fundamentally complex because of the need to translate all senses of a word including low frequency senses, which is very challenging for current machine translation approaches.", "Once we obtain a set of sense disambiguated sentences for each Wordnet entry, we start the translation approach. Our hunch is that correctly identified contextual information around the WordNet entry can guide the SMT system to correctly translate an ambiguous entry."]}
{"question_id": "25e6ba07285155266c3154d3e2ca1ae05c2f7f2d", "predicted_answer": "The speaker-conditioned models were compared to speaker-agnostic baselines.", "predicted_evidence": ["Speaker-conditioned models generate utterances closer to gold length than speaker-agnostic baselines, with significantly lower perplexity and higher BLEU scores. This indicates that including speaker information promotes the generation of higher fidelity responses. Our speaker models, especially Speaker GPT2, produce the most inquisitive responses (59.4% question-asking rate).", "Our models additionally exhibit several qualitative properties of high-quality and fluent conversation. We present a sample generation in tab:sampleconv (additional samples in the Appendix) that is indicative of broad trends across the test set. None of the models are able to introduce novel information (like Gold), but our speaker-conditioned models produce markedly better inquisitive responses. While GPT2 generates a natural-sounding short question with little relevance to the topic at hand, our Speaker DialoGPT model paraphrases previous turns and refers to existing entities to ask a substantial and coherent question. We further performed a human evaluation on a Likert scale to assess subjective dialog quality, with human raters preferring speaker model responses to speaker-agnostic models 62.5% of the time across 150 pairwise comparisons.", "For training and evaluation, we provide our model with up to 512 tokens of non-truncated historical turns. We use an 80-10-10 train/dev/test split with unique conversations in each split.", "To assess how well Interview represents open-domain dialog, we look to two datasets in widespread usage: DailyDialog BIBREF4, 13K short dialogs written to simulate simple conversations from daily life; and CALLHOME BIBREF11, transcriptions from 120 half-hour casual telephone conversations. We measure the language modeling performance of a pre-trained transformer model\u2014117M-parameter GPT2 BIBREF27\u2014both in its original form and versions fine-tuned (FT) on the training splits for Interview, DailyDialog, and CALLHOME. We evaluated the zero-shot performance of these models on the test splits of these datasets, with perplexities shown in tab:datasetcomparison.", "We decode the host response via top-$k$ sampling BIBREF27 with $k=5$. Results across all models on the test set are in tab:metrics."]}
{"question_id": "d68cc9aaf0466b97354600a5646c3be4512fc096", "predicted_answer": "The dialog tasks that the model was experimented on are:\n\n1. DailyDialog: 13K short dialogs written to simulate simple conversations from daily life.\n2. CALLHOME: Transcriptions from 120 half-hour casual telephone conversations.\n\nThese datasets were used to evaluate the language modeling performance of the pre-trained transformer model (GPT2) and its fine-tuned versions on the training splits for Interview, DailyDialog, and CALLHOME.", "predicted_evidence": ["To assess how well Interview represents open-domain dialog, we look to two datasets in widespread usage: DailyDialog BIBREF4, 13K short dialogs written to simulate simple conversations from daily life; and CALLHOME BIBREF11, transcriptions from 120 half-hour casual telephone conversations. We measure the language modeling performance of a pre-trained transformer model\u2014117M-parameter GPT2 BIBREF27\u2014both in its original form and versions fine-tuned (FT) on the training splits for Interview, DailyDialog, and CALLHOME. We evaluated the zero-shot performance of these models on the test splits of these datasets, with perplexities shown in tab:datasetcomparison.", "Broadly speaking, dialog and conversation datasets can be classified as constrained (goal-oriented) or open-domain, written or spoken, and scripted or spontaneous BIBREF5. In the realm of written dialog, the closest proxy to natural dialog comes in the form of role-play-style BIBREF6 conversations, featuring two agents instructed to participate in a constrained conversation. This setup has seen recent usage to construct goal-oriented BIBREF7, BIBREF8 and grounded conversations BIBREF9, BIBREF10. These datasets are expensive to collect at scale and are heavily constrained/guided by the instructions given to participants. Several initiatives have recorded and manually transcribed natural conversations occurring in the course of normal life, resulting in small, high-quality natural dialog datasets BIBREF11, BIBREF12, BIBREF13, BIBREF14. We explore an alternative venue for collecting a large-scale dataset of natural dialog: conversations and interviews on public radio.", "We generate a response conditioned on the host speaker role, to specifically model how an interview host speaks and inquires, contrary to speaker-agnostic dialog settings BIBREF28, BIBREF29. Individual guests appear sparsely and their utterances heavily rely on external world knowledge. Thus, we model host responses, which are generally aimed towards moderating the conversation via follow-up questions and acknowledgements. Role-specific generation like this can benefit the development of assistive technologies and role-dependent dialog systems.", "This suggests that language models trained on Interview can learn patterns characteristic of natural open-domain dialog in both simple daily conversation and informal long spoken exchanges. We also investigate DialoGPT, a model pre-trained on 147M Reddit threads as a proxy for dialog BIBREF22. Our results show that while Reddit threads can be used to emulate conversation, they may not resemble natural speech; DialoGPT posts by far the worst zero-shot modeling performance across all test datasets ($>$500 perplexity)\u2014inferior to zero-shot GPT2. These experiments confirm that Interview, a dataset of real, complex conversations, is useful for modeling patterns in natural spoken dialog. We show statistics for Interview compared to other dialog datasets in tab:nprstats.", "The US Defense Advanced Research Projects Agency (DARPA) has undertaken efforts to collect broadcast and informal conversation from public and private sources including messaging boards, SMS BIBREF15, and broadcast newswire content BIBREF16, BIBREF17. However, it proves difficult to adopt these datasets as widely available benchmarks on dialog modeling tasks, as they come with a substantial cost ($100-$1000 per dataset/year, covering up to a hundred hours of transcribed conversation). In this vein, we contribute an open-access large-scale corpus of cleanly annotated broadcast media dialog."]}
{"question_id": "d038e5d2a6f85e68422caaf8b96cb046db6599fa", "predicted_answer": "The annotation of the Interview dataset was done by manually transcribing conversations and interviews on public radio, and then annotating the transcriptions with specific speaker roles.", "predicted_evidence": ["The US Defense Advanced Research Projects Agency (DARPA) has undertaken efforts to collect broadcast and informal conversation from public and private sources including messaging boards, SMS BIBREF15, and broadcast newswire content BIBREF16, BIBREF17. However, it proves difficult to adopt these datasets as widely available benchmarks on dialog modeling tasks, as they come with a substantial cost ($100-$1000 per dataset/year, covering up to a hundred hours of transcribed conversation). In this vein, we contribute an open-access large-scale corpus of cleanly annotated broadcast media dialog.", "We additionally explore two tasks that are facilitated by speaker role annotations in Interview: 1) generating appropriate responses for a specific role given a conversation history (speaker role modeling); and 2) predicting whether a new speaker will interject on the next sentence of a conversation. These tasks are crucial components to building fluent and role-specific dialog systems, for settings such as healthcare and customer service.", "Large repositories of textual communications (e.g. forum and microblog posts) have gained recent popularity as proxies for dialog BIBREF0, BIBREF1, BIBREF2. However, conversations in these settings differ from natural dialog: turns may be sparsely scattered over a large temporal span, contain distinct syntax and vocabulary BIBREF3, and differ greatly in formality and focus BIBREF4. In this paper, we investigate how appropriate such data is for modeling natural dialog, and introduce Interview, a new high-quality large-scale open-domain conversational dataset grounded in interview settings with annotations for specific speaker roles.", "Broadly speaking, dialog and conversation datasets can be classified as constrained (goal-oriented) or open-domain, written or spoken, and scripted or spontaneous BIBREF5. In the realm of written dialog, the closest proxy to natural dialog comes in the form of role-play-style BIBREF6 conversations, featuring two agents instructed to participate in a constrained conversation. This setup has seen recent usage to construct goal-oriented BIBREF7, BIBREF8 and grounded conversations BIBREF9, BIBREF10. These datasets are expensive to collect at scale and are heavily constrained/guided by the instructions given to participants. Several initiatives have recorded and manually transcribed natural conversations occurring in the course of normal life, resulting in small, high-quality natural dialog datasets BIBREF11, BIBREF12, BIBREF13, BIBREF14. We explore an alternative venue for collecting a large-scale dataset of natural dialog: conversations and interviews on public radio.", "While models fine-tuned on the training set performed best on each dataset as expected, we observe that 1) models trained on other datasets obtain relatively poor zero-shot performance on Interview; and 2) the model trained on Interview achieved the best out-of-domain performance on DailyDialog and CALLHOME by large margins. This suggests that language models trained on Interview can learn patterns characteristic of natural open-domain dialog in both simple daily conversation and informal long spoken exchanges. We also investigate DialoGPT, a model pre-trained on 147M Reddit threads as a proxy for dialog BIBREF22. Our results show that while Reddit threads can be used to emulate conversation, they may not resemble natural speech; DialoGPT posts by far the worst zero-shot modeling performance across all test datasets ($>$500 perplexity)\u2014inferior to zero-shot GPT2."]}
{"question_id": "c66e0aa86b59bbf9e6a1dc725fb9785473bfa137", "predicted_answer": "The news outlets focused on in this study are:\n\n1. National Public Radio (NPR)\n2. Israeli news television programs (unspecified)", "predicted_evidence": ["The US Defense Advanced Research Projects Agency (DARPA) has undertaken efforts to collect broadcast and informal conversation from public and private sources including messaging boards, SMS BIBREF15, and broadcast newswire content BIBREF16, BIBREF17. However, it proves difficult to adopt these datasets as widely available benchmarks on dialog modeling tasks, as they come with a substantial cost ($100-$1000 per dataset/year, covering up to a hundred hours of transcribed conversation). In this vein, we contribute an open-access large-scale corpus of cleanly annotated broadcast media dialog.", "BIBREF18 explores the patterns and discourse within media dialog and contrast the associated speaker role dynamics with spontaneous natural conversation. The author manually annotates and investigates 24 hours of Israeli news television programs. We see an opportunity for the investigation of speaker dynamics and significance of speaker roles at scale with our dataset.", "We collect a novel dataset of 105K multi-party interview transcripts for 7 programs on National Public Radio (NPR) over 20 years (1999\u20132019), total of 10k hours. These transcripts contain a total of 3M turns comprising 7.5M sentences (127M words) from 184K speakers, of which 287 are hosts. To investigate role-play in media dialog, we curate a subset, Interview 2P, with two roles: a host and a guest, comprising 23K two-party conversations encompassing 455K turns, with 1.24M sentences and 21.7M words.", "We contribute a large-scale media dialog dataset that can act as a benchmark for complex open-domain, role-dependent grounded dialog. We present baseline model for role-conditioned dialog generation and show that they benefit from speaker information when added. In future work, we aim to perform temporal analyses of trends and biases within Interview and take advantage of the news setting to investigate external knowledge grounding in long natural conversations. These directions could potentially lead to more coherent free-form and assistive dialog systems.", "While models fine-tuned on the training set performed best on each dataset as expected, we observe that 1) models trained on other datasets obtain relatively poor zero-shot performance on Interview; and 2) the model trained on Interview achieved the best out-of-domain performance on DailyDialog and CALLHOME by large margins. This suggests that language models trained on Interview can learn patterns characteristic of natural open-domain dialog in both simple daily conversation and informal long spoken exchanges. We also investigate DialoGPT, a model pre-trained on 147M Reddit threads as a proxy for dialog BIBREF22. Our results show that while Reddit threads can be used to emulate conversation, they may not resemble natural speech; DialoGPT posts by far the worst zero-shot modeling performance across all test datasets ($>$500 perplexity)\u2014inferior to zero-shot GPT2."]}
{"question_id": "369d7bc5351409910c7a5e05c0cbb5abab8e50ec", "predicted_answer": "Yes, the interviews in the dataset fall under a specific news category, namely National Public Radio (NPR) programs, which are typically news-oriented or informative in nature.", "predicted_evidence": ["Large repositories of textual communications (e.g. forum and microblog posts) have gained recent popularity as proxies for dialog BIBREF0, BIBREF1, BIBREF2. However, conversations in these settings differ from natural dialog: turns may be sparsely scattered over a large temporal span, contain distinct syntax and vocabulary BIBREF3, and differ greatly in formality and focus BIBREF4. In this paper, we investigate how appropriate such data is for modeling natural dialog, and introduce Interview, a new high-quality large-scale open-domain conversational dataset grounded in interview settings with annotations for specific speaker roles.", "This suggests that language models trained on Interview can learn patterns characteristic of natural open-domain dialog in both simple daily conversation and informal long spoken exchanges. We also investigate DialoGPT, a model pre-trained on 147M Reddit threads as a proxy for dialog BIBREF22. Our results show that while Reddit threads can be used to emulate conversation, they may not resemble natural speech; DialoGPT posts by far the worst zero-shot modeling performance across all test datasets ($>$500 perplexity)\u2014inferior to zero-shot GPT2. These experiments confirm that Interview, a dataset of real, complex conversations, is useful for modeling patterns in natural spoken dialog. We show statistics for Interview compared to other dialog datasets in tab:nprstats.", "We collect a novel dataset of 105K multi-party interview transcripts for 7 programs on National Public Radio (NPR) over 20 years (1999\u20132019), total of 10k hours. These transcripts contain a total of 3M turns comprising 7.5M sentences (127M words) from 184K speakers, of which 287 are hosts. To investigate role-play in media dialog, we curate a subset, Interview 2P, with two roles: a host and a guest, comprising 23K two-party conversations encompassing 455K turns, with 1.24M sentences and 21.7M words.", "To assess how well Interview represents open-domain dialog, we look to two datasets in widespread usage: DailyDialog BIBREF4, 13K short dialogs written to simulate simple conversations from daily life; and CALLHOME BIBREF11, transcriptions from 120 half-hour casual telephone conversations. We measure the language modeling performance of a pre-trained transformer model\u2014117M-parameter GPT2 BIBREF27\u2014both in its original form and versions fine-tuned (FT) on the training splits for Interview, DailyDialog, and CALLHOME. We evaluated the zero-shot performance of these models on the test splits of these datasets, with perplexities shown in tab:datasetcomparison.", "In an interview setting, it is also important for host utterances to be related to the conversation at hand. We evaluate the content similarity between generated responses and the dialog history. We show that our speaker-conditioned models generate responses with the most noun-phrases / topical references. These also overlap the most with topics in the dialog history, indicating topical relatedness. We note that gold responses include more noun phrases with lower historical overlap, possibly due to hosts bringing up new topics."]}
{"question_id": "b9d9803ba24127f91ba4d7cff4da11492da20f09", "predicted_answer": "The speaker-conditioned models were compared to speaker-agnostic baselines.", "predicted_evidence": ["Speaker-conditioned models generate utterances closer to gold length than speaker-agnostic baselines, with significantly lower perplexity and higher BLEU scores. This indicates that including speaker information promotes the generation of higher fidelity responses. Our speaker models, especially Speaker GPT2, produce the most inquisitive responses (59.4% question-asking rate).", "Our models additionally exhibit several qualitative properties of high-quality and fluent conversation. We present a sample generation in tab:sampleconv (additional samples in the Appendix) that is indicative of broad trends across the test set. None of the models are able to introduce novel information (like Gold), but our speaker-conditioned models produce markedly better inquisitive responses. While GPT2 generates a natural-sounding short question with little relevance to the topic at hand, our Speaker DialoGPT model paraphrases previous turns and refers to existing entities to ask a substantial and coherent question. We further performed a human evaluation on a Likert scale to assess subjective dialog quality, with human raters preferring speaker model responses to speaker-agnostic models 62.5% of the time across 150 pairwise comparisons.", "For training and evaluation, we provide our model with up to 512 tokens of non-truncated historical turns. We use an 80-10-10 train/dev/test split with unique conversations in each split.", "To assess how well Interview represents open-domain dialog, we look to two datasets in widespread usage: DailyDialog BIBREF4, 13K short dialogs written to simulate simple conversations from daily life; and CALLHOME BIBREF11, transcriptions from 120 half-hour casual telephone conversations. We measure the language modeling performance of a pre-trained transformer model\u2014117M-parameter GPT2 BIBREF27\u2014both in its original form and versions fine-tuned (FT) on the training splits for Interview, DailyDialog, and CALLHOME. We evaluated the zero-shot performance of these models on the test splits of these datasets, with perplexities shown in tab:datasetcomparison.", "We decode the host response via top-$k$ sampling BIBREF27 with $k=5$. Results across all models on the test set are in tab:metrics."]}
{"question_id": "7625068cc22a095109580b83eff48616387167c2", "predicted_answer": "The dialog tasks they experimented on are:\n\n1. DailyDialog: 13K short dialogs written to simulate simple conversations from daily life.\n2. CALLHOME: Transcriptions from 120 half-hour casual telephone conversations.\n\nThese two datasets are used to assess the language modeling performance of the pre-trained transformer model (GPT2) and its fine-tuned versions on the Interview dataset.", "predicted_evidence": ["To assess how well Interview represents open-domain dialog, we look to two datasets in widespread usage: DailyDialog BIBREF4, 13K short dialogs written to simulate simple conversations from daily life; and CALLHOME BIBREF11, transcriptions from 120 half-hour casual telephone conversations. We measure the language modeling performance of a pre-trained transformer model\u2014117M-parameter GPT2 BIBREF27\u2014both in its original form and versions fine-tuned (FT) on the training splits for Interview, DailyDialog, and CALLHOME. We evaluated the zero-shot performance of these models on the test splits of these datasets, with perplexities shown in tab:datasetcomparison.", "This suggests that language models trained on Interview can learn patterns characteristic of natural open-domain dialog in both simple daily conversation and informal long spoken exchanges. We also investigate DialoGPT, a model pre-trained on 147M Reddit threads as a proxy for dialog BIBREF22. Our results show that while Reddit threads can be used to emulate conversation, they may not resemble natural speech; DialoGPT posts by far the worst zero-shot modeling performance across all test datasets ($>$500 perplexity)\u2014inferior to zero-shot GPT2. These experiments confirm that Interview, a dataset of real, complex conversations, is useful for modeling patterns in natural spoken dialog. We show statistics for Interview compared to other dialog datasets in tab:nprstats.", "Broadly speaking, dialog and conversation datasets can be classified as constrained (goal-oriented) or open-domain, written or spoken, and scripted or spontaneous BIBREF5. In the realm of written dialog, the closest proxy to natural dialog comes in the form of role-play-style BIBREF6 conversations, featuring two agents instructed to participate in a constrained conversation. This setup has seen recent usage to construct goal-oriented BIBREF7, BIBREF8 and grounded conversations BIBREF9, BIBREF10. These datasets are expensive to collect at scale and are heavily constrained/guided by the instructions given to participants. Several initiatives have recorded and manually transcribed natural conversations occurring in the course of normal life, resulting in small, high-quality natural dialog datasets BIBREF11, BIBREF12, BIBREF13, BIBREF14. We explore an alternative venue for collecting a large-scale dataset of natural dialog: conversations and interviews on public radio.", "See the following tables for sample dialog histories and generated host responses from each of our baseline and speaker-conditioned dialog models.", "We generate a response conditioned on the host speaker role, to specifically model how an interview host speaks and inquires, contrary to speaker-agnostic dialog settings BIBREF28, BIBREF29. Individual guests appear sparsely and their utterances heavily rely on external world knowledge. Thus, we model host responses, which are generally aimed towards moderating the conversation via follow-up questions and acknowledgements. Role-specific generation like this can benefit the development of assistive technologies and role-dependent dialog systems."]}
{"question_id": "be0b438952048fe6bb91c61ba48e529d784bdcea", "predicted_answer": "No, the text does not mention crowdsourcing for annotations. The authors mention that they have created a \"cleanly annotated\" dataset, but they do not specify how the annotations were obtained. It is likely that the annotations were done manually by the researchers or their team, rather than through crowdsourcing.", "predicted_evidence": ["Broadly speaking, dialog and conversation datasets can be classified as constrained (goal-oriented) or open-domain, written or spoken, and scripted or spontaneous BIBREF5. In the realm of written dialog, the closest proxy to natural dialog comes in the form of role-play-style BIBREF6 conversations, featuring two agents instructed to participate in a constrained conversation. This setup has seen recent usage to construct goal-oriented BIBREF7, BIBREF8 and grounded conversations BIBREF9, BIBREF10. These datasets are expensive to collect at scale and are heavily constrained/guided by the instructions given to participants. Several initiatives have recorded and manually transcribed natural conversations occurring in the course of normal life, resulting in small, high-quality natural dialog datasets BIBREF11, BIBREF12, BIBREF13, BIBREF14. We explore an alternative venue for collecting a large-scale dataset of natural dialog: conversations and interviews on public radio.", "The US Defense Advanced Research Projects Agency (DARPA) has undertaken efforts to collect broadcast and informal conversation from public and private sources including messaging boards, SMS BIBREF15, and broadcast newswire content BIBREF16, BIBREF17. However, it proves difficult to adopt these datasets as widely available benchmarks on dialog modeling tasks, as they come with a substantial cost ($100-$1000 per dataset/year, covering up to a hundred hours of transcribed conversation). In this vein, we contribute an open-access large-scale corpus of cleanly annotated broadcast media dialog.", "While models fine-tuned on the training set performed best on each dataset as expected, we observe that 1) models trained on other datasets obtain relatively poor zero-shot performance on Interview; and 2) the model trained on Interview achieved the best out-of-domain performance on DailyDialog and CALLHOME by large margins. This suggests that language models trained on Interview can learn patterns characteristic of natural open-domain dialog in both simple daily conversation and informal long spoken exchanges. We also investigate DialoGPT, a model pre-trained on 147M Reddit threads as a proxy for dialog BIBREF22. Our results show that while Reddit threads can be used to emulate conversation, they may not resemble natural speech; DialoGPT posts by far the worst zero-shot modeling performance across all test datasets ($>$500 perplexity)\u2014inferior to zero-shot GPT2.", "Large repositories of textual communications (e.g. forum and microblog posts) have gained recent popularity as proxies for dialog BIBREF0, BIBREF1, BIBREF2. However, conversations in these settings differ from natural dialog: turns may be sparsely scattered over a large temporal span, contain distinct syntax and vocabulary BIBREF3, and differ greatly in formality and focus BIBREF4. In this paper, we investigate how appropriate such data is for modeling natural dialog, and introduce Interview, a new high-quality large-scale open-domain conversational dataset grounded in interview settings with annotations for specific speaker roles.", "This suggests that language models trained on Interview can learn patterns characteristic of natural open-domain dialog in both simple daily conversation and informal long spoken exchanges. We also investigate DialoGPT, a model pre-trained on 147M Reddit threads as a proxy for dialog BIBREF22. Our results show that while Reddit threads can be used to emulate conversation, they may not resemble natural speech; DialoGPT posts by far the worst zero-shot modeling performance across all test datasets ($>$500 perplexity)\u2014inferior to zero-shot GPT2. These experiments confirm that Interview, a dataset of real, complex conversations, is useful for modeling patterns in natural spoken dialog. We show statistics for Interview compared to other dialog datasets in tab:nprstats."]}
{"question_id": "a97137318025a6642ed0634f7159255270ba3d4f", "predicted_answer": "Yes, annotations were done manually. According to the text, \"Several initiatives have recorded and manually transcribed natural conversations occurring in the course of normal life, resulting in small, high-quality natural dialog datasets...\"", "predicted_evidence": ["The US Defense Advanced Research Projects Agency (DARPA) has undertaken efforts to collect broadcast and informal conversation from public and private sources including messaging boards, SMS BIBREF15, and broadcast newswire content BIBREF16, BIBREF17. However, it proves difficult to adopt these datasets as widely available benchmarks on dialog modeling tasks, as they come with a substantial cost ($100-$1000 per dataset/year, covering up to a hundred hours of transcribed conversation). In this vein, we contribute an open-access large-scale corpus of cleanly annotated broadcast media dialog.", "Large repositories of textual communications (e.g. forum and microblog posts) have gained recent popularity as proxies for dialog BIBREF0, BIBREF1, BIBREF2. However, conversations in these settings differ from natural dialog: turns may be sparsely scattered over a large temporal span, contain distinct syntax and vocabulary BIBREF3, and differ greatly in formality and focus BIBREF4. In this paper, we investigate how appropriate such data is for modeling natural dialog, and introduce Interview, a new high-quality large-scale open-domain conversational dataset grounded in interview settings with annotations for specific speaker roles.", "We additionally explore two tasks that are facilitated by speaker role annotations in Interview: 1) generating appropriate responses for a specific role given a conversation history (speaker role modeling); and 2) predicting whether a new speaker will interject on the next sentence of a conversation. These tasks are crucial components to building fluent and role-specific dialog systems, for settings such as healthcare and customer service.", "Broadly speaking, dialog and conversation datasets can be classified as constrained (goal-oriented) or open-domain, written or spoken, and scripted or spontaneous BIBREF5. In the realm of written dialog, the closest proxy to natural dialog comes in the form of role-play-style BIBREF6 conversations, featuring two agents instructed to participate in a constrained conversation. This setup has seen recent usage to construct goal-oriented BIBREF7, BIBREF8 and grounded conversations BIBREF9, BIBREF10. These datasets are expensive to collect at scale and are heavily constrained/guided by the instructions given to participants. Several initiatives have recorded and manually transcribed natural conversations occurring in the course of normal life, resulting in small, high-quality natural dialog datasets BIBREF11, BIBREF12, BIBREF13, BIBREF14. We explore an alternative venue for collecting a large-scale dataset of natural dialog: conversations and interviews on public radio.", "This suggests that language models trained on Interview can learn patterns characteristic of natural open-domain dialog in both simple daily conversation and informal long spoken exchanges. We also investigate DialoGPT, a model pre-trained on 147M Reddit threads as a proxy for dialog BIBREF22. Our results show that while Reddit threads can be used to emulate conversation, they may not resemble natural speech; DialoGPT posts by far the worst zero-shot modeling performance across all test datasets ($>$500 perplexity)\u2014inferior to zero-shot GPT2. These experiments confirm that Interview, a dataset of real, complex conversations, is useful for modeling patterns in natural spoken dialog. We show statistics for Interview compared to other dialog datasets in tab:nprstats."]}
{"question_id": "a24b2269b292fd0ee81d50303d1315383c594382", "predicted_answer": "The news sources that the transcripts come from are National Public Radio (NPR) programs.", "predicted_evidence": ["We collect a novel dataset of 105K multi-party interview transcripts for 7 programs on National Public Radio (NPR) over 20 years (1999\u20132019), total of 10k hours. These transcripts contain a total of 3M turns comprising 7.5M sentences (127M words) from 184K speakers, of which 287 are hosts. To investigate role-play in media dialog, we curate a subset, Interview 2P, with two roles: a host and a guest, comprising 23K two-party conversations encompassing 455K turns, with 1.24M sentences and 21.7M words.", "The US Defense Advanced Research Projects Agency (DARPA) has undertaken efforts to collect broadcast and informal conversation from public and private sources including messaging boards, SMS BIBREF15, and broadcast newswire content BIBREF16, BIBREF17. However, it proves difficult to adopt these datasets as widely available benchmarks on dialog modeling tasks, as they come with a substantial cost ($100-$1000 per dataset/year, covering up to a hundred hours of transcribed conversation). In this vein, we contribute an open-access large-scale corpus of cleanly annotated broadcast media dialog.", "Broadly speaking, dialog and conversation datasets can be classified as constrained (goal-oriented) or open-domain, written or spoken, and scripted or spontaneous BIBREF5. In the realm of written dialog, the closest proxy to natural dialog comes in the form of role-play-style BIBREF6 conversations, featuring two agents instructed to participate in a constrained conversation. This setup has seen recent usage to construct goal-oriented BIBREF7, BIBREF8 and grounded conversations BIBREF9, BIBREF10. These datasets are expensive to collect at scale and are heavily constrained/guided by the instructions given to participants. Several initiatives have recorded and manually transcribed natural conversations occurring in the course of normal life, resulting in small, high-quality natural dialog datasets BIBREF11, BIBREF12, BIBREF13, BIBREF14. We explore an alternative venue for collecting a large-scale dataset of natural dialog: conversations and interviews on public radio.", "Large repositories of textual communications (e.g. forum and microblog posts) have gained recent popularity as proxies for dialog BIBREF0, BIBREF1, BIBREF2. However, conversations in these settings differ from natural dialog: turns may be sparsely scattered over a large temporal span, contain distinct syntax and vocabulary BIBREF3, and differ greatly in formality and focus BIBREF4. In this paper, we investigate how appropriate such data is for modeling natural dialog, and introduce Interview, a new high-quality large-scale open-domain conversational dataset grounded in interview settings with annotations for specific speaker roles.", "BIBREF18 explores the patterns and discourse within media dialog and contrast the associated speaker role dynamics with spontaneous natural conversation. The author manually annotates and investigates 24 hours of Israeli news television programs. We see an opportunity for the investigation of speaker dynamics and significance of speaker roles at scale with our dataset."]}
{"question_id": "7d8cd7d6c86349ef0bd4fdbd84c8dc49c7678f46", "predicted_answer": "The real-world datasets used in the experiment are not explicitly mentioned in the given text. However, it is mentioned that there are 6 datasets in total, consisting of 3 regular text datasets and 3 short text datasets, from various domains.", "predicted_evidence": ["In the experiments, three regular text datasets and three short text datasets were used:", "We conduct extensive experiments with several real datasets including regular and short texts in various domains. The experimental results demonstrate that MetaLDA achieves improved performance in terms of perplexity, topic coherence, and running time.", "In this experiment, the perplexity is computed only on the words that appear in the training vocabulary. Here we used 80% documents in each dataset as the training set and the remaining 20% as the test set.", "The results show that MetaLDA outperformed all the competitors in terms of perplexity on nearly all the datasets, showing the benefit of using both document and word meta information. Specifically, we have the following remarks:", "In this section, we evaluate the proposed MetaLDA against several recent advances that also incorporate meta information on 6 real datasets including both regular and short texts. The goal of the experimental work is to evaluate the effectiveness and efficiency of MetaLDA's incorporation of document and word meta information both separately and jointly compared with other methods. We report the performance in terms of perplexity, topic coherence, and running time per iteration."]}
{"question_id": "0fee37ebe0a010cf8bd665fa566306d8e7d12631", "predicted_answer": "According to the text, the authors compare MetaLDA against several recent advances that also incorporate meta information, but the specific models are not mentioned in the provided context.", "predicted_evidence": ["In this section, we review three lines of related work: models with document meta information, models with word meta information, and models for short texts.", "The intuition of our way of incorporating meta information is: At the document level, if two documents have more labels in common, their Dirichlet parameter INLINEFORM0 will be more similar, resulting in more similar topic distributions INLINEFORM1 ; At the word level, if two words have similar features, their INLINEFORM2 in topic INLINEFORM3 will be similar and then we can expect that their INLINEFORM4 could be more or less the same. Finally, the two words will have similar probabilities of showing up in topic INLINEFORM5 . In other words, if a topic \u201cprefers\u201d a certain word, we expect that it will also prefer other words with similar features to that word. Moreover, at both the document and the word level, different labels/features may have different impact on the topics ( INLINEFORM6 / INLINEFORM7 ), which is automatically learnt in MetaLDA.", "The results show that MetaLDA outperformed all the competitors in terms of perplexity on nearly all the datasets, showing the benefit of using both document and word meta information. Specifically, we have the following remarks:", "In practice, various kinds of meta information are available at the document level and the word level in many corpora. At the document level, labels of documents can be used to guide topic learning so that more meaningful topics can be discovered. Moreover, it is highly likely that documents with common labels discuss similar topics, which could further result in similar topic distributions. For example, if we use authors as labels for scientific papers, the topics of the papers published by the same researcher can be closely related.", "In this section, we evaluate the proposed MetaLDA against several recent advances that also incorporate meta information on 6 real datasets including both regular and short texts. The goal of the experimental work is to evaluate the effectiveness and efficiency of MetaLDA's incorporation of document and word meta information both separately and jointly compared with other methods. We report the performance in terms of perplexity, topic coherence, and running time per iteration."]}
{"question_id": "f8bba20d1781ce2b14fad28d6eff024e5a6c2c02", "predicted_answer": "According to the text, topic quality is measured using two metrics:\n\n1. **NPMI (Normalized Pointwise Mutual Information) scores**: Higher scores indicate better topic coherence. NPMI scores are used to evaluate the quality of the topics discovered by the topic models.\n2. **Perplexity scores**: Lower scores indicate higher modeling accuracy. Perplexity is a measure of how well a topic model predicts the test data. It is computed by training a topic model on a training set, then using the trained model to predict the second half of each test document, given the first half.\n\nIn the text, the authors use these metrics to compare the performance of different topic models, including MetaLDA, on short text datasets.", "predicted_evidence": ["With the rapid growth of the internet, huge amounts of text data are generated in social networks, online shopping and news websites, etc. These data create demand for powerful and efficient text analysis techniques. Probabilistic topic models such as Latent Dirichlet Allocation (LDA) BIBREF0 are popular approaches for this task, by discovering latent topics from text collections. Many conventional topic models discover topics purely based on the word-occurrences, ignoring the meta information (a.k.a., side information) associated with the content. In contrast, when we humans read text it is natural to leverage meta information to improve our comprehension, which includes categories, authors, timestamps, the semantic meanings of the words, etc. Therefore, topic models capable of using meta information should yield improved modelling accuracy and topic quality.", "It is known that conventional topic models directly applied to short texts suffer from low quality topics, caused by the insufficient word co-occurrence information. Here we study whether or not the meta information helps MetaLDA improve topic quality, compared with other topic models that can also handle short texts. Table TABREF65 shows the NPMI scores on the three short text datasets. Higher scores indicate better topic coherence. All the models were trained with 100 topics. Besides the NPMI scores averaged over all the 100 topics, we also show the scores averaged over top 20 topics with highest NPMI, where \u201crubbish\u201d topics are eliminated, following BIBREF22 . It is clear that MetaLDA performed significantly better than all the other models in WS and AN dataset in terms of NPMI, which indicates that MetaLDA can discover more meaningful topics with the document and word meta information. We would like to point out that on the TMN dataset, even though the average score of MetaLDA is still the best, the score of MetaLDA has overlapping with the others' in the standard deviation, which indicates the difference is not statistically significant.", "It is known that conventional topic models directly applied to short texts suffer from low quality topics, caused by the insufficient word co-occurrence information. Here we study whether or not the meta information helps MetaLDA improve topic quality, compared with other topic models that can also handle short texts. Table TABREF65 shows the NPMI scores on the three short text datasets. Higher scores indicate better topic coherence. All the models were trained with 100 topics. Besides the NPMI scores averaged over all the 100 topics, we also show the scores averaged over top 20 topics with highest NPMI, where \u201crubbish\u201d topics are eliminated, following BIBREF22 . It is clear that MetaLDA performed significantly better than all the other models in WS and AN dataset in terms of NPMI, which indicates that MetaLDA can discover more meaningful topics with the document and word meta information.", "Tables TABREF48 and TABREF49 show: the average perplexity scores with standard deviations for all the models. Note that: (1) The scores on AN with 150 and 200 topics are not reported due to overfitting observed in all the compared models. (2) Given the size of NYT, the scores of 200 and 500 topics are reported. (3) The number of latent topics in LLDA must equal to the number of document labels. (4) For PLLDA, we varied the number of topics per label from 5 to 50 (2 and 5 topics on NYT). The number of topics in PPLDA is the product of the numbers of labels and topics per label.", "Perplexity is a measure that is widely used BIBREF23 to evaluate the modelling accuracy of topic models. The lower the score, the higher the modelling accuracy. To compute perplexity, we randomly selected some documents in a dataset as the training set and the remaining as the test set. We first trained a topic model on the training set to get the word distributions of each topic INLINEFORM0 ( INLINEFORM1 ). Each test document INLINEFORM2 was split into two halves containing every first and every second words respectively. We then fixed the topics and trained the models on the first half to get the topic proportions ( INLINEFORM3 ) of test document INLINEFORM4 and compute perplexity for predicting the second half. In regard to MetaLDA, we fixed the matrices INLINEFORM5 and INLINEFORM6 output from the training procedure."]}
{"question_id": "252599e53f52b3375b26d4e8e8b66322a42d2563", "predicted_answer": "According to the text, the data augmentation technique used is to transform the posterior into a form that is conjugate to the gamma prior of INLINEFORM9.", "predicted_evidence": ["In the experiments, three regular text datasets and three short text datasets were used:", "MetaLDA and its variants: the proposed model and its variants. Here we use MetaLDA to indicate the model considering both document labels and word features. Several variants of MetaLDA with document labels and word features separately were also studied, which are shown in Table TABREF32 . These variants differ in the method of estimating INLINEFORM0 and INLINEFORM1 . All the models listed in Table TABREF32 were implemented on top of Mallet. The hyper-parameters INLINEFORM2 and INLINEFORM3 were set to INLINEFORM4 .", "Recall that all the document labels are binary and INLINEFORM0 is involved in computing INLINEFORM1 iff INLINEFORM2 . Extracting all the terms related to INLINEFORM3 in Eq. ( SECREF17 ), we get the marginal posterior of INLINEFORM4 : +rCl+x* e- l,k d=1:fd,l=1D 1qd d,kl,k l,k d=1D fd,l td,k where INLINEFORM5 is the value of INLINEFORM6 with INLINEFORM7 removed when INLINEFORM8 . With the data augmentation techniques, the posterior is transformed into a form that is conjugate to the gamma prior of INLINEFORM9 . Therefore, it is straightforward to yield the following sampling strategy for INLINEFORM10 : +rCl+x* l,k Ga( ', 1/\u201d)", "At the word level, MetaLDA-def-wf performed the best among the models with word features only. Moreover, our model has obvious advantage in running speed (see Table TABREF66 ). Furthermore, comparing MetaLDA-def-wf with MetaLDA-def-def and MetaLDA-0.1-wf with LDA, we can see using the word features indeed improved perplexity.", "0, otherwise . where INLINEFORM0 is the original embedding vector for word INLINEFORM1 , INLINEFORM2 is the binarised value for INLINEFORM3 element of INLINEFORM4 , and INLINEFORM5 and INLINEFORM6 are the average value of all the positive elements and negative elements respectively. The insight is that we only consider features with strong opinions (i.e., large positive or negative value) on each dimension. To transform INLINEFORM7 to the final INLINEFORM8 , we use two binary bits to encode one dimension of INLINEFORM9 : the first bit is on if INLINEFORM10 and the second is on if INLINEFORM11 . Besides, MetaLDA can work with other word features such as semantic similarity as well."]}
{"question_id": "e12166fa9d6f63c4e92252c95c6a7bc96977ebf4", "predicted_answer": "Yes, this is an English language corpus. The text mentions that the corpus is built upon Twitter data, and the language used in the tweets is English. The text also mentions the use of English language resources such as WordNet Lemmatizer, which is a tool specifically designed for English language processing. Additionally, the text does not mention any other language being used in the corpus.", "predicted_evidence": ["We relied on the textual representations\u2014a feature space of n-grams (unigrams, bigrams and trigrams)\u2014for training. Due to the noisy nature of Twitter, where users frequently write short, informal spellings and grammars, we pre-processed input data as the following steps: (1) utilized a revised Twokenizer system which was specially trained on Twitter texts BIBREF20 to tokenize raw messages, (2) completed stemming and lemmatization using WordNet Lemmatizer BIBREF21 .", "However, it is probably due to the natural challenges of Twitter messages \u2014 conversational style of interactions, lack of traditional spelling rules, and 140-character limit of each message\u2014we barely see similar public Twitter datasets investigating open-domain problems like job/employment in computational linguistic or social science field. Li et al. li2014major proposed a pipelined system to extract a wide variety of major life events, including job, from Twitter. Their key strategy was to build a relatively clean training dataset from large volume of Twitter data with minimum human efforts. Their real world testing demonstrates the capability of their system to identify major life events accurately. The most parallel work that we can leverage here is the method and corpus developed by Liu et al. liu2016understanding, which is an effective supervised learning system to detect job-related tweets from individual and business accounts. To fully utilize the existing resources, we build upon the corpus by Liu et al. liu2016understanding to construct and contribute our more fine-grained corpus of job-related discourse with improvements of the classification methods.", "The language differences exposed in social media have been observed and analyzed in relation to location BIBREF14 , gender, age, regional origin, and political orientation BIBREF15 . However, it is probably due to the natural challenges of Twitter messages \u2014 conversational style of interactions, lack of traditional spelling rules, and 140-character limit of each message\u2014we barely see similar public Twitter datasets investigating open-domain problems like job/employment in computational linguistic or social science field. Li et al. li2014major proposed a pipelined system to extract a wide variety of major life events, including job, from Twitter. Their key strategy was to build a relatively clean training dataset from large volume of Twitter data with minimum human efforts. Their real world testing demonstrates the capability of their system to identify major life events accurately. The most parallel work that we can leverage here is the method and corpus developed by Liu et al.", "Our dataset is available as a plain text file in JSON format. Each line represents one unique tweet with five attributes identifying the tweet id (tweet_id, a unique identification number generated by Twitter for each tweet), topics job vs. notjob labeled by human (topic_human) and machine (topic_machine), and sources personal vs. business labeled by human (source_human) and machine (source_machine). NA represents \u201cnot applicable.\u201d An example of tweet in our corpus is shown as follows:", "However, it is probably due to the natural challenges of Twitter messages \u2014 conversational style of interactions, lack of traditional spelling rules, and 140-character limit of each message\u2014we barely see similar public Twitter datasets investigating open-domain problems like job/employment in computational linguistic or social science field. Li et al. li2014major proposed a pipelined system to extract a wide variety of major life events, including job, from Twitter. Their key strategy was to build a relatively clean training dataset from large volume of Twitter data with minimum human efforts. Their real world testing demonstrates the capability of their system to identify major life events accurately. The most parallel work that we can leverage here is the method and corpus developed by Liu et al. liu2016understanding, which is an effective supervised learning system to detect job-related tweets from individual and business accounts. To fully utilize the existing resources, we build upon the corpus by Liu et al."]}
{"question_id": "d4cb704e93086a2246a8caa5c1035e8297b8f4c0", "predicted_answer": "Yes, the authors mention some relevant constraints on this corpus. They mention the natural challenges of Twitter messages, such as:\n\n* Conversational style of interactions\n* Lack of traditional spelling rules\n* 140-character limit of each message\n\nThese constraints make it difficult to analyze and extract meaningful information from Twitter data, especially when investigating open-domain problems like job/employment.", "predicted_evidence": ["We focus on a broad discourse and narrative theme that touches most adults worldwide. Measures of volume, content, affect of job-related discourse on social media may help understand the behavioral patterns of working people, predict labor market changes, monitor and control satisfaction/dissatisfaction with respect to their workplaces or colleagues, and help people strive for positive change BIBREF9 . The language differences exposed in social media have been observed and analyzed in relation to location BIBREF14 , gender, age, regional origin, and political orientation BIBREF15 . However, it is probably due to the natural challenges of Twitter messages \u2014 conversational style of interactions, lack of traditional spelling rules, and 140-character limit of each message\u2014we barely see similar public Twitter datasets investigating open-domain problems like job/employment in computational linguistic or social science field. Li et al.", "These studies are valuable in their own right, but one evident limitation is that each dataset is limited to depicting a particular company and excludes the populations who have no access to such restricted networks (e.g., people who are not employees of that company). Moreover, the workers may be unwilling to express, e.g., negative feelings about work (\u201cI don't wanna go to work today\u201d), unprofessional behavior (\u201cGot drunk as hell last night and still made it to work\u201d), or a desire to work elsewhere (\u201cI want to go work at Disney World so bad\u201d) on platforms controlled by their employers.", "We focus on a broad discourse and narrative theme that touches most adults worldwide. Measures of volume, content, affect of job-related discourse on social media may help understand the behavioral patterns of working people, predict labor market changes, monitor and control satisfaction/dissatisfaction with respect to their workplaces or colleagues, and help people strive for positive change BIBREF9 . The language differences exposed in social media have been observed and analyzed in relation to location BIBREF14 , gender, age, regional origin, and political orientation BIBREF15 . However, it is probably due to the natural challenges of Twitter messages \u2014 conversational style of interactions, lack of traditional spelling rules, and 140-character limit of each message\u2014we barely see similar public Twitter datasets investigating open-domain problems like job/employment in computational linguistic or social science field. Li et al. li2014major proposed a pipelined system to extract a wide variety of major life events, including job, from Twitter.", "This labeling round yielded 1,297 tweets labeled with unanimous agreement among five workers, i.e. five workers gave the same label to one tweet\u20141,027 of these were labeled job-related, and the rest 270 were not job-related. They composed the first part of our human-annotated dataset, named as Part-1.", "Measures of volume, content, affect of job-related discourse on social media may help understand the behavioral patterns of working people, predict labor market changes, monitor and control satisfaction/dissatisfaction with respect to their workplaces or colleagues, and help people strive for positive change BIBREF9 . The language differences exposed in social media have been observed and analyzed in relation to location BIBREF14 , gender, age, regional origin, and political orientation BIBREF15 . However, it is probably due to the natural challenges of Twitter messages \u2014 conversational style of interactions, lack of traditional spelling rules, and 140-character limit of each message\u2014we barely see similar public Twitter datasets investigating open-domain problems like job/employment in computational linguistic or social science field. Li et al. li2014major proposed a pipelined system to extract a wide variety of major life events, including job, from Twitter. Their key strategy was to build a relatively clean training dataset from large volume of Twitter data with minimum human efforts."]}
{"question_id": "a11b5eb928a6db9a0e3bb290ace468ff1685d253", "predicted_answer": "The type of annotation being performed is tweet annotation, specifically labeling tweets for a classification task.", "predicted_evidence": ["To assess the labeling quality of multiple annotators in crowdsourced annotation rounds (R1, R2 and R4), we calculated Fleiss' kappa BIBREF22 and Krippendorff's alpha BIBREF23 measures using the online tool BIBREF24 to assess inter-annotator reliability among the five annotators of each HIT. And then we calculated the average and standard deviation of inter-annotator scores for multiple HITs per round. Table TABREF36 records the inter-annotator agreement scores in three rounds of crowdsourced annotations.", "Table TABREF27 summarizes the results from multiple crowdsourced annotation rounds (R1, R2 and R4).", "These three learned labelers ( INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 ) are capable to annotate unseen tweets automatically. Their performances may vary due to the progressively increasing size of training data.", "Our conjecture about crowdsourced annotations, based on the experiments and conclusions from BIBREF17 , is that non-expert contributors could produce comparable quality of annotations when evaluating against those gold standard annotations from experts. And it is similarly effective to use the labeled tweets with high inter-annotator agreement among multiple non-expert annotators from crowdsourcing platforms to build robust models as doing so on expert-labeled data.", "Compared to the framework introduced in BIBREF16 , our improvements include: introducing a new rule-based classifier ( INLINEFORM0 ), conducting an additional round of crowdsourcing annotations (R4) to enrich the human labeled data, and training a classification model with enhanced performances ( INLINEFORM1 ) which was ultimately used to label the unseen data."]}
{"question_id": "275b2c22b6a733d2840324d61b5b101f2bbc5653", "predicted_answer": "The tweets were selected using a classifier with two rules:\n\n1. The tweet must contain at least one word in the Include lexicon.\n2. The tweet cannot contain any word in the Exclude lexicon.\n\nBefore applying these rules, each tweet was pre-processed by:\n\n1. Converting all words to lower cases.\n2. Stripping out punctuation and special characters.\n3. Normalizing the tweets by mapping out-of-vocabulary phrases to standard phrases using a dictionary of slang terms.\n\nThis filtering process yielded over 40,000 matched tweets having at least five words, referred to as job-likely.", "predicted_evidence": ["\"tweet_id\":\"409834886405832705\",", "Our dataset is available as a plain text file in JSON format. Each line represents one unique tweet with five attributes identifying the tweet id (tweet_id, a unique identification number generated by Twitter for each tweet), topics job vs. notjob labeled by human (topic_human) and machine (topic_machine), and sources personal vs. business labeled by human (source_human) and machine (source_machine). NA represents \u201cnot applicable.\u201d An example of tweet in our corpus is shown as follows:", "Classifier INLINEFORM0 consists of two rules: the matched tweet must contain at least one word in the Include lexicon and it cannot contain any word in the Exclude lexicon. Before applying filtering rules, we pre-processed each tweet by (1) converting all words to lower cases; (2) stripping out punctuation and special characters; and (3) normalizing the tweets by mapping out-of-vocabulary phrases (such as abbreviations and acronyms) to standard phrases using a dictionary of more than 5,400 slang terms in the Internet.", "This filtering yielded over 40,000 matched tweets having at least five words, referred as job-likely.", "Grouping Type-1 and Type-2 tweets with unanimous labels in R2 (bold columns in Table TABREF18 ), we had our second part of human-labeled dataset (Part-2)."]}
{"question_id": "f1f7a040545c9501215d3391e267c7874f9a6004", "predicted_answer": "The dataset used is a book, specifically the first 125 pages of the book, which were manually labelled for assessing the quality of the named entity extraction process.", "predicted_evidence": ["Table TABREF27 shows the most frequent `candidate entities' from the whole book, as extracted by Algorithm 1 and which of those candidate entities were considered as actual `named entities' by Algorithm 2.", "Considering the Portuguese text represented in Figure FIGREF37 (a) the PAMPO algorithm identifies the `named entities' listed in Figure FIGREF37 (b).", "To determine if the entity type contributes to output variability in the INLINEFORM0 , an analysis was conducted on the named entities for the classification types: `persons' (PER), `locations' (LOC), `organizations' (ORG) and `miscellaneous' (MISC).", "To compute the INLINEFORM0 , INLINEFORM1 and INLINEFORM2 measures presented in Table TABREF40 , we used Equations EQREF30 , EQREF31 and EQREF32 with a difference in the weight given to the partial identifications. Based on the example in Figure FIGREF39 , we observed that not all partial correspondences to the named entity on the text have necessarily the same value, i.e., `Atlanta', `Atlanta 1996', `Jogos Ol\u00edmpicos' or `Jogos Ol\u00edmpicos de Atlanta' as partial identifications of `Jogos Ol\u00edmpicos de Atlanta 1996' do not have the same information. Hence we adopted as weight criterion for the partial identifications, the fraction of the named entity that is identified. This means that the previous partial identifications have weights of INLINEFORM3 , INLINEFORM4 , INLINEFORM5 and INLINEFORM6 , respectively.", "From this book, a total of 12120 named entities were extracted by PAMPO, corresponding to 5159 unique named entities. To assess the quality of this process, the first 125 pages of the book were manually labelled (1/3 of the text book). The values of the computed measures are shown in Table TABREF29 . This part of the book contains 3836 named entities. INLINEFORM0 and INLINEFORM1 are estimated for the two phases based on the results obtained on the 125 pages of the book. A total of 5089 terms were labelled `candidate entities' in the first phase and 3075 were identified as `named entities' in the second phase. The true positives were 3205 in the first phase and 2982 in the second phase (partial identifications count as 1/2)."]}
{"question_id": "b6f4fd6bc76bfcbc15724a546445908afa6d922c", "predicted_answer": "According to the text, the INLINEFORM3 (F1-score) of the PAMPO extractor increases from 0.72 to 0.87, which is an improvement of 0.15 (or 20.8%).", "predicted_evidence": ["as a consequence the INLINEFORM0 of PAMPO is also the highest in almost all the news;", "the INLINEFORM0 of the PAMPO extractor is the highest in almost all the news;", "Equation ( EQREF32 ) defines another measure commonly used to assess the quality of the process, INLINEFORM0 . This measure allows interpreting the global quality, taking into account the decrease of INLINEFORM1 and the increase of INLINEFORM2 . The second phase of the PAMPO process increases the value of INLINEFORM3 from 0.72 to 0.87. DISPLAYFORM0", "The values of the computed measures are shown in Table TABREF29 . This part of the book contains 3836 named entities. INLINEFORM0 and INLINEFORM1 are estimated for the two phases based on the results obtained on the 125 pages of the book. A total of 5089 terms were labelled `candidate entities' in the first phase and 3075 were identified as `named entities' in the second phase. The true positives were 3205 in the first phase and 2982 in the second phase (partial identifications count as 1/2). This means that the INLINEFORM2 , given by Equation ( EQREF30 ), decreases from 0.84 to 0.78, and the INLINEFORM3 , given by Equation ( EQREF31 ), increases from 0.63 to 0.97. DISPLAYFORM0 DISPLAYFORM1", "The authors would like to thank SAPO Labs (http://labs.sapo.pt) for providing the data set of news from Lusa agency. The authors would also like to thank grant #2014/08996-0 and grant #2013/14757-6, S\u00e3o Paulo Research Foundation (FAPESP). This work is partially funded by FCT/MEC through PIDDAC and ERDF/ON2 within project NORTE-07-0124-FEDER-000059 and through the COMPETE Programme (operational programme for competitiveness) and by National Funds through the FCT - Funda\u00e7\u00e3o para a Ci\u00eancia e a Tecnologia (Portuguese Foundation for Science and Technology) within project FCOMP-01-0124-FEDER-037281."]}
{"question_id": "3614c1f1435b7c1fd1f7f0041219eebf5bcff473", "predicted_answer": "The text does not explicitly mention experimentation with other languages besides Portuguese. The focus is on the development of NER tools for Portuguese texts, and the authors mention that most services are devoted to English and few support NER on Portuguese texts. There is no indication that the authors or other researchers experimented with other languages besides Portuguese and English.", "predicted_evidence": ["The language is an important factor to be taken in consideration in the NER task. Most of the services are devoted to English and few support NER on Portuguese texts. The first reference to work developed in Portuguese texts was published in 1997 BIBREF14 ; the authors perform the NER task and compute some measures in a Portuguese corpus and other five corpora. Until now, we have only identified the Rembrandt tool as a service developed and devoted to extract named entities in Portuguese texts. Other tools (AlchemyAPI, NERD and Zemanta) have been adapted to work and accept Portuguese texts but were not specifically developed for that purpose. As recently pointed out by Taba and Caseli BIBREF15 , the Portuguese language still lacks high quality linguistic resources and tools.", "For several reasons, text mining tools are typically first developed for English and only afterwards extended to other languages. Thus, there are still relatively few text mining tools for Portuguese and even less that are freely accessible. In particular, for the named entities recognition task in Portuguese texts, we find three extractors available: Alchemy, Zemanta and Rembrandt BIBREF5 . We also find some studies where the measures ( INLINEFORM0 , INLINEFORM1 and INLINEFORM2 ) for those extractors are computed and compared BIBREF6 , but their comparative effectiveness remains domain and final purpose dependent.", "'Ado\u00e7\u00e3o', 'Atualiza\u00e7\u00e3o', '\u00c0s', '\u00c1', 'Capa', 'Convite', 'Compromisso', 'Condecora\u00e7\u00e3o', 'Convocat\u00f3ria', 'Cart\u00e3o', 'Causa', 'Comunica\u00e7\u00e3o', 'Corrup\u00e7\u00e3o', 'Converg\u00eancia', 'Decreto', 'Ditadura', 'Democracia', 'Democrata', 'Estrutura', 'Ficha', 'Fax', 'Fixa\u00e7\u00e3o', 'Futuro', 'Gabinete', 'Gl\u00f3ria', 'Janeiro', 'Fevereiro', 'Mar\u00e7o', 'Abril', 'Maio', 'Junho', 'Julho', 'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro', Di\u00e1rio', 'Semanal', 'Mensal', 'Minutos', 'Meses', 'Ano', 'Anos', 'Hoje'} INLINEFORM0 {Portuguese stopwords on R}", "The authors would like to thank SAPO Labs (http://labs.sapo.pt) for providing the data set of news from Lusa agency. The authors would also like to thank grant #2014/08996-0 and grant #2013/14757-6, S\u00e3o Paulo Research Foundation (FAPESP). This work is partially funded by FCT/MEC through PIDDAC and ERDF/ON2 within project NORTE-07-0124-FEDER-000059 and through the COMPETE Programme (operational programme for competitiveness) and by National Funds through the FCT - Funda\u00e7\u00e3o para a Ci\u00eancia e a Tecnologia (Portuguese Foundation for Science and Technology) within project FCOMP-01-0124-FEDER-037281.", "'Revolu\u00e7\u00e3o', 'Rep\u00fablica', 'Reequil\u00edbrio', 'Anexo', 'Abertura', 'Atestado', 'Ata', 'Ado\u00e7\u00e3o', 'Atualiza\u00e7\u00e3o', '\u00c0s', '\u00c1', 'Capa', 'Convite', 'Compromisso', 'Condecora\u00e7\u00e3o', 'Convocat\u00f3ria', 'Cart\u00e3o', 'Causa', 'Comunica\u00e7\u00e3o', 'Corrup\u00e7\u00e3o', 'Converg\u00eancia', 'Decreto', 'Ditadura', 'Democracia', 'Democrata', 'Estrutura', 'Ficha', 'Fax', 'Fixa\u00e7\u00e3o', 'Futuro', 'Gabinete', 'Gl\u00f3ria', 'Janeiro', 'Fevereiro', 'Mar\u00e7o', 'Abril', 'Maio', 'Junho', 'Julho', 'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro', Di\u00e1rio',"]}
{"question_id": "c316d7d0c80b8f720ff90a8bb84a8b879a3ef7ea", "predicted_answer": "There are 54 rules used.", "predicted_evidence": ["'visconde', 'viscondessa', 'rei', 'ra\u00ednha', 'pr\u00edncipe', 'princesa', 'marqu\u00eas', 'marquesa', 'bar\u00e3o', 'baronesa', 'bispo', 'presidente', 'secret\u00e1rio', 'secret\u00e1ria', 'ministro', 'ministra', 'primeiro', 'primeira', 'deputado', 'deputada', 'general', 'tenente', 'capit\u00e3o', 'capit\u00e3', 'sargento', 'governador', 'governadora', 'diretor', 'director', 'diretora', 'directora', 'ex', 'filho', 'filha', irm\u00e3o', 'irm\u00e3', 'pai', 'm\u00e3e', 'tio', 'tia', 'padrinho', 'madrinha', 'sobrinho', 'sobrinha', 'afilhado', 'afilhada',", "'marqu\u00eas', 'marquesa', 'bar\u00e3o', 'baronesa', 'bispo', 'presidente', 'secret\u00e1rio', 'secret\u00e1ria', 'ministro', 'ministra', 'primeiro', 'primeira', 'deputado', 'deputada', 'general', 'tenente', 'capit\u00e3o', 'capit\u00e3', 'sargento', 'governador', 'governadora', 'diretor', 'director', 'diretora', 'directora', 'ex', 'filho', 'filha', irm\u00e3o', 'irm\u00e3', 'pai', 'm\u00e3e', 'tio', 'tia', 'padrinho', 'madrinha', 'sobrinho', 'sobrinha', 'afilhado', 'afilhada', 'av\u00f3', 'av\u00f4', 'neto', 'neta', 'enteado', 'enteada', 'padrasto',", "'viscondessa', 'rei', 'ra\u00ednha', 'pr\u00edncipe', 'princesa', 'marqu\u00eas', 'marquesa', 'bar\u00e3o', 'baronesa', 'bispo', 'presidente', 'secret\u00e1rio', 'secret\u00e1ria', 'ministro', 'ministra', 'primeiro', 'primeira', 'deputado', 'deputada', 'general', 'tenente', 'capit\u00e3o', 'capit\u00e3', 'sargento', 'governador', 'governadora', 'diretor', 'director', 'diretora', 'directora', 'ex', 'filho', 'filha', irm\u00e3o', 'irm\u00e3', 'pai', 'm\u00e3e', 'tio', 'tia', 'padrinho', 'madrinha', 'sobrinho', 'sobrinha', 'afilhado', 'afilhada', 'av\u00f3',", "'pr\u00edncipe', 'princesa', 'marqu\u00eas', 'marquesa', 'bar\u00e3o', 'baronesa', 'bispo', 'presidente', 'secret\u00e1rio', 'secret\u00e1ria', 'ministro', 'ministra', 'primeiro', 'primeira', 'deputado', 'deputada', 'general', 'tenente', 'capit\u00e3o', 'capit\u00e3', 'sargento', 'governador', 'governadora', 'diretor', 'director', 'diretora', 'directora', 'ex', 'filho', 'filha', irm\u00e3o', 'irm\u00e3', 'pai', 'm\u00e3e', 'tio', 'tia', 'padrinho', 'madrinha', 'sobrinho', 'sobrinha', 'afilhado', 'afilhada', 'av\u00f3', 'av\u00f4', 'neto', 'neta',", "'marquesa', 'bar\u00e3o', 'baronesa', 'bispo', 'presidente', 'secret\u00e1rio', 'secret\u00e1ria', 'ministro', 'ministra', 'primeiro', 'primeira', 'deputado', 'deputada', 'general', 'tenente', 'capit\u00e3o', 'capit\u00e3', 'sargento', 'governador', 'governadora', 'diretor', 'director', 'diretora', 'directora', 'ex', 'filho', 'filha', irm\u00e3o', 'irm\u00e3', 'pai', 'm\u00e3e', 'tio', 'tia', 'padrinho', 'madrinha', 'sobrinho', 'sobrinha', 'afilhado', 'afilhada', 'av\u00f3', 'av\u00f4', 'neto', 'neta', 'enteado', 'enteada', 'padrasto', 'madrasta'}"]}
{"question_id": "a786cceba4372f6041187c426432853eda03dca6", "predicted_answer": "The state-of-the-art refers to the current best performance or result achieved by existing methods or models in a particular field or task, in this case, the traditional, fully supervised setting for detecting astroturfing or fake news.", "predicted_evidence": ["We improve over the state of the art in the traditional, fully supervised setting, where training labels are available.", "This research is part of the Tanbih project, which aims to limit the effect of \u201cfake news\u201d, propaganda and media bias by making users aware of what they are reading. The project is developed in collaboration between the Qatar Computing Research Institute, HBKU and the MIT Computer Science and Artificial Intelligence Laboratory.", "The contributions of this paper can be summarized as follows:", "We used two evaluation measures: accuracy, and macro-averaged F1 (the harmonic average of precision and recall). In the supervised scenario, we performed 5-fold cross-validation. In the distant-supervision scenario, we propagated labels from the media to the users. Therefore, in the latter case the user labels were only used for evaluation.", "Astroturfing is an effort to simulate a political grass-roots movement. It has attracted strong interest from political science, and research on it has focused on massive streams of microblogging data BIBREF6."]}
{"question_id": "a837dcbd339e27a974e28944178c790a5b0b37c0", "predicted_answer": "The dataset contains 2,973,371 tweets by 2,848 Twitter users.", "predicted_evidence": ["Table TABREF24 shows some basic statistics about the resulting media dataset. Similarly to the IRA dataset, the distribution is right-heavy.", "Our main dataset contains 2973371 tweets by 2848 Twitter users, which the US House Intelligence Committee has linked to the Russian Internet Research Agency (IRA). The data was collected and published by BIBREF0, and then made available online. The time span covers the period from February 2012 to May 2018.", "For each user in the IRA dataset, we extracted all the links in their tweets, we expanded them recursively if they were shortened, we extracted the domain of the link, and we checked whether it could be found in the MBFC dataset. By grouping these relationships by media, we constructed the sets of users $C_m$ that mention a given medium $m \\in M$.", "Table TABREF30 shows an experiment in distant supervision for reverse classification, where we trained a model on the IRA dataset with the troll labels, and then we applied that model to the representation of the media in the MBFC dataset, where each medium is represented as the average of the embeddings of the users who cited that medium. We can see that we could improve over the baseline by 20 points absolute in terms of accuracy and by 41 in terms absolute in terms of macro-averaged F1.", "For this purpose, they use the \u201cIRA Russian Troll\u201d dataset BIBREF0, which we also use in our experiments. However, we have a very different approach based on graph embeddings, which we show to be superior to their method in the supervised setup. We further experiment with a new, and arguably more realistic, setup based on distant supervision, where labels are not available. To the best of our knowledge, this setup has not been explored in previous work."]}
{"question_id": "c135e1f8ecaf7965f6a6d3e30b537eb37ad74230", "predicted_answer": "Labels for trolls are obtained by propagating labels from news media to the troll users that mention them, using a citation graph and distant supervision learning.", "predicted_evidence": ["A more realistic scenario assumes that labels for troll accounts are not available. In this case, we need to use some external information in order to learn a labeling function. Indeed, we leverage more persistent entities and their labels: news media. We assume a learning scenario with distant supervision where labels for news media are available. By combining these labels with a citation graph from the troll accounts to news media, we can infer the final labeling on the accounts themselves without any need for manual labeling.", "We propagate labels from the given media to the troll user that mentions them according to the following media-to-user mapping:", "A distant supervision learning task, in which labels for the troll roles are not available at training time, and thus we use labels for news media as a proxy, from which we infer labels for the troll users.", "A fully supervised learning task, where we have labeled training data with example troll and their roles;", "We can see in Table TABREF30 that the relative ordering in terms or performance for the different models is consistent with that for the experiments in the previous section. This suggests that the relationship between trolls and media goes both ways, and thus we can use labels for media as a way to label users, and we can also use labels for troll users as a way to label media."]}
{"question_id": "16a10c1681dc5a399b6d34b4eed7bb1fef816dd0", "predicted_answer": "No, they don't only look at tweets. They also analyze links shared in those tweets, extract domains from those links, and check if those domains are found in another dataset (MBFC).", "predicted_evidence": ["Our main dataset contains 2973371 tweets by 2848 Twitter users, which the US House Intelligence Committee has linked to the Russian Internet Research Agency (IRA). The data was collected and published by BIBREF0, and then made available online. The time span covers the period from February 2012 to May 2018.", "Left Trolls send socially liberal messages and discuss gender, sexual, religious, and -especially- racial identity. Many tweets are seemed intentionally divisive, attacking mainstream Democratic politicians, particularly Hillary Clinton, while supporting Bernie Sanders prior to the elections.", "Moreover, we found that using all of the data for learning the embeddings was better than focusing only on users that we target in this study, namely left, right, and news feed, i.e., using the rest of the data adds additional context to the embedding space, and makes the target labels more contextually distinguishable. Similarly, we observe 5\u20136 points of absolute drop in accuracy when training our embeddings on tweets by trolls labeled as left, right, and news feed.", "We have proposed a novel approach to analyze the behavior patterns of political trolls according to their political leaning (left vs. news feed vs. right) using features from social media, i.e., from Twitter. We experimented with two scenarios: (i) supervised learning, where labels for trolls are provided, and (ii) distant supervision, where such labels are not available, and we rely on more common labels for news outlets cited by the trolls. Technically, we leveraged the community structure and the text of the messages in the online social network of trolls represented as a graph, from which we extracted several types of representations, i.e., embeddings, for the trolls. Our experiments on the \u201cIRA Russian Troll\u201d dataset have shown improvements over the state-of-the-art in the supervised scenario, while providing a compelling case for the distant-supervision scenario, which has not been explored before.", "For each user in the IRA dataset, we extracted all the links in their tweets, we expanded them recursively if they were shortened, we extracted the domain of the link, and we checked whether it could be found in the MBFC dataset. By grouping these relationships by media, we constructed the sets of users $C_m$ that mention a given medium $m \\in M$."]}
{"question_id": "2ca3ca39d59f448e30be6798514709be7e3c62d8", "predicted_answer": "The model was trained on the following datasets:\n\n1. CNN dataset\n2. Daily Mail dataset\n3. CBT dataset", "predicted_evidence": ["Furthermore the named entities in the whole dataset were replaced by anonymous tokens which were further shuffled for each example so that the model cannot build up any world knowledge about the entities and hence has to genuinely rely on the context document to search for an answer to the question.", "In this section we introduce the task that we are seeking to solve and relevant large-scale datasets that have recently been introduced for this task.", "Performance of our models on the CNN and Daily Mail datasets is summarized in Table TABREF27 , Table TABREF28 shows results on the CBT dataset. The tables also list performance of other published models that were evaluated on these datasets. Ensembles of our models set new state-of-the-art results on all evaluated datasets.", "The training data consist of tuples INLINEFORM0 , where INLINEFORM1 is a question, INLINEFORM2 is a document that contains the answer to question INLINEFORM3 , INLINEFORM4 is a set of possible answers and INLINEFORM5 is the ground truth answer. Both INLINEFORM6 and INLINEFORM7 are sequences of words from vocabulary INLINEFORM8 . We also assume that all possible answers are words from the vocabulary, that is INLINEFORM9 , and that the ground truth answer INLINEFORM10 appears in the document, that is INLINEFORM11 .", "Basic statistics about the CNN, Daily Mail and CBT datasets are summarized in Table TABREF2 ."]}
{"question_id": "df7fb8e6e44c9c5af3f19dde762c75cbf2f8452f", "predicted_answer": "The performance of their model is as follows:\n\n* On the CNN dataset, the single model with the best validation accuracy achieves a test accuracy of 69.5%.\n* The average performance of the top 20% models according to validation accuracy is 69.9%, which is 0.5% better than the single best-validation model.", "predicted_evidence": ["For single models we are reporting results for the best model as well as the average of accuracies for the best 20% of models with best performance on validation data since single models display considerable variation of results due to random weight initialization even for identical hyperparameter values. Single model performance may consequently prove difficult to reproduce.", "Even though it is a simplification of the Attentive Reader this model performs significantly better than the original.", "During training we evaluated the model performance after each epoch and stopped the training when the error on the validation set started increasing. The models usually converged after two epochs of training. Time needed to complete a single epoch of training on each dataset on an Nvidia K40 GPU is shown in Table TABREF46 .", "On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5%. The average performance of the top 20% models according to validation accuracy is 69.9% which is even 0.5% better than the single best-validation model. This shows that there were many models that performed better on test set than the best-validation model. Fusing multiple models then gives a significant further increase in accuracy on both CNN and Daily Mail datasets..", "A high level structure of our model is shown in Figure FIGREF10 ."]}
{"question_id": "20e2b517fddb0350f5099c39b16c2ca66186d09b", "predicted_answer": "The baseline they compare against is not explicitly mentioned in the given context. However, it can be inferred that the baseline is likely the Attentive Reader model, which is mentioned as a key difference from the authors' model. The authors are comparing the performance of their model to that of the Attentive Reader model, but the specific baseline accuracy or performance metric is not provided.", "predicted_evidence": ["Table TABREF45 then measures accuracy as the proportion of test cases where the ground truth was among the top INLINEFORM0 answers proposed by the greedy ensemble model for INLINEFORM1 .", "To further analyze the properties of our model, we examined the dependence of accuracy on the length of the context document (Figure FIGREF33 ), the number of candidate answers (Figure FIGREF38 ) and the frequency of the correct answer in the context (Figure FIGREF41 ).", "In Section SECREF6 we analysed how the test accuracy depends on how frequent the correct answer is compared to other answer candidates for the news datasets. The plots for the Children's Book Test looks very similar, however we are adding it here for completeness.", "The key difference between the Attentive Reader and our model is that the Attentive Reader uses attention to compute a fixed length representation INLINEFORM0 of the document INLINEFORM1 that is equal to a weighted sum of contextual embeddings of words in INLINEFORM2 , that is INLINEFORM3 . A joint query and document embedding INLINEFORM4 is then a non-linear function of INLINEFORM5 and the query embedding INLINEFORM6 . This joint embedding INLINEFORM7 is in the end compared against all candidate answers INLINEFORM8 using the dot product INLINEFORM9 , in the end the scores are normalized by INLINEFORM10 . That is: INLINEFORM11 .", "The training data consist of tuples INLINEFORM0 , where INLINEFORM1 is a question, INLINEFORM2 is a document that contains the answer to question INLINEFORM3 , INLINEFORM4 is a set of possible answers and INLINEFORM5 is the ground truth answer. Both INLINEFORM6 and INLINEFORM7 are sequences of words from vocabulary INLINEFORM8 . We also assume that all possible answers are words from the vocabulary, that is INLINEFORM9 , and that the ground truth answer INLINEFORM10 appears in the document, that is INLINEFORM11 ."]}
{"question_id": "70512cc9dcd45157e40c8d1f85e82d21ade7645b", "predicted_answer": "The model is evaluated on the CNN, Daily Mail, and CBT datasets.", "predicted_evidence": ["Performance of our models on the CNN and Daily Mail datasets is summarized in Table TABREF27 , Table TABREF28 shows results on the CBT dataset. The tables also list performance of other published models that were evaluated on these datasets. Ensembles of our models set new state-of-the-art results on all evaluated datasets.", "In this section we evaluate our model on the CNN, Daily Mail and CBT datasets. We show that despite the model's simplicity its ensembles achieve state-of-the-art performance on each of these datasets.", "In the first part of this article we introduce the task at hand and the main aspects of the relevant datasets. Then we present our own model to tackle the problem. Subsequently we compare the model to previously proposed architectures and finally describe the experimental results on the performance of our model.", "We will now briefly summarize important features of the datasets.", "Basic statistics about the CNN, Daily Mail and CBT datasets are summarized in Table TABREF2 ."]}
{"question_id": "fd556a038c36abc88a800d9d4f2cfa0aef6f5aba", "predicted_answer": "The percentage of human judgment agreement on the set is not explicitly stated in the given context. However, it can be inferred that the human judgment agreement is not high, as the text states that \"humans performed only slightly better than the LSTM\" and that \"some ungrammatical sentences were rated as highly grammatical by humans\". This suggests that there is a significant amount of variability in human judgments, and that the agreement between human annotators is not high.", "predicted_evidence": ["To see the impact of such biases, we re-analysed the human and machine scores by restricting the AUCs to the non-permuted sentences, i.e, the sentences whose case assignments correspond to that of the original templates. These templates were constructed to be plausible, and indeed the average human plausibility scores for these non-permuted orders of 5.33 is higher than for the permuted ones 3.61. In this analysis, we therefore include the 6 valid grammatical argument order permutations and all 108 grammatical violations for each template sentence.", "In total, we collected 2,750 annotations from 55 annotators for sentence grammaticality (38% of the dataset) and 1,800 annotations from 36 annotators for sentence meaningfulness (100% of grammatical sentences). We do not have grammaticality annotations for all sentences due to a lack of proficient German annotators on Amazon Mechanical Turk. Our human results for grammaticality are computed on this subset of the dataset.", "The results are shown in Table TABREF24. As expected, the human AUC scores are higher in this restricted analysis than in the full dataset shown in Table TABREF18. Note that the model scores are also higher, which suggests that the algorithms are also sensitive to meaningfulness, probably through its effects on the distribution of case for the different nouns in the training corpus.", "As noted, we do not ask humans to compare minimally differing sentences, but rather to grade individual sentences. This setup differs from earlier work such as BIBREF6 who show both sentences simultaneously and ask humans to pick the most grammatical one. This approach prevents humans from using the differences between the sentences to form a judgment on grammaticality; rather they must judge each sentence on its own. In doing so, the human setup is closer to that of language models: when we use log probability scores of LMs, we do not enable them to learn from the differences between the sentences to form a judgment.", "Surprisingly, humans performed only slightly better than the LSTM. We believe that this is due two factors. First, we presented the sentences in a scrambled order and asked for an absolute grammaticality judgment. It may be more difficult to put a sentence on a 1 to 10 scale than making pairwise judgments. Second, our sentences may be particularly challenging. The grammatical sentences contained both unusual argument orders and semantically odd situations, thus inciting participants to rate them low. While these factors could be expected to impact the LSTM, it is more surprising that they impact humans, despite precise instructions to rate on grammaticality rather than meaning or frequency. In addition, as can be seen in Figure FIGREF11b, some ungrammatical sentences were rated as highly grammatical by humans."]}
{"question_id": "9119fbfba84d298014d1b74e0e3d30330320002c", "predicted_answer": "Yes, the orders of case assignment biases are motivated by frequency considerations. The text suggests that the frequency of certain case assignments in German, such as nominative being more frequent than accusative, and dative being even rarer, influences the biases in both human and model scores. The models, especially those based on frequency such as unigram and bigram models, tend to assign higher log probability scores to sentences with more frequent case assignments, leading to biases in their grammaticality scores. Even the LSTM model, which is not directly based on frequency, shows some frequency bias, suggesting that RNNs also rely on frequency cues.", "predicted_evidence": ["To see the impact of such biases, we re-analysed the human and machine scores by restricting the AUCs to the non-permuted sentences, i.e, the sentences whose case assignments correspond to that of the original templates. These templates were constructed to be plausible, and indeed the average human plausibility scores for these non-permuted orders of 5.33 is higher than for the permuted ones 3.61. In this analysis, we therefore include the 6 valid grammatical argument order permutations and all 108 grammatical violations for each template sentence.", "In Figure FIGREF20, we explore the effect of argument order. Despite the fact that all argument orderings should be equally valid from a grammatical perspective, we find that humans tend to favour more 'canonical' orders, with nominative-accusative-dative being the preferred order. Models also assign higher log probability scores to the canonical order compared to others. It is likely that some orders occur more frequently than others in German, thus leading to a frequency bias for both models and humans. Although sentences with shuffled argument order have the same meaning as those without shuffled order, we find a similar bias for the meaningfulness scores.", "In Table TABREF18, we further investigate our grammaticality results by segregating them by case violation type (duplicate nominative, accusative or dative). While humans tend to give similar scores for each violation type, models tend to assign higher log probability scores to sentences with doubled nominatives than to grammatical sentences, leading to worse than chance performance on Nominative violations. Conversely, models tend to assign lower log probability scores to sentences with doubled datives, likely because these sentences lack either a nominative or an accusative, both of which are more frequent than dative. This leads to better than human performance on this case violation. Such behavior is probably due to the fact that German being a non pro-drop language, every verb must have a nominative case, making nominative more frequent than accusative, and that dative even rarer. This frequency bias is worse for models that are directly based on frequency, such as our unigram and bigram models.", "While humans tend to give similar scores for each violation type, models tend to assign higher log probability scores to sentences with doubled nominatives than to grammatical sentences, leading to worse than chance performance on Nominative violations. Conversely, models tend to assign lower log probability scores to sentences with doubled datives, likely because these sentences lack either a nominative or an accusative, both of which are more frequent than dative. This leads to better than human performance on this case violation. Such behavior is probably due to the fact that German being a non pro-drop language, every verb must have a nominative case, making nominative more frequent than accusative, and that dative even rarer. This frequency bias is worse for models that are directly based on frequency, such as our unigram and bigram models. However, our LSTM is not exempt from it, confirming that RNNs rely in part on frequency cues.", "Interestingly, even though the case orders preferred by the LSTM correlate with those of humans, there are also subtle differences: we also find that models tend to prefer argument orders that start with dative to those that start with accusative, when the opposite is true for human grammaticality scores. The origin of such differences is unclear. Understanding it more fully would require to obtain distributional statistics on the order of such phrases in the original corpus."]}
{"question_id": "058b6e3fdbb607fa7dbfc688628b3e13e130c35a", "predicted_answer": "The paper lists the following heuristic biases in the LSTMs:\n\n1. Frequency bias: The LSTM tends to assign higher log probability scores to sentences with doubled nominatives than to grammatical sentences, likely because nominative is more frequent than accusative in German.\n2. Frequency bias: The LSTM tends to assign lower log probability scores to sentences with doubled datives, likely because dative is even rarer than accusative in German.\n3. Preference for argument orders that start with dative over those that start with accusative, which is the opposite of human grammaticality scores.\n\nAdditionally, the paper mentions that humans are also prone to biases, such as:\n\n1. Inattention: Humans may rate ungrammatical sentences as highly grammatical due to inattention, especially when the distinction between grammatical and ungrammatical sentences rests on a single word or character.", "predicted_evidence": ["Surprisingly, humans performed only slightly better than the LSTM. We believe that this is due two factors. First, we presented the sentences in a scrambled order and asked for an absolute grammaticality judgment. It may be more difficult to put a sentence on a 1 to 10 scale than making pairwise judgments. Second, our sentences may be particularly challenging. The grammatical sentences contained both unusual argument orders and semantically odd situations, thus inciting participants to rate them low. While these factors could be expected to impact the LSTM, it is more surprising that they impact humans, despite precise instructions to rate on grammaticality rather than meaning or frequency. In addition, as can be seen in Figure FIGREF11b, some ungrammatical sentences were rated as highly grammatical by humans. We suspect that these are cases of inattention, as in our test set the distinction between grammatical and ungrammatical rest on a single word, and even a single character (the distinction between 'der' and 'den', for instance).", "Surprisingly, humans performed only slightly better than the LSTM. We believe that this is due two factors. First, we presented the sentences in a scrambled order and asked for an absolute grammaticality judgment. It may be more difficult to put a sentence on a 1 to 10 scale than making pairwise judgments. Second, our sentences may be particularly challenging. The grammatical sentences contained both unusual argument orders and semantically odd situations, thus inciting participants to rate them low. While these factors could be expected to impact the LSTM, it is more surprising that they impact humans, despite precise instructions to rate on grammaticality rather than meaning or frequency. In addition, as can be seen in Figure FIGREF11b, some ungrammatical sentences were rated as highly grammatical by humans.", "While humans tend to give similar scores for each violation type, models tend to assign higher log probability scores to sentences with doubled nominatives than to grammatical sentences, leading to worse than chance performance on Nominative violations. Conversely, models tend to assign lower log probability scores to sentences with doubled datives, likely because these sentences lack either a nominative or an accusative, both of which are more frequent than dative. This leads to better than human performance on this case violation. Such behavior is probably due to the fact that German being a non pro-drop language, every verb must have a nominative case, making nominative more frequent than accusative, and that dative even rarer. This frequency bias is worse for models that are directly based on frequency, such as our unigram and bigram models. However, our LSTM is not exempt from it, confirming that RNNs rely in part on frequency cues.", "Interestingly, even though the case orders preferred by the LSTM correlate with those of humans, there are also subtle differences: we also find that models tend to prefer argument orders that start with dative to those that start with accusative, when the opposite is true for human grammaticality scores. The origin of such differences is unclear. Understanding it more fully would require to obtain distributional statistics on the order of such phrases in the original corpus.", "Figure FIGREF11 shows the distribution of the log probability scores predicted by the LSTM and the distribution of the grammaticality scores given by humans. Figure FIGREF16 presents the distributions and average of the AUC values computed per template (50 in total), both for the models' log probability scores and the human grammaticality scores. Performances are rather modest, with a mean AUC of 0.56 for the LTSM and of 0.58 for humans, compared to the chance score of 0.5 for the unigram and bigram models. As expected, the n-gram baselines perform exactly at chance, confirming that they do not represent verb argument structures and that LMs need a deeper encoding to be able capture syntax within sentences. We also notice that AUC varies relatively little across different templates for our models, indicating that the particular choice of template has little impact. For humans, the wider spread in results can be attributed partially to the fact that 55 random permutations out of the 144 permutations were annotated for each template."]}
{"question_id": "5b95665d44666a1dc9e568d2471e5edf8614859f", "predicted_answer": "The performances of LSTMs and humans on the task are:\n\n* LSTMs: mean AUC of 0.56\n* Humans: mean AUC of 0.58", "predicted_evidence": ["Surprisingly, humans performed only slightly better than the LSTM. We believe that this is due two factors. First, we presented the sentences in a scrambled order and asked for an absolute grammaticality judgment. It may be more difficult to put a sentence on a 1 to 10 scale than making pairwise judgments. Second, our sentences may be particularly challenging. The grammatical sentences contained both unusual argument orders and semantically odd situations, thus inciting participants to rate them low. While these factors could be expected to impact the LSTM, it is more surprising that they impact humans, despite precise instructions to rate on grammaticality rather than meaning or frequency. In addition, as can be seen in Figure FIGREF11b, some ungrammatical sentences were rated as highly grammatical by humans.", "Surprisingly, humans performed only slightly better than the LSTM. We believe that this is due two factors. First, we presented the sentences in a scrambled order and asked for an absolute grammaticality judgment. It may be more difficult to put a sentence on a 1 to 10 scale than making pairwise judgments. Second, our sentences may be particularly challenging. The grammatical sentences contained both unusual argument orders and semantically odd situations, thus inciting participants to rate them low. While these factors could be expected to impact the LSTM, it is more surprising that they impact humans, despite precise instructions to rate on grammaticality rather than meaning or frequency. In addition, as can be seen in Figure FIGREF11b, some ungrammatical sentences were rated as highly grammatical by humans. We suspect that these are cases of inattention, as in our test set the distinction between grammatical and ungrammatical rest on a single word, and even a single character (the distinction between 'der' and 'den', for instance).", "We evaluate three LMs on our dataset, the two-layer LSTM of BIBREF8 trained on German Wikipedia text, as well as n-gram baselines using the same corpus. We ask proficient German speakers to annotate our sentences for grammaticality, providing a human comparison. Since some of these sentences are rather implausible because of the permutations, we also collect human meaningfulness scores. We find that our dataset is challenging for both LMs and humans and that LMs lag behind human performance.", "Figure FIGREF11 shows the distribution of the log probability scores predicted by the LSTM and the distribution of the grammaticality scores given by humans. Figure FIGREF16 presents the distributions and average of the AUC values computed per template (50 in total), both for the models' log probability scores and the human grammaticality scores. Performances are rather modest, with a mean AUC of 0.56 for the LTSM and of 0.58 for humans, compared to the chance score of 0.5 for the unigram and bigram models. As expected, the n-gram baselines perform exactly at chance, confirming that they do not represent verb argument structures and that LMs need a deeper encoding to be able capture syntax within sentences. We also notice that AUC varies relatively little across different templates for our models, indicating that the particular choice of template has little impact. For humans, the wider spread in results can be attributed partially to the fact that 55 random permutations out of the 144 permutations were annotated for each template.", "Figure FIGREF11 shows the distribution of the log probability scores predicted by the LSTM and the distribution of the grammaticality scores given by humans. Figure FIGREF16 presents the distributions and average of the AUC values computed per template (50 in total), both for the models' log probability scores and the human grammaticality scores. Performances are rather modest, with a mean AUC of 0.56 for the LTSM and of 0.58 for humans, compared to the chance score of 0.5 for the unigram and bigram models. As expected, the n-gram baselines perform exactly at chance, confirming that they do not represent verb argument structures and that LMs need a deeper encoding to be able capture syntax within sentences. We also notice that AUC varies relatively little across different templates for our models, indicating that the particular choice of template has little impact."]}
{"question_id": "b9686a168366aafbab1737df426e031ad74a6284", "predicted_answer": "No, the authors do not offer a hypothesis for why Twitter data makes better predictions about the inventory of languages used in each country. They present the findings and discuss the differences between Twitter and web-crawled corpora, but they do not provide an explanation for why Twitter data is more accurate in certain regions.", "predicted_evidence": ["But the third important finding is that, given what ground-truth language-use data is available, there remain a number of countries where these corpora do not represent all the language produced by the local populations: not all languages from censuses are found in digital texts. In this case Twitter has fewer missing languages.", "In both Figures 4 and 5 a darker red indicates a higher number of languages from a census being found in the respective corpora. In many cases, the two corpora agree in which languages they predict to be used in each country: Europe, those parts of Africa for which there is ground-truth data, and South America. But Twitter provides a better representation of North America and Oceania. One factor that is disguised in these figures is that many countries have only a few languages, so that a high true positive rate for a country could reflect only one or two languages. For example, both English and Spanish are very common on Twitter (c.f. Table 3), so that any country which predominantly uses these two languages will have a good representation by default.", "We can also think about the false positive rate: what languages do the corpora find that are not contained in the census-based ground-truth? For example, if English and Spanish are used in a country on Twitter but not reflected on the census, this is a false positive. As shown in Figure 6, the web-crawled corpus has very few false positive outside of Russia and eastern Europe. The situation on Twitter is similar: most false positives are in Russia and Eastern Europe, but in Twitter there are also over-predicted languages in the US and Canada, South Africa, France, India, and Australia. This is important because it shows that relying on Twitter alone would indicate that there are more diverse language speaking populations in these countries. As shown in Table 1, Eastern Europe accounts for 2.4% of the world's population but 27.4% of the web corpus; this explains the region's general false positive rate.", "For example, if English and Spanish are used in a country on Twitter but not reflected on the census, this is a false positive. As shown in Figure 6, the web-crawled corpus has very few false positive outside of Russia and eastern Europe. The situation on Twitter is similar: most false positives are in Russia and Eastern Europe, but in Twitter there are also over-predicted languages in the US and Canada, South Africa, France, India, and Australia. This is important because it shows that relying on Twitter alone would indicate that there are more diverse language speaking populations in these countries. As shown in Table 1, Eastern Europe accounts for 2.4% of the world's population but 27.4% of the web corpus; this explains the region's general false positive rate. For Russia, on the other hand, which is not included in Eastern Europe, the false positive rate cannot be explained in reference to general over-representation. In this case, the false positives are other European languages: French, German, Spanish, Italian.", "We can also think about the false positive rate: what languages do the corpora find that are not contained in the census-based ground-truth? For example, if English and Spanish are used in a country on Twitter but not reflected on the census, this is a false positive. As shown in Figure 6, the web-crawled corpus has very few false positive outside of Russia and eastern Europe. The situation on Twitter is similar: most false positives are in Russia and Eastern Europe, but in Twitter there are also over-predicted languages in the US and Canada, South Africa, France, India, and Australia. This is important because it shows that relying on Twitter alone would indicate that there are more diverse language speaking populations in these countries. As shown in Table 1, Eastern Europe accounts for 2.4% of the world's population but 27.4% of the web corpus; this explains the region's general false positive rate. For Russia, on the other hand, which is not included in Eastern Europe, the false positive rate cannot be explained in reference to general over-representation."]}
{"question_id": "740cc392c0c8bfadfe6b3a60c0be635c03e17f2a", "predicted_answer": "The social media platforms represented are:\n\n1. Twitter", "predicted_evidence": ["The distribution of language data in Figures 2 and 3 raises an important distinction between types of users: locals vs. non-locals. For example, from internet usage statistics we know that many countries in Africa have less access to web-sites and thus produce much less web-crawled data. This is reflected in Figure 3. But many African countries are over-represented in Twitter data. Are these Twitter users members of the local populations or do they represent visitors? Note that Figure 2 does not reflect the popularity of Twitter as a platform because we start by normalizing the Twitter output for each country against the total Twitter usage. The over-represented countries in Figure 2, then, represent places where Twitter data is produced at a higher rate than expected. It has nothing to do with the relative popularity of the platform (e.g., Twitter vs. web pages).", "Each of these datasets rests on different assumptions and, as a result, is subject to different confounds. For example, using top-level domains to determine the country of origin for web pages is likely to over-represent countries like Estonia that use their TLD extensively and to under-represent countries like the United States that do not traditionally use their TLD. The Twitter dataset relies on georeferenced tweets, but not all users have GPS-enabled devices. For example, we might expect that countries with a lower per capita GDP have a lower percent of georeferenced tweets, in addition to having fewer tweets overall. The goal here is to establish a baseline of how well these corpora represent actual populations.", "We see that 87.9% and 80.4% of the data belongs to these twenty languages. The implication is that all the other languages make up less than 20% of both datasets. This is potentially problematic because majority languages such as English and Spanish (both very common) are used across widely different demographics. In other words, knowing that a population uses English or Spanish gives us relatively little information about that population. A different view of this is shown in Figure 1, with the distribution by percentage of the data for the top 100 languages in each dataset (not necessarily the same languages). There is a long-tail of minority languages with a relatively small representation. This trend is more extreme in the social media dataset, but it is found with the same order of magnitude in both datasets. The figure is cut off above 2.0% in order to visualize the long-tail of very infrequent languages. The biggest driver of this trend is English, accounting for 37.46% of social media and 29.96% of web data.", "In isolation, web-crawled data provides one observation of global language use. Another common source of data used for this purpose is Twitter BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21). A spatial search is used to collect tweets from within a 50km radius of 10k cities. This search avoids biasing the selection by using language-specific keywords or hashtags. The Twitter data covers the period from May of 2017 until early 2019. This creates a corpus containing 1,066,038,000 tweets, all connected with the city from which they were collected. Because the language identification component only provides reliable predictions for samples containing at least 50 characters, the corpus is pruned to that length threshold (this removes approximately 250 million tweets). As shown in Table 1, this produces a corpus containing 4.14 billion words.", "How well does language data represent both regional population densities and the social characteristics of regional populations? To answer this question, we collect and analyze two large global-scale datasets: web-crawled data from the Common Crawl (16.65 billion words) and social media data from Twitter (4.14 billion words). This paper evaluates demographic-type information that is derived from these datasets, information which traditionally has been collected using survey-instruments as part of a census."]}
{"question_id": "845bdcd900c0f96b2ae091d086fb1ab8bb1063f0", "predicted_answer": "The website used in the web crawl is the Common Crawl.", "predicted_evidence": ["Some countries are not available because their top-level domains are used for other purposes (i.e., .ai, .fm, .io, .ly, .ag, .tv). Domains that do not contain geographic information are also removed from consideration (e.g., .com sites). The Common Crawl dataset covers 2014 through the end of 2017, totalling 81.5 billion web pages. As shown in Table 1, after processing this produces a corpus of 16.65 billion words. Table 1 also shows the number of countries represented in the web corpus against the number of countries in the ground-truth UN dataset and in the collected Twitter corpus. Countries may be missing from the web dataset (i) because their domains are used for a different purpose or (ii) their domains are not widely used or the country does not produce a significant amount of data on the open internet.", "Data comes from two sources of digital texts: web pages from the Common Crawl and social media from Twitter. Starting with the web-crawled data, we can compare this dataset to previous georeferenced web corpora BIBREF12, BIBREF13. The basic pipeline is to process all text within $<p>$ tags, removing boilerplate content, navigation content, and noisy text. We view each web page as a document containing the remaining material. Documents are then deduplicated by site, by time, and by location.", "For web-crawled data, internet access provides a much better population weighting ($r=0.49$). This is perhaps not surprising because the internet usage statistics are directly relevant to the production of websites. But it is surprising that general internet access is not a good predictor of Twitter usage. Overall, we see that there is a definite relationship between populations and the amount of digital text produced per country, but there are clear regional biases.", "In isolation, web-crawled data provides one observation of global language use. Another common source of data used for this purpose is Twitter BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21). A spatial search is used to collect tweets from within a 50km radius of 10k cities. This search avoids biasing the selection by using language-specific keywords or hashtags. The Twitter data covers the period from May of 2017 until early 2019. This creates a corpus containing 1,066,038,000 tweets, all connected with the city from which they were collected. Because the language identification component only provides reliable predictions for samples containing at least 50 characters, the corpus is pruned to that length threshold (this removes approximately 250 million tweets). As shown in Table 1, this produces a corpus containing 4.14 billion words.", "How well does language data represent both regional population densities and the social characteristics of regional populations? To answer this question, we collect and analyze two large global-scale datasets: web-crawled data from the Common Crawl (16.65 billion words) and social media data from Twitter (4.14 billion words). This paper evaluates demographic-type information that is derived from these datasets, information which traditionally has been collected using survey-instruments as part of a census."]}
{"question_id": "8d1b6c88f06ee195d75af32ede85dbd6477c8497", "predicted_answer": "The datasets represent 199 countries in total, with the web-crawled data covering 166 countries and the Twitter data covering 169 countries. The languages represented in the datasets are not explicitly stated, but Table 3 shows the top 20 languages by their relative proportion of each dataset.", "predicted_evidence": ["What countries are specifically over-represented and under-represented in the two datasets? We first find the relative share of each dataset for each country. For example, what percentage of the web-corpus is from India? This assumes the total world population and the total corpus size as constants and finds the share of that total from each country. We then subtract the population estimates from the corpus-based estimates. In other words, we first normalize each representation (corpus size and population) and then find the difference between the normalized measures. This allows us to take into account the very different counts (words vs. persons).", "To what degree do these datasets represent majority languages? This is an important question because, with only language labels available, the prevalence of only a few languages will obscure important demographic information. Table 3 shows the top twenty languages (chosen from the web corpus) by their relative proportion of each dataset and, at the bottom, by their combined percent of the overall dataset. The two datasets do not agree in top languages given only the total number of words; however, these twenty languages make up a similar percent of each dataset.", "We begin by describing the corpora and how they were collected (Section 2) and the language identification model that is used to label them with language codes (Section 3). After looking at the frequency distribution of languages across the entire dataset (Section 4), we undertake a country-level evaluation of the datasets, first against population-density baselines (Section 5) and then against language-use baselines (Section 6).", "How well does the amount of data correspond with the population density of each country? In this section we compare the number of words in each corpus with the UN population estimates. These datasets cover 199 countries in total, although the web-crawled data only represents 166 countries and the Twitter data only represents 169 countries.", "When labeled with a language identification model, this data provides a representation of both (i) how much language a particular country produces, a proxy for population density and (ii) the mix of languages used in a country, a proxy for population demographics. These corpus-based representations are compared against four ground-truth baselines. First, the UN country-level population estimates BIBREF7. Second, because not all populations have equal access to internet technologies, we use per capita GDP BIBREF8 and internet-usage statistics BIBREF9 to adjust raw populations. Third, the UN country-level census aggregations are used to represent what languages are used in each country BIBREF10 and, where these are not available, the World Factbook BIBREF11 estimations are used. The goal is to measure how well corpus-based representations correspond with each of these ground-truth, survey-based representations."]}
{"question_id": "bc05503eef25c732f1785e29d59b6022f12ba094", "predicted_answer": "According to the text, the other evaluation metrics used, in addition to ROUGE-L, are not explicitly mentioned. However, it is mentioned that the performance is reported on \"all three metrics\", suggesting that there are two other metrics used in addition to ROUGE-L. Unfortunately, the text does not specify what these other two metrics are.", "predicted_evidence": ["All models are trained for 50,000 steps on 3 GPUs (GTX 1080 Ti) with gradient accumulation per two steps, which makes the batch size approximately equal to 36. Model checkpoints are saved and evaluated on the validation set every 1,000 steps. We select the top-3 checkpoints based on their evaluation losses on the validations set, and report the averaged results on the test set.", "As illustrated in the table, all BERT-based models outperformed previous state-of-the-art models by a large margin. Bertsum with Transformer achieved the best performance on all three metrics. The Bertsum with LSTM model does not have an obvious influence on the summarization performance compared to the Classifier model.", "Although many neural models have been proposed for extractive summarization recently BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , the improvement on automatic metrics like ROUGE has reached a bottleneck due to the complexity of the task. In this paper, we argue that, BERT BIBREF0 , with its pre-training on a huge dataset and the powerful architecture for learning complex features, can further boost the performance of extractive summarization .", "Both datasets contain abstractive gold summaries, which are not readily suited to training extractive summarization models. A greedy algorithm was used to generate an oracle summary for each document. The algorithm greedily select sentences which can maximize the ROUGE scores as the oracle sentences. We assigned label 1 to sentences selected in the oracle summary and 0 otherwise.", "When predicting summaries for a new document, we first use the models to obtain the score for each sentence. We then rank these sentences by the scores from higher to lower, and select the top-3 sentences as the summary."]}
{"question_id": "a6603305f4fd3dd0010ac31243c40999a116537e", "predicted_answer": "According to the context, they encode sentences separately. This is indicated by the use of a [CLS] token before each sentence and a [SEP] token after each sentence, as well as the assignment of interval segment embeddings (E_A or E_B) to each sentence individually.", "predicted_evidence": ["As illustrated in Figure 1, we insert a [CLS] token before each sentence and a [SEP] token after each sentence. In vanilla BERT, The [CLS] is used as a symbol to aggregate features from one sentence or a pair of sentences. We modify the model by using multiple [CLS] symbols to get features for sentences ascending the symbol.", "We use interval segment embeddings to distinguish multiple sentences within a document. For $sent_i$ we will assign a segment embedding $E_A$ or $E_B$ conditioned on $i$ is odd or even. For example, for $[sent_1, sent_2, sent_3, sent_4, sent_5]$ we will assign $[E_A, E_B, E_A,E_B, E_A]$ .", "where $h^0=\\mathrm {PosEmb}(T)$ and $T$ are the sentence vectors output by BERT, $\\mathrm {PosEmb}$ is the function of adding positional embeddings (indicating the position of each sentence) to $T$ ; $\\mathrm {LN}$ is the layer normalization operation BIBREF8 ; $\\mathrm {MHAtt}$ is the multi-head attention operation BIBREF1 ; the superscript $l$ indicates the depth of the stacked layer.", "Instead of a simple sigmoid classifier, Inter-sentence Transformer applies more Transformer layers only on sentence representations, extracting document-level features focusing on summarization tasks from the BERT outputs:", "We evaluated on two benchmark datasets, namely the CNN/DailyMail news highlights dataset BIBREF12 and the New York Times Annotated Corpus (NYT; BIBREF13 ). The CNN/DailyMail dataset contains news articles and associated highlights, i.e., a few bullet points giving a brief overview of the article. We used the standard splits of BIBREF12 for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. We first split sentences by CoreNLP and pre-process the dataset following methods in BIBREF14 ."]}
{"question_id": "2ba4477d597b1fd123d14be07a7780ccb5c4819b", "predicted_answer": "To encode the whole text, they use BERT to output sentence vectors, and then modify the input sequence and embeddings of BERT by inserting a [CLS] token before each sentence and a [SEP] token after each sentence. This allows BERT to output a representation for each sentence, which is then used as input to the summarization-specific layers. The [CLS] token is used to aggregate features from each sentence, and multiple [CLS] symbols are used to get features for sentences ascending the symbol.", "predicted_evidence": ["where $h^0=\\mathrm {PosEmb}(T)$ and $T$ are the sentence vectors output by BERT, $\\mathrm {PosEmb}$ is the function of adding positional embeddings (indicating the position of each sentence) to $T$ ; $\\mathrm {LN}$ is the layer normalization operation BIBREF8 ; $\\mathrm {MHAtt}$ is the multi-head attention operation BIBREF1 ; the superscript $l$ indicates the depth of the stacked layer.", "After obtaining the sentence vectors from BERT, we build several summarization-specific layers stacked on top of the BERT outputs, to capture document-level features for extracting summaries. For each sentence $sent_i$ , we will calculate the final predicted score $\\hat{Y}_i$ . The loss of the whole model is the Binary Classification Entropy of $\\hat{Y}_i$ against gold label $Y_i$ . These summarization layers are jointly fine-tuned with BERT.", "To use BERT for extractive summarization, we require it to output the representation for each sentence. However, since BERT is trained as a masked-language model, the output vectors are grounded to tokens instead of sentences. Meanwhile, although BERT has segmentation embeddings for indicating different sentences, it only has two labels (sentence A or sentence B), instead of multiple sentences as in extractive summarization. Therefore, we modify the input sequence and embeddings of BERT to make it possible for extracting summaries.", "As illustrated in Figure 1, we insert a [CLS] token before each sentence and a [SEP] token after each sentence. In vanilla BERT, The [CLS] is used as a symbol to aggregate features from one sentence or a pair of sentences. We modify the model by using multiple [CLS] symbols to get features for sentences ascending the symbol.", "Instead of a simple sigmoid classifier, Inter-sentence Transformer applies more Transformer layers only on sentence representations, extracting document-level features focusing on summarization tasks from the BERT outputs:"]}
{"question_id": "027814f3a879a6c7852e033f9d99519b8729e444", "predicted_answer": "The ROUGE-L score of the baseline method is not explicitly mentioned in the given context. The context describes the experimental setup, model architecture, and training process, but it does not provide the ROUGE-L score of the baseline method.", "predicted_evidence": ["When predicting summaries for a new document, we first use the models to obtain the score for each sentence. We then rank these sentences by the scores from higher to lower, and select the top-3 sentences as the summary.", "$$\\nonumber lr = 2e^{-3}\\cdot min(step^{-0.5}, step \\cdot warmup^{-1.5})$$   (Eq. 17)", "All models are trained for 50,000 steps on 3 GPUs (GTX 1080 Ti) with gradient accumulation per two steps, which makes the batch size approximately equal to 36. Model checkpoints are saved and evaluated on the validation set every 1,000 steps. We select the top-3 checkpoints based on their evaluation losses on the validations set, and report the averaged results on the test set.", "Both datasets contain abstractive gold summaries, which are not readily suited to training extractive summarization models. A greedy algorithm was used to generate an oracle summary for each document. The algorithm greedily select sentences which can maximize the ROUGE scores as the oracle sentences. We assigned label 1 to sentences selected in the oracle summary and 0 otherwise.", "Like in the original BERT paper, the Simple Classifier only adds a linear layer on the BERT outputs and use a sigmoid function to get the predicted score:"]}
{"question_id": "00df1ff914956d4d23299d02fd44e4c985bb61fa", "predicted_answer": "The baseline method is the one that uses the scores from the model to rank the sentences, and then selects the top-3 sentences as the summary. This is described in the first sentence of the given context.", "predicted_evidence": ["When predicting summaries for a new document, we first use the models to obtain the score for each sentence. We then rank these sentences by the scores from higher to lower, and select the top-3 sentences as the summary.", "where $h^0=\\mathrm {PosEmb}(T)$ and $T$ are the sentence vectors output by BERT, $\\mathrm {PosEmb}$ is the function of adding positional embeddings (indicating the position of each sentence) to $T$ ; $\\mathrm {LN}$ is the layer normalization operation BIBREF8 ; $\\mathrm {MHAtt}$ is the multi-head attention operation BIBREF1 ; the superscript $l$ indicates the depth of the stacked layer.", "$$\\nonumber lr = 2e^{-3}\\cdot min(step^{-0.5}, step \\cdot warmup^{-1.5})$$   (Eq. 17)", "During the predicting process, Trigram Blocking is used to reduce redundancy. Given selected summary $S$ and a candidate sentence $c$ , we will skip $c$ is there exists a trigram overlapping between $c$ and $S$ . This is similar to the Maximal Marginal Relevance (MMR) BIBREF11 but much simpler.", "As illustrated in Figure 1, we insert a [CLS] token before each sentence and a [SEP] token after each sentence. In vanilla BERT, The [CLS] is used as a symbol to aggregate features from one sentence or a pair of sentences. We modify the model by using multiple [CLS] symbols to get features for sentences ascending the symbol."]}
{"question_id": "b57ad10468e1ba2a7a34396688dbb10a575d89f5", "predicted_answer": "The loss function used is the negative log-likelihood.", "predicted_evidence": ["$$loss = -\\frac{1}{|D|}\\sum _{(t_a, q) \\in D} \\log (f_{nn}(t_a,q)) \\nonumber $$   (Eq. 20)", "We separately train the parameters for each aspect with back-propagation. We use negative log-likelihood as the loss function.", "where $w_i$ is the weight associated with the $i$ -th regression tree, and $tr_i( \\cdot )$ is the value of a leaf node obtained by evaluating $i$ -th tree with features $\\left[ f_1(q,t), ... ,f_K(q,t) \\right]$ . The values of $w_i$ and the parameters in $tr_i(\\cdot )$ are learned with gradient descent during training.", "$$f_{s2}(t_a, q)=cosine(vec\\_avg(t_a), vec\\_avg(q)) \\nonumber $$   (Eq. 12)", "$$\\alpha _i = \\frac{exp(tanh(W [m_i; v_q] + b))}{\\sum _{j=1}^k exp(tanh(W [m_j; v_q] + b))}\\nonumber $$   (Eq. 15)"]}
{"question_id": "9d6d17120c42a834b2b5d96f2120d646218ed4bb", "predicted_answer": "No, they do not use the unstructured text on the webpage that was the source of the table. According to the text, they focus on making a thorough exploration of table content, and they do not utilize surrounding text of a table or pagerank score of a web page, unlike existing studies in the database community.", "predicted_evidence": ["We also conduct a synthetic experiment for table retrieval on WikiTableQuestions BIBREF0 , which is a widely used dataset for table-based question answering. It contains 2,108 HTML tables extracted from Wikipedia. Workers from Amazon Mechanical Turk are asked to write several relevant questions for each table. Since each query is written for a specific table, we believe that each pair of query-table can also be used as an instance for table retrieval. The difference between WikiTableQuestions and WebQueryTable is that the questions in WikiTableQuestions mainly focus on the local regions, such as cells or columns, of a table while the queries in WebQueryTable mainly focus on the global content of a table. The number of table index in WikiTableQuestions is 2,108, which is smaller than the number of table index in WebQueryTable. We randomly split the 22,033 question-table pairs into training (70%), development (10%) and test (20%).", "This is different from the phenomenon in WebQueryTable that the most effective aspect is caption. We believe that this is because the questions in WikiTableQuestions typically include content constrains from cells or headers. Two randomly sampled questions are \u201cwhich country won the 1994 europeans men's handball championship's preliminary round?\" and \u201cwhat party had 7,115 inactive voters as of october 25, 2005?\". On the contrary, queries from WebTableQuery usually do not use information from specific headers or cells. Examples include \u201cpolish rivers\", \u201cworld top 5 mountains\" and \u201clist of american cruise lines\". From Table 1 , we can also find that the question in WikiTableQuestions are longer than the queries in WebQueryTable. In addition, we observe that not all the questions from WikiTableQuestions are suitable for table retrieval. An example is \u201cwhat was the first player to be drafted in this table?\".", "To the best of our knowledge, there is no publicly available dataset for table retrieval. We introduce WebQueryTable, an open-domain dataset consisting of query-table pairs. We use search logs from a commercial search engine to get a list of queries that could be potentially answered by web tables. Each query in query logs is paired with a list of web pages, ordered by the number of user clicks for the query. We select the tables occurred in the top ranked web page, and ask annotators to label whether a table is relevant to a query or not. In this way, we get 21,113 query-table pairs. In the real scenario of table retrieval, a system is required to find a table from a huge collection of tables. Therefore, in order to enlarge the search space of our dataset, we extract 252,703 web tables from Wikipedia and regard them as searchable tables as well. Data statistics are given in Table 1 .", "We also find that the most effective aspect for WikiTableQuestions is header. This is different from the phenomenon in WebQueryTable that the most effective aspect is caption. We believe that this is because the questions in WikiTableQuestions typically include content constrains from cells or headers. Two randomly sampled questions are \u201cwhich country won the 1994 europeans men's handball championship's preliminary round?\" and \u201cwhat party had 7,115 inactive voters as of october 25, 2005?\". On the contrary, queries from WebTableQuery usually do not use information from specific headers or cells. Examples include \u201cpolish rivers\", \u201cworld top 5 mountains\" and \u201clist of american cruise lines\". From Table 1 , we can also find that the question in WikiTableQuestions are longer than the queries in WebQueryTable. In addition, we observe that not all the questions from WikiTableQuestions are suitable for table retrieval. An example is \u201cwhat was the first player to be drafted in this table?", "Unlike existing studies in database community BIBREF1 , BIBREF2 that utilize surrounding text of a table or pagerank score of a web page, we focus on making a thorough exploration of table content in this work. We believe that content-based table retrieval has the following challenges. The first challenge is how to effectively represent a table, which is semi-structured and includes many aspects such as headers, cells and caption. The second challenge is how to build a robust model that measures the relevance between an unstructured natural language query and a semi-structured table. Table retrieval could be viewed as a multi-modal task because the query and the table are of different forms. Moreover, to the best of our knowledge, there is no publicly available dataset for table retrieval. Further progress towards improving this area requires richer training and evaluation resources."]}
{"question_id": "965e0ce975a0b8612a30cfc31bbfd4b8a57aa138", "predicted_answer": "No, their method does not rely on the column headings of the table. According to the text, the most effective aspect for table retrieval in WikiTableQuestions is the content constraints from cells or headers, but in WebQueryTable, the caption is the most effective aspect, and the queries do not use information from specific headers or cells.", "predicted_evidence": ["We formulate the task of table retrieval in this section. Given a query $q$ and a collection of tables $T=\\lbrace t_1, ..., t_N\\rbrace $ , the goal of table search is to find a table $t_i$ that is most relevant to $q$ .", "Table 2 shows the performance of different approaches on the WebQueryTable dataset.", "Typically, a query $q$ is a natural language expression that consists of a list of words, such as \u201cmajor cities of netherlands\u201d. A table $t$ is a set of data elements arranged by vertical columns and horizontal rows. Formally, we define a table as a triple $t=\\lbrace headers,\\ cells,\\ caption\\rbrace $ that consists of three aspects. A table could have multiple $headers$ , each of which indicates the property of a column and could be used to identify a column. A table could have multiple $cells$ , each of which is a unit where a row and a column intersects. A table could have a $caption$ , which is typically an explanatory text about the table. Figure 1 gives an example to illustrate different aspects of a table.", "This is different from the phenomenon in WebQueryTable that the most effective aspect is caption. We believe that this is because the questions in WikiTableQuestions typically include content constrains from cells or headers. Two randomly sampled questions are \u201cwhich country won the 1994 europeans men's handball championship's preliminary round?\" and \u201cwhat party had 7,115 inactive voters as of october 25, 2005?\". On the contrary, queries from WebTableQuery usually do not use information from specific headers or cells. Examples include \u201cpolish rivers\", \u201cworld top 5 mountains\" and \u201clist of american cruise lines\". From Table 1 , we can also find that the question in WikiTableQuestions are longer than the queries in WebQueryTable. In addition, we observe that not all the questions from WikiTableQuestions are suitable for table retrieval. An example is \u201cwhat was the first player to be drafted in this table?\".", "It is helpful to note that tables from the web are not always \u201cregular\u201d. We regard a table as a \u201cregular\u201d table if it contains header, cell and caption, and the number of cells in each row is equal to the number of header cells. In this work, we make a comprehensive study of table retrieval on regular tables, and would like to release benchmark datasets of good quality. It is trivial to implement heuristic rules so as to convert the irregular tables to regular one, so we leave it to the future work."]}
{"question_id": "8dfdd1ed805bb23c774fbb032ef1d97c6802e07c", "predicted_answer": "No, not all the tables in the dataset are from the same website. The dataset consists of tables from two sources: the top-ranked web pages from a commercial search engine's query logs, and Wikipedia.", "predicted_evidence": ["To the best of our knowledge, there is no publicly available dataset for table retrieval. We introduce WebQueryTable, an open-domain dataset consisting of query-table pairs. We use search logs from a commercial search engine to get a list of queries that could be potentially answered by web tables. Each query in query logs is paired with a list of web pages, ordered by the number of user clicks for the query. We select the tables occurred in the top ranked web page, and ask annotators to label whether a table is relevant to a query or not. In this way, we get 21,113 query-table pairs. In the real scenario of table retrieval, a system is required to find a table from a huge collection of tables. Therefore, in order to enlarge the search space of our dataset, we extract 252,703 web tables from Wikipedia and regard them as searchable tables as well. Data statistics are given in Table 1 .", "It is helpful to note that tables from the web are not always \u201cregular\u201d. We regard a table as a \u201cregular\u201d table if it contains header, cell and caption, and the number of cells in each row is equal to the number of header cells. In this work, we make a comprehensive study of table retrieval on regular tables, and would like to release benchmark datasets of good quality. It is trivial to implement heuristic rules so as to convert the irregular tables to regular one, so we leave it to the future work.", "Table 2 shows the performance of different approaches on the WebQueryTable dataset.", "Table 4 shows the results of table ranking on the WikiTableQuestions dataset.", "This paper has the following contributions. We develop both feature-based and neural network based approaches, and conduct thorough experiments on real world and synthetic datasets. We release an open-domain dataset for table retrieval."]}
{"question_id": "c21675d8a90bda624d27e5535d1c10f08fcbc16b", "predicted_answer": "The tables are extracted from the HTML of web pages, specifically from Wikipedia, using a process that is not fully described in the given context. However, it is mentioned that the extracted tables are considered \"searchable tables\" and are added to the dataset to enlarge the search space. It is also mentioned that the tables are extracted from the top-ranked web pages in the search logs, and annotators are asked to label whether a table is relevant to a query or not.", "predicted_evidence": ["Typically, a query $q$ is a natural language expression that consists of a list of words, such as \u201cmajor cities of netherlands\u201d. A table $t$ is a set of data elements arranged by vertical columns and horizontal rows. Formally, we define a table as a triple $t=\\lbrace headers,\\ cells,\\ caption\\rbrace $ that consists of three aspects. A table could have multiple $headers$ , each of which indicates the property of a column and could be used to identify a column. A table could have multiple $cells$ , each of which is a unit where a row and a column intersects. A table could have a $caption$ , which is typically an explanatory text about the table. Figure 1 gives an example to illustrate different aspects of a table.", "To the best of our knowledge, there is no publicly available dataset for table retrieval. We introduce WebQueryTable, an open-domain dataset consisting of query-table pairs. We use search logs from a commercial search engine to get a list of queries that could be potentially answered by web tables. Each query in query logs is paired with a list of web pages, ordered by the number of user clicks for the query. We select the tables occurred in the top ranked web page, and ask annotators to label whether a table is relevant to a query or not. In this way, we get 21,113 query-table pairs. In the real scenario of table retrieval, a system is required to find a table from a huge collection of tables. Therefore, in order to enlarge the search space of our dataset, we extract 252,703 web tables from Wikipedia and regard them as searchable tables as well. Data statistics are given in Table 1 .", "There exists several works in database community that aims at finding related tables from keyword queries. A representative work is given by VLDB2008GG, which considers table search as a special case of document search task and represent a table with its surrounding text and page title. VLDB2010india use YAGO ontology to annotate tables with column and relationship labels. VLDB2011GG go one step further and use labels and relationships extracted from the web. VLDB2012IBM focus on the queries that describe table columns, and retrieve tables based on column mapping. There also exists table-related studies such as searching related tables from a table BIBREF16 , assembling a table from list in web page BIBREF17 and extracting tables using tabular structure from web page BIBREF18 . Our work differs from this line of research in that we focus on exploring the content of table to find relevant tables from web queries.", "It is helpful to note that tables from the web are not always \u201cregular\u201d. We regard a table as a \u201cregular\u201d table if it contains header, cell and caption, and the number of cells in each row is equal to the number of header cells. In this work, we make a comprehensive study of table retrieval on regular tables, and would like to release benchmark datasets of good quality. It is trivial to implement heuristic rules so as to convert the irregular tables to regular one, so we leave it to the future work.", "We formulate the task of table retrieval in this section. Given a query $q$ and a collection of tables $T=\\lbrace t_1, ..., t_N\\rbrace $ , the goal of table search is to find a table $t_i$ that is most relevant to $q$ ."]}
{"question_id": "da077b385d619305033785af5b204696d6145bd8", "predicted_answer": "Yes, the query-bag matching model uses a neural network. Specifically, it uses a combination of CNNs (Convolutional Neural Networks) and an MLP (Multi-Layer Perceptron) to predict the matching score between the query and the bag.", "predicted_evidence": ["Query-bag Matching To verify the effectiveness of our proposed models, We design a new query-bag matching based baseline. We concatenate the questions in the bag to form a new long \u201cquestion\u201d, then the hCNN model is applied to measure the matching degree of the original query and the new \u201cquestion\u201d, namely Bag-Con (Bag Concatenation).", "In the following subsections, we will introduce our proposed Query-bag Matching (QBM) model which output is the matching probability indicating whether the query and bag are asking the same questions. The basic Q-Q (query-question) matching model hybrid CNN (hCNN) BIBREF5 is presented as the background. Then we will show the base model and its two components designed to promote the performance: Mutual Coverage and Bag Representation. For better understanding, the whole model is shown in Figure FIGREF2.", "After getting the Q-Q matching representation $r_i$, we combine the $\\lbrace r_1, \\dots , r_n\\rbrace $ by element-wise max and mean pooling in order to get $r_p$ to represent the query-bag matching representation: rp = [ max_pooling { r1, ..., rn }; mean_pooling { r1, ..., rn } ] where [;] denotes concatenation. After that, an MLP with softmax is applied to predict whether the query and the bag is asking the same question. Finally, the loss function minimizes the cross entropy of the training data. Due to the out-of-order of the bag, we do not model the bag representation by CNN or LSTM, and experiments show the pooling-based method works well.", "Each element of $M^i$ is defined as $M^i_{a,b}=q_a^\\top \\cdot b_{i,b}$ where $q_a$ is the word embedding of the $a$-th word in query $q$ and $b_{i,b}$ is the embedding of the $b$-th word in $b_i$. Finally, the outputs of CNNs are combined via Equation SECREF3 to get the representation $r_i$, which indicates the matching representation of the query $q$ and the $i$-th question $b_i$ in the bag. For the Q-Q matching task, the $r_i$ is fed into an MLP (Multi-Layer Perceptron) to predict the matching score. In our query-bag matching setting, we will aggregate the $\\lbrace r_1, \\dots , r_n\\rbrace $ to predict the query-bag matching score. Due to the page limitation, please refer to BIBREF5 BIBREF5 for more details on hCNN.", "Finally, the outputs of CNNs are combined via Equation SECREF3 to get the representation $r_i$, which indicates the matching representation of the query $q$ and the $i$-th question $b_i$ in the bag. For the Q-Q matching task, the $r_i$ is fed into an MLP (Multi-Layer Perceptron) to predict the matching score. In our query-bag matching setting, we will aggregate the $\\lbrace r_1, \\dots , r_n\\rbrace $ to predict the query-bag matching score. Due to the page limitation, please refer to BIBREF5 BIBREF5 for more details on hCNN. h1 = CNN1(q) h2i = CNN1(bi) hmi = CNN2(qbi)"]}
{"question_id": "6d8a51e2790043497ed2637a1abc36bdffb39b71", "predicted_answer": "The two datasets used for experiments are:\n\n1. AliMe dataset\n2. Quora dataset", "predicted_evidence": ["We conduct experiments on two datasets: AliMe and Quora. The AliMe dataset is collected from the AliMe intelligent assistant system and the Quora dataset is composed of a public dataset.", "Quora The Quora dataset is originally released for the duplicated question detection task. The dataset contains 400,000 question pairs and each pair is marked whether they are asking the same question. Due to the huge amount of duplicated question pairs, we group the questions as question bag via the union-find algorithm from the duplicated questions. We get 60,400 bags, and all the questions in a bag are asking the same question. We filter the bags that contain questions less than 3 to make the bag not too small. The new bag dataset will help similar questions recommendation on the Quora website. We then extract one question in the bag as query and the other questions make up the bag in our task. Considering the negative samples, we follow the same strategy as AliMe dataset. Finally, we get 20,354 training set, 2,000 validation set, and 10,000 test set. To facilitate the research in this area, the composed Quora dataset are released.", "Analysis of the Bag Representation Coverage is also applied in the bag representation layer. The results of the bag representation without coverage component (Base+(BR w/o Cov)) is shown in Table TABREF6. Compared with the Base+BR and BR without coverage, it shows that the coverage component contributes a lot on both the two datasets. The bag representation with coverage (Base+BR) gains improvement over Base model, especially in Quora dataset.", "We conduct experiments on the AliMe and Quora dataset for the query-bag matching based information-seeking conversation. Compared with baselines, we verify the effectiveness of our model. Our model obtains 0.05 and 0.03 $\\text{R}_{10}@1$ gains comparing to the strongest baseline in the two datasets. The ablation study shows the usefulness of the components. The contributions in this paper are summarized as follows: 1) To the best of our knowledge, we are the first to adopt query-bag matching in the information-seeking conversation. 2) We propose the mutual coverage model to measure the information coverage in the query-bag matching. 3) We release the composite Quora dataset to facilitate the research in this area.", "Results and Ablation Study The results are shown in Table TABREF6. Our model (QBM) performs best compared to baselines (Q-Q Mean, Q-Q Max, Bag-con). Comparing Bag-Con and Base model, we find that modelling the query-question relationship following aggregation works better. We assume that the pooling-based aggregation can reduce the redundant information cross sentences in a bag. Considering the Q-Q matching based methods and query-bag based methods. In AliMe dataset, the query-bag matching outperforms the Q-Q matching based methods which shows the necessity to perform query-bag matching. The ablation study shows that the mutual coverage component and bag representation component achieve better performance than the base model, especially in the Quora dataset. The two components work independently and their combination gets the best performance."]}
{"question_id": "de4cc9e7fa5d700f5046d60789770f47911b3dd7", "predicted_answer": "The natural language(s) studied are Chinese (AliMe dataset) and English (Quora dataset).", "predicted_evidence": ["We conduct experiments on two datasets: AliMe and Quora. The AliMe dataset is collected from the AliMe intelligent assistant system and the Quora dataset is composed of a public dataset.", "This task aims at predicting whether a query $q$ matches a bag $b$, where the bag is composed of some questions $b=\\lbrace b_1, \\dots , b_n \\rbrace $ and $n$ is the number of questions in a bag. For the $q$ and $b_i$, an embedding layer is first applied to transform words to word embeddings via looking up word embedding table which is initialized by pre-trained word embeddings as in Section SECREF12.", "Recalling the text matching task BIBREF0, recently, researchers have adopted the deep neural network to model the matching relationship. ESIM BIBREF1 judges the inference relationship between two sentences by enhanced LSTM and interaction space. SMN BIBREF2 performs the context-response matching for the open-domain dialog system. BIBREF3 BIBREF3 explores the usefulness of noisy pre-training in the paraphrase identification task. BIBREF4 BIBREF4 surveys the methods in query-document matching in web search which focuses on the topic model, the dependency model, etc. However, none of them pays attention to the query-bag matching which concentrates on the matching for a query and a bag containing multiple questions.", "We conduct experiments on the AliMe and Quora dataset for the query-bag matching based information-seeking conversation. Compared with baselines, we verify the effectiveness of our model. Our model obtains 0.05 and 0.03 $\\text{R}_{10}@1$ gains comparing to the strongest baseline in the two datasets. The ablation study shows the usefulness of the components. The contributions in this paper are summarized as follows: 1) To the best of our knowledge, we are the first to adopt query-bag matching in the information-seeking conversation. 2) We propose the mutual coverage model to measure the information coverage in the query-bag matching. 3) We release the composite Quora dataset to facilitate the research in this area.", "We use the Adam optimizer with learning rate 0.0001 to optimize the parameters. The batch size is 32. The dropout rate is 0.5. The max length of the query and questions is 20 to cover most of the words in a sentence. We use padding to handle the various lengths of the text. The model checkpoint is chosen according to the best F-score on the validation set. The word embedding dimension is 300, and the pre-trained word embedding is from Sina and Glove for AliMe and Quora dataset respectively. Besides, the embedding is tuned while the model training to get better performance."]}
{"question_id": "8ad5ebca2f69023b60ccfa3aac0ed426234437ac", "predicted_answer": "Yes, the model is compared to two baselines: the Q-Q matching based baseline and the query-bag matching based baseline.", "predicted_evidence": ["To prove the effectiveness of our models, we propose two baselines from different aspects: the Q-Q matching based baseline and the query-bag matching based baseline.", "Following BIBREF7, we evaluate the model performance on five automatic evaluation metrics: MRR, $\\text{R}_{10}@1$, $\\text{R}_{10}@2$, $\\text{R}_{10}@5$, and $\\text{R}_{2}@1$. $\\text{R}_n@k$ calculates the recall of the true positive pre-defined questions among the $k$ selected candidates from $n$ available candidates. And Mean Reciprocal Rank (MRR) is another popular measurement for ranking problems.", "Analysis of the Bag Representation Coverage is also applied in the bag representation layer. The results of the bag representation without coverage component (Base+(BR w/o Cov)) is shown in Table TABREF6. Compared with the Base+BR and BR without coverage, it shows that the coverage component contributes a lot on both the two datasets. The bag representation with coverage (Base+BR) gains improvement over Base model, especially in Quora dataset.", "Q-Q Matching One starting point behind our work is that the query-bag matching may work better than the Q-Q matching for the information-seeking conversation. To verify such opinion, we propose the Q-Q matching based baseline and compare our model with two instances of the baseline. We extract the query-question pairs form the query-bag pair. The label of the query-bag pair is assigned to the new query-question pairs. An hCNN model is applied to train the new dataset. In the testing stage, each query-question pair is assigned with a probability indicating the matching degree. To compare with our model, we rank the bags based on the query-bag matching scores and the scores are defined as the max or mean matching probability of the query-question pairs in the query-bag pair. We name the two instances Q-Q Max and Q-Q Mean respectively.", "Query-bag Matching To verify the effectiveness of our proposed models, We design a new query-bag matching based baseline. We concatenate the questions in the bag to form a new long \u201cquestion\u201d, then the hCNN model is applied to measure the matching degree of the original query and the new \u201cquestion\u201d, namely Bag-Con (Bag Concatenation)."]}
{"question_id": "4afd4cfcb30433714b135b977baff346323af1e3", "predicted_answer": "The two datasets used in the experiments are:\n\n1. AliMe dataset\n2. Quora dataset", "predicted_evidence": ["We conduct experiments on two datasets: AliMe and Quora. The AliMe dataset is collected from the AliMe intelligent assistant system and the Quora dataset is composed of a public dataset.", "Analysis of the Bag Representation Coverage is also applied in the bag representation layer. The results of the bag representation without coverage component (Base+(BR w/o Cov)) is shown in Table TABREF6. Compared with the Base+BR and BR without coverage, it shows that the coverage component contributes a lot on both the two datasets. The bag representation with coverage (Base+BR) gains improvement over Base model, especially in Quora dataset.", "Quora The Quora dataset is originally released for the duplicated question detection task. The dataset contains 400,000 question pairs and each pair is marked whether they are asking the same question. Due to the huge amount of duplicated question pairs, we group the questions as question bag via the union-find algorithm from the duplicated questions. We get 60,400 bags, and all the questions in a bag are asking the same question. We filter the bags that contain questions less than 3 to make the bag not too small. The new bag dataset will help similar questions recommendation on the Quora website. We then extract one question in the bag as query and the other questions make up the bag in our task. Considering the negative samples, we follow the same strategy as AliMe dataset. Finally, we get 20,354 training set, 2,000 validation set, and 10,000 test set. To facilitate the research in this area, the composed Quora dataset are released.", "We conduct experiments on the AliMe and Quora dataset for the query-bag matching based information-seeking conversation. Compared with baselines, we verify the effectiveness of our model. Our model obtains 0.05 and 0.03 $\\text{R}_{10}@1$ gains comparing to the strongest baseline in the two datasets. The ablation study shows the usefulness of the components. The contributions in this paper are summarized as follows: 1) To the best of our knowledge, we are the first to adopt query-bag matching in the information-seeking conversation. 2) We propose the mutual coverage model to measure the information coverage in the query-bag matching. 3) We release the composite Quora dataset to facilitate the research in this area.", "Results and Ablation Study The results are shown in Table TABREF6. Our model (QBM) performs best compared to baselines (Q-Q Mean, Q-Q Max, Bag-con). Comparing Bag-Con and Base model, we find that modelling the query-question relationship following aggregation works better. We assume that the pooling-based aggregation can reduce the redundant information cross sentences in a bag. Considering the Q-Q matching based methods and query-bag based methods. In AliMe dataset, the query-bag matching outperforms the Q-Q matching based methods which shows the necessity to perform query-bag matching. The ablation study shows that the mutual coverage component and bag representation component achieve better performance than the base model, especially in the Quora dataset. The two components work independently and their combination gets the best performance."]}
{"question_id": "b2dc0c813da92cf13d86528bd32c12286ec9b9cd", "predicted_answer": "According to the given context, one lexical feature is considered, which is lexical predicates (e.g. open(e)).", "predicted_evidence": ["We perform an error analysis to assess the quality of the prediction for operators (i.e. logic operators like \u201cNot\u201d as well as discourse relations \u201cContrast\u201d), non-lexical predicates, such as binary predicates (e.g. Agent(e,x)) as well as unary predicates (e.g. time(t), entity(x), etc.), as well as for lexical predicates (e.g. open(e)). Results in Table TABREF15 show that predicting operators and binary predicates across language is hard, compared to the other two categories. Prediction of lexical predicates is relatively good even though most tokens in the test set where never seen during training; this can be attributable to the copy mechanism that is able to transfer tokens from the input directly during predication.", "We use the PMB v.2.1.0 for the experiments. The dataset consists of 4405 English sentences, 1173 German sentences, 633 Italian sentences and 583 Dutch sentences. We divide the English sentences into 3072 training sentences, 663 development and 670 testing sentences. We consider all the sentences in other languages as test set.", "To show how this approach performs, we focus on the Parallel Meaning Bank BIBREF13 \u2013 a multilingual semantic bank, where sentences in English, German, Italian and Dutch have been annotated with their meaning representations. The annotations in the PMB are based on Discourse Representation Theory BIBREF3, a popular theory of meaning representation designed to account for intra and inter-sentential phenomena, like temporal expressions and anaphora. Figure 1 shows an example DRT for the sentence `I sat down and opened my laptop' in its canonical `box' representation. A DRS is a nested structure with the top part containing the discourse references and the bottom with unary and binary predicates, as well as semantic constants (e.g. `speaker'). DRS can be linked to each other via logic operator (e.g. $\\lnot $, $\\rightarrow $, $\\diamond $) or, as in this case, discourse relations (e.g.", "Whereas the majority of semantic banks focus on English, recent effort has focussed on building multilingual representations, e.g. PMB BIBREF9, MRS BIBREF10 and FrameNetBIBREF11. However, manually annotating meaning representations in a new language is a painstaking process which explains why there are only a few datasets available for different formalisms in languages other than English. As a consequence, whereas the field has made great advances for English, little work has been done in other languages.", "To show how this approach performs, we focus on the Parallel Meaning Bank BIBREF13 \u2013 a multilingual semantic bank, where sentences in English, German, Italian and Dutch have been annotated with their meaning representations. The annotations in the PMB are based on Discourse Representation Theory BIBREF3, a popular theory of meaning representation designed to account for intra and inter-sentential phenomena, like temporal expressions and anaphora. Figure 1 shows an example DRT for the sentence `I sat down and opened my laptop' in its canonical `box' representation. A DRS is a nested structure with the top part containing the discourse references and the bottom with unary and binary predicates, as well as semantic constants (e.g. `speaker'). DRS can be linked to each other via logic operator (e.g. $\\lnot $, $\\rightarrow $, $\\diamond $) or, as in this case, discourse relations (e.g. CONTINUATION, RESULT, ELABORATION, etc."]}
{"question_id": "c4c06f36454fbfdc5d218fb84ce74eaf7f78fc98", "predicted_answer": "According to the text, the performance of the cross-lingual models in the three languages tested is as follows:\n\n* German: TreeLSTMs slightly improve performance compared to the baseline BiLSTM.\n* Italian: TreeLSTMs do not outperform the baseline BiLSTM.\n* Dutch: TreeLSTMs boost performance only when dependency features are available.", "predicted_evidence": ["We use the PMB v.2.1.0 for the experiments. The dataset consists of 4405 English sentences, 1173 German sentences, 633 Italian sentences and 583 Dutch sentences. We divide the English sentences into 3072 training sentences, 663 development and 670 testing sentences. We consider all the sentences in other languages as test set.", "Table TABREF12 shows the performance of our cross-lingual models in German, Italian and Dutch. We summarize the results as follows:", "In order to make the model directly transferable to the German, Italian and Dutch test data, we use the following language-independent features.", "TreeLSTMs slightly improve performance only for German. TreeLSTMs do not outperform a baseline BiLSTM for Italian and Dutch and they show little improvement in performance for German. This might be due to different factors that deserve more analysis including the performance of the parsers and syntactic similarity between these languages. When only dependency features are available, we found treeLSTM to boost performance only for Dutch.", "In this paper we explore parameter-shared models instead, where a model is trained on English using language independent features and tested in a target language."]}
{"question_id": "347dc2fd6427b39cf2358d43864750044437dff8", "predicted_answer": "According to the text, 3 Universal Dependency features are considered.", "predicted_evidence": ["What would that require? We show that universal dependency features can dramatically improve the performance of a cross-lingual semantic parser but modelling the tree structure directly does not outperform sequential BiLSTM architectures, not even when the two are combined together.", "TreeLSTM. To model the dependency structure directly, we use a child-sum tree-LSTM BIBREF15, where each word in the input sentence corresponds to a node in the dependency tree. In particular, summing across children is advantageous for cross-lingual tasks since languages might display different word orders. Computation follows Equation (1).", "We are planning to extend this initial survey to other DRS parsers that does not exclude presupposition and sense as well as to other semantic formalisms (e.g. AMR, MRS) where data sets annotated in languages other than English are available. Finally, we want to understand whether adding a bidirectionality to the treeLSTM will help improving the performance on modelling the dependency structure directly.", "Dependency features are crucial for zero-shot cross-lingual semantic parsing. Adding dependency features dramatically improves the performance in all three languages, when compared to using multilingual word-embedding and universal PoS embeddings alone. We hypothesize that the quality of the multilingual word-embeddings is poor, given that models using embeddings for the dependency relations alone outperform those using the other two features.", "Results show that language-independent features are a valid alternative to projection methods for cross-lingual semantic parsing. We show that adding dependency relation as features is beneficial, even when they are the only feature used during encoding. However, we also show that modeling the dependency structure directly via tree encoders does not outperform a sequential BiLSTM architecture for the three languages we have experimented with."]}
{"question_id": "6911e8724dfdb178fa81bf58019947b71ef8fbe7", "predicted_answer": "No, they do not evaluate any non-zero-shot parsers on the three languages (Italian, German, and Dutch). The paper only discusses a zero-shot cross-lingual semantic parsing method, where a parser trained in English is used to parse sentences in the three languages.", "predicted_evidence": ["Can we train a semantic parser in a language where annotation is available?. In this paper we show that this is indeed possible and we propose a zero-shot cross-lingual semantic parsing method based on language-independent features, where a parser trained in English \u2013 where labelled data is available, is used to parse sentences in three languages, Italian, German and Dutch.", "Dependency features are crucial for zero-shot cross-lingual semantic parsing. Adding dependency features dramatically improves the performance in all three languages, when compared to using multilingual word-embedding and universal PoS embeddings alone. We hypothesize that the quality of the multilingual word-embeddings is poor, given that models using embeddings for the dependency relations alone outperform those using the other two features.", "UD relations and structure. We use UDPipe BIBREF18 to obtain parses for English, German, Italian and Dutch. UD relation embeddings are randomly initialized and updated.", "We are planning to extend this initial survey to other DRS parsers that does not exclude presupposition and sense as well as to other semantic formalisms (e.g. AMR, MRS) where data sets annotated in languages other than English are available. Finally, we want to understand whether adding a bidirectionality to the treeLSTM will help improving the performance on modelling the dependency structure directly.", "What would that require? We show that universal dependency features can dramatically improve the performance of a cross-lingual semantic parser but modelling the tree structure directly does not outperform sequential BiLSTM architectures, not even when the two are combined together."]}
{"question_id": "b012df09fa2a3d6b581032d68991768cf4bc9d7b", "predicted_answer": "The size of the Parallel Meaning Bank (PMB) is not explicitly stated in the given context. The text only mentions that it is a multilingual semantic bank with annotated sentences in English, German, Italian, and Dutch, but it does not provide a specific number of sentences or annotations.", "predicted_evidence": ["To show how this approach performs, we focus on the Parallel Meaning Bank BIBREF13 \u2013 a multilingual semantic bank, where sentences in English, German, Italian and Dutch have been annotated with their meaning representations. The annotations in the PMB are based on Discourse Representation Theory BIBREF3, a popular theory of meaning representation designed to account for intra and inter-sentential phenomena, like temporal expressions and anaphora. Figure 1 shows an example DRT for the sentence `I sat down and opened my laptop' in its canonical `box' representation. A DRS is a nested structure with the top part containing the discourse references and the bottom with unary and binary predicates, as well as semantic constants (e.g. `speaker'). DRS can be linked to each other via logic operator (e.g. $\\lnot $, $\\rightarrow $, $\\diamond $) or, as in this case, discourse relations (e.g.", "To show how this approach performs, we focus on the Parallel Meaning Bank BIBREF13 \u2013 a multilingual semantic bank, where sentences in English, German, Italian and Dutch have been annotated with their meaning representations. The annotations in the PMB are based on Discourse Representation Theory BIBREF3, a popular theory of meaning representation designed to account for intra and inter-sentential phenomena, like temporal expressions and anaphora. Figure 1 shows an example DRT for the sentence `I sat down and opened my laptop' in its canonical `box' representation. A DRS is a nested structure with the top part containing the discourse references and the bottom with unary and binary predicates, as well as semantic constants (e.g. `speaker'). DRS can be linked to each other via logic operator (e.g. $\\lnot $, $\\rightarrow $, $\\diamond $) or, as in this case, discourse relations (e.g. CONTINUATION, RESULT, ELABORATION, etc.", "Whereas the majority of semantic banks focus on English, recent effort has focussed on building multilingual representations, e.g. PMB BIBREF9, MRS BIBREF10 and FrameNetBIBREF11. However, manually annotating meaning representations in a new language is a painstaking process which explains why there are only a few datasets available for different formalisms in languages other than English. As a consequence, whereas the field has made great advances for English, little work has been done in other languages.", "To answer this question, previous work have leveraged machine translation techniques to map the semantics from a language to another BIBREF12. However, these methods require parallel corpora to extract automatic alignments which are often noisy or not available at all.", "We use Counter BIBREF20 to evaluate the performance of our models. Counter looks for the best alignment between the predicted and gold DRS and computes precision, recall and F1. For further details about Counter, the reader is referred to van2018evaluating. It is worth reminding that unlike other work on the PMB BIBREF21, BIBREF0 does not deal with presupposition. In the PMB, presupposed variables are extracted from a main box and included in a separate one. In our work, we revert this process so to ignore presupposed boxes. Similarly, we also do not deal with sense tags which we aim to include in future work."]}
{"question_id": "62edffd051d056cf60e17deafcc55a8c9af398cb", "predicted_answer": "The source of the crosslingual word embeddings is MUSE (Multilingual Unsupervised and Supervised Embeddings) pre-trained multilingual word embeddings, specifically the MUSE BIBREF17 version.", "predicted_evidence": ["Multilingual word embeddings. We use the MUSE BIBREF17 pre-trained multilingual word embeddings and keep them fixed during training.", "Dependency features are crucial for zero-shot cross-lingual semantic parsing. Adding dependency features dramatically improves the performance in all three languages, when compared to using multilingual word-embedding and universal PoS embeddings alone. We hypothesize that the quality of the multilingual word-embeddings is poor, given that models using embeddings for the dependency relations alone outperform those using the other two features.", "Table TABREF12 shows the performance of our cross-lingual models in German, Italian and Dutch. We summarize the results as follows:", "We use the BiLSTM model as baseline (Bi) and compare it to the child-sum tree-LSTM (tree) with positional information added (Po/tree), as well as to a treeLSTM initialized with the hidden states of the BiLSTM(Bi/tree). We also conduct an ablation study on the features used, where WE, PE and DE are the word-embedding, PoS embedding and dependency relation embedding respectively. For completeness, along with the results for the cross-lingual task, we also report results for monolingual English semantic parsing, where word embedding features are randomly initialized.", "Previous work have explored two main methods for cross-lingual semantic parsing. One method requires parallel corpora to extract alignments between source and target languages using machine translation BIBREF11, BIBREF22, BIBREF23 The other method is to use parameter-shared models in the target language and the source language by leveraging language-independent features such as multilingual word embeddings, Universal POS tags and UD BIBREF24, BIBREF25, BIBREF26, BIBREF27. For semantic parsing, encoder-decoder models have achieved great success. Amongst these, tree or graph-structured decoders have recently shown to be state-of-the-art BIBREF5, BIBREF7, BIBREF0, BIBREF28, BIBREF8."]}
{"question_id": "d5c393df758dec6ea6827ae5b887eb6c303a4f4d", "predicted_answer": "Yes, they do compare against manually-created lexicons. In the last experiment, they manually translate a small part of the lexicon (up to 2,000 words) and use it as a bilingual dictionary to train the transformation matrix, and compare the performance with that of the statistical translation system.", "predicted_evidence": ["Lexicons: number of words present in each lexicon", "In the literature, it has been proposed to extend existing lexicons without supervision BIBREF4 , BIBREF5 , or to automatically translate existing lexicons from resourceful languages with statistical machine translation (SMT) systems BIBREF6 . While the former requires seed lexicons, the later are very interesting because they can automate the process of generating sentiment lexicons without any human expertise. But automatically translating sentiment lexicons leads to two problems: (1) out-of-vocabulary words, such as mis-spellings, morphological variants and slang, cannot be translated, and (2) machine translation performance strongly depends on available training resources such as bi-texts.", "Many methods have been proposed for extending polarity lexicons: propagate polarity along thesaurus relations BIBREF8 , BIBREF9 , BIBREF10 or use cooccurrence statistics to identify similar words BIBREF11 , BIBREF12 .", "In a last experiment, we look into the gains that can be obtained by manually translating a small part of the lexicon and use it as bilingual dictionary when training the transformation matrix. Figure FIGREF21 shows average macro-fmeasure on the four languages when translating up to 2,000 words from the MPQA lexicon (out of 8k). It can be observed that from 600 words on, performance is better than that of the statistical translation system.", "Our second experiment consists in varying the size of the bilingual dictionary used to train INLINEFORM0 . Figure FIGREF20 shows the evolution of average macro f-measure (over the four languages) when the INLINEFORM1 most frequent words from Wikipedia are part of the bilingual dictionary. It can be observed that using the 50k most frequent words leads to the best performance (an average macro-fmeasure of 61.72) while only 1,000 words already brings nice improvements."]}
{"question_id": "11a3af3f056e0fb5559fe5cbff1640e022732735", "predicted_answer": "The text does not explicitly compare the lexicon-based methods to non-lexicon methods. It primarily focuses on exploring different approaches to extend and port polarity lexicons to other languages, and evaluating their performance. Therefore, it does not provide a direct comparison with non-lexicon methods.", "predicted_evidence": ["Lexicons: number of words present in each lexicon", "Many methods have been proposed for extending polarity lexicons: propagate polarity along thesaurus relations BIBREF8 , BIBREF9 , BIBREF10 or use cooccurrence statistics to identify similar words BIBREF11 , BIBREF12 .", "Porting lexicons to other languages has also been studied: use aligned thesauri and propagate at the sense level BIBREF13 , BIBREF14 , translate the lexicon directly BIBREF15 , BIBREF16 , take advantage of off-the-shelf translation and include sample word context to get better translations BIBREF17 or use crowd sourcing to quickly bootstrap lexicons in non-english languages BIBREF18 .", "In a last experiment, we look into the gains that can be obtained by manually translating a small part of the lexicon and use it as bilingual dictionary when training the transformation matrix. Figure FIGREF21 shows average macro-fmeasure on the four languages when translating up to 2,000 words from the MPQA lexicon (out of 8k). It can be observed that from 600 words on, performance is better than that of the statistical translation system.", "Our second experiment consists in varying the size of the bilingual dictionary used to train INLINEFORM0 . Figure FIGREF20 shows the evolution of average macro f-measure (over the four languages) when the INLINEFORM1 most frequent words from Wikipedia are part of the bilingual dictionary. It can be observed that using the 50k most frequent words leads to the best performance (an average macro-fmeasure of 61.72) while only 1,000 words already brings nice improvements."]}
{"question_id": "07a214748a69b31400585aef7aba6af3e3d9cce2", "predicted_answer": "The language pairs considered are not explicitly mentioned in the given context. However, based on the mention of \"source language\" and \"target language\", it can be inferred that the approach is applicable to any language pair, as long as a small bilingual dictionary of frequent words is available.", "predicted_evidence": ["Moses and BWE can be combined by creating a lexicon from the union of the lexicons obtained by those systems. This combination yields even better results than translation or mapping alone.", "Our approach consists in creating distributional word representations in the source and target languages, and map them to each other with a linear transform trained given a small bilingual dictionary of frequent words. Then, source language words from the polarity lexicon can be projected in the target language embedding. The closest words to the projecting are used as translation.", "Lexicons: number of words present in each lexicon", "After presenting related work in Section SECREF2 , the extraction of word gs and their mapping across languages are detailed in Section SECREF3 . The corpus on which experiments are carried out and the results of our experiments are presented in Section SECREF4 . Finally, we conclude with a discussion of possible directions in Section SECREF5 .", "We did not implement part-of-speech and cluster features as they cannot be assumed to be available in the target languages. This system was part of the system combination that obtained the best results at the TASS 2015 BIBREF25 , BIBREF33 and DEFT 2015 BIBREF34 , BIBREF35 evaluation campaigns."]}
{"question_id": "44bf3047ff7e5c6b727b2aaa0805dd66c907dcd6", "predicted_answer": "According to the text, there is one abstractive summarization for each dialogue in the SAMSum Corpus, which contains over 16k chat dialogues with manually annotated summaries.", "predicted_evidence": ["As shown, summarization of dialogues is much more challenging than of news. In order to perform well, it may require designing dedicated tools, but also new, non-standard measures to capture the quality of abstractive dialogue summaries in a relevant way. We hope to tackle these issues in future work.", "In our paper we have studied the challenges of abstractive dialogue summarization. We have addressed a major factor that prevents researchers from engaging into this problem: the lack of a proper dataset. To the best of our knowledge, this is the first attempt to create a comprehensive resource of this type which can be used in future research. The next step could be creating an even more challenging dataset with longer dialogues that not only cover one topic, but span over numerous different ones.", "In the present paper, we further investigate the problem of abstractive dialogue summarization. With the growing popularity of online conversations via applications like Messenger, WhatsApp and WeChat, summarization of chats between a few participants is a new interesting direction of summarization research. For this purpose we have created the SAMSum Corpus which contains over 16k chat dialogues with manually annotated summaries. The dataset is freely available for the research community.", "This paper is a step towards abstractive summarization of dialogues by (1) introducing a new dataset, created for this task, (2) comparison with news summarization by the means of automated (ROUGE) and human evaluation.", "In order to build a dialogue summarization model, we adopt the following strategies: (1) each candidate architecture is trained and evaluated on the dialogue dataset; (2) each architecture is trained on the train set of CNN/Daily Mail joined together with the train set of the dialogue data, and evaluated on the dialogue test set."]}
{"question_id": "c6f2598b85dc74123fe879bf23aafc7213853f5b", "predicted_answer": "According to the text, human evaluators' judgement was measured using a rating system, where annotators assigned a score of -1 or 1 to the summaries. The criteria for evaluation are not explicitly stated, but it can be inferred that the annotators were assessing the quality and accuracy of the summaries, with 1 indicating a good summary and -1 indicating a poor summary.", "predicted_evidence": ["For manually evaluated samples, we calculated ROUGE metrics and the mean of two human ratings; the prepared statistics is presented in Table TABREF27. As we can see, models generating dialogue summaries can obtain high ROUGE results, but their outputs are marked as poor by human annotators. Our conclusion is that the ROUGE metric corresponds with the quality of generated summaries for news much better than for dialogues, confirmed by Pearson's correlation between human evaluation and the ROUGE metric, shown in Table TABREF28.", "We noticed a few annotations (7 for news and 4 for dialogues) with opposite marks (i.e. one annotator judgement was $-1$, whereas the second one was 1) and decided to have them annotated once again by another annotator who had to resolve conflicts. For the rest, we calculated the linear weighted Cohen's kappa coefficient BIBREF22 between annotators' scores. For news examples, we obtained agreement on the level of $0.371$ and for dialogues \u2013 $0.506$. The annotators' agreement is higher on dialogues than on news, probably because of structures of those data \u2013 articles are often long and it is difficult to decide what the key-point of the text is; dialogues, on the contrary, are rather short and focused mainly on one topic.", "This section contains a description of setting used in the experiments carried out.", "We show that the most popular summarization metric ROUGE does not reflect the quality of a summary. Looking at the ROUGE scores, one concludes that the dialogue summarization models perform better than the ones for news summarization. In fact, this hypothesis is not true \u2013 we performed an independent, manual analysis of summaries and we demonstrated that high ROUGE results, obtained for automatically-generated dialogue summaries, correspond with lower evaluation marks given by human annotators. An interesting example of the misleading behavior of the ROUGE metrics is presented in Table TABREF35 for Dialogue 4, where a wrong summary \u2013 'paul and cindy don't like red roses.' \u2013 obtained all ROUGE values higher than a correct summary \u2013 'paul asks cindy what color flowers should buy.'. Despite lower ROUGE values, news summaries were scored higher by human evaluators.", "We show that the most popular summarization metric ROUGE does not reflect the quality of a summary. Looking at the ROUGE scores, one concludes that the dialogue summarization models perform better than the ones for news summarization. In fact, this hypothesis is not true \u2013 we performed an independent, manual analysis of summaries and we demonstrated that high ROUGE results, obtained for automatically-generated dialogue summaries, correspond with lower evaluation marks given by human annotators. An interesting example of the misleading behavior of the ROUGE metrics is presented in Table TABREF35 for Dialogue 4, where a wrong summary \u2013 'paul and cindy don't like red roses.' \u2013 obtained all ROUGE values higher than a correct summary \u2013 'paul asks cindy what color flowers should buy.'. Despite lower ROUGE values, news summaries were scored higher by human evaluators. We conclude that when measuring the quality of model-generated summaries, the ROUGE metrics are more indicative for news than for dialogues, and a new metric should be designed to measure the quality of abstractive dialogue summaries."]}
{"question_id": "bdae851d4cf1d05506cf3e8359786031ac4f756f", "predicted_answer": "The following models have been evaluated:\n\n1. Fast Abs RL model (with original utterances and with enhanced ones)\n2. Lead-3 baseline\n3. MIDDLE baseline\n4. LONGEST-3 baseline\n5. DynamicConv with GPT-2 embeddings (trained on joined news and dialogue data with an utterance separation token)", "predicted_evidence": ["We evaluate models with the standard ROUGE metric BIBREF21, reporting the $F_1$ scores (with stemming) for ROUGE-1, ROUGE-2 and ROUGE-L following previous works BIBREF5, BIBREF4. We obtain scores using the py-rouge package.", "Having compared two variants of the Fast Abs RL model \u2013 with original utterances and with enhanced ones (see Section SECREF11), we conclude that enhancing utterances with information about the other interlocutors helps achieve higher ROUGE values.", "For manually evaluated samples, we calculated ROUGE metrics and the mean of two human ratings; the prepared statistics is presented in Table TABREF27. As we can see, models generating dialogue summaries can obtain high ROUGE results, but their outputs are marked as poor by human annotators. Our conclusion is that the ROUGE metric corresponds with the quality of generated summaries for news much better than for dialogues, confirmed by Pearson's correlation between human evaluation and the ROUGE metric, shown in Table TABREF28.", "Results of the evaluation of the above models are reported in Table TABREF9. There is no obvious baseline for the task of dialogues summarization. We expected rather low results for Lead-3, as the beginnings of the conversations usually contain greetings, not the main part of the discourse. However, it seems that in our dataset greetings are frequently combined with question-asking or information passing (sometimes they are even omitted) and such a baseline works even better than the MIDDLE baseline (taking utterances from the middle of a dialogue). Nevertheless, the best dialogue baseline turns out to be the LONGEST-3 model.", "According to ROUGE metrics, the best performing model is DynamicConv with GPT-2 embeddings, trained on joined news and dialogue data with an utterance separation token."]}
{"question_id": "894bbb1e42540894deb31c04cba0e6cfb10ea912", "predicted_answer": "No, the authors do not propose a specific better metric than ROUGE for measuring abstractive dialogue summarization. They do, however, conclude that ROUGE is not suitable for this task and suggest that a new metric should be designed to measure the quality of abstractive dialogue summaries. They also provide a manual evaluation method using human annotators, but this is not a proposed metric to replace ROUGE.", "predicted_evidence": ["As shown, summarization of dialogues is much more challenging than of news. In order to perform well, it may require designing dedicated tools, but also new, non-standard measures to capture the quality of abstractive dialogue summaries in a relevant way. We hope to tackle these issues in future work.", "We show that the most popular summarization metric ROUGE does not reflect the quality of a summary. Looking at the ROUGE scores, one concludes that the dialogue summarization models perform better than the ones for news summarization. In fact, this hypothesis is not true \u2013 we performed an independent, manual analysis of summaries and we demonstrated that high ROUGE results, obtained for automatically-generated dialogue summaries, correspond with lower evaluation marks given by human annotators. An interesting example of the misleading behavior of the ROUGE metrics is presented in Table TABREF35 for Dialogue 4, where a wrong summary \u2013 'paul and cindy don't like red roses.' \u2013 obtained all ROUGE values higher than a correct summary \u2013 'paul asks cindy what color flowers should buy.'. Despite lower ROUGE values, news summaries were scored higher by human evaluators.", "We show that the most popular summarization metric ROUGE does not reflect the quality of a summary. Looking at the ROUGE scores, one concludes that the dialogue summarization models perform better than the ones for news summarization. In fact, this hypothesis is not true \u2013 we performed an independent, manual analysis of summaries and we demonstrated that high ROUGE results, obtained for automatically-generated dialogue summaries, correspond with lower evaluation marks given by human annotators. An interesting example of the misleading behavior of the ROUGE metrics is presented in Table TABREF35 for Dialogue 4, where a wrong summary \u2013 'paul and cindy don't like red roses.' \u2013 obtained all ROUGE values higher than a correct summary \u2013 'paul asks cindy what color flowers should buy.'. Despite lower ROUGE values, news summaries were scored higher by human evaluators. We conclude that when measuring the quality of model-generated summaries, the ROUGE metrics are more indicative for news than for dialogues, and a new metric should be designed to measure the quality of abstractive dialogue summaries.", "This paper is a step towards abstractive summarization of dialogues by (1) introducing a new dataset, created for this task, (2) comparison with news summarization by the means of automated (ROUGE) and human evaluation.", "ROUGE is a standard way of evaluating the quality of machine generated summaries by comparing them with reference ones. The metric based on n-gram overlapping, however, may not be very informative for abstractive summarization, where paraphrasing is a keypoint in producing high-quality sentences. To quantify this conjecture, we manually evaluated summaries generated by the models for 150 news and 100 dialogues. We asked two linguists to mark the quality of every summary on the scale of $-1$, 0, 1, where $-1$ means that a summarization is poor, extracts irrelevant information or does not make sense at all, 1 \u2013 it is understandable and gives a brief overview of the text, and 0 stands for a summarization that extracts only a part of relevant information, or makes some mistakes in the produced summary."]}
{"question_id": "75b3e2d2caec56e5c8fbf6532070b98d70774b95", "predicted_answer": "The SAMSum Corpus contains 16,369 conversations.", "predicted_evidence": ["After collecting all of the conversations, we asked language experts to annotate them with summaries, assuming that they should (1) be rather short, (2) extract important pieces of information, (3) include names of interlocutors, (4) be written in the third person. Each dialogue contains only one reference summary. Validation. Since the SAMSum corpus contains dialogues created by linguists, the question arises whether such conversations are really similar to those typically written via messenger apps. To find the answer, we performed a validation task. We asked two linguists to doubly annotate 50 conversations in order to verify whether the dialogues could appear in a messenger app and could be summarized (i.e. a dialogue is not too general or unintelligible) or not (e.g. a dialogue between two people in a shop). The results revealed that 94% of examined dialogues were classified by both annotators as good i.e. they do look like conversations from a messenger app and could be condensed in a reasonable way.", "Each dialogue contains only one reference summary. Validation. Since the SAMSum corpus contains dialogues created by linguists, the question arises whether such conversations are really similar to those typically written via messenger apps. To find the answer, we performed a validation task. We asked two linguists to doubly annotate 50 conversations in order to verify whether the dialogues could appear in a messenger app and could be summarized (i.e. a dialogue is not too general or unintelligible) or not (e.g. a dialogue between two people in a shop). The results revealed that 94% of examined dialogues were classified by both annotators as good i.e. they do look like conversations from a messenger app and could be condensed in a reasonable way. In a similar validation task, conducted for the existing dialogue-type datasets (described in the Initial approach section), the annotators agreed that only 28% of the dialogues resembled conversations from a messenger app.", "Each dialogue was created by one person. After collecting all of the conversations, we asked language experts to annotate them with summaries, assuming that they should (1) be rather short, (2) extract important pieces of information, (3) include names of interlocutors, (4) be written in the third person. Each dialogue contains only one reference summary. Validation. Since the SAMSum corpus contains dialogues created by linguists, the question arises whether such conversations are really similar to those typically written via messenger apps. To find the answer, we performed a validation task. We asked two linguists to doubly annotate 50 conversations in order to verify whether the dialogues could appear in a messenger app and could be summarized (i.e. a dialogue is not too general or unintelligible) or not (e.g. a dialogue between two people in a shop).", "Each dialogue was created by one person. After collecting all of the conversations, we asked language experts to annotate them with summaries, assuming that they should (1) be rather short, (2) extract important pieces of information, (3) include names of interlocutors, (4) be written in the third person. Each dialogue contains only one reference summary. Validation. Since the SAMSum corpus contains dialogues created by linguists, the question arises whether such conversations are really similar to those typically written via messenger apps. To find the answer, we performed a validation task. We asked two linguists to doubly annotate 50 conversations in order to verify whether the dialogues could appear in a messenger app and could be summarized (i.e. a dialogue is not too general or unintelligible) or not (e.g. a dialogue between two people in a shop). The results revealed that 94% of examined dialogues were classified by both annotators as good i.e.", "Description. The created dataset is made of 16369 conversations distributed uniformly into 4 groups based on the number of utterances in conversations: 3-6, 7-12, 13-18 and 19-30. Each utterance contains the name of the speaker. Most conversations consist of dialogues between two interlocutors (about 75% of all conversations), the rest is between three or more people. Table TABREF3 presents the size of the dataset split used in our experiments. The example of a dialogue from this corpus is shown in Table TABREF4."]}
{"question_id": "573b8b1ad919d3fd0ef7df84e55e5bfd165b3e84", "predicted_answer": "No, they do not manually check all adversarial examples that fooled some model for potential valid examples. Instead, they conducted an experiment where human workers annotated a randomly sampled 100 adversarial examples that were successful in fooling the target classifier, to evaluate the effectiveness of their model in generating adversarial examples that are semantics-preserving and do not affect human judgments significantly.", "predicted_evidence": ["In this section, we describe the evaluation setup used to measure the effectiveness of our model in generating adversarial examples. The success of our model lies in its ability to fool the target classifier. We pretrain our models with dataset that generates a number of character and word perturbations. We elaborate on the experimental setup and the results below.", "We also evaluated our model based on human judgments. We conducted an experiment where the workers were presented with randomly sampled 100 adversarial examples generated by our model which were successful in fooling the target classifier. The examples were shuffled to mitigate ordering bias, and every example was annotated by three workers. The workers were asked to label the sentiment of the sampled adversarial example. For every adversarial example shown, we also showed the original text and asked them to rate their similarity on a scale from 0 (Very Different) to 3 (Very Similar). We found that the perturbations produced by our model do not affect the human judgments significantly as $94.6\\%$ of the human annotations matched with the ground-truth label of the original text. The average similarity rating of $1.916$ also indicated that the generated adversarial sequences are semantics-preserving.", "We fine-tune our model to fool a target classifier by learning a policy that maximizes a specific discrete metric formulated based on the constraints required to generate adversarial examples. In our work, we use the self-critical approach of Rennie et al. BIBREF36 as our policy gradient training algorithm.", "Adversarial examples are generally minimal perturbations applied to the input data in an effort to expose the regions of the input space where a trained model performs poorly. Prior works BIBREF0, BIBREF1 have demonstrated the ability of an adversary to evade state-of-the-art classifiers by carefully crafting attack examples which can be even imperceptible to humans. Following such approaches, there has been a number of techniques aimed at generating adversarial examples BIBREF2, BIBREF3. Depending on the degree of access to the target model, an adversary may operate in one of the two different settings: (a) black-box setting, where an adversary doesn't have access to target model's internal architecture or its parameters, (b) white-box setting, where an adversary has access to the target model, its parameters, and input feature representations. In both these settings, the adversary cannot alter the training data or the target model itself.", "Following such approaches, there has been a number of techniques aimed at generating adversarial examples BIBREF2, BIBREF3. Depending on the degree of access to the target model, an adversary may operate in one of the two different settings: (a) black-box setting, where an adversary doesn't have access to target model's internal architecture or its parameters, (b) white-box setting, where an adversary has access to the target model, its parameters, and input feature representations. In both these settings, the adversary cannot alter the training data or the target model itself. Depending on the purpose of the adversary, adversarial attacks can be categorized as (a) targeted attack and (b) non-targeted attack. In a targeted attack, the output category of a generated example is intentionally controlled to a specific target category with limited change in semantic information. While a non-targeted attack doesn't care about the category of misclassified results."]}
{"question_id": "07d98dfa88944abd12acd45e98fb7d3719986aeb", "predicted_answer": "According to the text, the answer is: Not all generated examples are semantics-preserving perturbations to the original text, but a significant majority (94.6%) are. The average similarity rating of 1.916 also indicates that the generated adversarial sequences are semantics-preserving. However, it is possible that some perturbations may not preserve the semantics of the original text, as mentioned in the context of DeepWordBug, which can introduce excessive misspellings that irreversibly change the semantics of the text.", "predicted_evidence": ["We also evaluated our model based on human judgments. We conducted an experiment where the workers were presented with randomly sampled 100 adversarial examples generated by our model which were successful in fooling the target classifier. The examples were shuffled to mitigate ordering bias, and every example was annotated by three workers. The workers were asked to label the sentiment of the sampled adversarial example. For every adversarial example shown, we also showed the original text and asked them to rate their similarity on a scale from 0 (Very Different) to 3 (Very Similar). We found that the perturbations produced by our model do not affect the human judgments significantly as $94.6\\%$ of the human annotations matched with the ground-truth label of the original text. The average similarity rating of $1.916$ also indicated that the generated adversarial sequences are semantics-preserving.", "Let us consider a target model $T$ and $(x,l)$ refers to the samples from the dataset. Given an instance $x$, the goal of the adversary is to generate adversarial examples $x^{\\prime }$ such that $T(x^{\\prime }) \\ne l$, where $l$ denotes the true label i.e take one of the $K$ classes of the target classification model. The changes made to $x$ to get $x^{\\prime }$ are called perturbations. We would like to have $x^{\\prime }$ close to the original instance $x$. In a black box setting, we do not have knowledge about the internals of the target model or its training data. Previous work by Papernot et al. BIBREF14 train a separate substitute classifier such that it can mimic the decision boundaries of the target classifier. The substitute classifier is then used to craft adversarial examples. While these techniques have been applied for image classification models, such methods have not been explored extensively for text.", "Let us consider a target model $T$ and $(x,l)$ refers to the samples from the dataset. Given an instance $x$, the goal of the adversary is to generate adversarial examples $x^{\\prime }$ such that $T(x^{\\prime }) \\ne l$, where $l$ denotes the true label i.e take one of the $K$ classes of the target classification model. The changes made to $x$ to get $x^{\\prime }$ are called perturbations. We would like to have $x^{\\prime }$ close to the original instance $x$. In a black box setting, we do not have knowledge about the internals of the target model or its training data. Previous work by Papernot et al. BIBREF14 train a separate substitute classifier such that it can mimic the decision boundaries of the target classifier. The substitute classifier is then used to craft adversarial examples.", "However, this approach was restricted to binary text classifiers. Iyyer et al. BIBREF23 crafted adversarial examples using their proposed Syntactically Controlled Paraphrase Networks (SCPNs). They designed this model for generating syntactically adversarial examples without compromising on the quality of the input semantics. The general process is based on the encoder-decoder architecture of SCPN. Gao et al. BIBREF24 implemented an algorithm called DeepWordBug that generates small text perturbations in a black box setting forcing the deep learning model to make mistakes. DeepWordBug used a scoring function to determine important tokens and then applied character-level transformations to those tokens. Though the algorithm successfully generates adversarial examples by introducing character-level attacks, most of the introduced perturbations are constricted to misspellings. The semantics of the text may be irreversibly changed if excessive misspellings are introduced to fool the target classifier.", "Before the substitute network is trained, we pretrain our encoder-decoder model on common misspellings and paraphrase datasets to empower the model to produce character and word perturbations in the form of misspellings or paraphrases. For training substitute network and generation of adversarial examples, we randomly draw data that is disjoint from the training data of the black-box model since we assume the adversaries have no prior knowledge about the training data or the model. Specifically, we consider attacking a target classifier by generating adversarial examples based on unseen input examples. This is done by dividing the dataset into training, validation and test using 60-30-10 ratio. The training data is used by the target model, while the unseen validation samples are used with necessary data augmentation for our AEG model. We further improve our model by using a self-critical approach to finally generate better adversarial examples. The rewards are formulated based on the following goals: (a) fool the target classifier, (b) minimize the number of perturbations and (c) preserve the semantics of the text."]}
{"question_id": "3a40559e5a3c2a87c7b9031c89e762b828249c05", "predicted_answer": "The success rate of fooling the tested models in experiments is not explicitly stated as a single percentage value. However, it can be inferred from the text that the success rate is high, as the authors mention that the perturbations produced by their model do not affect human judgments significantly, with 94.6% of human annotations matching the ground-truth label of the original text. Additionally, the authors aim to promote the target classifier to make mistakes, which suggests that their model is effective in fooling the target classifier.", "predicted_evidence": ["In this section, we describe the evaluation setup used to measure the effectiveness of our model in generating adversarial examples. The success of our model lies in its ability to fool the target classifier. We pretrain our models with dataset that generates a number of character and word perturbations. We elaborate on the experimental setup and the results below.", "The performance of these methods are measured by the percentage fall in accuracy of these models on the generated adversarial texts. Higher the percentage dip in the accuracy of the target classifier, more effective is our model.", "We also evaluated our model based on human judgments. We conducted an experiment where the workers were presented with randomly sampled 100 adversarial examples generated by our model which were successful in fooling the target classifier. The examples were shuffled to mitigate ordering bias, and every example was annotated by three workers. The workers were asked to label the sentiment of the sampled adversarial example. For every adversarial example shown, we also showed the original text and asked them to rate their similarity on a scale from 0 (Very Different) to 3 (Very Similar). We found that the perturbations produced by our model do not affect the human judgments significantly as $94.6\\%$ of the human annotations matched with the ground-truth label of the original text. The average similarity rating of $1.916$ also indicated that the generated adversarial sequences are semantics-preserving.", "Table TABREF29 summarizes the data and models used in our experiments. We compare our proposed model with the following black-box non-targeted attacks:", "Given a target model $T$, it takes a text sequence $y$ and outputs prediction probabilities $P$ across various categories of the target model. Given an input sample $(x, l)$, we compute a perturbation using our AEG model and produce a sequence $y$. We compute the adversarial reward as $R_{A}=(1-P_l)$, where the ground truth $l$ is an index to the list of categories and $P_l$ is the probability that the perturbed generated sequence $y$ belongs to target ground truth $l$. Since we want the target classifier to make mistakes, we promote it by rewarding higher when the sequences produce low target probabilities."]}
{"question_id": "5db47bbb97282983e10414240db78154ea7ac75f", "predicted_answer": "The models that are able to be fooled for AG's news corpus news categorization task by this approach are not explicitly stated in the given context. However, it can be inferred that the character-based convolutional model (CNN-Char) is one of the models that can be fooled, since the approach is evaluated on AG's news categorization task and the CNN-Char model is used as a baseline model in this task.", "predicted_evidence": ["News categorization: We perform our experiments on AG's news corpus with a character-based convolutional model (CNN-Char) BIBREF12. The news corpus contains titles and descriptions of various news articles along with their respective categories. There are four categories: World, Sports, Business and Sci/Tech. The trained CNN-Char model achieves a test accuracy of 89.11%.", "We evaluate our models on two different datasets associated with two different tasks: IMDB sentiment classification and AG's news categorization task. We run ablation studies on various components of the model and provide insights into decisions of our model.", "Promisingly, it identifies the most important words that contribute to particular categorization. The model introduces misspellings or word replacements without significant change in semantics of the text.", "We adopt a self-critical sequence training technique to train our model to generate examples that can fool or increase the probability of misclassification in text classifiers.", "We conduct experiments on different datasets to verify if the accuracy of the deep learning models decrease when fed with the adversarial examples generated by our model. We use benchmark sentiment classification and news categorization datasets and the details are as follows:"]}
{"question_id": "c589d83565f528b87e355b9280c1e7143a42401d", "predicted_answer": "According to the text, the models that can be fooled by this approach for the IMDB sentiment classification task are:\n\n* Word-based models\n* Character-based models", "predicted_evidence": ["We evaluate our models on two different datasets associated with two different tasks: IMDB sentiment classification and AG's news categorization task. We run ablation studies on various components of the model and provide insights into decisions of our model.", "Sentiment classification: We trained a word-based convolutional model (CNN-Word) BIBREF11 on IMDB sentiment dataset . The dataset contains 50k movie reviews in total which are labeled as positive or negative. The trained model achieves a test accuracy of 89.95% which is relatively close to the state-of-the-art results on this dataset.", "By generating adversarial examples for target word and character-based models trained on IMDB reviews and AG's news dataset, we find that our model is capable of generating semantics-preserving perturbations that leads to steep decrease in accuracy of those target models. We conducted ablation studies to find the importance of individual components of our system. Extremely low values of the certain reward coefficient constricts the quantitative performance of the model can also lead to semantic divergence. Therefore, the choice of a particular value for this model should be motivated by the demands of the context in which it is applied. One of the main challenges of such approaches lies in the ability to produce more synthetic data to train the generator model in the distribution of the target model's training data. This can significantly improve the performance of our model. We hope that our method motivates a more nuanced exploration into generating adversarial examples and adversarial training for building robust classification models.", "We conduct experiments on different datasets to verify if the accuracy of the deep learning models decrease when fed with the adversarial examples generated by our model. We use benchmark sentiment classification and news categorization datasets and the details are as follows:", "In this work, we have introduced a $AEG$, a model capable of generating adversarial text examples to fool the black-box text classification models. Since we do not have access to gradients or parameters of the target model, we modelled our problem using a reinforcement learning based approach. In order to effectively baseline the REINFORCE algorithm for policy-gradients, we implemented a self-critical approach that normalizes the rewards obtained by sampled sentences with the rewards obtained by the model under test-time inference algorithm. By generating adversarial examples for target word and character-based models trained on IMDB reviews and AG's news dataset, we find that our model is capable of generating semantics-preserving perturbations that leads to steep decrease in accuracy of those target models. We conducted ablation studies to find the importance of individual components of our system. Extremely low values of the certain reward coefficient constricts the quantitative performance of the model can also lead to semantic divergence."]}
{"question_id": "7f90e9390ad58b22b362a57330fff1c7c2da7985", "predicted_answer": "No, they do not use an already trained model on some task in their reinforcement learning approach. The text states \"No-RL: We use our pretrained model without the reinforcement learning objective.\" This suggests that they have a pre-trained model, but they do not use it as a starting point for their reinforcement learning approach. Instead, they use reinforcement learning to train their model from scratch.", "predicted_evidence": ["No-RL: We use our pretrained model without the reinforcement learning objective.", "Since we do not have access to gradients or parameters of the target model, we modelled our problem using a reinforcement learning based approach. In order to effectively baseline the REINFORCE algorithm for policy-gradients, we implemented a self-critical approach that normalizes the rewards obtained by sampled sentences with the rewards obtained by the model under test-time inference algorithm. By generating adversarial examples for target word and character-based models trained on IMDB reviews and AG's news dataset, we find that our model is capable of generating semantics-preserving perturbations that leads to steep decrease in accuracy of those target models. We conducted ablation studies to find the importance of individual components of our system. Extremely low values of the certain reward coefficient constricts the quantitative performance of the model can also lead to semantic divergence. Therefore, the choice of a particular value for this model should be motivated by the demands of the context in which it is applied. One of the main challenges of such approaches lies in the ability to produce more synthetic data to train the generator model in the distribution of the target model's training data.", "In this work, we have introduced a $AEG$, a model capable of generating adversarial text examples to fool the black-box text classification models. Since we do not have access to gradients or parameters of the target model, we modelled our problem using a reinforcement learning based approach. In order to effectively baseline the REINFORCE algorithm for policy-gradients, we implemented a self-critical approach that normalizes the rewards obtained by sampled sentences with the rewards obtained by the model under test-time inference algorithm. By generating adversarial examples for target word and character-based models trained on IMDB reviews and AG's news dataset, we find that our model is capable of generating semantics-preserving perturbations that leads to steep decrease in accuracy of those target models. We conducted ablation studies to find the importance of individual components of our system. Extremely low values of the certain reward coefficient constricts the quantitative performance of the model can also lead to semantic divergence. Therefore, the choice of a particular value for this model should be motivated by the demands of the context in which it is applied.", "In order to effectively baseline the REINFORCE algorithm for policy-gradients, we implemented a self-critical approach that normalizes the rewards obtained by sampled sentences with the rewards obtained by the model under test-time inference algorithm. By generating adversarial examples for target word and character-based models trained on IMDB reviews and AG's news dataset, we find that our model is capable of generating semantics-preserving perturbations that leads to steep decrease in accuracy of those target models. We conducted ablation studies to find the importance of individual components of our system. Extremely low values of the certain reward coefficient constricts the quantitative performance of the model can also lead to semantic divergence. Therefore, the choice of a particular value for this model should be motivated by the demands of the context in which it is applied. One of the main challenges of such approaches lies in the ability to produce more synthetic data to train the generator model in the distribution of the target model's training data. This can significantly improve the performance of our model.", "In this work, we have introduced a $AEG$, a model capable of generating adversarial text examples to fool the black-box text classification models. Since we do not have access to gradients or parameters of the target model, we modelled our problem using a reinforcement learning based approach. In order to effectively baseline the REINFORCE algorithm for policy-gradients, we implemented a self-critical approach that normalizes the rewards obtained by sampled sentences with the rewards obtained by the model under test-time inference algorithm. By generating adversarial examples for target word and character-based models trained on IMDB reviews and AG's news dataset, we find that our model is capable of generating semantics-preserving perturbations that leads to steep decrease in accuracy of those target models. We conducted ablation studies to find the importance of individual components of our system. Extremely low values of the certain reward coefficient constricts the quantitative performance of the model can also lead to semantic divergence."]}
{"question_id": "3e3e45094f952704f1f679701470c3dbd845999e", "predicted_answer": "The proposed reinforcement learning based approach generates adversarial examples in black-box settings by:\n\n1. Training a substitute network using a disjoint dataset from the target model's training data.\n2. Formulating the generation of adversarial examples as a reinforcement learning task, where the rewards are based on three goals:\n\t* Fooling the target classifier\n\t* Minimizing the number of perturbations\n\t* Preserving the semantics of the text\n3. Using a self-critical approach to improve the generation of adversarial examples.\n4. Querying the target classifier with the generated adversarial examples to get their corresponding predictions, and using this feedback to update the Adversarial Example Generator (AEG) model.\n\nBy using reinforcement learning, the AEG model learns to generate adversarial examples that are effective in fooling the target classifier while minimizing the number of perturbations and preserving the semantics of the text.", "predicted_evidence": ["We propose a black-box non-targeted attack strategy by combining ideas of substitute network and adversarial example generation. We formulate it as a reinforcement learning task.", "For training substitute network and generation of adversarial examples, we randomly draw data that is disjoint from the training data of the black-box model since we assume the adversaries have no prior knowledge about the training data or the model. Specifically, we consider attacking a target classifier by generating adversarial examples based on unseen input examples. This is done by dividing the dataset into training, validation and test using 60-30-10 ratio. The training data is used by the target model, while the unseen validation samples are used with necessary data augmentation for our AEG model. We further improve our model by using a self-critical approach to finally generate better adversarial examples. The rewards are formulated based on the following goals: (a) fool the target classifier, (b) minimize the number of perturbations and (c) preserve the semantics of the text. In the following sections, we explain the encoder-decoder model and then describe the reinforcement learning framing towards generation of adversarial examples.", "Given different settings of the adversary, there are other works that have designed attacks in \u201cgray-box\u201d settings BIBREF8, BIBREF9, BIBREF10. However, the definitions of \u201cgray-box\u201d attacks are quite different in each of these approaches. In this paper, we focus on \u201cblack-box\u201d setting where we assume that the adversary possesses a limited set of labeled data, which is different from the target's training data, and also has an oracle access to the system, i.e., one can query the target classifier with any input and get its corresponding predictions. We propose an effective technique to generate adversarial examples in a black-box setting. We develop an Adversarial Example Generator (AEG) model that uses a reinforcement learning framing to generate adversarial examples. We evaluate our models using a word-based BIBREF11 and character-based BIBREF12 text classification model on benchmark classification tasks: sentiment classification and news categorization.", "Adversarial examples are generally minimal perturbations applied to the input data in an effort to expose the regions of the input space where a trained model performs poorly. Prior works BIBREF0, BIBREF1 have demonstrated the ability of an adversary to evade state-of-the-art classifiers by carefully crafting attack examples which can be even imperceptible to humans. Following such approaches, there has been a number of techniques aimed at generating adversarial examples BIBREF2, BIBREF3. Depending on the degree of access to the target model, an adversary may operate in one of the two different settings: (a) black-box setting, where an adversary doesn't have access to target model's internal architecture or its parameters, (b) white-box setting, where an adversary has access to the target model, its parameters, and input feature representations. In both these settings, the adversary cannot alter the training data or the target model itself.", "Given different settings of the adversary, there are other works that have designed attacks in \u201cgray-box\u201d settings BIBREF8, BIBREF9, BIBREF10. However, the definitions of \u201cgray-box\u201d attacks are quite different in each of these approaches. In this paper, we focus on \u201cblack-box\u201d setting where we assume that the adversary possesses a limited set of labeled data, which is different from the target's training data, and also has an oracle access to the system, i.e., one can query the target classifier with any input and get its corresponding predictions. We propose an effective technique to generate adversarial examples in a black-box setting. We develop an Adversarial Example Generator (AEG) model that uses a reinforcement learning framing to generate adversarial examples. We evaluate our models using a word-based BIBREF11 and character-based BIBREF12 text classification model on benchmark classification tasks: sentiment classification and news categorization. The adversarial sequences generated are able to effectively fool the classifiers without changing the semantics of the text."]}
{"question_id": "475ef4ad32a8589dae9d97048166d732ae5d7beb", "predicted_answer": "The languages with different scripts that M-Bert looks at are English, Japanese, Hindi (written in Devanagari), and Bulgarian.", "predicted_evidence": ["Code-switching (CS)\u2014the mixing of multiple languages within a single utterance\u2014and transliteration\u2014writing that is not in the language's standard script\u2014present unique test cases for M-Bert, which is pre-trained on monolingual, standard-script corpora. Generalizing to code-switching is similar to other cross-lingual transfer scenarios, but would benefit to an even larger degree from a shared multilingual representation. Likewise, generalizing to transliterated text is similar to other cross-script transfer experiments, but has the additional caveat that M-Bert was not pre-trained on text that looks like the target.", "However, cross-script transfer is less accurate for other pairs, such as English and Japanese, indicating that M-Bert's multilingual representation is not able to generalize equally well in all cases. A possible explanation for this, as we will see in section SECREF18 , is typological similarity. English and Japanese have a different order of subject, verb and object, while English and Bulgarian have the same, and M-Bert may be having trouble generalizing across different orderings.", "For script-corrected inputs, i.e., when Hindi is written in Devanagari, M-Bert's performance when trained only on monolingual corpora is comparable to performance when training on code-switched data, and it is likely that some of the remaining difference is due to domain mismatch. This provides further evidence that M-Bert uses a representation that is able to incorporate information from multiple languages.", "M-Bert's ability to transfer between languages that are written in different scripts, and thus have effectively zero lexical overlap, is surprising given that it was trained on separate monolingual corpora and not with a multilingual objective. To probe deeper into how the model is able to perform this generalization, Table TABREF14 shows a sample of pos results for transfer across scripts.", "In this work, we showed that M-Bert's robust, often surprising, ability to generalize cross-lingually is underpinned by a multilingual representation, without being explicitly trained for it. The model handles transfer across scripts and to code-switching fairly well, but effective transfer to typologically divergent and transliterated targets will likely require the model to incorporate an explicit multilingual training objective, such as that used by BIBREF15 or BIBREF16 ."]}
{"question_id": "3fd8eab282569b1c18b82f20d579b335ae70e79f", "predicted_answer": "The languages experimented with are not explicitly mentioned in the given context. However, it can be inferred that the experiments involve multiple languages, including those with different scripts and typological features, such as subject/object/verb order and adjective/noun order.", "predicted_evidence": ["It is our hope that these kinds of probing experiments will help steer researchers toward the most promising lines of inquiry by encouraging them to focus on the places where current contextualized word representation approaches fall short.", "In this work, we showed that M-Bert's robust, often surprising, ability to generalize cross-lingually is underpinned by a multilingual representation, without being explicitly trained for it. The model handles transfer across scripts and to code-switching fairly well, but effective transfer to typologically divergent and transliterated targets will likely require the model to incorporate an explicit multilingual training objective, such as that used by BIBREF15 or BIBREF16 .", "Code-switching (CS)\u2014the mixing of multiple languages within a single utterance\u2014and transliteration\u2014writing that is not in the language's standard script\u2014present unique test cases for M-Bert, which is pre-trained on monolingual, standard-script corpora. Generalizing to code-switching is similar to other cross-lingual transfer scenarios, but would benefit to an even larger degree from a shared multilingual representation. Likewise, generalizing to transliterated text is similar to other cross-script transfer experiments, but has the additional caveat that M-Bert was not pre-trained on text that looks like the target.", "Our results show that M-Bert is able to perform cross-lingual generalization surprisingly well. More importantly, we present the results of a number of probing experiments designed to test various hypotheses about how the model is able to perform this transfer. Our experiments show that while high lexical overlap between languages improves transfer, M-Bert is also able to transfer between languages written in different scripts\u2014thus having zero lexical overlap\u2014indicating that it captures multilingual representations. We further show that transfer works best for typologically similar languages, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order.", "Table TABREF20 shows macro-averaged pos accuracies for transfer between languages grouped according to two typological features: subject/object/verb order, and adjective/noun order BIBREF11 . The results reported include only zero-shot transfer, i.e. they do not include cases training and testing on the same language. We can see that performance is best when transferring between languages that share word order features, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order."]}
{"question_id": "8e9561541f2e928eb239860c2455a254b5aceaeb", "predicted_answer": "The language pairs affected are:\n\n1. English and Japanese\n2. English and Bulgarian", "predicted_evidence": ["Code-switching (CS)\u2014the mixing of multiple languages within a single utterance\u2014and transliteration\u2014writing that is not in the language's standard script\u2014present unique test cases for M-Bert, which is pre-trained on monolingual, standard-script corpora. Generalizing to code-switching is similar to other cross-lingual transfer scenarios, but would benefit to an even larger degree from a shared multilingual representation. Likewise, generalizing to transliterated text is similar to other cross-script transfer experiments, but has the additional caveat that M-Bert was not pre-trained on text that looks like the target.", "In this work, we showed that M-Bert's robust, often surprising, ability to generalize cross-lingually is underpinned by a multilingual representation, without being explicitly trained for it. The model handles transfer across scripts and to code-switching fairly well, but effective transfer to typologically divergent and transliterated targets will likely require the model to incorporate an explicit multilingual training objective, such as that used by BIBREF15 or BIBREF16 .", "However, cross-script transfer is less accurate for other pairs, such as English and Japanese, indicating that M-Bert's multilingual representation is not able to generalize equally well in all cases. A possible explanation for this, as we will see in section SECREF18 , is typological similarity. English and Japanese have a different order of subject, verb and object, while English and Bulgarian have the same, and M-Bert may be having trouble generalizing across different orderings.", "Because M-Bert uses a single, multilingual vocabulary, one form of cross-lingual transfer occurs when word pieces present during fine-tuning also appear in the evaluation languages. In this section, we present experiments probing M-Bert's dependence on this superficial form of generalization: How much does transferability depend on lexical overlap? And is transfer possible to languages written in different scripts (no overlap)?", "M-Bert's ability to transfer between languages that are written in different scripts, and thus have effectively zero lexical overlap, is surprising given that it was trained on separate monolingual corpora and not with a multilingual objective. To probe deeper into how the model is able to perform this generalization, Table TABREF14 shows a sample of pos results for transfer across scripts."]}
{"question_id": "50c1bf8b928069f3ffc7f0cb00aa056a163ef336", "predicted_answer": "The evaluation metrics used in this context are:\n\n1. Accuracy\n2. F1 score\n3. Nearest neighbor accuracy\n\nThese metrics are used to evaluate the performance of M-Bert and En-Bert in various experiments, including zero-shot transfer, entity recognition, and language transfer.", "predicted_evidence": ["We perform pos experiments using Universal Dependencies (UD) BIBREF7 data for 41 languages. We use the evaluation sets from BIBREF8 . Table TABREF7 shows M-Bert zero-shot results for four European languages. We see that M-Bert generalizes well across languages, achieving over INLINEFORM0 accuracy for all pairs.", "If M-Bert's ability to generalize were mostly due to vocabulary memorization, we would expect zero-shot performance on ner to be highly dependent on word piece overlap, since entities are often similar across languages. To measure this effect, we compute INLINEFORM0 and INLINEFORM1 , the sets of word pieces used in entities in the training and evaluation datasets, respectively, and define overlap as the fraction of common word pieces used in the entities: INLINEFORM2 .", "Because M-Bert uses a single, multilingual vocabulary, one form of cross-lingual transfer occurs when word pieces present during fine-tuning also appear in the evaluation languages. In this section, we present experiments probing M-Bert's dependence on this superficial form of generalization: How much does transferability depend on lexical overlap? And is transfer possible to languages written in different scripts (no overlap)?", "We sample 5000 pairs of sentences from WMT16 BIBREF14 and feed each sentence (separately) to M-Bert with no fine-tuning. We then extract the hidden feature activations at each layer for each of the sentences, and average the representations for the input tokens except [cls] and [sep], to get a vector for each sentence, at each layer INLINEFORM0 , INLINEFORM1 . For each pair of sentences, e.g. INLINEFORM2 , we compute the vector pointing from one to the other and average it over all pairs: INLINEFORM3 , where INLINEFORM4 is the number of pairs. Finally, we translate each sentence, INLINEFORM5 , by INLINEFORM6 , find the closest German sentence vector, and measure the fraction of times the nearest neighbour is the correct pair, which we call the \u201cnearest neighbor accuracy\u201d.", "Figure FIGREF9 plots ner F1 score versus entity overlap for zero-shot transfer between every language pair in an in-house dataset of 16 languages, for both M-Bert and En-Bert. We can see that performance using En-Bert depends directly on word piece overlap: the ability to transfer deteriorates as word piece overlap diminishes, and F1 scores are near zero for languages written in different scripts. M-Bert's performance, on the other hand, is flat for a wide range of overlaps, and even for language pairs with almost no lexical overlap, scores vary between INLINEFORM0 and INLINEFORM1 , showing that M-Bert's pretraining on multiple languages has enabled a representational capacity deeper than simple vocabulary memorization."]}
{"question_id": "2ddfb40a9e73f382a2eb641c8e22bbb80cef017b", "predicted_answer": "The datasets used are:\n\n1. CoNLL-2002 and CoNLL-2003 datasets, which contain Dutch, Spanish, English, and German languages.\n2. An in-house dataset with 16 languages, using the same CoNLL categories.\n3. Universal Dependencies (UD) data for 41 languages.\n4. Evaluation sets from a specific reference (BIBREF8).", "predicted_evidence": ["We perform ner experiments on two datasets: the publicly available CoNLL-2002 and -2003 sets, containing Dutch, Spanish, English, and German BIBREF5 , BIBREF6 ; and an in-house dataset with 16 languages, using the same CoNLL categories. Table TABREF4 shows M-Bert zero-shot performance on all language pairs in the CoNLL data.", "In Figure FIGREF27 , we plot the nearest neighbor accuracy for en-de (solid line). It achieves over INLINEFORM0 accuracy for all but the bottom layers, which seems to imply that the hidden representations, although separated in space, share a common subspace that represents useful linguistic information, in a language-agnostic way. Similar curves are obtained for en-ru, and ur-hi (in-house dataset), showing this works for multiple languages.", "We perform pos experiments using Universal Dependencies (UD) BIBREF7 data for 41 languages. We use the evaluation sets from BIBREF8 . Table TABREF7 shows M-Bert zero-shot results for four European languages. We see that M-Bert generalizes well across languages, achieving over INLINEFORM0 accuracy for all pairs.", "It is our hope that these kinds of probing experiments will help steer researchers toward the most promising lines of inquiry by encouraging them to focus on the places where current contextualized word representation approaches fall short.", "Figure FIGREF9 plots ner F1 score versus entity overlap for zero-shot transfer between every language pair in an in-house dataset of 16 languages, for both M-Bert and En-Bert. We can see that performance using En-Bert depends directly on word piece overlap: the ability to transfer deteriorates as word piece overlap diminishes, and F1 scores are near zero for languages written in different scripts. M-Bert's performance, on the other hand, is flat for a wide range of overlaps, and even for language pairs with almost no lexical overlap, scores vary between INLINEFORM0 and INLINEFORM1 , showing that M-Bert's pretraining on multiple languages has enabled a representational capacity deeper than simple vocabulary memorization."]}
{"question_id": "65b39676db60f914f29f74b7c1264422ee42ad5c", "predicted_answer": "The other methods they compare to are:\n\n1. Calculating sentiment ratio using moving average (BIBREF3)\n2. Naive Bayesian models (BIBREF2)\n\nThese methods are from recent studies that analyzed sentiment on tweets related to U.S. Presidential Elections.", "predicted_evidence": ["We explain our data preparation methods in the next section. It is followed by our research methodology in Section III. We present our results in Section IV, which is followed by discussion and conclusion in Section V and VI.", "The models show good accuracy and INLINEFORM0 score (Table III). It shows that the model can predict the test data almost perfectly (95.8%) with slightly better result on positive sentiment than negative ones, which can be seen by the larger value of INLINEFORM1 than INLINEFORM2 .", "Our models' accuracy is tested using 10-fold cross validation. Model validation is done using scikit-learn library. The accuracy is calculated by checking the confusion matrix BIBREF12 , BIBREF13 and its INLINEFORM0 score BIBREF14 .", "Some of the most recent studies are BIBREF3 , BIBREF2 , BIBREF1 , BIBREF10 . Below we discuss these three recent studies and explain how our study relates to theirs. The first study is done by BIBREF3 , which analyzed the sentiment on 2008 U.S. Presidential Candidates by calculating sentiment ratio using moving average. They counted the sentiment value for Obama and McCain based on number of the positive and negative words stated on each tweet. The tweets were gathered during 2008-2009, whereas the positive and negative words were acquired from OpinionFinder. They found that the comparison between sentiment on tweets and polls were complex since people might choose \"Obama\", \"McCain\", \"have not decided\", \"not going to vote\", or any independent candidate on the polls.", "The second study predicted the outcome of 2012 U.S. Presidential Election polls using Naive Bayesian models BIBREF2 . They collected over 32 million tweets from September 29 until November 16, 2012. They used Tweepy and set keywords for each candidate to collect the tweets, such as mitt romney, barack obama, us election. The collected tweets passed some preprocessing stages: (1) URL, mentions, hashtags, RT, and stop words removal; (2) tokenization; and (3) additional not_ for negation. They analyzed 10,000 randomly selected tweets which only contain a candidate name. The analysis results were compared to Huffington Post's polls and they found that Obama's popularity on Twitter represented the polls result. This research didn't use tweets with two or more candidate names since it requires more complex preprocessing methods."]}
{"question_id": "a2baa8e266318f23f43321c4b2b9cf467718c94a", "predicted_answer": "The preprocessing method introduced is a two-step process:\n\n1. Removing URLs and pictures from the tweets.\n2. Filtering out tweets that do not contain a candidate's name.\n\nThis preprocessing stage removes 41% of the data.", "predicted_evidence": ["We explain our data preparation methods in the next section. It is followed by our research methodology in Section III. We present our results in Section IV, which is followed by discussion and conclusion in Section V and VI.", "We preprocess the data by: (1) removing URLs and pictures, also (2) by filtering tweets which have candidates' name. Hashtags, mentions, and retweets are not removed in order to maintain the original meaning of a tweet. We only save tweets which have passed the two requirements such as in Table 1. The first example shows no change in the tweet's content, since there isn't any URLs or pictures, and it contains a candidate's name: Bernie Sanders. The second example shows a removed tweet, which doesn't contain any candidates' name. The preprocessing stage changes the third tweet's contents. It removes the URLs and still keeps the tweet because it contains \"Hillary Clinton\" and \"Donald Trump\". The preprocessing stage removes 41% of the data (Figure 2).", "where INLINEFORM0 and n equals the number of candidates. Po and Pre are the poll and prediction ranks associated with RealClearPolitics.com and the model respectively.", "The preprocessed tweets are labeled manually by 11 annotators who understand English. All annotators are given either grade as part of their coursework or souvenirs for their work. The given label consists of the intended candidate and the sentiment. The annotators interpret the tweet and decide whom the tweet relates to. If they think the tweets does not relate to particular candidate nor understand the content, they can choose \"not clear\" as the label. Otherwise, they can relate it to one candidate and label it as positive or negative. We divide the tweets and annotators into three groups (Table II). They label as many tweets as they can since January 24 until April 16, 2016.", "Our models' accuracy is tested using 10-fold cross validation. Model validation is done using scikit-learn library. The accuracy is calculated by checking the confusion matrix BIBREF12 , BIBREF13 and its INLINEFORM0 score BIBREF14 ."]}
{"question_id": "97ff88c31dac9a3e8041a77fa7e34ce54eef5a76", "predicted_answer": "The performance of the model on the recommendation task is not explicitly stated in the given context. The text discusses the importance of evaluating the model on specific tasks, including recommendation, and mentions that the bAbI Movie Dialog dataset provides a benchmark for this task. However, it does not provide specific results or metrics on how well the model performs on the recommendation task.", "predicted_evidence": ["The paper is organized as follows: Section SECREF2 describes our model, while Section SECREF3 summarizes the evaluation of the model on the two above-mentioned tasks and the comparison with respect to state-of-the-art approaches. Section SECREF4 gives an overview of the literature of both QA and recommender systems, while final remarks and our long-term vision are reported in Section SECREF5 .", "In order to fulfill our long-term goal of building a conversational recommender system we need to assess the performance of our model on specific tasks involved in this scenario. A recent work which goes in this direction is reported in BIBREF2 , which presents the bAbI Movie Dialog dataset, composed by different tasks such as factoid QA, top-n recommendation and two more complex tasks, one which mixes QA and recommendation and one which contains turns of dialogs taken from Reddit. Having more specific tasks like QA and recommendation, and a more complex one which mixes both tasks gives us the possibility to evaluate our model on different levels of granularity. Moreover, the subdivision in turns of the more complex task provides a proper benchmark of the model capability to handle an effective dialog with the user.", "We think that it is necessary to consider models and techniques coming from research both in QA and recommender systems in order to pursue our desire to build an intelligent agent able to assist the user in decision-making tasks. We cannot fill the gap between the above-mentioned research areas if we do not consider the proposed models in a synergic way by virtue of the proposed analogy between the user profile (the set of user preferences) and the items to be recommended, as the question and the correct answers. The first work which goes in this direction is reported in BIBREF12 , which exploits movie descriptions to suggest appealing movies for a given user using an architecture tipically used for QA tasks. In fact, most of the research in the recommender systems field presents ad-hoc systems which exploit neighbourhood information like in Collaborative Filtering techniques BIBREF13 , item descriptions and metadata like in Content-based systems BIBREF14 .", "We think that it is necessary to consider models and techniques coming from research both in QA and recommender systems in order to pursue our desire to build an intelligent agent able to assist the user in decision-making tasks. We cannot fill the gap between the above-mentioned research areas if we do not consider the proposed models in a synergic way by virtue of the proposed analogy between the user profile (the set of user preferences) and the items to be recommended, as the question and the correct answers. The first work which goes in this direction is reported in BIBREF12 , which exploits movie descriptions to suggest appealing movies for a given user using an architecture tipically used for QA tasks. In fact, most of the research in the recommender systems field presents ad-hoc systems which exploit neighbourhood information like in Collaborative Filtering techniques BIBREF13 , item descriptions and metadata like in Content-based systems BIBREF14 . Recently presented neural network models BIBREF15 , BIBREF16 systems are able to learn latent representations in the network weights leveraging information coming from user preferences and item information.", "In a common recommendation system scenario, when a user enters a search query, it is assumed that his preferences are known. This is a stringent requirement because users cannot have a clear idea of their preferences at that point. Conversational recommender systems support users to fulfill their information needs through an interactive process. In this way, the system can provide a personalized experience dynamically adapting the user model with the possibility to enhance the generated predictions. Moreover, the system capability can be further enhanced giving explanations to the user about the given suggestions."]}
{"question_id": "272defe245d1c5c091d3bc51399181da2da5e5f0", "predicted_answer": "The knowledge base facts are retrieved from the knowledge base in triple form distributed with the dataset, using the Elasticsearch engine.", "predicted_evidence": ["Differently from BIBREF2 , the relevant knowledge base facts, taken from the knowledge base in triple form distributed with the dataset, are retrieved by INLINEFORM0 implemented by exploiting the Elasticsearch engine and not according to an hash lookup operator which applies a strict filtering procedure based on word frequency. In our work, INLINEFORM1 returns at most the top 30 relevant facts for INLINEFORM2 . Each entity in questions and documents is recognized using the list of entities provided with the dataset and considered as a single word of the dictionary INLINEFORM3 .", "In recent days, a lot of effort is devoted to create benchmarks for artificial agents to assess their ability to comprehend natural language and to reason over facts. One of the first attempt is the bAbI BIBREF17 dataset which is a synthetic dataset containing elementary tasks such as selecting an answer between one or more candidate facts, answering yes/no questions, counting operations over lists and sets and basic induction and deduction tasks. Another relevant benchmark is the one described in BIBREF18 , which provides CNN/Daily Mail datasets consisting of document-query-answer triples where an entity in the query is replaced by a placeholder and the system should identify the correct entity by reading and comprehending the given document. MCTest BIBREF19 requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Finally, SQuAD BIBREF20 consists in a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage.", "In recent days, a lot of effort is devoted to create benchmarks for artificial agents to assess their ability to comprehend natural language and to reason over facts. One of the first attempt is the bAbI BIBREF17 dataset which is a synthetic dataset containing elementary tasks such as selecting an answer between one or more candidate facts, answering yes/no questions, counting operations over lists and sets and basic induction and deduction tasks. Another relevant benchmark is the one described in BIBREF18 , which provides CNN/Daily Mail datasets consisting of document-query-answer triples where an entity in the query is replaced by a placeholder and the system should identify the correct entity by reading and comprehending the given document. MCTest BIBREF19 requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension.", "People have information needs of varying complexity, ranging from simple questions about common facts which can be found in encyclopedias, to more sophisticated cases in which they need to know what movie to watch during a romantic evening. These tasks can be solved by an intelligent agent able to answer questions formulated in a proper way, eventually considering user context and preferences.", "Following the experimental design, the results in Table TABREF10 are promising because our model outperforms all other systems on both tasks except for the QA SYSTEM on the QA task. Despite the advantage of the QA SYSTEM, it is a carefully designed system to handle knowledge base data in the form of triples, but our model can leverage data in the form of documents, without making any assumption about the form of the input data and can be applied to different kind of tasks. Additionally, the model MEMN2N is a neural network whose weights are pre-trained on the same dataset without using the long-term memory and the models JOINT SUPERVISED EMBEDDINGS and JOINT MEMN2N are models trained across all the tasks of the dataset in order to boost performance. Despite that, our model outperforms the three above-mentioned ones without using any supplementary trick."]}
{"question_id": "860257956b83099cccf1359e5d960289d7d50265", "predicted_answer": "Based on the given context, the neural network architecture used is a combination of a Gated Recurrent Unit (GRU) and a feedforward neural network with a hidden layer.\n\nHere's a breakdown of the architecture:\n\n1. GRU: The GRU output sizes are fixed to 128, and the number of inference steps is set to 3.\n2. Feedforward neural network: The prediction neural network has a hidden layer with a size of 4096, and it uses the ReLU activation function. The output of this layer is passed through a sigmoid activation function to treat the problem as a multi-label classification problem.\n\nSo, the overall architecture is a GRU-based encoder followed by a feedforward neural network with a hidden layer and a sigmoid output layer.", "predicted_evidence": ["where INLINEFORM0 is the hidden layer size, INLINEFORM1 and INLINEFORM2 are weight matrices, INLINEFORM3 , INLINEFORM4 are bias vectors, INLINEFORM5 is the sigmoid function and INLINEFORM6 is the ReLU activation function, which are applied pointwise to the given input vector.", "The model is trained using ADAM BIBREF7 optimizer (learning rate= INLINEFORM0 ) with a batch size of 128 for at most 100 epochs considering the best model until the HITS@k on the validation set decreases for 5 consecutive times. Dropout BIBREF8 is applied on INLINEFORM1 and on INLINEFORM2 with a rate of INLINEFORM3 and on the prediction neural network hidden layer with a rate of INLINEFORM4 . L2 regularization is applied to the embedding matrix INLINEFORM5 with a coefficient equal to INLINEFORM6 . We clipped the gradients if their norm is greater than 5 to stabilize learning BIBREF9 . Embedding size INLINEFORM7 is fixed to 50. All GRU output sizes are fixed to 128. The number of inference steps INLINEFORM8 is set to 3. The size of the prediction neural network hidden layer INLINEFORM9 is fixed to 4096. Biases INLINEFORM10 and INLINEFORM11 are initialized to zero vectors.", "L2 regularization is applied to the embedding matrix INLINEFORM5 with a coefficient equal to INLINEFORM6 . We clipped the gradients if their norm is greater than 5 to stabilize learning BIBREF9 . Embedding size INLINEFORM7 is fixed to 50. All GRU output sizes are fixed to 128. The number of inference steps INLINEFORM8 is set to 3. The size of the prediction neural network hidden layer INLINEFORM9 is fixed to 4096. Biases INLINEFORM10 and INLINEFORM11 are initialized to zero vectors. All weight matrices are initialized sampling from the normal distribution INLINEFORM12 . The ReLU activation function in the prediction neural network has been experimentally chosen comparing different activation functions such as sigmoid and tanh and taking the one which leads to the best performance. The model is implemented in TensorFlow BIBREF10 and executed on an NVIDIA TITAN X GPU.", "Dropout BIBREF8 is applied on INLINEFORM1 and on INLINEFORM2 with a rate of INLINEFORM3 and on the prediction neural network hidden layer with a rate of INLINEFORM4 . L2 regularization is applied to the embedding matrix INLINEFORM5 with a coefficient equal to INLINEFORM6 . We clipped the gradients if their norm is greater than 5 to stabilize learning BIBREF9 . Embedding size INLINEFORM7 is fixed to 50. All GRU output sizes are fixed to 128. The number of inference steps INLINEFORM8 is set to 3. The size of the prediction neural network hidden layer INLINEFORM9 is fixed to 4096. Biases INLINEFORM10 and INLINEFORM11 are initialized to zero vectors. All weight matrices are initialized sampling from the normal distribution INLINEFORM12 . The ReLU activation function in the prediction neural network has been experimentally chosen comparing different activation functions such as sigmoid and tanh and taking the one which leads to the best performance.", "The neural network weights are supposed to learn latent features which encode relationships between the most relevant words for the given query to predict the correct answers. The outer sigmoid activation function is used to treat the problem as a multi-label classification problem, so that each candidate answer is independent and not mutually exclusive. In this way the neural network generates a score which represents the probability that the candidate answer is correct. Moreover, differently from BIBREF3 , the candidate answer INLINEFORM0 can be any word, even those which not belong to the documents related to the query."]}
{"question_id": "1b1849ad0bdd79c6645572849fe7873ec7bd7e6d", "predicted_answer": "The author is arguing that Reddit and Twitter datasets, although prevalent, are not effective in addressing the problems of bias and canonical data in NLP. The reason is that these datasets, although large and easily collectible, are still biased and may not be representative of the diversity of language use. The author suggests that even user-generated data like Wikipedia has its own peculiarities and biases. Therefore, relying solely on these datasets may not be sufficient to address the challenges of bias and canonical data in NLP. Instead, the author proposes the use of \"fortuitous data\" and more unsupervised and weakly supervised approaches to gather annotated data quickly and semi-automatically.", "predicted_evidence": ["However, while it has advanced the field in so many ways, it has also introduced almost imperceptible biases: why is newswire considered more standard or more canonical than other text types? Journalists are trained writers who make fewer errors and adhere to a codified norm. But let us pause for a minute. If NLP had emerged only in the last decade, would newswire data still be our canon? Or would, say, Wikipedia be considered canonical? User-generated data is less standardized, but is highly available. If we take this thought further and start over today, maybe we would be in an `inverted' world: social media is standard and newswire with its `headlinese' is the `bad language' BIBREF0 . It is easy to collect large quantities of social media data. Whatever we consider canonical, all data comes with its biases, even more democratic media like Wikipedia carry their own peculiarities.", "In the remainder I will look at the NLP community's approach to face these challenges. I will outline one potential way to go about it, arguing for the use of fortuitous data, and end by returning to the question of domain.", "Our datasets are subspaces of this high-dimensional space. Depending on our task, instances are sentences, documents etc. In the following I will use POS tagging as a running example to analyze what's in a domain, by referring to the datasets with the typically used categories.", "What we need is quick ways to semi-automatically gather annotated data, and use more unsupervised and weakly supervised approaches.", "Annotating more data is a first and intuitive solution. However, it is na\u00efve, for several reasons."]}
{"question_id": "deb0c3524a3b3707e8b20abd27f54ad6188d6e4e", "predicted_answer": "The answer is no, they did not experiment with other languages. The text only mentions English-language tweets and does not mention any other languages.", "predicted_evidence": ["There has been several works on generating embeddings for words, most famously Word2Vec by Mikolov et al. BIBREF2 ). There has also been a number of different works that use encoder-decoder models based on long short-term memory (LSTM) BIBREF3 , and gated recurrent neural networks (GRU) BIBREF4 . These methods have been used mostly in the context of machine translation. The encoder maps the sentence from the source language to a vector representation, while the decoder conditions on this encoded vector for translating it to the target language. Perhaps the work most related to ours is the work of Le and Mikolov le2014distributed, where they extended the Word2Vec model to generate representations for sentences (called ParagraphVec). However, these models all function at the word level, making them ill-suited to the extremely noisy and idiosyncratic nature of tweets.", "We adapted this model, which employs temporal convolution and pooling operations, for tweets. The character set includes the English alphabets, numbers, special characters and unknown character. There are 70 characters in total, given below:", "There has been several works on generating embeddings for words, most famously Word2Vec by Mikolov et al. BIBREF2 ). There has also been a number of different works that use encoder-decoder models based on long short-term memory (LSTM) BIBREF3 , and gated recurrent neural networks (GRU) BIBREF4 . These methods have been used mostly in the context of machine translation. The encoder maps the sentence from the source language to a vector representation, while the decoder conditions on this encoded vector for translating it to the target language. Perhaps the work most related to ours is the work of Le and Mikolov le2014distributed, where they extended the Word2Vec model to generate representations for sentences (called ParagraphVec). However, these models all function at the word level, making them ill-suited to the extremely noisy and idiosyncratic nature of tweets. Our character-level model, on the other hand, can better deal with the noise and idiosyncrasies in tweets.", "BIBREF2 ). There has also been a number of different works that use encoder-decoder models based on long short-term memory (LSTM) BIBREF3 , and gated recurrent neural networks (GRU) BIBREF4 . These methods have been used mostly in the context of machine translation. The encoder maps the sentence from the source language to a vector representation, while the decoder conditions on this encoded vector for translating it to the target language. Perhaps the work most related to ours is the work of Le and Mikolov le2014distributed, where they extended the Word2Vec model to generate representations for sentences (called ParagraphVec). However, these models all function at the word level, making them ill-suited to the extremely noisy and idiosyncratic nature of tweets. Our character-level model, on the other hand, can better deal with the noise and idiosyncrasies in tweets. We plan to make our model and the data used to train it publicly available to be used by other researchers that work with tweets.", "We trained the CNN-LSTM encoder-decoder model on 3 million randomly selected English-language tweets populated using data augmentation techniques, which are useful for controlling generalization error for deep learning models. Data augmentation, in our context, refers to replicating tweet and replacing some of the words in the replicated tweets with their synonyms. These synonyms are obtained from WordNet BIBREF8 which contains words grouped together on the basis of their meanings. This involves selection of replaceable words (example of non-replaceable words are stopwords, user names, hash tags, etc) from the tweet and the number of words INLINEFORM0 to be replaced. The probability of the number, INLINEFORM1 , is given by a geometric distribution with parameter INLINEFORM2 in which INLINEFORM3 . Words generally have several synonyms, thus the synonym index INLINEFORM4 , of a given word is also determined by another geometric distribution in which INLINEFORM5 ."]}
{"question_id": "d7e43a3db8616a106304ac04ba729c1fee78761d", "predicted_answer": "The answer is not explicitly stated in the text, but we can infer that the system outperformed previous tasks by an unspecified amount, since it is stated that \"Our model outperforms all these models...\" without providing a specific margin of improvement.", "predicted_evidence": [".91", ".91", ".91", "In recent years, the micro-blogging site Twitter has become a major social media platform with hundreds of millions of users. The short (140 character limit), noisy and idiosyncratic nature of tweets make standard information retrieval and data mining methods ill-suited to Twitter. Consequently, there has been an ever growing body of IR and data mining literature focusing on Twitter. However, most of these works employ extensive feature engineering to create task-specific, hand-crafted features. This is time consuming and inefficient as new features need to be engineered for every task.", "As with the last task, we first extract the vector representation of all the tweets in the dataset using Tweet2Vec and use that to train a logistic regression classifier using the vector representations. Even though there are three classes, the SemEval task is a binary task. The performance is measured as the average F1-score of the positive and the negative class. Table 3 shows the performance of our model compared to the top four models in the SemEval 2015 competition (note that only the F1-score is reported by SemEval for this task) and ParagraphVec. Our model outperforms all these models, again without resorting to any feature engineering."]}
{"question_id": "0ba8f04c3fd64ee543b9b4c022310310bc5d3c23", "predicted_answer": "The previous state of the art for sentiment categorization, as mentioned in the text, are the top four models in the SemEval 2015 competition, as well as ParagraphVec, which is an extension of Word2Vec to handle sentences.", "predicted_evidence": ["The vector representations generated by our model are generic, and thus can be applied to tasks of different nature. We evaluated our model using two different SemEval 2015 tasks: Twitter semantic relatedness, and sentiment classification. Simple, off-the-shelf logistic regression classifiers trained using the vector representations generated by our model outperformed the top-performing methods for both tasks, without the need for any extensive feature engineering. This was despite the fact that due to resource limitations, our Tweet2Vec model was trained on a relatively small set (3 million tweets). Also, our method outperformed ParagraphVec, which is an extension of Word2Vec to handle sentences. This is a small but noteworthy illustration of why our tweet embeddings are best-suited to deal with the noise and idiosyncrasies of tweets.", "We evaluated our model using two classification tasks: Tweet semantic relatedness and Tweet sentiment classification.", "The second evaluation is based on the SemEval 2015-Task 10B: Twitter Message Polarity Classification BIBREF12 . Given a tweet, the task is to classify it as either positive, negative or neutral in sentiment. The size of the training and test sets were 9,520 tweets and 2,380 tweets respectively ( INLINEFORM0 positive, INLINEFORM1 negative, and INLINEFORM2 neutral).", "In this paper, we present Tweet2Vec, a method for generating general-purpose vector representation of tweets that can be used for any classification task. Tweet2Vec removes the need for expansive feature engineering and can be used to train any standard off-the-shelf classifier (e.g., logistic regression, svm, etc). Tweet2Vec uses a CNN-LSTM encoder-decoder model that operates at the character level to learn and generate vector representation of tweets. Our method is especially useful for natural language processing tasks on Twitter where it is particularly difficult to engineer features, such as speech-act classification and stance detection (as shown in our previous works on these topics BIBREF0 , BIBREF1 ).", "As with the last task, we first extract the vector representation of all the tweets in the dataset using Tweet2Vec and use that to train a logistic regression classifier using the vector representations. Even though there are three classes, the SemEval task is a binary task. The performance is measured as the average F1-score of the positive and the negative class. Table 3 shows the performance of our model compared to the top four models in the SemEval 2015 competition (note that only the F1-score is reported by SemEval for this task) and ParagraphVec. Our model outperforms all these models, again without resorting to any feature engineering."]}
{"question_id": "b7d02f12baab5db46ea9403d8932e1cd1b022f79", "predicted_answer": "The previous state of the art for tweet semantic similarity is not explicitly mentioned in the given context. However, it can be inferred that the top-performing methods for the SemEval 2015 tasks, specifically Task 1: Paraphrase and Semantic Similarity in Twitter, are the previous state of the art.\n\nThe context mentions that the authors' model, Tweet2Vec, outperformed the top-performing methods for both tweet semantic relatedness and sentiment classification tasks, without the need for extensive feature engineering. This implies that the authors' model achieved better results than the previous state-of-the-art methods for these tasks.\n\nAdditionally, the context mentions that the authors' method outperformed ParagraphVec, which is an extension of Word2Vec to handle sentences. This suggests that ParagraphVec was one of the previous state-of-the-art methods for tweet semantic similarity.", "predicted_evidence": ["The first evaluation is based on the SemEval 2015-Task 1: Paraphrase and Semantic Similarity in Twitter BIBREF10 . Given a pair of tweets, the goal is to predict their semantic equivalence (i.e., if they express the same or very similar meaning), through a binary yes/no judgement. The dataset provided for this task contains 18K tweet pairs for training and 1K pairs for testing, with INLINEFORM0 of these pairs being paraphrases, and INLINEFORM1 non-paraphrases. We first extract the vector representation of all the tweets in the dataset using our Tweet2Vec model. We use two features to represent a tweet pair. Given two tweet vectors INLINEFORM2 and INLINEFORM3 , we compute their element-wise product INLINEFORM4 and their absolute difference INLINEFORM5 and concatenate them together (Similar to BIBREF11 ). We then train a logistic regression model on these features using the dataset.", "The first evaluation is based on the SemEval 2015-Task 1: Paraphrase and Semantic Similarity in Twitter BIBREF10 . Given a pair of tweets, the goal is to predict their semantic equivalence (i.e., if they express the same or very similar meaning), through a binary yes/no judgement. The dataset provided for this task contains 18K tweet pairs for training and 1K pairs for testing, with INLINEFORM0 of these pairs being paraphrases, and INLINEFORM1 non-paraphrases. We first extract the vector representation of all the tweets in the dataset using our Tweet2Vec model. We use two features to represent a tweet pair. Given two tweet vectors INLINEFORM2 and INLINEFORM3 , we compute their element-wise product INLINEFORM4 and their absolute difference INLINEFORM5 and concatenate them together (Similar to BIBREF11 ). We then train a logistic regression model on these features using the dataset. Cross-validation is used for tuning the threshold for classification.", "We evaluated our model using two classification tasks: Tweet semantic relatedness and Tweet sentiment classification.", "The vector representations generated by our model are generic, and thus can be applied to tasks of different nature. We evaluated our model using two different SemEval 2015 tasks: Twitter semantic relatedness, and sentiment classification. Simple, off-the-shelf logistic regression classifiers trained using the vector representations generated by our model outperformed the top-performing methods for both tasks, without the need for any extensive feature engineering. This was despite the fact that due to resource limitations, our Tweet2Vec model was trained on a relatively small set (3 million tweets). Also, our method outperformed ParagraphVec, which is an extension of Word2Vec to handle sentences. This is a small but noteworthy illustration of why our tweet embeddings are best-suited to deal with the noise and idiosyncrasies of tweets.", "These synonyms are obtained from WordNet BIBREF8 which contains words grouped together on the basis of their meanings. This involves selection of replaceable words (example of non-replaceable words are stopwords, user names, hash tags, etc) from the tweet and the number of words INLINEFORM0 to be replaced. The probability of the number, INLINEFORM1 , is given by a geometric distribution with parameter INLINEFORM2 in which INLINEFORM3 . Words generally have several synonyms, thus the synonym index INLINEFORM4 , of a given word is also determined by another geometric distribution in which INLINEFORM5 . In our encoder-decoder model, we decode the encoded representation to the actual tweet or a synonym-replaced version of the tweet from the augmented data. We used INLINEFORM6 , INLINEFORM7 for our training. We also make sure that the POS tags of the replaced words are not completely different from the actual words. For regularization, we apply a dropout mechanism after the penultimate layer."]}
{"question_id": "ff2b58c90784eda6dddd8a92028e6432442c1093", "predicted_answer": "The text does not provide a specific numerical value for how much the method outperforms the baselines. It only states that the average improvement in accuracy across OMICS tasks between SEM-HMM and each baseline is statistically significant at a .01 level, indicating that the method outperforms the baselines, but the exact magnitude of the improvement is not specified.", "predicted_evidence": ["The average accuracy across the 84 domains for each method is found in Table 1. On average our method significantly out-performed all the baselines, with the average improvement in accuracy across OMICS tasks between SEM-HMM and each baseline being statistically significant at a .01 level across all pairs and on sizes of INLINEFORM0 and INLINEFORM1 using one-sided paired t-tests. For INLINEFORM2 improvement was not statistically greater than zero. We see that the results improve with batch size INLINEFORM3 until INLINEFORM4 for SEM-HMM and BMM+EM, but they decrease with batch size for BMM without EM. Both of the methods which use EM depend on statistics to be robust and hence need a larger INLINEFORM5 value to be accurate. However for BMM, a smaller INLINEFORM6 size means it reconciles a couple of documents with the current model in each iteration which ultimately helps guide the structure search.", "The average accuracy across the 84 domains for each method is found in Table 1. On average our method significantly out-performed all the baselines, with the average improvement in accuracy across OMICS tasks between SEM-HMM and each baseline being statistically significant at a .01 level across all pairs and on sizes of INLINEFORM0 and INLINEFORM1 using one-sided paired t-tests. For INLINEFORM2 improvement was not statistically greater than zero. We see that the results improve with batch size INLINEFORM3 until INLINEFORM4 for SEM-HMM and BMM+EM, but they decrease with batch size for BMM without EM. Both of the methods which use EM depend on statistics to be robust and hence need a larger INLINEFORM5 value to be accurate. However for BMM, a smaller INLINEFORM6 size means it reconciles a couple of documents with the current model in each iteration which ultimately helps guide the structure search. The accuracy for \u201cSEM-HMM Approx.\u201d is close to the exact version at each batch level, while only taking half the time on average.", "We now describe how we score the structures produced by our algorithm to select the best structure. We employ a Bayesian scoring function, which is the posterior probability of the model given the data, denoted INLINEFORM0 . The score is decomposed via Bayes Rule (i.e., INLINEFORM1 ), and the denominator is omitted since it is invariant with regards to the model.", "We now present our experimental results on SEM-HMM and SEM-HMM-Approx. The evaluation task is to predict missing events from an observed sequence of events. For comparison, four baselines were also evaluated. The \u201cFrequency\u201d baseline predicts the most frequent event in the training set that is not found in the observed test sequence. The \u201cConditional\u201d baseline predicts the next event based on what most frequently follows the prior event. A third baseline, referred to as \u201cBMM,\u201d is a version of our algorithm that does not use EM for parameter estimation and instead only incrementally updates the parameters starting from the raw document counts. Further, it learns a standard HMM, that is, with no INLINEFORM0 transitions. This is very similar to the Bayesian Model Merging approach for HMMs BIBREF9 . The fourth baseline is the same as above, but uses our EM algorithm for parameter estimation without INLINEFORM1 transitions. It is referred to as \u201cBMM + EM.\u201d", "The 84 domains with at least 50 narratives and 3 event types were used for evaluation. For each domain, forty percent of the narratives were withheld for testing, each with one randomly-chosen event omitted. The model was evaluated on the proportion of correctly predicted events given the remaining sequence. On average each domain has 21.7 event types with a standard deviation of 4.6. Further, the average narrative length across domains is 3.8 with standard deviation of 1.7. This implies that only a frcation of the event types are present in any given narrative. There is a high degree of omission of events and many different ways of accomplishing each task. Hence, the prediction task is reasonably difficult, as evidenced by the simple baselines. Neither the frequency of events nor simple temporal structure is enough to accurately fill in the gaps which indicates that most sophisticated modeling such as SEM-HMM is needed."]}
{"question_id": "5e4eac0b0a73d465d74568c21819acaec557b700", "predicted_answer": "The baselines used are:\n\n1. \"Frequency\" baseline: predicts the most frequent event in the training set that is not found in the observed test sequence.\n2. \"Conditional\" baseline: predicts the next event based on what most frequently follows the prior event.\n3. \"BMM\" baseline: a version of the algorithm that does not use EM for parameter estimation and instead only incrementally updates the parameters starting from the raw document counts, learning a standard HMM with no INLINEFORM0 transitions.\n4. \"BMM + EM\" baseline: the same as BMM, but uses the EM algorithm for parameter estimation without INLINEFORM1 transitions.", "predicted_evidence": ["The average accuracy across the 84 domains for each method is found in Table 1. On average our method significantly out-performed all the baselines, with the average improvement in accuracy across OMICS tasks between SEM-HMM and each baseline being statistically significant at a .01 level across all pairs and on sizes of INLINEFORM0 and INLINEFORM1 using one-sided paired t-tests. For INLINEFORM2 improvement was not statistically greater than zero. We see that the results improve with batch size INLINEFORM3 until INLINEFORM4 for SEM-HMM and BMM+EM, but they decrease with batch size for BMM without EM. Both of the methods which use EM depend on statistics to be robust and hence need a larger INLINEFORM5 value to be accurate. However for BMM, a smaller INLINEFORM6 size means it reconciles a couple of documents with the current model in each iteration which ultimately helps guide the structure search.", "The average accuracy across the 84 domains for each method is found in Table 1. On average our method significantly out-performed all the baselines, with the average improvement in accuracy across OMICS tasks between SEM-HMM and each baseline being statistically significant at a .01 level across all pairs and on sizes of INLINEFORM0 and INLINEFORM1 using one-sided paired t-tests. For INLINEFORM2 improvement was not statistically greater than zero. We see that the results improve with batch size INLINEFORM3 until INLINEFORM4 for SEM-HMM and BMM+EM, but they decrease with batch size for BMM without EM. Both of the methods which use EM depend on statistics to be robust and hence need a larger INLINEFORM5 value to be accurate. However for BMM, a smaller INLINEFORM6 size means it reconciles a couple of documents with the current model in each iteration which ultimately helps guide the structure search. The accuracy for \u201cSEM-HMM Approx.\u201d is close to the exact version at each batch level, while only taking half the time on average.", "The semantic constraints are learned from the event sequences for use in the model prior. The constraints take the simple form \u201c INLINEFORM0 never follows INLINEFORM1 .\u201d They are learned by generating all possible such rules using pairwise permutations of event types, and evaluating them on the training data. In particular, the number of times each rule is violated is counted and a INLINEFORM2 -test is performed to determine if the violation rate is lower than a predetermined error rate. Those rules that pass the hypothesis test with a threshold of INLINEFORM3 are included. When evaluating a model, these contraints are considered violated if the model could generate a sequence of observations that violates the constraint.", "We now present our experimental results on SEM-HMM and SEM-HMM-Approx. The evaluation task is to predict missing events from an observed sequence of events. For comparison, four baselines were also evaluated. The \u201cFrequency\u201d baseline predicts the most frequent event in the training set that is not found in the observed test sequence. The \u201cConditional\u201d baseline predicts the next event based on what most frequently follows the prior event. A third baseline, referred to as \u201cBMM,\u201d is a version of our algorithm that does not use EM for parameter estimation and instead only incrementally updates the parameters starting from the raw document counts. Further, it learns a standard HMM, that is, with no INLINEFORM0 transitions. This is very similar to the Bayesian Model Merging approach for HMMs BIBREF9 . The fourth baseline is the same as above, but uses our EM algorithm for parameter estimation without INLINEFORM1 transitions. It is referred to as \u201cBMM + EM.\u201d", "We now describe how we score the structures produced by our algorithm to select the best structure. We employ a Bayesian scoring function, which is the posterior probability of the model given the data, denoted INLINEFORM0 . The score is decomposed via Bayes Rule (i.e., INLINEFORM1 ), and the denominator is omitted since it is invariant with regards to the model."]}
{"question_id": "bc6ad5964f444cf414b661a4b942dafb7640c564", "predicted_answer": "The datasets used for evaluation are not explicitly named, but they consist of 84 domains with at least 50 narratives and 3 event types each. Each domain has an average of 21.7 event types and an average narrative length of 3.8 sentences.", "predicted_evidence": ["The average accuracy across the 84 domains for each method is found in Table 1. On average our method significantly out-performed all the baselines, with the average improvement in accuracy across OMICS tasks between SEM-HMM and each baseline being statistically significant at a .01 level across all pairs and on sizes of INLINEFORM0 and INLINEFORM1 using one-sided paired t-tests. For INLINEFORM2 improvement was not statistically greater than zero. We see that the results improve with batch size INLINEFORM3 until INLINEFORM4 for SEM-HMM and BMM+EM, but they decrease with batch size for BMM without EM. Both of the methods which use EM depend on statistics to be robust and hence need a larger INLINEFORM5 value to be accurate. However for BMM, a smaller INLINEFORM6 size means it reconciles a couple of documents with the current model in each iteration which ultimately helps guide the structure search.", "At the top level, the algorithm is input a set of documents INLINEFORM0 , where each document is a sequence of natural language sentences that describes the same stereotypical activity. The output of the algorithm is a Left-to-Right HMM that represents that activity.", "We now describe how we score the structures produced by our algorithm to select the best structure. We employ a Bayesian scoring function, which is the posterior probability of the model given the data, denoted INLINEFORM0 . The score is decomposed via Bayes Rule (i.e., INLINEFORM1 ), and the denominator is omitted since it is invariant with regards to the model.", "We would like to thank Nate Chambers, Frank Ferraro, and Ben Van Durme for their helpful comments, criticism, and feedback. Also we would like to thank the SCALE 2013 workshop. This work was supported by the DARPA and AFRL under contract No. FA8750-13-2-0033. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the DARPA, the AFRL, or the US government.", "The 84 domains with at least 50 narratives and 3 event types were used for evaluation. For each domain, forty percent of the narratives were withheld for testing, each with one randomly-chosen event omitted. The model was evaluated on the proportion of correctly predicted events given the remaining sequence. On average each domain has 21.7 event types with a standard deviation of 4.6. Further, the average narrative length across domains is 3.8 with standard deviation of 1.7. This implies that only a frcation of the event types are present in any given narrative. There is a high degree of omission of events and many different ways of accomplishing each task. Hence, the prediction task is reasonably difficult, as evidenced by the simple baselines. Neither the frequency of events nor simple temporal structure is enough to accurately fill in the gaps which indicates that most sophisticated modeling such as SEM-HMM is needed."]}
{"question_id": "380e71848d4b0d1e983d504b1249119612f00bcb", "predicted_answer": "The deep learning methods mentioned in the paper are:\n\n1. Deep Neural Networks (DNNs)\n2. Convolutional Neural Networks (CNNs)\n3. Long Short-Term Memory Networks (LSTMs)\n4. FastText", "predicted_evidence": ["In this paper, we experiment with multiple classifiers such as Logistic Regression, Random Forest, SVMs, Gradient Boosted Decision Trees (GBDTs) and Deep Neural Networks(DNNs). The feature spaces for these classifiers are in turn defined by task-specific embeddings learned using three deep learning architectures: FastText, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs). As baselines, we compare with feature spaces comprising of char n-grams BIBREF0 , TF-IDF vectors, and Bag of Words vectors (BoWV).", "We use `adam' for CNN and LSTM, and `RMS-Prop' for FastText as our optimizer. We perform training in batches of size 128 for CNN & LSTM and 64 for FastText. More details on the experimental setup can be found from our publicly available source code.", "To verify the task-specific nature of the embeddings, we show top few similar words for a few chosen words in Table TABREF7 using the original GloVe embeddings and also embeddings learned using DNNs. The similar words obtained using deep neural network learned embeddings clearly show the \u201chatred\u201d towards the target words, which is in general not visible at all in similar words obtained using GloVe.", "As the table shows, our proposed methods in part B are significantly better than the baseline methods in part A. Among the baseline methods, the word TF-IDF method is better than the character n-gram method. Among part B methods, CNN performed better than LSTM which was better than FastText. Surprisingly, initialization with random embeddings is slightly better than initialization with GloVe embeddings when used along with GBDT. Finally, part C methods are better than part B methods. The best method is \u201cLSTM + Random Embedding + GBDT\u201d where tweet embeddings were initialized to random vectors, LSTM was trained using back-propagation, and then learned embeddings were used to train a GBDT classifier. Combinations of CNN, LSTM, FastText embeddings as features for GBDTs did not lead to better results. Also note that the standard deviation for all these methods varies from 0.01 to 0.025.", "All of these networks are trained (fine-tuned) using labeled data with back-propagation. Once the network is learned, a new tweet is tested against the network which classifies it as racist, sexist or neither. Besides learning the network weights, these methods also learn task-specific word embeddings tuned towards the hate speech labels. Therefore, for each of the networks, we also experiment by using these embeddings as features and various other classifiers like SVMs and GBDTs as the learning method."]}
{"question_id": "21c89ee0281f093b209533453196306b9699b552", "predicted_answer": "The baseline methods mentioned in the text are:\n\n1. Character n-gram method\n2. Word TF-IDF method\n3. Bag of Words vectors (BoWV)\n\nThese methods are used as a baseline to compare the performance of the proposed approach.", "predicted_evidence": ["We first discuss a few baseline methods and then discuss the proposed approach. In all these methods, an embedding is generated for a tweet and is used as its feature representation with a classifier.", "As the table shows, our proposed methods in part B are significantly better than the baseline methods in part A. Among the baseline methods, the word TF-IDF method is better than the character n-gram method. Among part B methods, CNN performed better than LSTM which was better than FastText. Surprisingly, initialization with random embeddings is slightly better than initialization with GloVe embeddings when used along with GBDT. Finally, part C methods are better than part B methods. The best method is \u201cLSTM + Random Embedding + GBDT\u201d where tweet embeddings were initialized to random vectors, LSTM was trained using back-propagation, and then learned embeddings were used to train a GBDT classifier. Combinations of CNN, LSTM, FastText embeddings as features for GBDTs did not lead to better results. Also note that the standard deviation for all these methods varies from 0.01 to 0.025.", "In this paper, we experiment with multiple classifiers such as Logistic Regression, Random Forest, SVMs, Gradient Boosted Decision Trees (GBDTs) and Deep Neural Networks(DNNs). The feature spaces for these classifiers are in turn defined by task-specific embeddings learned using three deep learning architectures: FastText, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs). As baselines, we compare with feature spaces comprising of char n-grams BIBREF0 , TF-IDF vectors, and Bag of Words vectors (BoWV).", "We experimented with a dataset of 16K annotated tweets made available by the authors of BIBREF0 . Of the 16K tweets, 3383 are labeled as sexist, 1972 as racist, and the remaining are marked as neither sexist nor racist. For the embedding based methods, we used the GloVe BIBREF5 pre-trained word embeddings. GloVe embeddings have been trained on a large tweet corpus (2B tweets, 27B tokens, 1.2M vocab, uncased). We experimented with multiple word embedding sizes for our task. We observed similar results with different sizes, and hence due to lack of space we report results using embedding size=200. We performed 10-Fold Cross Validation and calculated weighted macro precision, recall and F1-scores.", "We use `adam' for CNN and LSTM, and `RMS-Prop' for FastText as our optimizer. We perform training in batches of size 128 for CNN & LSTM and 64 for FastText. More details on the experimental setup can be found from our publicly available source code."]}
{"question_id": "5096aaea2d0f4bea4c12e14f4f7735e1aea1bfa6", "predicted_answer": "The architectures they experiment with are:\n\n1. FastText\n2. Convolutional Neural Networks (CNNs)\n3. Long Short-Term Memory Networks (LSTMs)\n4. Deep Neural Networks (DNNs)\n\nAdditionally, they also experiment with traditional machine learning models such as:\n\n1. Logistic Regression\n2. Random Forest\n3. SVMs\n4. Gradient Boosted Decision Trees (GBDTs)", "predicted_evidence": ["In this paper, we experiment with multiple classifiers such as Logistic Regression, Random Forest, SVMs, Gradient Boosted Decision Trees (GBDTs) and Deep Neural Networks(DNNs). The feature spaces for these classifiers are in turn defined by task-specific embeddings learned using three deep learning architectures: FastText, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs). As baselines, we compare with feature spaces comprising of char n-grams BIBREF0 , TF-IDF vectors, and Bag of Words vectors (BoWV).", "All of these networks are trained (fine-tuned) using labeled data with back-propagation. Once the network is learned, a new tweet is tested against the network which classifies it as racist, sexist or neither. Besides learning the network weights, these methods also learn task-specific word embeddings tuned towards the hate speech labels. Therefore, for each of the networks, we also experiment by using these embeddings as features and various other classifiers like SVMs and GBDTs as the learning method.", "Proposed Methods: We investigate three neural network architectures for the task, described as follows. For each of the three methods, we initialize the word embeddings with either random embeddings or GloVe embeddings. (1) CNN: Inspired by Kim et. al BIBREF3 's work on using CNNs for sentiment classification, we leverage CNNs for hate speech detection. We use the same settings for the CNN as described in BIBREF3 . (2) LSTM: Unlike feed-forward neural networks, recurrent neural networks like LSTMs can use their internal memory to process arbitrary sequences of inputs. Hence, we use LSTMs to capture long range dependencies in tweets, which may play a role in hate speech detection.", "We use `adam' for CNN and LSTM, and `RMS-Prop' for FastText as our optimizer. We perform training in batches of size 128 for CNN & LSTM and 64 for FastText. More details on the experimental setup can be found from our publicly available source code.", "To verify the task-specific nature of the embeddings, we show top few similar words for a few chosen words in Table TABREF7 using the original GloVe embeddings and also embeddings learned using DNNs. The similar words obtained using deep neural network learned embeddings clearly show the \u201chatred\u201d towards the target words, which is in general not visible at all in similar words obtained using GloVe."]}
{"question_id": "452e2d7d7d9e1bb4914903479cd7caff9f6fae42", "predicted_answer": "Yes, pretrained embeddings are used in this study. Specifically, the authors use GloVe pre-trained word embeddings, which were trained on a large tweet corpus (2B tweets, 27B tokens, 1.2M vocab, uncased).", "predicted_evidence": ["As the table shows, our proposed methods in part B are significantly better than the baseline methods in part A. Among the baseline methods, the word TF-IDF method is better than the character n-gram method. Among part B methods, CNN performed better than LSTM which was better than FastText. Surprisingly, initialization with random embeddings is slightly better than initialization with GloVe embeddings when used along with GBDT. Finally, part C methods are better than part B methods. The best method is \u201cLSTM + Random Embedding + GBDT\u201d where tweet embeddings were initialized to random vectors, LSTM was trained using back-propagation, and then learned embeddings were used to train a GBDT classifier. Combinations of CNN, LSTM, FastText embeddings as features for GBDTs did not lead to better results. Also note that the standard deviation for all these methods varies from 0.01 to 0.025.", "To verify the task-specific nature of the embeddings, we show top few similar words for a few chosen words in Table TABREF7 using the original GloVe embeddings and also embeddings learned using DNNs. The similar words obtained using deep neural network learned embeddings clearly show the \u201chatred\u201d towards the target words, which is in general not visible at all in similar words obtained using GloVe.", "In this paper, we experiment with multiple classifiers such as Logistic Regression, Random Forest, SVMs, Gradient Boosted Decision Trees (GBDTs) and Deep Neural Networks(DNNs). The feature spaces for these classifiers are in turn defined by task-specific embeddings learned using three deep learning architectures: FastText, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs). As baselines, we compare with feature spaces comprising of char n-grams BIBREF0 , TF-IDF vectors, and Bag of Words vectors (BoWV).", "We experimented with a dataset of 16K annotated tweets made available by the authors of BIBREF0 . Of the 16K tweets, 3383 are labeled as sexist, 1972 as racist, and the remaining are marked as neither sexist nor racist. For the embedding based methods, we used the GloVe BIBREF5 pre-trained word embeddings. GloVe embeddings have been trained on a large tweet corpus (2B tweets, 27B tokens, 1.2M vocab, uncased). We experimented with multiple word embedding sizes for our task. We observed similar results with different sizes, and hence due to lack of space we report results using embedding size=200. We performed 10-Fold Cross Validation and calculated weighted macro precision, recall and F1-scores.", "We first discuss a few baseline methods and then discuss the proposed approach. In all these methods, an embedding is generated for a tweet and is used as its feature representation with a classifier."]}
{"question_id": "cdb211be0340bb18ba5a9ee988e9df0e2ba8b793", "predicted_answer": "The paper does not explicitly discuss the limitations of considering only data from Twitter. While it provides a detailed description of the Twitter data set used and the methodology employed, it does not acknowledge or address potential limitations or biases associated with relying solely on Twitter data.", "predicted_evidence": ["Twitter is an online social networking platform. Users post 140-character messages, which appear in their followers' timelines. Because follower ties can be asymmetric, Twitter serves multiple purposes: celebrities share messages with millions of followers, while lower-degree users treat Twitter as a more intimate social network for mutual communication BIBREF13 . In this paper, we use a large-scale Twitter data set, acquired via an agreement between Microsoft and Twitter. This data set contains all public messages posted between June 2013 and June 2014 by several million users, augmented with social network and geolocation metadata. We excluded retweets, which are explicitly marked with metadata, and focused on messages that were posted in English from within the United States.", "Our main research goal is to test whether and how geographically distinctive linguistic markers spread through Twitter. With this goal in mind, our first question is whether the adoption of these markers can be viewed as a form of complex contagion. To answer this question, we computed the fraction of users who used one of the words listed in sec:data-language after being exposed to that word by one of their social network connections. Formally, we say that user $i$ exposed user $j$ to word $w$ at time $t$ if and only if the following conditions hold: $i$ used $w$ at time $t$ ; $j$ had not used $w$ before time $t$ ; the social network connection $j$0 was formed before time $j$1 .", "Following a probabilistic modeling approach, we treated our Twitter data set as a set of cascades of timestamped events, with one cascade for each of the geographically distinctive words described in sec:data-language. Each event in a word's cascade corresponds to a tweet containing that word. We modeled each cascade as a probabilistic process, and estimated the parameters of this process. By comparing nested models that make progressively finer distinctions between social network connections, we were able to quantitatively test our hypotheses.", "Our main research goal is to test whether and how geographically distinctive linguistic markers spread through Twitter. With this goal in mind, our first question is whether the adoption of these markers can be viewed as a form of complex contagion. To answer this question, we computed the fraction of users who used one of the words listed in sec:data-language after being exposed to that word by one of their social network connections. Formally, we say that user $i$ exposed user $j$ to word $w$ at time $t$ if and only if the following conditions hold: $i$ used $w$ at time $t$ ; $j$ had not used $w$ before time $t$ ; the social network connection $j$0 was formed before time $j$1 . We define the infection risk for word $j$2 to be the number of users who use word $j$3 after being exposed divided by the total number of users who were exposed.", "In this paper, we show that large-scale social media data can shed new light on how language changes propagate through social networks. We use a data set of Twitter users that contains all public messages for several million accounts, augmented with social network and geolocation metadata. This data set makes it possible to track, and potentially explain, every usage of a linguistic variable as it spreads through social media. Overall, we make the following contributions:"]}
{"question_id": "4cb2e80da73ae36de372190b4c1c490b72977ef8", "predicted_answer": "No, they did not represent tie strength only as the number of social ties in a network. They used the normalized mutual friends metric introduced by Adamic and Adar, which takes into account the embeddedness of ties, i.e., the number of mutual friends shared by two individuals. This metric provides a more nuanced measure of tie strength than simply counting the number of social ties.", "predicted_evidence": ["We quantify tie strength in terms of embeddedness. Specifically, we use the normalized mutual friends metric introduced by Adamic and Adar BIBREF39 :", "Features F3 and F4 did not improve the goodness of fit for less frequent words, such as ain, graffiti, and yeen, which occur fewer than $10^4$ times. Below this count threshold, there is not enough data to statistically distinguish between different types of social network connections. However, above this count threshold, adding in F3 (tie strength) yielded a statistically significant increase in goodness of fit for ard, asl, cookout, hella, jawn, mfs, and tfti. This finding provides evidence in favor of hypothesis H1\u2014that the linguistic influence exerted across densely embedded ties is greater than the linguistic influence exerted across other ties.", "The linguistic influence exerted across ties with a high embeddedness value $s_{i,j}$ will be greater than the linguistic influence exerted across other ties.", "Social networks are often characterized in terms of strong and weak ties BIBREF37 , BIBREF3 , with strong ties representing more important social relationships. Strong ties are often densely embedded, meaning that the nodes in question share many mutual friends; in contrast, weak ties often bridge disconnected communities. Bakshy et al. investigated the role of weak ties in information diffusion, through resharing of URLs on Facebook BIBREF38 . They found that URLs shared across strong ties are more likely to be reshared. However, they also found that weak ties play an important role, because users tend to have more weak ties than strong ties, and because weak ties are more likely to be a source of new information. In some respects, language change is similar to traditional information diffusion scenarios, such as resharing of URLs. But, in contrast, language connects with personal identity on a much deeper level than a typical URL. As a result, strong, deeply embedded ties may play a greater role in enforcing community norms.", "We chose a set of four binary features that would enable us to test our hypotheses about the roles of different types of social network connections:"]}
{"question_id": "a064337bafca8cf01e222950ea97ebc184c47bc0", "predicted_answer": "The sociolinguistic variables (phonetic spellings) analyzed in this study are not explicitly defined, but based on the context, it can be inferred that they refer to non-standard dialectal pronunciations that are represented through unconventional spellings, such as \"gonna\" for \"going to\" or \"wanna\" for \"want to\". These phonetic spellings are considered a type of linguistic variable that is associated with local identity and non-standard dialects.", "predicted_evidence": ["An open question in sociolinguistics is whether and how local covert prestige\u2014i.e., the positive social evaluation of non-standard dialects\u2014affects the adoption of new linguistic forms BIBREF6 . Speakers often explain their linguistic choices in terms of their relationship with their local identity BIBREF40 , but this may be a post-hoc rationalization made by people whose language is affected by factors beyond their control. Indeed, some sociolinguists have cast doubt on the role of \u201clocal games\u201d in affecting the direction of language change BIBREF41 .", "We note that this hypothesis is restricted in scope to geographically distinctive words. We do not consider the more general hypothesis that geographically local ties are more influential for all types of language change, such as change involving linguistic variables that are associated with gender or socioeconomic status.", "In sec:influence we found that phonetic spellings and abbreviations exhibit complex contagion, while lexical words do not. Here, however, we found no such systematic differences between the three linguistic classes. Although we hypothesize that lexical words propagate mainly outside of social media, we nonetheless see that when these words do propagate across Twitter, their adoption is modulated by tie strength, as is the case for phonetic spellings and abbreviations.", "fig:risk-by-exposure depicts the ratios between the words' observed infection risks and the words' infection risks under the null hypothesis, after exposures across one, two, and three or more distinct connections. We computed 95% confidence intervals across the words and across the permutations used in the shuffle test. For all three linguistic classes defined in sec:data-language, the risk ratio for even a single exposure is significantly greater than one, suggesting the existence of social influence. The risk ratio for a single exposure is nearly identical across the three classes. For phonetic spellings and abbreviations, the risk ratio grows with the number of exposures. This pattern suggests that words in these classes exhibit complex contagion\u2014i.e., multiple exposures increase the likelihood of adoption BIBREF35 . In contrast, the risk ratio for lexical words remains the same as the number of exposures increases, suggesting that these words spread by simple contagion.", "But, while the basic outline of the interaction between language change and social structure is understood, the fine details are still missing: What types of social network connections are most important for language change? To what extent do considerations of identity affect linguistic differences, particularly in an online context? Traditional sociolinguistic approaches lack the data and the methods for asking such detailed questions about language variation and change."]}
{"question_id": "993d5bef2bf1c0cd537342ef76d4b952f0588b83", "predicted_answer": "The older dialect markers that they explored are not explicitly mentioned in the given context. However, it can be inferred that they are referring to dialect markers that were used in the past, such as those that may have been used in Renaissance-era texts like The Canterbury Tales.", "predicted_evidence": ["The origins of cookout, graffiti, hella, phony, and stamp can almost certainly be traced back to spoken language. Some of these words (e.g., cookout and graffiti) are known to all fluent English speakers, but are preferred in certain cities simply as a matter of topic. Other words (e.g., hella BIBREF25 and jawn BIBREF26 ) are dialect markers that are not widely used outside their regions of origin, even after several decades of use in spoken language.", "An open question in sociolinguistics is whether and how local covert prestige\u2014i.e., the positive social evaluation of non-standard dialects\u2014affects the adoption of new linguistic forms BIBREF6 . Speakers often explain their linguistic choices in terms of their relationship with their local identity BIBREF40 , but this may be a post-hoc rationalization made by people whose language is affected by factors beyond their control. Indeed, some sociolinguists have cast doubt on the role of \u201clocal games\u201d in affecting the direction of language change BIBREF41 .", "Change is a universal property of language. For example, English has changed so much that Renaissance-era texts like The Canterbury Tales must now be read in translation. Even contemporary American English continues to change and diversify at a rapid pace\u2014to such an extent that some geographical dialect differences pose serious challenges for comprehensibility BIBREF0 . Understanding language change is therefore crucial to understanding language itself, and has implications for the design of more robust natural language processing systems BIBREF1 .", "We note that this hypothesis is restricted in scope to geographically distinctive words. We do not consider the more general hypothesis that geographically local ties are more influential for all types of language change, such as change involving linguistic variables that are associated with gender or socioeconomic status.", "The influence toward geographically distinctive linguistic markers is greater when exerted across geographically local ties than across other ties."]}
{"question_id": "a8e5e10d13b3f21dd11e8eb58e30cc25efc56e93", "predicted_answer": "The answer is 3. The paper creates ontologies for three domains: European Capital Cities, Office Buildings in Chicago, and Birds of the United States.", "predicted_evidence": ["In this paper, we have proposed a completely unsupervised model, hrLDA, for terminological ontology learning. hrLDA is a domain-independent and self-learning model, which means it is very promising for learning ontologies in new domains and thus can save significant time and effort in ontology acquisition.", "Although researchers have made significant progress on knowledge acquisition and have proposed many ontologies, for instance, WordNet BIBREF0 , DBpedia BIBREF1 , YAGO BIBREF2 , Freebase, BIBREF3 Nell BIBREF4 , DeepDive BIBREF5 , Domain Cartridge BIBREF6 , Knowledge Vault BIBREF7 , INS-ES BIBREF8 , iDLER BIBREF9 , and TransE-NMM BIBREF10 , current ontology construction methods still rely heavily on manual parsing and existing knowledge bases. This raises challenges for learning ontologies in new domains. While a strong ontology parser is effective in small-scale corpora, an unsupervised model is beneficial for learning new entities and their relations from new data sources, and is likely to perform better on larger corpora.", "We use precision, recall and F-measure for this ontology evaluation. A true positive case is an ontology rule that can be found in an extracted ontology and the associated ontology of DBpedia. A false positive case is an incorrectly identified ontology rule. A false negative case is a missed ontology rule. Table TABREF61 shows the evaluation results of ontologies extracted from Wikipedia articles pertaining to European Capital Cities (Corpus E), Office Buildings in Chicago (Corpus O) and Birds of the United States (Corpus B) using hrLDA, KB-LDA, phrase_hLDA (tree depth INLINEFORM0 = 3), and LDA+GSHL in contrast to these gold ontologies belonging to DBpedia. The three corpora used in this evaluation were collected from Wikipedia abstracts, the same text source of DBpedia. The seeds of hrLDA and the root concepts of LDA+GSHL are capital, building, and bird.", "We use precision, recall and F-measure for this ontology evaluation. A true positive case is an ontology rule that can be found in an extracted ontology and the associated ontology of DBpedia. A false positive case is an incorrectly identified ontology rule. A false negative case is a missed ontology rule. Table TABREF61 shows the evaluation results of ontologies extracted from Wikipedia articles pertaining to European Capital Cities (Corpus E), Office Buildings in Chicago (Corpus O) and Birds of the United States (Corpus B) using hrLDA, KB-LDA, phrase_hLDA (tree depth INLINEFORM0 = 3), and LDA+GSHL in contrast to these gold ontologies belonging to DBpedia. The three corpora used in this evaluation were collected from Wikipedia abstracts, the same text source of DBpedia.", "The rest of this paper is organized into five parts. In Section 2, we provide a brief background of hLDA. In Section 3, we present our hrLDA model and the ontology generation method. In Section 4, we demonstrate empirical results regarding topic hierarchies and generated terminological ontologies. Finally, in Section 5, we present some concluding remarks and discuss avenues for future work and improvements."]}
{"question_id": "949a2bc34176e47a4d895bcc3223f2a960f15a81", "predicted_answer": "According to the given context, the answer is yes, they do separately extract topic relations and topic hierarchies in their model. This is evident from the sentence: \"Finally, we demonstrate how to build topic hierarchies that bind with extracted relation triplets.\" This implies that they first extract relation triplets (topic relations) and then use them to build topic hierarchies.", "predicted_evidence": ["We begin by describing the process of reconstructing LDA. Subsequently, we explain relation extraction from heterogeneous documents. Next, we propose an improved topic partition method over CRP. Finally, we demonstrate how to build topic hierarchies that bind with extracted relation triplets.", "We have compared hrLDA with popular topic models to interpret how our algorithm learns meaningful hierarchies. By taking syntax and document structures into consideration, hrLDA is able to extract more descriptive topics. In addition, hrLDA eliminates the restrictions on the fixed topic tree depth and the limited number of topic paths. Furthermore, ACRP allows hrLDA to create more reasonable topics and to converge faster in Gibbs sampling.", "Topic modeling was originally used for topic extraction and document clustering. The classical topic model, latent Dirichlet allocation (LDA) BIBREF11 , simplifies a document as a bag of its words and describes a topic as a distribution of words. Prior research BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 has shown that LDA-based approaches are adequate for (terminological) ontology learning. However, these models are deficient in that they still need human supervision to decide the number of topics, and to pick meaningful topic labels usually from a list of unigrams. Among models not using unigrams, LDA-based Global Similarity Hierarchy Learning (LDA+GSHL) BIBREF13 only extracts a subset of relations: \u201cbroader\" and \u201crelated\" relations. In addition, the topic hierarchies of KB-LDA BIBREF17 rely on hypernym-hyponym pairs capturing only a subset of hierarchies.", "Topic modeling was originally used for topic extraction and document clustering. The classical topic model, latent Dirichlet allocation (LDA) BIBREF11 , simplifies a document as a bag of its words and describes a topic as a distribution of words. Prior research BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 has shown that LDA-based approaches are adequate for (terminological) ontology learning. However, these models are deficient in that they still need human supervision to decide the number of topics, and to pick meaningful topic labels usually from a list of unigrams. Among models not using unigrams, LDA-based Global Similarity Hierarchy Learning (LDA+GSHL) BIBREF13 only extracts a subset of relations: \u201cbroader\" and \u201crelated\" relations.", "We extract a single level topic tree using each of the four models; hrLDA becomes rLDA, and phrase_hLDA becomes phrase-based LDA. We have tested the average perplexity and running time performance of ten independent runs on each of the four models BIBREF31 , BIBREF32 . Equation EQREF41 defines the perplexity, which we employed as an empirical measure. DISPLAYFORM0"]}
{"question_id": "70abb108c3170e81f8725ddc1a3f2357be5a4959", "predicted_answer": "They measure the usefulness of obtained ontologies compared to domain expert ones by evaluating the precision, recall, and F-measure of the extracted ontologies against the gold ontologies from DBpedia. A true positive case is an ontology rule that can be found in both the extracted ontology and the associated DBpedia ontology, a false positive case is an incorrectly identified ontology rule, and a false negative case is a missed ontology rule.", "predicted_evidence": ["We use KB-LDA, phrase_hLDA, and LDA+GSHL as our baseline methods, and compare ontologies extracted from hrLDA, KB-LDA, phrase_hLDA, and LDA+GSHL with DBpedia ontologies. We use precision, recall and F-measure for this ontology evaluation. A true positive case is an ontology rule that can be found in an extracted ontology and the associated ontology of DBpedia. A false positive case is an incorrectly identified ontology rule. A false negative case is a missed ontology rule.", "In this paper, we have proposed a completely unsupervised model, hrLDA, for terminological ontology learning. hrLDA is a domain-independent and self-learning model, which means it is very promising for learning ontologies in new domains and thus can save significant time and effort in ontology acquisition.", "We use precision, recall and F-measure for this ontology evaluation. A true positive case is an ontology rule that can be found in an extracted ontology and the associated ontology of DBpedia. A false positive case is an incorrectly identified ontology rule. A false negative case is a missed ontology rule. Table TABREF61 shows the evaluation results of ontologies extracted from Wikipedia articles pertaining to European Capital Cities (Corpus E), Office Buildings in Chicago (Corpus O) and Birds of the United States (Corpus B) using hrLDA, KB-LDA, phrase_hLDA (tree depth INLINEFORM0 = 3), and LDA+GSHL in contrast to these gold ontologies belonging to DBpedia. The three corpora used in this evaluation were collected from Wikipedia abstracts, the same text source of DBpedia.", "We use precision, recall and F-measure for this ontology evaluation. A true positive case is an ontology rule that can be found in an extracted ontology and the associated ontology of DBpedia. A false positive case is an incorrectly identified ontology rule. A false negative case is a missed ontology rule. Table TABREF61 shows the evaluation results of ontologies extracted from Wikipedia articles pertaining to European Capital Cities (Corpus E), Office Buildings in Chicago (Corpus O) and Birds of the United States (Corpus B) using hrLDA, KB-LDA, phrase_hLDA (tree depth INLINEFORM0 = 3), and LDA+GSHL in contrast to these gold ontologies belonging to DBpedia. The three corpora used in this evaluation were collected from Wikipedia abstracts, the same text source of DBpedia. The seeds of hrLDA and the root concepts of LDA+GSHL are capital, building, and bird.", "Table TABREF61 shows the evaluation results of ontologies extracted from Wikipedia articles pertaining to European Capital Cities (Corpus E), Office Buildings in Chicago (Corpus O) and Birds of the United States (Corpus B) using hrLDA, KB-LDA, phrase_hLDA (tree depth INLINEFORM0 = 3), and LDA+GSHL in contrast to these gold ontologies belonging to DBpedia. The three corpora used in this evaluation were collected from Wikipedia abstracts, the same text source of DBpedia. The seeds of hrLDA and the root concepts of LDA+GSHL are capital, building, and bird. For both KB-LDA and phrase_hLDA we kept the top five tokens in each topic as each node of their topic trees is a distribution/list of phrases. hrLDA achieves the highest precision and F-measure scores in the three experiments compared to the other models. KB-LDA performs better than phrase_hLDA and LDA+GSHL, and phrase_hLDA performs similarly to LDA+GSHL."]}
{"question_id": "ce504a7ee2c1f068ef4dde8d435245b4e77bb0b5", "predicted_answer": "They obtain syntax from raw documents in hrLDA by using a language parser such as the Stanford NLP parser and Ollie to find all syntactically related noun phrases and their connections.", "predicted_evidence": ["We start with the root node ( INLINEFORM0 ) and apply rLDA to all the documents in a corpus.", "Extracting relation triplets is the essential step of hrLDA, and it is also the key process for converting a hierarchical topic tree to an ontology structure. The idea is to find all syntactically related noun phrases and their connections using a language parser such as the Stanford NLP parser BIBREF24 and Ollie BIBREF25 . Generally, there are two types of relation triplets:", "Instead of a predefined tree depth INLINEFORM0 , the tree depth for hrLDA is optional and data-driven. More importantly, clustering decisions are made given a global distribution of all current non-partitioned phrases (leaves) in our algorithm. This means there can be multiple paths traversed down a topic tree for each document. With reference to the topic tree, every node has a noun phrase as its label and represents a topic that may have multiple sub-topics. The root node is visited by all phrases. In practice, we do not link any phrases to the root node, as it contains the entire vocabulary. An inner node of a topic tree contains a selected topic label. A leaf node contains an unprocessed noun phrase. We define a hashmap INLINEFORM1 with a document ID as the key and the current leaf nodes of the document as the value. We denote the current tree level by INLINEFORM2 .", "The main problem we address in this section is generating terminological ontologies in an unsupervised fashion. The fundamental concept of hrLDA is as follows. When people construct a document, they start with selecting several topics. Then, they choose some noun phrases as subjects for each topic. Next, for each subject they come up with relation triplets to describe this subject or its relationships with other subjects. Finally, they connect the subject phrases and relation triplets to sentences via reasonable grammar. The main topic is normally described with the most important relation triplets. Sentences in one paragraph, especially adjacent sentences, are likely to express the same topic.", "The procedure for extending ACRP to hierarchies is essential to why hrLDA outperforms hLDA. Instead of a predefined tree depth INLINEFORM0 , the tree depth for hrLDA is optional and data-driven. More importantly, clustering decisions are made given a global distribution of all current non-partitioned phrases (leaves) in our algorithm. This means there can be multiple paths traversed down a topic tree for each document. With reference to the topic tree, every node has a noun phrase as its label and represents a topic that may have multiple sub-topics. The root node is visited by all phrases. In practice, we do not link any phrases to the root node, as it contains the entire vocabulary. An inner node of a topic tree contains a selected topic label. A leaf node contains an unprocessed noun phrase."]}
{"question_id": "468eb961215a554ace8088fa9097a7ad239f2d71", "predicted_answer": "The datasets available for the Cross-Domain Sentiment Analysis (CDSA) task are not explicitly mentioned in the given context. However, the table mentioned (TABREF31) suggests that there are 20 domains (D1 to D20) used in the experiment, and the results are presented for each domain. The specific datasets used for these domains are not provided.", "predicted_evidence": ["In general, we show which similarity metrics are crucial indicators of the benefit to a target domain, in terms of source domain selection for CDSA.", "We show the results of the classifier's CDSA performance followed by metrics evaluation on the top 10 domains. Finally, we present an overall comparison of metrics for all the domains.", "A possible future work is to use a weighted combination of multiple metrics for source domain selection. These similarity metrics may be used to extract suitable data or features for efficient CDSA. Similarity metrics may also be used as features to predict the CDSA performance in terms of accuracy degradation.", "We compare eleven similarity metrics (four that use labelled data for the target domain, seven that do not use labelled data for the target domain) with the CDSA performance of 20 domains. Out of these eleven metrics, we introduce two new metrics.", "Table TABREF31 shows the average CDSA accuracy degradation in each domain when it is selected as the source domain, and the rest of the domains are selected as the target domain. We also show in-domain sentiment analysis accuracy, the best source domain (on which CDSA classifier is trained), and the best target domain (on which CDSA classifier is tested) in the table. D15 suffers from the maximum average accuracy degradation, and D18 performs the best with least average accuracy degradation, which is also supported by its number of appearances i.e., 4, as the best source domain in the table. As for the best target domain, D9 appears the maximum number of times."]}
{"question_id": "57d07d2b509c5860880583efe2ed4c5620a96747", "predicted_answer": "The two novel metrics proposed are Precision and Ranking Accuracy.", "predicted_evidence": ["We explain all our metrics in detail later in this section. These 11 metrics can also be classified into two categories:", "Asymmetric Metrics - The metrics which are 2-way in nature i.e., $(D_1,D_2)$ and $(D_2,D_1)$ have different similarity values viz. Entropy Change, Doc2Vec embeddings, and FastText sentence embeddings. These metrics offer additional advantage as they can help decide which domain to train from and which domain to test on amongst $D_1$ and $D_2$.", "To compare metrics, we use two parameters: Precision and Ranking Accuracy.", "Note that this metric offers the advantage of asymmetricity, unlike the other three metrics for labelled data.", "We use a total of 11 metrics over two scenarios: the first that uses labelled data, while the second that uses unlabelled data."]}
{"question_id": "d126d5d6b7cfaacd58494f1879547be9e91d1364", "predicted_answer": "The similarity metrics that have been tried are:\n\nAsymmetric Metrics:\n\n1. Entropy Change\n2. Doc2Vec embeddings\n3. FastText sentence embeddings\n\nSymmetric Metrics:\n\n1. Significant Words Overlap\n2. Chameleon Words Similarity\n3. Symmetric KL Divergence\n4. Word2Vec embeddings\n5. GloVe embeddings\n6. FastText word embeddings\n7. ELMo based embeddings\n8. Universal Sentence Encoder based embeddings\n\nAdditionally, four metrics that use labelled data for the target domain are mentioned, but not specified.", "predicted_evidence": ["To compare metrics, we use two parameters: Precision and Ranking Accuracy.", "Asymmetric Metrics - The metrics which are 2-way in nature i.e., $(D_1,D_2)$ and $(D_2,D_1)$ have different similarity values viz. Entropy Change, Doc2Vec embeddings, and FastText sentence embeddings. These metrics offer additional advantage as they can help decide which domain to train from and which domain to test on amongst $D_1$ and $D_2$.", "We compare eleven similarity metrics (four that use labelled data for the target domain, seven that do not use labelled data for the target domain) with the CDSA performance of 20 domains. Out of these eleven metrics, we introduce two new metrics.", "Symmetric Metrics - The metrics which consider domain-pairs $(D_1,D_2)$ and $(D_2,D_1)$ as the same and provide similar results for them viz. Significant Words Overlap, Chameleon Words Similarity, Symmetric KL Divergence, Word2Vec embeddings, GloVe embeddings, FastText word embeddings, ELMo based embeddings and Universal Sentence Encoder based embeddings.", "Note that this metric offers the advantage of asymmetricity, unlike the other three metrics for labelled data."]}
{"question_id": "7dca806426058d59f4a9a4873e9219d65aea0987", "predicted_answer": "There are 20 domains available for selection of source domain.", "predicted_evidence": ["When target domain data is labelled, we use the following four metrics for comparing and ranking source domains for a particular target domain:", "We then say that a source domain $D_2$ is more suitable for target domain $D_1$ as compared to source domain $D_3$ if;", "For a target domain, source domains are ranked in decreasing order of final similarity value.", "Figure FIGREF36 shows the number of true positives (precision) when K = 5 for each metric over the top 10 domains. The X-axis denotes the domains, whereas the Y-axis in the bar graph indicates the precision achieved by all metrics in each domain. We observe that the highest precision attained is 5, by 4 different metrics. We also observe that all the metrics reach a precision of at least 1. A similar observation is made for the remaining domains as well. Figure FIGREF37 displays the RA values of K = 5 in each metric for the top 10 domains. Here, the highest number of correct source domain rankings attained is 4 by ULM6 (ELMo) for domain D5.", "Based on CDSA results, we create a recommendation chart that prescribes domains that are the best as the source or target domain, for each of the domains."]}
{"question_id": "800fcd8b08d36c5276f9e5e1013208d41b46de59", "predicted_answer": "They think sentiment features do not result in improvement because they experimented with various sentiment and emotion lexicons, including manually created and automatically generated ones, but did not observe any improvement in the cross-validation experiments.", "predicted_evidence": ["Sentiment Lexicon Features", "In this task, the general textual features (row b) played a bigger role in the overall performance than the domain-specific (row c) or sentiment lexicon (row d) features. Removing this group of features results in more than 2.5 percentage points drop in the F-measure affecting both precision and recall (row b). However, removing any one feature subgroup in this group (e.g., general INLINEFORM0 -grams, general clusters, general embeddings, etc.) results only in slight drop or even increase in the performance (rows b.1\u2013b.4). This indicates that the features in this group capture similar information. Among the domain-specific features, the INLINEFORM1 -grams generalized over domain terms are the most useful. The model trained without these INLINEFORM2 -grams features performs almost one percentage point worse than the model that uses all the features (row c.1). The sentiment lexicon features were not helpful (row d).", "We experimented with a number of other existing manually created or automatically generated sentiment and emotion lexicons, such as the NRC Emotion Lexicon BIBREF21 and the NRC Hashtag Emotion Lexicon BIBREF22 (http://saifmohammad.com/ WebPages/lexicons.html), but did not observe any improvement in the cross-validation experiments. None of the sentiment lexicon features were effective in the cross-validation experiments on Task 1; therefore, we did not include them in the final feature set for this task.", "Table TABREF42 shows the performance of our best system (submission 1) when one of the feature groups is removed. In this task, the general textual features (row b) played a bigger role in the overall performance than the domain-specific (row c) or sentiment lexicon (row d) features. Removing this group of features results in more than 2.5 percentage points drop in the F-measure affecting both precision and recall (row b). However, removing any one feature subgroup in this group (e.g., general INLINEFORM0 -grams, general clusters, general embeddings, etc.) results only in slight drop or even increase in the performance (rows b.1\u2013b.4). This indicates that the features in this group capture similar information. Among the domain-specific features, the INLINEFORM1 -grams generalized over domain terms are the most useful.", "Table TABREF42 shows the performance of our best system (submission 1) when one of the feature groups is removed. In this task, the general textual features (row b) played a bigger role in the overall performance than the domain-specific (row c) or sentiment lexicon (row d) features. Removing this group of features results in more than 2.5 percentage points drop in the F-measure affecting both precision and recall (row b). However, removing any one feature subgroup in this group (e.g., general INLINEFORM0 -grams, general clusters, general embeddings, etc.) results only in slight drop or even increase in the performance (rows b.1\u2013b.4). This indicates that the features in this group capture similar information. Among the domain-specific features, the INLINEFORM1 -grams generalized over domain terms are the most useful. The model trained without these INLINEFORM2 -grams features performs almost one percentage point worse than the model that uses all the features (row c.1)."]}
{"question_id": "cdbbba22e62bc9402aea74ac5960503f59e984ff", "predicted_answer": "The sizes of the datasets are:\n\n* Task 1: \n  - Training set: 8,000 tweets\n  - Development set: 2,260 tweets\n  - Test set: 7,513 tweets\n* Task 2: \n  - Training set: 10,822 tweets\n  - Development set: 4,845 tweets\n  - Test set: 9,961 tweets", "predicted_evidence": ["Information on how the data was collected and annotated was not available until after the evaluation.", "Two labeled datasets were provided to the participants: a training set containing 8,000 tweets and a development set containing 2,260 tweets. As for Task 1, the training and development sets were distributed through tweet IDs and a download script. Around 95% of the tweets were accessible through download. Again, we removed duplicate and near-duplicate messages. A separate test set of 7,513 tweets was provided without labels at the evaluation period. This set was distributed to the participants, in full, by email. Table TABREF7 shows the number of instances we used for training and testing our model.", "Two labeled datasets were provided to the participants: a training set containing 10,822 tweets and a development set containing 4,845 tweets. These datasets were distributed as lists of tweet IDs, and the participants needed to download the tweets using the provided Python script. However, only about 60\u201370% of the tweets were accessible at the time of download (May 2017). The training set contained several hundreds of duplicate or near-duplicate messages, which we decided to remove. Near-duplicates were defined as tweets containing mostly the same text but differing in user mentions, punctuation, or other non-essential context. A separate test set of 9,961 tweets was provided without labels at the evaluation period. This set was distributed to the participants, in full, by email. Table TABREF1 shows the number of instances we used for training and testing our model.", "For each task, our team submitted three sets of predictions. The submissions differed in the sets of features and parameters used to train the classification models (Table TABREF32 ).", "16. weight"]}
{"question_id": "301a453abaa3bc15976817fefce7a41f3b779907", "predicted_answer": "The evaluation metrics mentioned are:\n\n1. F-score for class 1 (ADR)\n2. Micro-averaged F-score of class 1 (intake) and class 2 (possible intake)\n3. Total score = score of the last token in the tweet", "predicted_evidence": ["The official evaluation metric was the F-score for class 1 (ADR): INLINEFORM0", "The official evaluation metric for this task was micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1", "Information on how the data was collected and annotated was not available until after the evaluation.", "the total score = INLINEFORM0 ;", "the score of the last token in the tweet."]}
{"question_id": "f3673f6375f065014e8e4bb8c7adf54c1c7d7862", "predicted_answer": "The results for Task 1 are not explicitly stated in the given context. The text only mentions the formulation of Task 1 as a binary classification task to determine whether a tweet mentions an adverse drug reaction, but it does not provide the results.\n\nThe results are only provided for Task 2, which are presented in Table TABREF41. The best results are achieved with submission 1, with precision and recall similar to submissions 2 and 3. The system is ranked third among the nine teams that participated in Task 2.", "predicted_evidence": ["Below we describe in detail the two tasks we participated in, Task 1 and Task 2.", "Task 2", "4 Task 1", "Task 1 was formulated as follows: given a tweet, determine whether it mentions an adverse drug reaction. This was a binary classification task:", "The results for our three official submissions on Task 2 are presented in Table TABREF41 (rows c.1\u2013c.3). The best results in INLINEFORM0 are achieved with submission 1 (row c.1). The results for the other two submissions, submission 2 and submission 3, are quite similar to the results of submission 1 in both precision and recall (rows c.2\u2013c.3). Adding the features from the ADR lexicon and the Pronoun lexicon did not result in performance improvement on the test set. Our best system is ranked third among the nine teams participated in this task (rows b.1\u2013b.3)."]}
{"question_id": "0bd3bea892c34a3820e98c4a42cdeda03753146b", "predicted_answer": "According to the text, the domain-specific features that were trained on are:\n\n1. INLINEFORM1-grams generalized over domain terms\n2. Domain word embeddings", "predicted_evidence": ["From these resources, the following domain-specific features were generated:", "Domain-Specific Features", "To generate domain-specific features, we used the following domain resources:", "Submission 2: the features and parameters were selected based on the performance of the model trained on the full training set and tested on the full development set.", "To investigate the impact of each feature group on the overall performance, we conduct ablation experiments where we repeat the same classification process but remove one feature group at a time. Table TABREF40 shows the results of these ablation experiments for our best system (submission 1). Comparing the two major groups of features, general textual features (row b) and domain-specific features (row c), we observe that they both have a substantial impact on the performance. Removing one of these groups leads to a two percentage points drop in INLINEFORM0 . The general textual features mostly affect recall of the ADR class (row b) while the domain-specific features impact precision (row c). Among the general textual features, the most influential feature is general-domain word embeddings (row b.2). Among the domain-specific features, INLINEFORM1 -grams generalized over domain terms (row c.1) and domain word embeddings (row c.3) provide noticeable contribution to the overall performance."]}
{"question_id": "8cf5abf0126f19253930478b02f0839af28e4093", "predicted_answer": "According to the text, the sentiment features used are:\n\n1. Hu and Liu Lexicon\n2. Norms of Valence, Arousal, and Dominance\n3. labMT\n4. NRC Emoticon Lexicon\n\nThese features were calculated separately for each tweet and each lexicon.", "predicted_evidence": ["Sentiment Lexicon Features", "We generated features using the sentiment scores provided in the following lexicons: Hu and Liu Lexicon BIBREF17 , Norms of Valence, Arousal, and Dominance BIBREF18 , labMT BIBREF19 , and NRC Emoticon Lexicon BIBREF20 . The first three lexicons were created through manual annotation while the last one, NRC Emoticon Lexicon, was generated automatically from a large collection of tweets with emoticons. The following set of features were calculated separately for each tweet and each lexicon:", "The classification model leverages a variety of general textual features as well as sentiment and domain-specific features described below. Many features were inspired by previous work on ADR BIBREF12 , BIBREF8 , BIBREF9 and our work on sentiment analysis (such as the winning system in the SemEval-2013 task on sentiment analysis in Twitter BIBREF13 and best performing stance detection system BIBREF14 ).", "We experimented with a number of other existing manually created or automatically generated sentiment and emotion lexicons, such as the NRC Emotion Lexicon BIBREF21 and the NRC Hashtag Emotion Lexicon BIBREF22 (http://saifmohammad.com/ WebPages/lexicons.html), but did not observe any improvement in the cross-validation experiments. None of the sentiment lexicon features were effective in the cross-validation experiments on Task 1; therefore, we did not include them in the final feature set for this task.", "General Textual Features"]}
{"question_id": "d211a37830c59aeab4970fdb2e03d9b7368b421c", "predicted_answer": "The surface-form features used are not specified in the given context. The text only mentions that certain surface-form features were used, but it does not provide information on what those features are.", "predicted_evidence": ["The following surface-form features were used:", "From these resources, the following domain-specific features were generated:", "Submission 1: we used the features and parameters that gave the best results during cross-validation.", "To generate domain-specific features, we used the following domain resources:", "Domain-Specific Features"]}
{"question_id": "c3ce95658eea1e62193570955f105839de3d7e2d", "predicted_answer": "The BERT-based model works as follows:\n\n1. The query and document are flattened and packed as a token sequence, with each token represented by a sum of token, segmentation, and position embeddings.\n2. The input sequence is constructed by inserting special tokens, such as [CLS], [SEP], and [L#], to indicate sentence boundaries and query structure.\n3. The input sequence is fed into a BERT model with randomly initialized parameters, which encodes the query relevance, document context, and salient meanings of each sentence into a vector representation.\n4. The vector representations are then fed into a simple output layer to predict the label or estimate the score of each sentence.\n5. The model is fine-tuned on the WikiRef dataset and then further fine-tuned on the DUC datasets to utilize the general language representations of BERT and adapt to the specific task of query-focused extractive summarization.\n\nOverall, the model leverages the self-attention mechanism of BERT to jointly model the relationships between the query and sentences, as well as between sentences, to generate a summary that is relevant to the query.", "predicted_evidence": ["uses the same structure as the BERT with randomly initialized parameters.", "The query $\\mathcal {Q}$ and document $\\mathcal {D}$ are flattened and packed as a token sequence as input. Following the standard practice of BERT, the input representation of each token is constructed by summing the corresponding token, segmentation and position embeddings. Token embeddings project the one-hot input tokens into dense vector representations. Two segment embeddings $\\mathbf {E}_Q$ and $\\mathbf {E}_D$ are used to indicate query and document tokens respectively. Position embeddings indicate the absolute position of each token in the input sequence. To embody the hierarchical structure of the query in a sequence, we insert a [L#] token before the #-th query token sequence. For each sentence, we insert a [CLS] token at the beginning and a [SEP] token at the end to draw a clear sentence boundary.", "Figure FIGREF2 gives an overview of our BERT-based extractive query-focused summmarization model. For each sentence, we use BERT to encode its query relevance, document context and salient meanings into a vector representation. Then the vector representations are fed into a simple output layer to predict the label or estimate the score of each sentence.", "In this paper, we develop a BERT-based model for query-focused extractive summarization. The model takes the concatenation of the query and the document as input. The query-sentence and sentence-sentence relationships are jointly modeled by the self-attention mechanism BIBREF12. The model is fine-tuned to utilize the general language representations of BERT BIBREF13.", "The WikiRef dataset is used as augmentation data for DUC datasets in two steps. We first fine-tune BERT on the WikiRef dataset. Subsequently, we use the DUC datasets to further fine-tune parameters of the best pre-trained model."]}
{"question_id": "389cc454ac97609e9d0f2b2fe70bf43218dd8ba7", "predicted_answer": "According to the text, they use Wikipedia to automatically collect a query-focused summarization dataset (WikiRef) by:\n\n1. Taking a statement from a Wikipedia article as the summary.\n2. Using the supporting citation of the statement as the source document.\n3. Forming the query by combining the article title and section titles of the statement.\n\nThis process allows them to automatically construct a large-scale query-focused summarization dataset using Wikipedia articles and their corresponding references.", "predicted_evidence": ["We automatically construct a query-focused summarization dataset (named as WikiRef) using Wikipedia and corresponding reference web pages. In the following sections, we will first elaborate the creation process. Then we will analyze the queries, documents and summaries quantitatively and qualitatively.", "In order to advance query-focused summarization with limited data, we improve the summarization model with data augmentation. Specifically, we transform Wikipedia into a large-scale query-focused summarization dataset (named as WikiRef). To automatically construct query-focused summarization examples using Wikipedia, the statements' citations in Wikipedia articles as pivots to align the queries and documents. Figure FIGREF1 shows an example that is constructed by the proposed method. We first take the highlighted statement as the summary. Its supporting citation is expected to provide an adequate context to derive the statement, thus can serve as the source document. On the other hand, the section titles give a hint about which aspect of the document is the summary's focus. Therefore, we use the article title and the section titles of the statement to form the query. Given that Wikipedia is the largest online encyclopedia, we can automatically construct massive query-focused summarization examples.", "In this paper, we propose to automatically construct a large-scale query-focused summarization dataset WikiRef using Wikipedia articles and the corresponding references. The statements, supporting citations and article title along with section titles of the statements are used as summaries, documents and queries respectively. The WikiRef dataset serves as a means of data augmentation on DUC benchmarks. It also is shown to be a eligible query-focused summarization benchmark. Moreover, we develop a BERT-based extractive query-focused summarization model to extract summaries from the documents. The model makes use of the query-sentence relationships and sentence-sentence relationships jointly to score sentences. The results on DUC benchmarks show that our model with data augmentation outperforms the state-of-the-art. As for future work, we would like to model relationships among documents for multi-document summarization.", "We compare our method with several previous query-focused summarization models, of which the AttSum is the state-of-the-art model:", "DUC 2005-2007 are query-focused multi-document summarization benchmarks. The documents are from the news domain and grouped into clusters according to their topics. And the summary is required to be no longer than 250 tokens. Table TABREF29 shows statistics of the DUC datasets. Each document cluster has several reference summaries generated by humans and a query that specifies the focused aspects and desired information. We show an example query from the DUC 2006 dataset below:"]}
{"question_id": "2c4db4398ecff7e4c1c335a2cb3864bfdc31df1a", "predicted_answer": "Based on the provided context, the GPU-based self-critical Reinforcement Learning model is designed as follows:\n\n1. **Policy Gradient Method**: The model uses policy gradient methods to directly optimize non-differentiable metrics such as ROUGE and METEOR scores.\n2. **Markov Decision Process**: The problem is posed as a Markov Decision Process, where the set of actions $\\mathcal{A}$ is the vocabulary, and the reward $\\mathcal{R}$ is the ROUGE score itself.\n3. **Policy $\\pi(\\theta)$**: The goal is to find a policy $\\pi(\\theta)$ that generates a set of sampled words $\\tilde{y} = \\{\\tilde{y}_1, \\tilde{y}_2, ..., \\tilde{y}_T\\}$ that achieves the highest ROUGE score among all possible summaries.\n4. **Sampling Probability**: The sampling probability $p(\\tilde{y}_t)$ is defined as $p(\\tilde{y}_t | \\tilde{y}_1, \\tilde{y}_2, ..., \\tilde{y}_{t-1}, x)$, where $x$ is the input.\n5. **Regularization Coefficient**: The model uses a regularization coefficient $\\alpha$ to control the exploration-exploitation trade-off, where a higher $\\alpha$ corresponds to more exploration, and a lower $\\alpha$ corresponds to more exploitation.\n6. **GPU-based Implementation**: The model is implemented on GPU using 7-NVIDIA Tesla-P100 GPUs or 8-NVIDIA Tesla-V100 GPUs, depending on the model architecture, with a batch size of 448 or 96, respectively.\n7. **Training**: The model is trained using a learning rate of 0.00005, which is manually changed to 0.00001 when needed, with $\\alpha = 0.0001$. The reported results are obtained after training for 15 days.\n\nThe figure FIGREF38 shows the self-critical model architecture, which is not provided in the text.", "predicted_evidence": ["Where, $p(\\tilde{y}_{t})=p(\\tilde{y}_{t} | \\tilde{y}_{1}, \\tilde{y}_{2}, ..., \\tilde{y}_{t-1}, x)$ is the sampling probability and $V$ is the size of the vocabulary. It is similar to the exploration-exploitation trade-off. $\\alpha $ is the regularization coefficient that explicitly controls this trade-off: a higher $\\alpha $ corresponds to more exploration, and a lower $\\alpha $ corresponds to more exploitation. We have found that all TensorFlow based open-source implementations of self-critic models use a function (tf.py_func) that runs only on CPU and it is very slow. To the best of our knowledge, ours is the first GPU based implementation.", "The Hierarchical models process one sentence at a time, and hence attention distributions need less memory, and therefore, a larger batch size can be used, which in turn speeds up the training process. The non-factored model is trained on 7-NVIDIA Tesla-P100 GPUs with a batch size of 448 (64 examples per GPU); it takes approximately 45 minutes per epoch. Since the factored sequences are long, we used a batch size of 96 (12 examples per GPU) on 8-NVIDIA Tesla-V100 GPUs. The Hier model reaches optimal cross-entropy loss in just 8 epochs, unlike 33-35 epochs for both BIBREF7 and BIBREF2. For the self-critical model, training is started from the best supervised model with a learning rate of 0.00005 and manually changed to 0.00001 when needed with $\\alpha =0.0001$ and the reported results are obtained after training for 15 days.", "As discussed earlier, training in a supervised learning setting creates a mismatch between training and testing objectives. Also, feeding the ground-truth labels in training time-step creates an exposure bias while testing in which we feed the predictions from the previous time-step. Policy gradient methods overcome this by directly optimizing the non-differentiable metrics such as ROUGE BIBREF12 and METEOR BIBREF19. It can be posed as a Markov Decision Process in which the set of actions $\\mathcal {A}$ is the vocabulary and reward $\\mathcal {R}$ is the ROUGE score itself. So, we should find a policy $\\pi (\\theta )$ such that the set of sampled words $\\tilde{y} = \\lbrace \\tilde{y}_{1}, \\tilde{y}_{2}, ..., \\tilde{y}_{T}\\rbrace $ achieves highest ROUGE score among all possible summaries.", "The Hierarchical models process one sentence at a time, and hence attention distributions need less memory, and therefore, a larger batch size can be used, which in turn speeds up the training process. The non-factored model is trained on 7-NVIDIA Tesla-P100 GPUs with a batch size of 448 (64 examples per GPU); it takes approximately 45 minutes per epoch. Since the factored sequences are long, we used a batch size of 96 (12 examples per GPU) on 8-NVIDIA Tesla-V100 GPUs. The Hier model reaches optimal cross-entropy loss in just 8 epochs, unlike 33-35 epochs for both BIBREF7 and BIBREF2.", "Figure FIGREF38 below shows the self-critical model. All the examples shown in Tables TABREF39-TABREF44 are chosen as per the shortest article lengths available due to space constraints."]}
{"question_id": "4738158f92b5b520ceba6207e8029ae082786dbe", "predicted_answer": "The authors are referring to previous works cited as BIBREF7, which have used large vocabulary sizes of 150,000 words.", "predicted_evidence": ["Figure FIGREF38 below shows the self-critical model. All the examples shown in Tables TABREF39-TABREF44 are chosen as per the shortest article lengths available due to space constraints.", "Where $C$ is a compressor we intend to learn. Figure FIGREF3 represents the fundamental idea when using a sequence-to-sequence architecture. For a sentence $s$ in summary, the representations of all the related document sentences $d_{1}, d_{2}, ..., d_{K}$ are expected to form a cluster that represents a part of the highlight of the document.", "For all the plain NSE models, we have truncated the article to a maximum of 400 tokens and the summary to 100 tokens. For the hierarchical NSE models, articles are truncated to have a maximum of 20 sentences and 20 words per sentence each. Shorter sequences are padded with `PAD` tokens. Since the factored models have lemma, PoS tag and the separator `|` for each word, sequence lengths should be close to 3 times the non-factored counterparts. For practical reasons of memory and time, we have used 800 tokens per article and 300 tokens for the summary.", "For all the models, including the pointer-generator model, we use a vocabulary size of 50,000 words for both source and target. Though some previous works BIBREF7 have used large vocabulary sizes of 150,000, since our models have a copy mechanism, smaller vocabulary is enough to obtain good performance. Large vocabularies increase the computation time. Since memory plays a prominent role in retrieval and update, it is vital to start with a good initialization. We have used 300-dimensional pre-trained GloVe BIBREF23 word-vectors to represent the input sequence to a model. Sentence memories are initialized with GloVe word-vectors of all the words in that sentence. Document memories are initialized with vector representations of all the sentences where a sentence is represented with the average of the GloVe word-vectors of all its words. All the models are trained using the Adam optimizer with the default learning rate of 0.001.", "For all the models, including the pointer-generator model, we use a vocabulary size of 50,000 words for both source and target. Though some previous works BIBREF7 have used large vocabulary sizes of 150,000, since our models have a copy mechanism, smaller vocabulary is enough to obtain good performance. Large vocabularies increase the computation time. Since memory plays a prominent role in retrieval and update, it is vital to start with a good initialization. We have used 300-dimensional pre-trained GloVe BIBREF23 word-vectors to represent the input sequence to a model. Sentence memories are initialized with GloVe word-vectors of all the words in that sentence. Document memories are initialized with vector representations of all the sentences where a sentence is represented with the average of the GloVe word-vectors of all its words. All the models are trained using the Adam optimizer with the default learning rate of 0.001. We have not applied any regularization as the usage of dropout, and $L_{2}$ penalty resulted in similar performance, however with a drastically increased training time."]}
{"question_id": "4dadde7c61230553ef14065edd8c1c7e41b9c329", "predicted_answer": "The previous state of the art on factored datasets was that factoring lemma and Part-of-Speech (PoS) tag of surface words increases the performance of NMT models in terms of BLEU score drastically, as observed in BIBREF22. This is due to the improvement of vocabulary coverage and better generalization.", "predicted_evidence": ["The Hierarchical models process one sentence at a time, and hence attention distributions need less memory, and therefore, a larger batch size can be used, which in turn speeds up the training process. The non-factored model is trained on 7-NVIDIA Tesla-P100 GPUs with a batch size of 448 (64 examples per GPU); it takes approximately 45 minutes per epoch. Since the factored sequences are long, we used a batch size of 96 (12 examples per GPU) on 8-NVIDIA Tesla-V100 GPUs. The Hier model reaches optimal cross-entropy loss in just 8 epochs, unlike 33-35 epochs for both BIBREF7 and BIBREF2.", "The factoring of lemma and Part-of-Speech (PoS) tag of surface words, are observed BIBREF22 to increase the performance of NMT models in terms of BLEU score drastically. This is due to the improvement of the vocabulary coverage and better generalization. We have added a pre-processing step by incorporating the lemma and PoS tag to every word of the dataset and training the supervised model on the factored data. The process of extracting the lemma and the PoS tags has been described in BIBREF22. Please refer to the appendix for an example of factoring.", "The Hierarchical models process one sentence at a time, and hence attention distributions need less memory, and therefore, a larger batch size can be used, which in turn speeds up the training process. The non-factored model is trained on 7-NVIDIA Tesla-P100 GPUs with a batch size of 448 (64 examples per GPU); it takes approximately 45 minutes per epoch. Since the factored sequences are long, we used a batch size of 96 (12 examples per GPU) on 8-NVIDIA Tesla-V100 GPUs. The Hier model reaches optimal cross-entropy loss in just 8 epochs, unlike 33-35 epochs for both BIBREF7 and BIBREF2. For the self-critical model, training is started from the best supervised model with a learning rate of 0.00005 and manually changed to 0.00001 when needed with $\\alpha =0.0001$ and the reported results are obtained after training for 15 days.", "For all the plain NSE models, we have truncated the article to a maximum of 400 tokens and the summary to 100 tokens. For the hierarchical NSE models, articles are truncated to have a maximum of 20 sentences and 20 words per sentence each. Shorter sequences are padded with `PAD` tokens. Since the factored models have lemma, PoS tag and the separator `|` for each word, sequence lengths should be close to 3 times the non-factored counterparts. For practical reasons of memory and time, we have used 800 tokens per article and 300 tokens for the summary.", "Where $C$ is a compressor we intend to learn. Figure FIGREF3 represents the fundamental idea when using a sequence-to-sequence architecture. For a sentence $s$ in summary, the representations of all the related document sentences $d_{1}, d_{2}, ..., d_{K}$ are expected to form a cluster that represents a part of the highlight of the document."]}
{"question_id": "014830892d93e3c01cb659ad31c90de4518d48f3", "predicted_answer": "Unfortunately, the provided context does not mention how much the model outperformed other models or baselines. The text only describes the experimental setup, hyperparameter tuning, and evaluation metrics used to evaluate the performance of the proposed model and other QG methods. It does not provide any results or comparisons with other models.", "predicted_evidence": ["The following values of hyperparameters are found to be optimal: (i) $\\gamma _1=0.99$, $\\gamma _2=0.01$, $\\gamma _3=0.1$, (ii) $d_1=300$, $d_2=d_3=3$, (iii) $\\alpha =0.9, \\beta = 10$, $h=5000$. Adam BIBREF37 optimizer is used to train the model with (i) $ \\beta _{1} = 0.9 $, (ii) $ \\beta _{2} = 0.999 $, and (iii) $ \\epsilon =10^{-8} $. For MTL-QG training, the initial learning rate is set to $0.01$. For our proposed model training the learning rate is set to $0.00001$. We also apply gradient clipping BIBREF38 with range $ [-5, 5] $.", "We conduct experiments to evaluate the performance of our proposed and other QG methods using the evaluation metrics: BLEU-1, BLEU-2, BLEU-3, BLEU-4 BIBREF39, ROUGE-L BIBREF40 and METEOR BIBREF41.", "Next, we initialize our model with the pre-trained ML weights and train the network with the mixed-objective learning function. The following values of hyperparameters are found to be optimal: (i) $\\gamma _1=0.99$, $\\gamma _2=0.01$, $\\gamma _3=0.1$, (ii) $d_1=300$, $d_2=d_3=3$, (iii) $\\alpha =0.9, \\beta = 10$, $h=5000$. Adam BIBREF37 optimizer is used to train the model with (i) $ \\beta _{1} = 0.9 $, (ii) $ \\beta _{2} = 0.999 $, and (iii) $ \\epsilon =10^{-8} $. For MTL-QG training, the initial learning rate is set to $0.01$. For our proposed model training the learning rate is set to $0.00001$.", "where $\\gamma _1$, $\\gamma _2$, and $\\gamma _3$ correspond to the weights of $\\mathcal {L}_{rl}$, $\\mathcal {L}_{ml}$, and $\\mathcal {L}_{sp}$, respectively. In our experiments, we use the same vocabulary for both the encoder and decoder. Our vocabulary consists of the top 50,000 frequent words from the training data. We use the development dataset for hyper-parameter tuning. Pre-trained GloVe embeddings BIBREF34 of dimension 300 are used in the document encoding step. The hidden dimension of all the LSTM cells is set to 512. Answer tagging features and supporting facts position features are embedded to 3-dimensional vectors. The dropout BIBREF35 probability $p$ is set to $0.3$. The beam size is set to 4 for beam search. We initialize the model parameters randomly using a Gaussian distribution with Xavier scheme BIBREF36.", "In our experiments, we use the same vocabulary for both the encoder and decoder. Our vocabulary consists of the top 50,000 frequent words from the training data. We use the development dataset for hyper-parameter tuning. Pre-trained GloVe embeddings BIBREF34 of dimension 300 are used in the document encoding step. The hidden dimension of all the LSTM cells is set to 512. Answer tagging features and supporting facts position features are embedded to 3-dimensional vectors. The dropout BIBREF35 probability $p$ is set to $0.3$. The beam size is set to 4 for beam search. We initialize the model parameters randomly using a Gaussian distribution with Xavier scheme BIBREF36. We first pre-train the network by minimizing only the maximum likelihood (ML) loss. Next, we initialize our model with the pre-trained ML weights and train the network with the mixed-objective learning function. The following values of hyperparameters are found to be optimal: (i) $\\gamma _1=0.99$, $\\gamma _2=0.01$, $\\gamma _3=0.1$, (ii) $d_1=300$, $d_2=d_3=3$, (iii) $\\alpha =0.9, \\beta = 10$, $h=5000$."]}
{"question_id": "ae7c5cf9c2c121097eb00d389cfd7cc2a5a7d577", "predicted_answer": "The language in the dataset is English. This is inferred from the fact that the dataset is based on Wikipedia articles and uses pre-trained GloVe embeddings, which are typically trained on English language texts. Additionally, the references to other datasets such as SQuAD, NewsQA, and MSMARCO, which are all English language datasets, further support this conclusion.", "predicted_evidence": ["We use the HotPotQA BIBREF11 dataset to evaluate our methods. This dataset consists of over 113k Wikipedia-based question-answer pairs, with each question requiring multi-step reasoning across multiple supporting documents to infer the answer. While there exists other multi-hop datasets BIBREF9, BIBREF10, only HotPotQA dataset provides the sentence-level ground-truth labels to locate the supporting facts in the list of documents. We combine the training set ($90,564$) and development set ($7,405$) and randomly split the resulting data, with 80% for training, 10% for development, 10% for testing.", "where $\\gamma _1$, $\\gamma _2$, and $\\gamma _3$ correspond to the weights of $\\mathcal {L}_{rl}$, $\\mathcal {L}_{ml}$, and $\\mathcal {L}_{sp}$, respectively. In our experiments, we use the same vocabulary for both the encoder and decoder. Our vocabulary consists of the top 50,000 frequent words from the training data. We use the development dataset for hyper-parameter tuning. Pre-trained GloVe embeddings BIBREF34 of dimension 300 are used in the document encoding step. The hidden dimension of all the LSTM cells is set to 512. Answer tagging features and supporting facts position features are embedded to 3-dimensional vectors. The dropout BIBREF35 probability $p$ is set to $0.3$. The beam size is set to 4 for beam search. We initialize the model parameters randomly using a Gaussian distribution with Xavier scheme BIBREF36.", "where $\\gamma _1$, $\\gamma _2$, and $\\gamma _3$ correspond to the weights of $\\mathcal {L}_{rl}$, $\\mathcal {L}_{ml}$, and $\\mathcal {L}_{sp}$, respectively. In our experiments, we use the same vocabulary for both the encoder and decoder. Our vocabulary consists of the top 50,000 frequent words from the training data. We use the development dataset for hyper-parameter tuning. Pre-trained GloVe embeddings BIBREF34 of dimension 300 are used in the document encoding step. The hidden dimension of all the LSTM cells is set to 512. Answer tagging features and supporting facts position features are embedded to 3-dimensional vectors. The dropout BIBREF35 probability $p$ is set to $0.3$. The beam size is set to 4 for beam search.", "where $N$ is the number of document list, $S$ the number of candidate sentences in a particular training example, $\\delta _i^j$ and $p_i^{j}$ represent the ground truth supporting facts label and the output Sigmoid probability, respectively.", "In the past, question generation has been tackled using rule-based approaches such as question templates BIBREF0 or utilizing named entity information and predictive argument structures of sentences BIBREF1. Recently, neural-based approaches have accomplished impressive results BIBREF2, BIBREF3, BIBREF4 for the task of question generation. The availability of large-scale machine reading comprehension datasets such as SQuAD BIBREF5, NewsQA BIBREF6, MSMARCO BIBREF7 etc. have facilitated research in question answering task. SQuAD BIBREF5 dataset itself has been the de facto choice for most of the previous works in question generation. However, 90% of the questions in SQuAD can be answered from a single sentence BIBREF8, hence former QG systems trained on SQuAD are not capable of distilling and utilizing information from multiple sentences. Recently released multi-hop datasets such as QAngaroo BIBREF9, ComplexWebQuestions BIBREF10 and HotPotQA BIBREF11 are more suitable for building QG systems that required to gather and utilize information across multiple documents as opposed to a single paragraph or sentence."]}
{"question_id": "af948ea91136c700957b438d927f58d9b051c97c", "predicted_answer": "The HotPotQA dataset consists of over 113,000 Wikipedia-based question-answer pairs.", "predicted_evidence": ["We also measure the multi-hopping in terms of SF coverage and reported the results in Table TABREF26 and Table TABREF27. We achieve skyline performance of $80.41$ F1 value on the ground-truth questions of the test dataset of HotPotQA.", "We use the HotPotQA BIBREF11 dataset to evaluate our methods. This dataset consists of over 113k Wikipedia-based question-answer pairs, with each question requiring multi-step reasoning across multiple supporting documents to infer the answer. While there exists other multi-hop datasets BIBREF9, BIBREF10, only HotPotQA dataset provides the sentence-level ground-truth labels to locate the supporting facts in the list of documents. We combine the training set ($90,564$) and development set ($7,405$) and randomly split the resulting data, with 80% for training, 10% for development, 10% for testing.", "The automatic evaluation scores of our proposed method, baselines, and state-of-the-art single-hop question generation model on the HotPotQA test set are shown in Table TABREF26. The performance improvements with our proposed model over the baselines and state-of-the-arts are statistically significant as $(p <0.005)$. For the question-aware supporting fact prediction model (c.f. SECREF21), we obtain the F1 and EM scores of $84.49$ and $44.20$, respectively, on the HotPotQA development dataset. We can not directly compare the result ($21.17$ BLEU-4) on the HotPotQA dataset reported in BIBREF44 as their dataset split is different and they only use the ground-truth supporting facts to generate the questions.", "Our main contributions in this work are: (i). We introduce the problem of multi-hop question generation and propose a multi-task training framework to condition the shared encoder with supporting facts information. (ii). We formulate a novel reward function, multihop-enhanced reward via question-aware supporting fact predictions to enforce the maximum utilization of supporting facts to generate a question; (iii). We introduce an automatic evaluation metric to measure the coverage of supporting facts in the generated question. (iv). Empirical results show that our proposed method outperforms the current state-of-the-art single-hop QG models over several automatic and human evaluation metrics on the HotPotQA dataset.", "where $\\gamma _1$, $\\gamma _2$, and $\\gamma _3$ correspond to the weights of $\\mathcal {L}_{rl}$, $\\mathcal {L}_{ml}$, and $\\mathcal {L}_{sp}$, respectively. In our experiments, we use the same vocabulary for both the encoder and decoder. Our vocabulary consists of the top 50,000 frequent words from the training data. We use the development dataset for hyper-parameter tuning. Pre-trained GloVe embeddings BIBREF34 of dimension 300 are used in the document encoding step. The hidden dimension of all the LSTM cells is set to 512. Answer tagging features and supporting facts position features are embedded to 3-dimensional vectors. The dropout BIBREF35 probability $p$ is set to $0.3$. The beam size is set to 4 for beam search."]}
{"question_id": "a913aa14d4e05cc9d658bf6697fe5b2652589b1b", "predicted_answer": "The labeling schemes used are:\n\n* For constituency parsing: a single label of the form INLINEFORM0 = INLINEFORM1 for each token.\n* For dependency parsing: a single dependency label of the form INLINEFORM2 = INLINEFORM3 for each token.", "predicted_evidence": ["We predict the partial labels from one of the parsing abstractions as main tasks. The partial labels from the other parsing paradigm are used as auxiliary tasks. The loss is computed as INLINEFORM0 = INLINEFORM1 , where INLINEFORM2 is an auxiliary loss and INLINEFORM3 its specific weighting factor. Figure FIGREF17 shows the architecture used in this and the following multi-paradigm model.", "In general, one can observe different range of gains of the models across languages. In terms of uas, the differences between single-task and mtl models span between INLINEFORM0 (Basque) and INLINEFORM1 (Hebrew); for las, INLINEFORM2 and INLINEFORM3 (both for Hebrew); and for F1, INLINEFORM4 (Hebrew) and INLINEFORM5 (Korean). Since the sequence labeling encoding used for dependency parsing heavily relies on PoS tags, the result for a given language can be dependent on the degree of the granularity of its PoS tags.", "For the evaluation on English language we use the English Penn Treebank BIBREF40 , transformed into Stanford dependencies BIBREF41 with the predicted PoS tags as in BIBREF32 .", "For constituency parsing, we use the single-task model by BIBREF10 . The input is the raw sentence and the output for each token a single label of the form INLINEFORM0 = INLINEFORM1 . For dependency parsing we use the model by BIBREF11 to predict a single dependency label of the form INLINEFORM2 = INLINEFORM3 for each token.", "We test different sequence labeling parsers to determine whether there are any benefits in learning across representations. We compare: (i) a single-task model for constituency parsing and another one for dependency parsing, (ii) a multi-task model for constituency parsing (and another for dependency parsing) where each element of the 3-tuple is predicted as a partial label in a separate subtask instead of as a whole, (iii) different mtl models where the partial labels from a specific parsing abstraction are used as auxiliary tasks for the other one, and (iv) an mtl model that learns to produce both abstractions as main tasks."]}
{"question_id": "b065a3f598560fdeba447f0a100dd6c963586268", "predicted_answer": "The parts of their multitask model that are shared are the stacked BiLSTMs that process the sentence. These shared layers are used across all tasks, and then task-dependent feed-forward networks are used on top of them to compute each task's outputs.", "predicted_evidence": ["All tasks are learned as main tasks instead.", "To learn across representations we cast the problem as multi-task learning. mtl enables learning many tasks jointly, encapsulating them in a single model and leveraging their shared representation BIBREF12 , BIBREF22 . In particular, we will use a hard-sharing architecture: the sentence is first processed by stacked bilstms shared across all tasks, with a task-dependent feed-forward network on the top of it, to compute each task's outputs. In particular, to benefit from a specific parsing abstraction we will be using the concept of auxiliary tasks BIBREF23 , BIBREF24 , BIBREF25 , where tasks are learned together with the main task in the mtl setup even if they are not of actual interest by themselves, as they might help to find out hidden patterns in the data and lead to better generalization of the model. For instance, BIBREF26 have shown that semantic parsing benefits from that approach.", "Table TABREF22 shows the speeds (sentences/second) on a single core of a CPU. The d-mtl setup comes at almost no added computational cost, so the very good speed-accuracy tradeoff already provided by the single-task models is improved.", "mtl models that use auxiliary tasks (d-mtl-aux) consistently outperform the single-task models (s-s) in all datasets, both for constituency parsing and for dependency parsing in terms of uas. However, this does not extend to las. This different behavior between uas and las seems to be originated by the fact that 2-task dependency parsing models, which are the basis for the corresponding auxiliary task and mtl models, improve uas but not las with respect to single-task dependency parsing models. The reason might be that the single-task setup excludes unlikely combinations of dependency labels with PoS tags or dependency directions that are not found in the training set, while in the 2-task setup, both components are treated separately, which may be having a negative influence on dependency labeling accuracy.", "In general, one can observe different range of gains of the models across languages. In terms of uas, the differences between single-task and mtl models span between INLINEFORM0 (Basque) and INLINEFORM1 (Hebrew); for las, INLINEFORM2 and INLINEFORM3 (both for Hebrew); and for F1, INLINEFORM4 (Hebrew) and INLINEFORM5 (Korean). Since the sequence labeling encoding used for dependency parsing heavily relies on PoS tags, the result for a given language can be dependent on the degree of the granularity of its PoS tags."]}
{"question_id": "9d963d385bd495a7e193f8a498d64c1612e6c20c", "predicted_answer": "The datasets used are the PTB (Penn Treebank) test set and the SPMRL (Syntactic Parsing of Morphologically Rich Languages) test sets.", "predicted_evidence": ["Table TABREF22 shows the speeds (sentences/second) on a single core of a CPU. The d-mtl setup comes at almost no added computational cost, so the very good speed-accuracy tradeoff already provided by the single-task models is improved.", "Table TABREF30 shows model hyperparameters.", "In general, one can observe different range of gains of the models across languages. In terms of uas, the differences between single-task and mtl models span between INLINEFORM0 (Basque) and INLINEFORM1 (Hebrew); for las, INLINEFORM2 and INLINEFORM3 (both for Hebrew); and for F1, INLINEFORM4 (Hebrew) and INLINEFORM5 (Korean). Since the sequence labeling encoding used for dependency parsing heavily relies on PoS tags, the result for a given language can be dependent on the degree of the granularity of its PoS tags.", "In the following experiments we use two parallel datasets that provide syntactic analyses for both dependency and constituency parsing.", "In addition, Table TABREF19 provides a comparison of the d-mtl-aux models for dependency and constituency parsing against existing models on the PTB test set. Tables TABREF20 and TABREF21 shows the results for various existing models on the SPMRL test sets."]}
{"question_id": "179bc57b7b5231ea6ad3e93993a6935dda679fa2", "predicted_answer": "Yes, they do compare against Reinforcement-Learning (RL) approaches. They mention that RL has been used to optimize non-differentiable metrics, but it has not been very successful, possibly due to the discrepancy between sampling decisions at training time and choosing the highest ranking ones at test time. They also compare their method to RL approaches, such as clark-manning:2016:EMNLP2016, and show that their method provides a viable alternative to using RL and reward rescaling, with a higher absolute improvement in performance.", "predicted_evidence": ["When comparing to clark-manning:2016:EMNLP2016 (the second half of Table TABREF25 ), we can see that the absolute improvement over the baselines (i.e. `heuristic loss' for them and the heuristic cross entropy loss for us) is higher than that of reward rescaling but with much shorter training time: INLINEFORM0 (7 days) and INLINEFORM1 (15 hours) on the CoNLL metric for clark-manning:2016:EMNLP2016 and ours, respectively. It is worth noting that our absolute scores are weaker than these of clark-manning:2016:EMNLP2016, as they build on top of a similar but stronger mention-ranking baseline, which employs deeper neural networks and requires a much larger number of epochs to train (300 epochs, including pretraining).", "When comparing to clark-manning:2016:EMNLP2016 (the second half of Table TABREF25 ), we can see that the absolute improvement over the baselines (i.e. `heuristic loss' for them and the heuristic cross entropy loss for us) is higher than that of reward rescaling but with much shorter training time: INLINEFORM0 (7 days) and INLINEFORM1 (15 hours) on the CoNLL metric for clark-manning:2016:EMNLP2016 and ours, respectively. It is worth noting that our absolute scores are weaker than these of clark-manning:2016:EMNLP2016, as they build on top of a similar but stronger mention-ranking baseline, which employs deeper neural networks and requires a much larger number of epochs to train (300 epochs, including pretraining). For the purpose of illustrating the proposed losses, we started with a simpler model by P15-1137 which requires a much smaller number of epochs, thus faster, to train (20 epochs, including pretraining).", "Using differentiable relaxations of evaluation metrics as in our work is related to a line of research in reinforcement learning where a non-differentiable action-value function is replaced by a differentiable critic BIBREF26 , BIBREF27 . The critic is trained so that it is as close to the true action-value function as possible. This technique is applied to machine translation BIBREF28 where evaluation metrics (e.g., BLUE) are non-differentiable. A disadvantage of using critics is that there is no guarantee that the critic converges to the true evaluation metric given finite training data. In contrast, our differentiable relaxations do not need to train, and the convergence is guaranteed as INLINEFORM0 .", "One way to deal with this challenge is to optimize directly the non-differentiable metrics using reinforcement learning (RL), for example, relying on the REINFORCE policy gradient algorithm BIBREF2 . However, this approach has not been very successful, which, as suggested by clark-manning:2016:EMNLP2016, is possibly due to the discrepancy between sampling decisions at training time and choosing the highest ranking ones at test time. A more successful alternative is using a `roll-out' stage to associate cost with possible decisions, as in clark-manning:2016:EMNLP2016, but it is computationally expensive. Imitation learning BIBREF3 , BIBREF4 , though also exploiting metrics, requires access to an expert policy, with exact policies not directly computable for the metrics of interest.", "Consequently, non-differentiable functions used within the metrics (e.g. the set size function INLINEFORM2 ) become differentiable ( INLINEFORM3 ). Though we assume that the scores of the underlying statistical model can be used to define a probability model, we show that this is not a serious limitation. Specifically, as a baseline we use a probabilistic version of the neural mention-ranking model of P15-1137, which on its own outperforms the original one and achieves similar performance to its global version BIBREF5 . Importantly when we use the introduced differentiable relaxations in training, we observe a substantial gain in performance over our probabilistic baseline. Interestingly, the absolute improvement (+0.52) is higher than the one reported in clark-manning:2016:EMNLP2016 using RL (+0.05) and the one using reward rescaling (+0.37). This suggests that our method provides a viable alternative to using RL and reward rescaling."]}
{"question_id": "a59e86a15405c8a11890db072b99fda3173e5ab2", "predicted_answer": "The training dataset is not explicitly mentioned in the given context. However, it is mentioned that the model requires 20 epochs, including pretraining, to train, which suggests that the training dataset is not extremely large.", "predicted_evidence": ["When comparing to clark-manning:2016:EMNLP2016 (the second half of Table TABREF25 ), we can see that the absolute improvement over the baselines (i.e. `heuristic loss' for them and the heuristic cross entropy loss for us) is higher than that of reward rescaling but with much shorter training time: INLINEFORM0 (7 days) and INLINEFORM1 (15 hours) on the CoNLL metric for clark-manning:2016:EMNLP2016 and ours, respectively. It is worth noting that our absolute scores are weaker than these of clark-manning:2016:EMNLP2016, as they build on top of a similar but stronger mention-ranking baseline, which employs deeper neural networks and requires a much larger number of epochs to train (300 epochs, including pretraining).", "When comparing to clark-manning:2016:EMNLP2016 (the second half of Table TABREF25 ), we can see that the absolute improvement over the baselines (i.e. `heuristic loss' for them and the heuristic cross entropy loss for us) is higher than that of reward rescaling but with much shorter training time: INLINEFORM0 (7 days) and INLINEFORM1 (15 hours) on the CoNLL metric for clark-manning:2016:EMNLP2016 and ours, respectively. It is worth noting that our absolute scores are weaker than these of clark-manning:2016:EMNLP2016, as they build on top of a similar but stronger mention-ranking baseline, which employs deeper neural networks and requires a much larger number of epochs to train (300 epochs, including pretraining). For the purpose of illustrating the proposed losses, we started with a simpler model by P15-1137 which requires a much smaller number of epochs, thus faster, to train (20 epochs, including pretraining).", "To train these resolvers we use AdaGrad BIBREF16 to minimize their loss functions with the learning rate tuned on the development set and with one-document mini-batches. Note that we use the baseline as the initialization point to train the other three resolvers.", "Consequently, non-differentiable functions used within the metrics (e.g. the set size function INLINEFORM2 ) become differentiable ( INLINEFORM3 ). Though we assume that the scores of the underlying statistical model can be used to define a probability model, we show that this is not a serious limitation. Specifically, as a baseline we use a probabilistic version of the neural mention-ranking model of P15-1137, which on its own outperforms the original one and achieves similar performance to its global version BIBREF5 . Importantly when we use the introduced differentiable relaxations in training, we observe a substantial gain in performance over our probabilistic baseline. Interestingly, the absolute improvement (+0.52) is higher than the one reported in clark-manning:2016:EMNLP2016 using RL (+0.05) and the one using reward rescaling (+0.37). This suggests that our method provides a viable alternative to using RL and reward rescaling.", "Using differentiable relaxations of evaluation metrics as in our work is related to a line of research in reinforcement learning where a non-differentiable action-value function is replaced by a differentiable critic BIBREF26 , BIBREF27 . The critic is trained so that it is as close to the true action-value function as possible. This technique is applied to machine translation BIBREF28 where evaluation metrics (e.g., BLUE) are non-differentiable. A disadvantage of using critics is that there is no guarantee that the critic converges to the true evaluation metric given finite training data. In contrast, our differentiable relaxations do not need to train, and the convergence is guaranteed as INLINEFORM0 ."]}
{"question_id": "9489b0ecb643c1fc95c001c65d4e9771315989aa", "predicted_answer": "The dataset used is the English portion of CoNLL 2012 data.", "predicted_evidence": ["We use five most popular metrics,", "We run experiments on the English portion of CoNLL 2012 data BIBREF15 which consists of 3,492 documents in various domains and formats. The split provided in the CoNLL 2012 shared task is used. In all our resolvers, we use not the original features of P15-1137 but their slight modification described in N16-1114 (section 6.1).", "where INLINEFORM0 is called temperature BIBREF14 .", "Let INLINEFORM0 and INLINEFORM1 be the gold-standard entity set and an entity set given by a resolver. Recall that an entity is a set of mentions. The recall and precision of the B INLINEFORM2 metric is computed by: INLINEFORM3", "for evaluation. However, because MUC is the least discriminative metric BIBREF13 , whereas CEAF is slow to compute, out of the five most popular metrics we incorporate into our loss only B INLINEFORM0 . In addition, we integrate LEA, as it has been shown to provide a good balance between discriminativity and interpretability."]}
{"question_id": "b210c3e48c15cdc8c47cf6f4b6eb1c29a1933654", "predicted_answer": "The parent model is trained on French-English, which is a high-resource language pair.", "predicted_evidence": ["The results of this experiment are shown in Table 6 . We get a 4.3 Bleu improvement with an unrelated parent (i.e. French-parent and Uzbek-child), but we get a 6.7 Bleu improvement with a `closely related' parent (i.e. French-parent and French'-child). We conclude that the choice of parent model can have a strong impact on transfer models, and choosing better parents for our low-resource languages (if data for such parents can be obtained) could improve the final results.", "For all of our experiments with low-resource languages we use French as the parent source language and for child source languages we use Hausa, Turkish, Uzbek, and Urdu. The target language is always English. Table 1 shows parallel training data set sizes for the child languages, where the language with the most data has only 1.8m English tokens. For comparison, our parent French-English model uses a training set with 300 million English tokens and achieves 26 Bleu on the development set. Table 1 also shows the SBMT system scores along with the NMT baselines that do not use transfer. There is a large gap between the SBMT and NMT systems without using our transfer method.", "A justification for this approach is that in scenarios where we have limited training data, we need a strong prior distribution over models. The parent model trained on a large amount of bilingual data can be considered an anchor point, the peak of our prior distribution in model space. When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child-model training. In the French-English to Uzbek-English example, as a result of the initialization, the English word embeddings from the parent model are copied, but the Uzbek words are initially mapped to random French embeddings. The English embeddings should be kept but the Uzbek embeddings should be modified during training of the child model. Freezing certain portions of the parent model and fine tuning others can be considered a hard approximation to a tight prior or strong regularization applied to some of the parameters.", "Our experimental results are shown in Table 5 , where we use French and German as parent languages. If we just train a model with no transfer on a small Spanish-English training set we get a Bleu score of 16.4. When using our transfer method using French and German as parent languages, we get Bleu scores of 31.0 and 29.8 respectively. As expected, French is a better parent than German for Spanish, which could be the result of the parent language being more similar to the child language.", "In all the above experiments, only the target input and output embeddings are fixed during training. In this section we analyze what happens when different parts of the model are fixed, in order to see what yields optimal performance. Figure 2 shows a diagram of the components of a sequence-to-sequence model. Table 7 shows how we begin to allow more components of the child NMT model to be trained and see the effect on performance in the model. We see that the optimal setting for transferring from French-English to Uzbek-English in terms of Bleu performance is to allow all of the components of the child model to be trained except for the input and output target embeddings."]}
{"question_id": "00341a46a67d31d36e6dc54d5297626319584891", "predicted_answer": "According to the text, they did experiment with ordinary L2 regularization, but found that it did not significantly improve over the parameter freezing approach they used. They also mention that freezing certain portions of the parent model and fine-tuning others can be considered a hard approximation to a tight prior or strong regularization applied to some of the parameters.", "predicted_evidence": ["A justification for this approach is that in scenarios where we have limited training data, we need a strong prior distribution over models. The parent model trained on a large amount of bilingual data can be considered an anchor point, the peak of our prior distribution in model space. When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child-model training. In the French-English to Uzbek-English example, as a result of the initialization, the English word embeddings from the parent model are copied, but the Uzbek words are initially mapped to random French embeddings. The English embeddings should be kept but the Uzbek embeddings should be modified during training of the child model. Freezing certain portions of the parent model and fine tuning others can be considered a hard approximation to a tight prior or strong regularization applied to some of the parameters. We also experiment with ordinary L2 regularization, but find it does not significantly improve over the parameter freezing described above.", "In all the above experiments, only the target input and output embeddings are fixed during training. In this section we analyze what happens when different parts of the model are fixed, in order to see what yields optimal performance. Figure 2 shows a diagram of the components of a sequence-to-sequence model. Table 7 shows how we begin to allow more components of the child NMT model to be trained and see the effect on performance in the model. We see that the optimal setting for transferring from French-English to Uzbek-English in terms of Bleu performance is to allow all of the components of the child model to be trained except for the input and output target embeddings.", "A justification for this approach is that in scenarios where we have limited training data, we need a strong prior distribution over models. The parent model trained on a large amount of bilingual data can be considered an anchor point, the peak of our prior distribution in model space. When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child-model training. In the French-English to Uzbek-English example, as a result of the initialization, the English word embeddings from the parent model are copied, but the Uzbek words are initially mapped to random French embeddings. The English embeddings should be kept but the Uzbek embeddings should be modified during training of the child model. Freezing certain portions of the parent model and fine tuning others can be considered a hard approximation to a tight prior or strong regularization applied to some of the parameters.", "We analyze the effects of using different parent models, regularizing different parts of the child model and trying different regularization techniques.", "The fact that the two models start from and converge to very different points, yet have similar training set performances, indicates that our architecture and training algorithm are able to reach a good minimum of the training objective regardless of the initialization. However, the training objective seems to have a large basin of models with similar performance and not all of them generalize well to the development set. The transfer model, starting with and staying close to a point known to perform well on a related task, is guided to a final point in the weight space that generalizes to the development set much better."]}
{"question_id": "d0dc6729b689561370b6700b892c9de8871bb44d", "predicted_answer": "According to the text, they constrained training using the parameters by:\n\n* Fixing the target input and output embeddings during training\n* Freezing certain portions of the parent model and fine-tuning others\n* Setting a specific dropout probability (0.5 for child models and 0.2 for the parent model)\n* Setting a specific learning rate (0.5) with a decay rate (0.9) when the development perplexity does not improve\n* Setting a specific minibatch size (128), hidden state size (1000), target vocabulary size (15K), and source vocabulary size (30K)\n* Rescaling the gradient when the gradient norm is greater than 5\n* Initializing parameters within a specific range ([ -0.08, +0.08 ])\n* Training the child models for a fixed number of epochs (100)", "predicted_evidence": ["The fact that the two models start from and converge to very different points, yet have similar training set performances, indicates that our architecture and training algorithm are able to reach a good minimum of the training objective regardless of the initialization. However, the training objective seems to have a large basin of models with similar performance and not all of them generalize well to the development set. The transfer model, starting with and staying close to a point known to perform well on a related task, is guided to a final point in the weight space that generalizes to the development set much better.", "For our NMT system, we use development sets for Hausa, Turkish, Uzbek, and Urdu to tune the learning rate, parameter initialization range, dropout rate and hidden state size for all the experiments. For training we use a minibatch size of 128, hidden state size of 1000, a target vocabulary size of 15K and a source vocabulary size of 30K. The child models are trained with a dropout probability of 0.5 as in dropout. The common parent model is trained with a dropout probability of 0.2. The learning rate used for both child and parents is 0.5 with a decay rate of 0.9 when the development perplexity does not improve. The child models are all trained for 100 epochs. We re-scale the gradient when the gradient norm is greater than 5. The initial parameter range is $[$ -0.08, +0.08 $]$ .", "In all the above experiments, only the target input and output embeddings are fixed during training. In this section we analyze what happens when different parts of the model are fixed, in order to see what yields optimal performance. Figure 2 shows a diagram of the components of a sequence-to-sequence model. Table 7 shows how we begin to allow more components of the child NMT model to be trained and see the effect on performance in the model. We see that the optimal setting for transferring from French-English to Uzbek-English in terms of Bleu performance is to allow all of the components of the child model to be trained except for the input and output target embeddings.", "In Figure 3 we plot learning curves for both a transfer and a non-transfer model on training and development sets. We see that the final training set perplexities for both the transfer and non-transfer model are very similar, but the development set perplexity for the transfer model is much better.", "A justification for this approach is that in scenarios where we have limited training data, we need a strong prior distribution over models. The parent model trained on a large amount of bilingual data can be considered an anchor point, the peak of our prior distribution in model space. When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child-model training. In the French-English to Uzbek-English example, as a result of the initialization, the English word embeddings from the parent model are copied, but the Uzbek words are initially mapped to random French embeddings. The English embeddings should be kept but the Uzbek embeddings should be modified during training of the child model. Freezing certain portions of the parent model and fine tuning others can be considered a hard approximation to a tight prior or strong regularization applied to some of the parameters."]}
{"question_id": "17fd6deb9e10707f9d1b70165dedb045e1889aac", "predicted_answer": "The evaluation metric mentioned in the text is the average F1-score, which is used to evaluate the performance of the end-to-end query generation task.", "predicted_evidence": ["All the experiments were carried out on a machine with an Intel Xeon E3-1225 3.2GHz processor, 32 GB of RAM, and an NVIDIA GTX1080Ti GPU. For the embedding layer, we used random embedding. For each dataset, we performed 5-fold cross-validation with the train set (70%), development set (10%), and test set (20%). The threshold INLINEFORM0 for frequent query substructures is set to 30, the maximum iteration number INLINEFORM1 for merging is set to 2, INLINEFORM2 in Algorithm SECREF22 is set to INLINEFORM3 , the maximum triple number INLINEFORM4 for merged results is set to 5, and the maximum aggregation number INLINEFORM5 is set to 2. Other detailed statistics are shown in Table TABREF33 .", "2. Rank existing query structures. To find an appropriate query structure for the input question, we rank existing query structures ( INLINEFORM0 ) by using a scoring function, see Section SECREF20 .", "The average F1-scores for the end-to-end query generation task are reported in Table TABREF35 . All these results are based on the gold standard entity/relation linking result as input. Our approach SubQG outperformed all the comparative approaches on both datasets. Furthermore, as the results shown in Table TABREF36 , it gained a more significant improvement on complex questions compared with CompQA.", "Rank w/o substructures. We replaced the query substructure prediction and query structure ranking module, by choosing an existing query structure in the training data for the input question, using a BiLSTM multiple classification network.", "In this step, we use a combinational function to score each query structure in the training data for the input question. Since the prediction result for each query substructure is independent, the score for query structure INLINEFORM0 is measured by joint probability, which is DISPLAYFORM0"]}
{"question_id": "c4a3f270e942803dab9b40e5e871a2e8886ce444", "predicted_answer": "Based on the provided context, it appears that formal queries are indeed tree-structured. \n\nA formal query is defined as a pair (V, E), where V is the set of vertices and E is the set of labeled edges. This definition suggests a graph structure, where vertices are connected by edges with labels. \n\nIn a tree structure, each vertex has at most one parent vertex, and edges connect vertices in a hierarchical manner. Although the definition does not explicitly state that the graph is a tree, the use of terms like \"vertices\" and \"edges\" implies a graph structure, which can be a tree.\n\nAdditionally, the examples provided, such as INLINEFORM4 and INLINEFORM7, suggest a hierarchical structure, where edges connect vertices in a way that resembles a tree.\n\nTherefore, based on the provided context, it is reasonable to conclude that formal queries are indeed tree-structured.", "predicted_evidence": ["To classify various queries with similar query intentions and narrow the search space for query generation, we introduce the notion of query structures. A query structure is a set of structurally-equivalent queries. Let INLINEFORM0 and INLINEFORM1 denote two queries. INLINEFORM2 is structurally-equivalent to INLINEFORM3 , denoted by INLINEFORM4 , if and only if there exist two bijections INLINEFORM5 and INLINEFORM6 such that:", "1. Collect query structures. For questions in the training data, we first discover the structurally-equivalent queries, and then extract the set of all query structures, denoted by INLINEFORM0 .", "A formal query (or simply called query) is the structured representation of a natural language question executable on a given KB. Formally, a query is a pair INLINEFORM0 , where INLINEFORM1 denotes the set of vertices, and INLINEFORM2 denotes the set of labeled edges. A vertex can be either a variable, an entity or a literal, and the label of an edge can be either a built-in property or a user-defined one. For simplicity, the set of all edge labels are denoted by INLINEFORM3 . In this paper, the built-in properties include Count, Avg, Max, Min, MaxAtN, MinAtN and IsA (rdf:type), where the former four are used to connect two variables. For example, INLINEFORM4 represents that INLINEFORM5 is the counting result of INLINEFORM6 . MaxAtN and MinAtN take the meaning of Order By in SPARQL BIBREF0 .", "A formal query (or simply called query) is the structured representation of a natural language question executable on a given KB. Formally, a query is a pair INLINEFORM0 , where INLINEFORM1 denotes the set of vertices, and INLINEFORM2 denotes the set of labeled edges. A vertex can be either a variable, an entity or a literal, and the label of an edge can be either a built-in property or a user-defined one. For simplicity, the set of all edge labels are denoted by INLINEFORM3 . In this paper, the built-in properties include Count, Avg, Max, Min, MaxAtN, MinAtN and IsA (rdf:type), where the former four are used to connect two variables. For example, INLINEFORM4 represents that INLINEFORM5 is the counting result of INLINEFORM6 . MaxAtN and MinAtN take the meaning of Order By in SPARQL BIBREF0 . For instance, INLINEFORM7 means Order By Desc INLINEFORM8 Limit 1 Offset 1.", "For many simple questions, two query structures, i.e., INLINEFORM0 INLINEFORM1 and INLINEFORM2 INLINEFORM3 , are sufficient. However, for complex questions, a diversity of query structures exist and some of them share a set of frequently-appeared substructures, each of which delivers a part of the query intention. We give the definition of query substructures as follows."]}
{"question_id": "1faccdc78bbd99320c160ac386012720a0552119", "predicted_answer": "They rely on knowledge bases (KBs) such as DBpedia and Freebase.", "predicted_evidence": ["Knowledge-based question answering (KBQA) aims to answer natural language questions over knowledge bases (KBs) such as DBpedia and Freebase. Formal query generation is an important component in many KBQA systems BIBREF0 , BIBREF1 , BIBREF2 , especially for answering complex questions. Given entity and relation linking results, formal query generation aims to generate correct executable queries, e.g., SPARQL queries, for the input natural language questions. An example question and its formal query are shown in Figure FIGREF1 . Generally speaking, formal query generation is expected to include but not be limited to have the capabilities of (i) recognizing and paraphrasing different kinds of constraints, including triple-level constraints (e.g., \u201cmovies\" corresponds to a typing constraint for the target variable) and higher level constraints (e.g., subgraphs).", "Knowledge-based question answering (KBQA) aims to answer natural language questions over knowledge bases (KBs) such as DBpedia and Freebase. Formal query generation is an important component in many KBQA systems BIBREF0 , BIBREF1 , BIBREF2 , especially for answering complex questions. Given entity and relation linking results, formal query generation aims to generate correct executable queries, e.g., SPARQL queries, for the input natural language questions. An example question and its formal query are shown in Figure FIGREF1 . Generally speaking, formal query generation is expected to include but not be limited to have the capabilities of (i) recognizing and paraphrasing different kinds of constraints, including triple-level constraints (e.g., \u201cmovies\" corresponds to a typing constraint for the target variable) and higher level constraints (e.g., subgraphs). For instance, \u201cthe same ... as\" represents a complex structure shown in the middle of Figure FIGREF1 ; (ii) recognizing and paraphrasing aggregations (e.g., \u201chow many\" corresponds to Count); and (iii) organizing all the above to generate an executable query BIBREF3 , BIBREF4 .", "Rank w/o substructures. We replaced the query substructure prediction and query structure ranking module, by choosing an existing query structure in the training data for the input question, using a BiLSTM multiple classification network.", "1. Collect query structures. For questions in the training data, we first discover the structurally-equivalent queries, and then extract the set of all query structures, denoted by INLINEFORM0 .", "We employed the same datasets as BIBREF3 ( BIBREF3 ) and BIBREF4 ( BIBREF4 ): (i) the large-scale complex question answering dataset (LC-QuAD) BIBREF8 , containing 3,253 questions with non-empty results on DBpedia (2016-04), and (ii) the fifth edition of question answering over linked data (QALD-5) dataset BIBREF9 , containing 311 questions with non-empty results on DBpedia (2015-10). Both datasets are widely used in KBQA studies BIBREF10 , BIBREF6 , and have become benchmarks for some annual KBQA competitions. We did not employ the WebQuestions BIBREF11 dataset, since approximately 85% of its questions are simple."]}
{"question_id": "804466848f4fa1c552f0d971dce226cd18b9edda", "predicted_answer": "According to the text, SubQG recovers from noisy entity linking in several ways:\n\n1. **Ranking query structures first**: SubQG ranks query structures first and considers linking results in the last step, which helps to filter out many error linking results.\n2. **Empty query check**: SubQG performs an empty query check, which helps to eliminate incorrect linking results.\n3. **Domain/range check**: SubQG performs a domain/range check, which also helps to eliminate incorrect linking results.\n4. **Validation**: SubQG performs validation, including grammar check, domain/range check, and empty query check, to ensure that the generated query is correct.\n\nBy using these mechanisms, SubQG is able to recover from noisy entity linking and generate correct answers in 85% of cases, even when the precision of the first output declines by 11.4%.", "predicted_evidence": ["We simulated the real KBQA environment by considering noisy entity/relation linking results. We firstly mixed the correct linking result for each mention with the top-5 candidates generated from EARL BIBREF6 , which is a joint entity/relation linking system with state-of-the-art performance on LC-QuAD. The result is shown in the second row of Table TABREF42 . Although the precision for first output declined 11.4%, in 85% cases we still can generate correct answer in top-5. This is because SubQG ranked query structures first and considered linking results in the last step. Many error linking results can be filtered out by the empty query check or domain/range check.", "4. Grounding and validation. We leverage the query structure ranking result, alongside with the entity/relation linking result from some existing black box systems BIBREF6 to generate executable formal query for the input question. For each query structure, we try all possible combinations of the linking results according to the descending order of the overall linking score, and perform validation including grammar check, domain/range check and empty query check. The first non-empty query passing all validations is considered as the output for SubQG. The grounding and validation results for the example question are shown in the bottom of Figure FIGREF12 .", "The structure of the network is shown in Figure FIGREF14 . Before the input question is fed into the network, we replace all entity mentions with INLINEFORM0 Entity INLINEFORM1 using EARL BIBREF6 , to enhance the generalization ability. Given the question word sequence { INLINEFORM2 }, we first use a word embedding matrix to convert the original sequence into word vectors { INLINEFORM3 }, followed by a BiLSTM network to generate the context-sensitive representation { INLINEFORM4 } for each word, where DISPLAYFORM0", "Grounding errors (29%) occurred when SubQG generated wrong queries with correct query structures. For example, for the question \u201cWas Kevin Rudd the prime minister of Julia Gillard\", SubQG cannot distinguish INLINEFORM0 from INLINEFORM1 INLINEFORM2 , since both triples exist in DBpedia. We believe that extra training data are required for fixing this problem.", "Query structure errors (71%) occurred due to multiple reasons. Firstly, 21% of error cases have entity mentions that are not correctly detected before query substructure prediction, which highly influenced the prediction result. Secondly, in 39% of the cases a part of substructure predictors provided wrong prediction, which led to wrong structure ranking results. Finally, in the remaining 11% of the cases the correct query structure did not appear in the training data, and they cannot be generated by merging substructures."]}
{"question_id": "8d683d2e1f46626ceab60ee4ab833b50b346c29e", "predicted_answer": "The authors evaluate their approach on two datasets:\n\n1. LC-QuAD (Large-scale Complex Question Answering Dataset), which contains 3,253 questions with non-empty results on DBpedia (2016-04).\n2. QALD-5 (Fifth edition of Question Answering over Linked Data), which contains 311 questions with non-empty results on DBpedia (2015-10).", "predicted_evidence": ["We employed the same datasets as BIBREF3 ( BIBREF3 ) and BIBREF4 ( BIBREF4 ): (i) the large-scale complex question answering dataset (LC-QuAD) BIBREF8 , containing 3,253 questions with non-empty results on DBpedia (2016-04), and (ii) the fifth edition of question answering over linked data (QALD-5) dataset BIBREF9 , containing 311 questions with non-empty results on DBpedia (2015-10). Both datasets are widely used in KBQA studies BIBREF10 , BIBREF6 , and have become benchmarks for some annual KBQA competitions. We did not employ the WebQuestions BIBREF11 dataset, since approximately 85% of its questions are simple.", "We employed the same datasets as BIBREF3 ( BIBREF3 ) and BIBREF4 ( BIBREF4 ): (i) the large-scale complex question answering dataset (LC-QuAD) BIBREF8 , containing 3,253 questions with non-empty results on DBpedia (2016-04), and (ii) the fifth edition of question answering over linked data (QALD-5) dataset BIBREF9 , containing 311 questions with non-empty results on DBpedia (2015-10). Both datasets are widely used in KBQA studies BIBREF10 , BIBREF6 , and have become benchmarks for some annual KBQA competitions. We did not employ the WebQuestions BIBREF11 dataset, since approximately 85% of its questions are simple. Also, we did not employ the ComplexQuestions BIBREF0 and ComplexWebQuestions BIBREF12 dataset, since the existing works on these datasets have not reported the formal query generation result, and it is difficult to separate the formal query generation component from the end-to-end KBQA systems in these works.", "where INLINEFORM0 denotes the set of training data.", "The results on QALD-5 dataset is not as high as the result on LC-QuAD. This is because QALD-5 contains 11% of very difficult questions, requiring complex filtering conditions such as Regex and numerical comparison. These questions are currently beyond our approach's ability. Also, the size of training data is significant smaller.", "All the experiments were carried out on a machine with an Intel Xeon E3-1225 3.2GHz processor, 32 GB of RAM, and an NVIDIA GTX1080Ti GPU. For the embedding layer, we used random embedding. For each dataset, we performed 5-fold cross-validation with the train set (70%), development set (10%), and test set (20%). The threshold INLINEFORM0 for frequent query substructures is set to 30, the maximum iteration number INLINEFORM1 for merging is set to 2, INLINEFORM2 in Algorithm SECREF22 is set to INLINEFORM3 , the maximum triple number INLINEFORM4 for merged results is set to 5, and the maximum aggregation number INLINEFORM5 is set to 2. Other detailed statistics are shown in Table TABREF33 ."]}
{"question_id": "5ae005917efc17a505ba1ba5e996c4266d6c74b6", "predicted_answer": "No, they did not use the same dataset to train. The Skip-gram model was trained on a 100 billion word corpus from Google News, while the SubGram model was trained on the same training data as the third column of the table, which is a smaller dataset.", "predicted_evidence": ["When comparing models on our test set (Table TABREF19 ), we see that given the same training set, SubGram significantly outperforms Skip-gram model (22.4% vs. 9.7%). The performance of Skip-gram trained on the much larger dataset is higher (43.5%) and it would be interesting to see the SubGram model, if we could get access to such training data. Note however, that the Rule-based baseline is significantly better on both test sets.", "Table TABREF18 and Table TABREF19 report the results. The first column shows the rule-based approach. The column \u201cReleased Skip-gram\u201d shows results of the model released by Mikolov and was trained on a 100 billion word corpus from Google News and generates 300 dimensional vector representation. The third column shows Skip-gram model trained on our training data, the same data as used for the training of the SubGram. Last column shows the results obtained from our SubGram model.", "The Skip-gram model is a classic NN, where activation functions are removed and hierarchical soft-max BIBREF8 is used instead of soft-max normalization. The input representation is one-hot so the activation function is not needed on hidden layer, there is nothing to be summed up. This way, the model is learned much faster than comparable non-linear NNs and lends itself to linear vector operations possibly useful for finding semantically or syntactically related words.", "Comparing Skip-gram and SubGram on the original test set (Table TABREF18 ), we see that our SubGram outperforms Skip-gram in several morpho-syntactic question sets but over all performs similarly (42.5% vs. 42.3%). On the other hand, it does not capture the tested semantic relations at all, getting a zero score on average.", "We consider only the 141K most frequent word forms to simplify the training. The remaining word forms fall out of vocabulary (OOV), so the original Skip-gram cannot provide them with any vector representation. Our SubGram relies on known substrings and always provides at least some approximation."]}
{"question_id": "72c04eb3fc323c720f7f8da75c70f09a35abf3e6", "predicted_answer": "The question is not answered in the given context. The context appears to be discussing a method for handling out-of-vocabulary (OOV) words in a Skip-gram model, but it does not mention any specific gains obtained.", "predicted_evidence": ["past-tense: Remove ing and add ed at the end of the verb.", "where INLINEFORM0 is the number of substrings of the word INLINEFORM1 .", "The accuracy is computed as the number of correctly answered questions divided by the total number of questions in the set. Because the Skip-gram cannot answer questions containing OOV words, we also provide results with such questions excluded from the test set (scores in brackets).", "plural: Add s at the end of the word.", "We consider only the 141K most frequent word forms to simplify the training. The remaining word forms fall out of vocabulary (OOV), so the original Skip-gram cannot provide them with any vector representation. Our SubGram relies on known substrings and always provides at least some approximation."]}
{"question_id": "0715d510359eb4c851cf063c8b3a0c61b8a8edc0", "predicted_answer": "The extractive technique used for summarization mentioned in the text is:\n\n* TextRank\n* Greedy KL\n* Lead baselines", "predicted_evidence": ["In this paper, we begin by discussing how this task relates to the current state of text summarization and similar tasks in Section SECREF2 . We then introduce the novel dataset and provide details on the level of abstraction, compression, and readability in Section SECREF3 . Next, we provide results and analysis on the performance of extractive summarization baselines on our data in Section SECREF5 . Finally, we discuss the potential for unsupervised systems in this genre in Section SECREF6 .", "In initial experimentation using this dataset, we employ popular unsupervised extractive summarization models such as TextRank BIBREF12 and Greedy KL BIBREF13 , as well as lead baselines. We show that such methods do not perform well on this dataset when compared to the same methods on DUC 2002. These results highlight the fact that this is a very challenging task. As there is not currently a dataset in this domain large enough for supervised methods, we suggest the use of methods developed for simplification and/or style transfer.", "We present our legal dataset as a test set for contracts summarization. In this section, we report baseline performances of unsupervised, extractive methods as most recent supervised abstractive summarization methods, e.g., BIBREF33 , BIBREF14 , would not have enough training data in this domain. We chose to look at the following common baselines:", "In this paper, we propose the task of summarizing legal documents in plain English and present an initial evaluation dataset for this task. We gather our dataset from online sources dedicated to explaining sections of contracts in plain English and manually verify the quality of the summaries. We show that our dataset is highly abstractive and that the summaries are much simpler to read. This task is challenging, as popular unsupervised extractive summarization methods do not perform well on this dataset and, as discussed in section SECREF6 , current methods that address the change in register are mostly supervised as well. We call for the development of resources for unsupervised simplification and style transfer in this domain.", "Rather than attempt to summarize an entire document, these sources summarize each document at the section level. In this way, the reader can reference the more detailed text if need be. The summaries in this dataset are reviewed for quality by the first author, who has 3 years of professional contract drafting experience."]}
{"question_id": "4e106b03cc2f54373e73d5922e97f7e5e9bf03e4", "predicted_answer": "The dataset contains 446 sets of parallel text, sourced from 166 documents by 122 companies for TOS;DR, and 84 sets sourced from 9 documents for TL;DRLegal.", "predicted_evidence": ["Furthermore, as shown in Figure FIGREF15 , the dataset is very compressive, with a mean compression rate of 0.31 (std 0.23). The original texts have a mean of 3.6 (std 3.8) sentences per document and a mean of 105.6 (std 147.8) words per document. The reference summaries have a mean of 1.2 (std 0.6) sentences per document, and a mean of 17.2 (std 11.8) words per document.", "TOS;DR tends to focus on topics related to user data and privacy. We scraped 421 sets of parallel text sourced from 166 documents by 122 companies. Each set consists of a portion of an agreement text (e.g., Terms of Use, Privacy Policy, Terms of Service) and 1-3 human-written summaries.", "To understand the level of abstraction of the proposed dataset, we first calculate the number of n-grams that appear only in the reference summaries and not in the original texts they summarize BIBREF14 , BIBREF27 . As shown in Figure FIGREF14 , 41.4% of words in the reference summaries did not appear in the original text. Additionally, 78.5%, 88.4%, and 92.3% of 2-, 3-, and 4-grams in the reference summaries did not appear in the original text. When compared to a standard abstractive news dataset also shown in the graph (DUC 2002), the legal dataset is significantly more abstractive.", "TL;DRLegal focuses mostly on software licenses, however, we only scraped documents related to specific companies rather than generic licenses (i.e. Creative Commons, etc). The scraped data consists of 84 sets sourced from 9 documents: Pokemon GO Terms of Service, TLDRLegal Terms of Service, Minecraft End User Licence Agreement, YouTube Terms of Service, Android SDK License Agreement (June 2014), Google Play Game Services (May 15th, 2013), Facebook Terms of Service (Statement of Rights and Responsibilities), Dropbox Terms of Service, and Apple Website Terms of Service.", "The dataset we propose contains 446 sets of parallel text. We show the level of abstraction through the number of novel words in the reference summaries, which is significantly higher than the abstractive single-document summaries created for the shared tasks of the Document Understanding Conference (DUC) in 2002 BIBREF11 , a standard dataset used for single document news summarization. Additionally, we utilize several common readability metrics to show that there is an average of a 6 year reading level difference between the original documents and the reference summaries in our legal dataset."]}
{"question_id": "f8edc911f9e16559506f3f4a6bda74cde5301a9a", "predicted_answer": "The text does not provide a specific answer to the question of how much the models outperform the baseline in terms of a numerical value. It only mentions that there is an improvement in terms of precision and NMI, but a degradation of the recall.", "predicted_evidence": ["We then evaluated the effect of the informative prior on the acoustic unit discovery (Table TABREF23 ). On all 4 combinations (2 features sets INLINEFORM0 2 models) we observe an improvement in terms of precision and NMI but a degradation of the recall. This result is encouraging since the informative prior was trained on English data (TIMIT) which is very different from Mboshi. Indeed, this suggests that even speech from an unrelated language can be of some help in the design of an ASR for a very low resource language. Finally, similarly to the SVAE/HMM case described above, we found that the degradation of the recall is due to longer units discovered for models with an informative prior (numbers omitted due to lack of space).", "where INLINEFORM0 is the Kullback-Leibler divergence. Then, the posterior distribution of the parameters given the target data INLINEFORM1 can be estimated by optimizing the evidence of the target data INLINEFORM2 : DISPLAYFORM0", "To evaluate our work we measured how the discovered units compared to the forced aligned phones in term of segmentation and information. The accuracy of the segmentation was measured in term of Precision, Recall and F-score. If a unit boundary occurs at the same time (+/- 10ms) of an actual phone boundary it is considered as a true positive, otherwise it is considered to be a false positive. If no match is found with a true phone boundary, this is considered to be a false negative. The consistency of the units was evaluated in term of normalized mutual information (NMI - see BIBREF1 , BIBREF3 , BIBREF5 for details) which measures the statistical dependency between the units and the forced aligned phones. A NMI of 0 % means that the units are completely independent of the phones whereas a NMI of 100 % indicates that the actual phones could be retrieved without error given the sequence of discovered units.", "Practically, the computation of the informative prior as well as the final posterior distribution is intractable and we seek for an approximation by means of the well known Variational Bayes Inference BIBREF12 . The approximate informative prior INLINEFORM0 is estimated by optimizing the variational lower bound of the evidence of the prior data INLINEFORM1 : DISPLAYFORM0", "Indeed, Variational Bayesian Inference algorithm converges only to a local optimum of the objective function and is therefore dependent of the initialization. We believe the second explanation is the more likely since, as we shall see shortly, the best results in term of word segmentation and NMI are eventually obtained with the MBN features when the inference is done with the informative prior. Next, we compared the HMM and the SVAE models when trained with an uninformative prior (lines with \"Inf. Prior\" set to \"no\" in Table TABREF23 ). The SVAE significantly improves the NMI and the precision showing that it extracts more consistent units than the HMM model. However, it also degrades the segmentation in terms of recall. We further investigated this behavior by looking at the duration of the units found by both models compared to the true phones (Table TABREF22 ). We observe that the SVAE model favors longer units than the HMM model hence leading to fewer boundaries and consequently smaller recall."]}
{"question_id": "8c288120139615532838f21094bba62a77f92617", "predicted_answer": "The length of the datasets is not explicitly mentioned in the given context. However, it is mentioned that the evaluation metrics used include precision, recall, F-score, and normalized mutual information (NMI), which suggests that the datasets are likely to be of varying lengths. Additionally, the context mentions the use of lattices as input for the word segmentation task, which implies that the datasets may be complex and contain multiple sequences of acoustic units.", "predicted_evidence": ["We then evaluated the effect of the informative prior on the acoustic unit discovery (Table TABREF23 ). On all 4 combinations (2 features sets INLINEFORM0 2 models) we observe an improvement in terms of precision and NMI but a degradation of the recall. This result is encouraging since the informative prior was trained on English data (TIMIT) which is very different from Mboshi. Indeed, this suggests that even speech from an unrelated language can be of some help in the design of an ASR for a very low resource language. Finally, similarly to the SVAE/HMM case described above, we found that the degradation of the recall is due to longer units discovered for models with an informative prior (numbers omitted due to lack of space).", "To evaluate our work we measured how the discovered units compared to the forced aligned phones in term of segmentation and information. The accuracy of the segmentation was measured in term of Precision, Recall and F-score. If a unit boundary occurs at the same time (+/- 10ms) of an actual phone boundary it is considered as a true positive, otherwise it is considered to be a false positive. If no match is found with a true phone boundary, this is considered to be a false negative. The consistency of the units was evaluated in term of normalized mutual information (NMI - see BIBREF1 , BIBREF3 , BIBREF5 for details) which measures the statistical dependency between the units and the forced aligned phones. A NMI of 0 % means that the units are completely independent of the phones whereas a NMI of 100 % indicates that the actual phones could be retrieved without error given the sequence of discovered units.", "In order to provide an extrinsic metric to evaluate the quality of the acoustic units discovered by our different methods, we performed an unsupervised word segmentation task on the acoustic units sequences, and evaluated the accuracy of the discovered word boundaries. We also wanted to experiment using lattices as an input for the word segmentation task, instead of using single sequences of units, so as to better mitigate the uncertainty of the AUD task and provide a companion metric that would be more robust to noise. A model capable of performing word segmentation both on lattices and text sequences was introduced by BIBREF6 . Building on the work of BIBREF17 , BIBREF18 they combine a nested hierarchical Pitman-Yor language model with a Weighted Finite State Transducer approach. Both for lattices and acoustic units sequences, we use the implementation of the authors with a bigram language model and a unigram character model. Word discovery is evaluated using the Boundary metric from the Zero Resource Challenge 2017 BIBREF20 and BIBREF21 .", "We also wanted to experiment using lattices as an input for the word segmentation task, instead of using single sequences of units, so as to better mitigate the uncertainty of the AUD task and provide a companion metric that would be more robust to noise. A model capable of performing word segmentation both on lattices and text sequences was introduced by BIBREF6 . Building on the work of BIBREF17 , BIBREF18 they combine a nested hierarchical Pitman-Yor language model with a Weighted Finite State Transducer approach. Both for lattices and acoustic units sequences, we use the implementation of the authors with a bigram language model and a unigram character model. Word discovery is evaluated using the Boundary metric from the Zero Resource Challenge 2017 BIBREF20 and BIBREF21 . This metric measures the quality of a word segmentation and the discovered boundaries with respect to a gold corpus (Precision, Recall and F-score are computed).", "We have conducted an analysis of the state-of-the-art Bayesian approach for acoustic unit discovery on a real case of low-resource language. This analysis was focused on the quality of the discovered units compared to the gold standard phone alignments. Outcomes of the analysis are i) the combination of neural network and Bayesian model (SVAE) yields a significant improvement in the AUD in term of consistency ii) Bayesian models can naturally embed information from a resourceful language and consequently improve the consistency of the discovered units. Finally, we hope this work can serve as a baseline for future research on unsupervised acoustic unit discovery in very low resource scenarios."]}
{"question_id": "a464052fd11af1d2d99e407c11791269533d43d1", "predicted_answer": "The Bayesian model being trained is a Variational Bayes Inference model for Acoustic Unit Discovery (AUD), specifically a Bayesian approach that uses an informative prior to incorporate knowledge from a resourceful language to improve the discovery of phone-like units in a target language.", "predicted_evidence": ["which is the same as Eq. EQREF8 but for the distribution of the acoustic parameters which is based on the data of the resourceful language. In contrast of the term uninformative prior we denote INLINEFORM0 as an informative prior. As illustrated by Eq. EQREF9 , a characteristic of Bayesian inference is that it naturally leads to a sequential inference. Therefore, model training can be summarized as:", "Bayesian Inference differs from other machine learning techniques by introducing a distribution INLINEFORM0 over the parameters of the model. A major concern in Bayesian Inference is usually to define a prior that makes as little assumption as possible. Such a prior is usually known as uninformative prior. Having a completely uninformative prior has the practical advantage that the prior distribution will have a minimal impact on the outcome of the inference leading to a model which bases its prediction purely and solely on the data. In the present work, we aim at the opposite behavior, we wish our AUD model to learn phone-like units from the unlabeled speech data of a target language given the knowledge that was previously accumulated from another resourceful language. More formally, the original AUD model training consists in estimate the a posteriori distribution of the parameters given the unlabeled speech data of a target language INLINEFORM1 : DISPLAYFORM0", "Note that when the model is trained with an uninformative prior the loss function is the as in Eq. EQREF13 but with INLINEFORM0 instead of the INLINEFORM1 . For the case of the uninformative prior, the Variational Bayes Inference was initialized as described in BIBREF1 . In the informative prior case, we initialized the algorithm by setting INLINEFORM2 .", "Practically, the computation of the informative prior as well as the final posterior distribution is intractable and we seek for an approximation by means of the well known Variational Bayes Inference BIBREF12 . The approximate informative prior INLINEFORM0 is estimated by optimizing the variational lower bound of the evidence of the prior data INLINEFORM1 : DISPLAYFORM0", "We have conducted an analysis of the state-of-the-art Bayesian approach for acoustic unit discovery on a real case of low-resource language. This analysis was focused on the quality of the discovered units compared to the gold standard phone alignments. Outcomes of the analysis are i) the combination of neural network and Bayesian model (SVAE) yields a significant improvement in the AUD in term of consistency ii) Bayesian models can naturally embed information from a resourceful language and consequently improve the consistency of the discovered units. Finally, we hope this work can serve as a baseline for future research on unsupervised acoustic unit discovery in very low resource scenarios."]}
{"question_id": "5f6c1513cbda9ae711bc38df08fe72e3d3028af2", "predicted_answer": "The low-resource language considered in this context is Mboshi, a Bantu language spoken in Congo-Brazzaville.", "predicted_evidence": ["We used the Mboshi5K corpus BIBREF13 as a test set for all the experiments reported here. Mboshi (Bantu C25) is a typical Bantu language spoken in Congo-Brazzaville. It is one of the languages documented by the BULB (Breaking the Unwritten Language Barrier) project BIBREF14 . This speech dataset was collected following a real language documentation scenario, using Lig_Aikuma, a mobile app specifically dedicated to fieldwork language documentation, which works both on android powered smartphones and tablets BIBREF15 . The corpus is multilingual (5130 Mboshi speech utterances aligned to French text) and contains linguists' transcriptions in Mboshi (in the form of a non-standard graphemic form close to the language phonology). It is also enriched with automatic forced-alignment between speech and transcriptions. The dataset is made available to the research community. More details on this corpus can be found in BIBREF13 .", "We have conducted an analysis of the state-of-the-art Bayesian approach for acoustic unit discovery on a real case of low-resource language. This analysis was focused on the quality of the discovered units compared to the gold standard phone alignments. Outcomes of the analysis are i) the combination of neural network and Bayesian model (SVAE) yields a significant improvement in the AUD in term of consistency ii) Bayesian models can naturally embed information from a resourceful language and consequently improve the consistency of the discovered units. Finally, we hope this work can serve as a baseline for future research on unsupervised acoustic unit discovery in very low resource scenarios.", "We then evaluated the effect of the informative prior on the acoustic unit discovery (Table TABREF23 ). On all 4 combinations (2 features sets INLINEFORM0 2 models) we observe an improvement in terms of precision and NMI but a degradation of the recall. This result is encouraging since the informative prior was trained on English data (TIMIT) which is very different from Mboshi. Indeed, this suggests that even speech from an unrelated language can be of some help in the design of an ASR for a very low resource language. Finally, similarly to the SVAE/HMM case described above, we found that the degradation of the recall is due to longer units discovered for models with an informative prior (numbers omitted due to lack of space).", "In order to provide an extrinsic metric to evaluate the quality of the acoustic units discovered by our different methods, we performed an unsupervised word segmentation task on the acoustic units sequences, and evaluated the accuracy of the discovered word boundaries. We also wanted to experiment using lattices as an input for the word segmentation task, instead of using single sequences of units, so as to better mitigate the uncertainty of the AUD task and provide a companion metric that would be more robust to noise. A model capable of performing word segmentation both on lattices and text sequences was introduced by BIBREF6 . Building on the work of BIBREF17 , BIBREF18 they combine a nested hierarchical Pitman-Yor language model with a Weighted Finite State Transducer approach. Both for lattices and acoustic units sequences, we use the implementation of the authors with a bigram language model and a unigram character model. Word discovery is evaluated using the Boundary metric from the Zero Resource Challenge 2017 BIBREF20 and BIBREF21 .", "The parameters are divided into two subgroups INLINEFORM0 where INLINEFORM1 are the global parameters of the model, and INLINEFORM2 are the latent variables which, in our case, correspond to the sequences of acoustic units. The global parameters are separated into two independent subsets : INLINEFORM3 , corresponding to the acoustic parameters ( INLINEFORM4 ) and the \"phonotactic\" language model parameters ( INLINEFORM5 ). Replacing INLINEFORM6 and following the conditional independence of the variable induced by the model (see BIBREF1 for details) leads to: DISPLAYFORM0"]}
{"question_id": "130d73400698e2b3c6860b07f2e957e3ff022d48", "predicted_answer": "Cluster purity is measured using the V-measure.", "predicted_evidence": ["Figure FIGREF30 shows the V-measures of the clusters of the most biased words in Wikipedia for each embedding. Gigaword patterns similarly (see appendix). Figure FIGREF31 shows example tSNE projections for the Gigaword embeddings (\u201c$\\mathrm {V}$\u201d refers to their V-measures; these examples were chosen as they represent the best results achieved by BIBREF1's (BIBREF1) method, BIBREF5's (BIBREF5) method, and our new names variant). On both corpora, the new nCDA and nCDS techniques have significantly lower purity of biased-word cluster than all other evaluated mitigation techniques (0.420 for nCDS on Gigaword, which corresponds to a reduction of purity by 58% compared to the unmitigated embedding, and 0.609 (39%) on Wikipedia).", "Figure FIGREF30 shows the V-measures of the clusters of the most biased words in Wikipedia for each embedding. Gigaword patterns similarly (see appendix). Figure FIGREF31 shows example tSNE projections for the Gigaword embeddings (\u201c$\\mathrm {V}$\u201d refers to their V-measures; these examples were chosen as they represent the best results achieved by BIBREF1's (BIBREF1) method, BIBREF5's (BIBREF5) method, and our new names variant). On both corpora, the new nCDA and nCDS techniques have significantly lower purity of biased-word cluster than all other evaluated mitigation techniques (0.420 for nCDS on Gigaword, which corresponds to a reduction of purity by 58% compared to the unmitigated embedding, and 0.609 (39%) on Wikipedia). nWED70's V-Measure is significantly higher than either of the other Names variants (reduction of 11% on Gigaword, only 1% on Wikipedia), suggesting that the success of nCDS and nCDA is not merely due to their larger list of gender-words.", "Figure FIGREF33 shows the results of the second test of indirect bias, and reports the accuracy of a classifier trained to reclassify previously gender biased words on the Wikipedia embeddings (Gigaword patterns similarly). These results reinforce the finding of the clustering experiment: once again, nCDS outperforms all other methods significantly on both corpora ($p<0.01$), although it should be noted that the successful reclassification rate remains relatively high (e.g. 88.9% on Wikipedia).", "We have replicated two state-of-the-art bias mitigation techniques, WED and CDA, on two large corpora, Wikipedia and the English Gigaword. In our empirical comparison, we found that although both methods mitigate direct gender bias and maintain the interpretability of the space, WED failed to maintain a robust representation of gender (the best variants had an error rate of 23% average when drawing non-biased analogies, suggesting that too much gender information was removed). A new variant of CDA we propose (the Names Intervention) is the only to successfully mitigate indirect gender bias: following its application, previously biased words are significantly less clustered according to gender, with an average of 49% reduction in cluster purity when clustering the most biased words.", "The quality of a space is traditionally measured by how well it replicates human judgements of word similarity. The SimLex-999 dataset BIBREF17 provides a ground-truth measure of similarity produced by 500 native English speakers. Similarity scores in an embedding are computed as the cosine angle between word-vector pairs, and Spearman correlation between embedding and human judgements are reported. We measure correlative significance at $\\alpha = 0.01$."]}
{"question_id": "7e9aec2bdf4256c6249cad9887c168d395b35270", "predicted_answer": "The previous state of the art for bias mitigation includes two popular methods: WED (Word Embedding Debiasing) and CDA (Counterfactual Data Augmentation). Specifically, the method introduced by Bolukbasi et al. (2016) measures the genderedness of words by the extent to which they point in a gender direction, assuming a linear subspace $B \\subset \\mathbb {R}^d$ that contains most of the gender bias in the space of word embeddings.", "predicted_evidence": ["The measurement and mitigation of gender bias relies on the chosen operationalisation of gender bias. As a direct consequence, how researchers choose to operationalise bias determines both the techniques at one's disposal to mitigate the bias, as well as the yardstick by which success is determined.", "We note that nullifying indirect bias associations entirely is not necessarily the goal of debiasing, since some of these may result from causal links in the domain. For example, whilst associations between man and engineer and between man and car are each stereotypic (and thus could be considered examples of direct bias), an association between engineer and car might well have little to do with gender bias, and so should not be mitigated.", "We prefer the philosophy of CDA over WED as it makes fewer assumptions about the operationalisation of the bias it is meant to mitigate.", "We have replicated two state-of-the-art bias mitigation techniques, WED and CDA, on two large corpora, Wikipedia and the English Gigaword. In our empirical comparison, we found that although both methods mitigate direct gender bias and maintain the interpretability of the space, WED failed to maintain a robust representation of gender (the best variants had an error rate of 23% average when drawing non-biased analogies, suggesting that too much gender information was removed). A new variant of CDA we propose (the Names Intervention) is the only to successfully mitigate indirect gender bias: following its application, previously biased words are significantly less clustered according to gender, with an average of 49% reduction in cluster purity when clustering the most biased words.", "One popular method for the mitigation of gender bias, introduced by DBLP:conf/nips/BolukbasiCZSK16, measures the genderedness of words by the extent to which they point in a gender direction. Suppose we embed our words into $\\mathbb {R}^d$. The fundamental assumption is that there exists a linear subspace $B \\subset \\mathbb {R}^d$ that contains (most of) the gender bias in the space of word embeddings. (Note that $B$ is a direction when it is a single vector.) We term this assumption the gender subspace hypothesis."]}
{"question_id": "1acf06105f6c1930f869347ef88160f55cbf382b", "predicted_answer": "In the Names Intervention, names are paired by fixedly associating pairs of names for swapping, taking into account both name frequency and the degree of gender-specificity. This is done to ensure that the pairing is balanced and does not negate the gender intervention or distort the overall frequency distribution of names.", "predicted_evidence": ["Future work could extend the Names Intervention to names from other languages beyond the US-based gazetteer used here. Our method only allows for there to be an equal number of male and female names, but if this were not the case one ought to explore the possibility of a many-to-one mapping, or perhaps a probablistic approach (though difficulties would be encountered sampling simultaneously from two distributions, frequency and gender-specificity). A mapping between nicknames (not covered by administrative sources) and formal names could be learned from a corpus for even wider coverage, possibly via the intermediary of coreference chains. Finally, given that names have been used in psychological literature as a proxy for race (e.g. BIBREF12), the Names Intervention could also be used to mitigate racial biases (something which, to the authors' best knowledge, has never been attempted), but finding pairings could prove problematic.", "Future work could extend the Names Intervention to names from other languages beyond the US-based gazetteer used here. Our method only allows for there to be an equal number of male and female names, but if this were not the case one ought to explore the possibility of a many-to-one mapping, or perhaps a probablistic approach (though difficulties would be encountered sampling simultaneously from two distributions, frequency and gender-specificity). A mapping between nicknames (not covered by administrative sources) and formal names could be learned from a corpus for even wider coverage, possibly via the intermediary of coreference chains. Finally, given that names have been used in psychological literature as a proxy for race (e.g. BIBREF12), the Names Intervention could also be used to mitigate racial biases (something which, to the authors' best knowledge, has never been attempted), but finding pairings could prove problematic. It is important that other work looks into operationalising bias beyond the subspace definition proposed by BIBREF1, as it is becoming increasingly evident that gender bias is not linear in embedding space.", "We fixedly associate pairs of names for swapping, thus expanding BIBREF5's short list of gender pairs vastly. Clearly both name frequency and the degree of gender-specificity are relevant to this bipartite matching. If only frequency were considered, a more gender-neutral name (e.g. Taylor) could be paired with a very gender-specific name (e.g. John), which would negate the gender intervention in many cases (namely whenever a male occurrence of Taylor is transformed into John, which would also result in incorrect pronouns, if present). If, on the other hand, only the degree of gender-specificity were considered, we would see frequent names (like James) being paired with far less frequent names (like Sybil), which would distort the overall frequency distribution of names.", "We fixedly associate pairs of names for swapping, thus expanding BIBREF5's short list of gender pairs vastly. Clearly both name frequency and the degree of gender-specificity are relevant to this bipartite matching. If only frequency were considered, a more gender-neutral name (e.g. Taylor) could be paired with a very gender-specific name (e.g. John), which would negate the gender intervention in many cases (namely whenever a male occurrence of Taylor is transformed into John, which would also result in incorrect pronouns, if present). If, on the other hand, only the degree of gender-specificity were considered, we would see frequent names (like James) being paired with far less frequent names (like Sybil), which would distort the overall frequency distribution of names. This might also result in the retention of a gender signal: for instance, swapping a highly frequent male name with a rare female name might simply make the rare female name behave as a new link between masculine contexts (instead of the original male name), as it rarely appears in female contexts.", "Our main technical contribution in this paper is to provide a method for better counterfactual augmentation, which is based on bipartite-graph matching of names. Instead of Lu et. al's (2018) solution of not treating words which corefer to proper nouns in order to maintain grammaticality, we propose an explicit treatment of first names. This is because we note that as a result of not swapping the gender of words which corefer with proper nouns, CDA could in fact reinforce certain biases instead of mitigate them. Consider the sentence `Tom ...He is a successful and powerful executive.' Since he and Tom corefer, the counterfactual corpus copy will not replace he with she in this instance, and as the method involves a duplication of text, this would result in a stronger, not weaker, association between he and gender-stereotypic concepts present like executive."]}
{"question_id": "9ce90f4132b34a328fa49a63e897f376a3ad3ca8", "predicted_answer": "The tasks that quantify embedding quality are:\n\n1. Spearman correlation between embedding and human judgements of word similarity (using SimLex-999 dataset)\n2. Word similarity task\n3. Sentiment classification task (using Stanford Large Movie Review dataset)\n4. Novel task: quantifying how well the embedding spaces capture an understanding of gender using non-biased analogies.", "predicted_evidence": ["The quality of a space is traditionally measured by how well it replicates human judgements of word similarity. The SimLex-999 dataset BIBREF17 provides a ground-truth measure of similarity produced by 500 native English speakers. Similarity scores in an embedding are computed as the cosine angle between word-vector pairs, and Spearman correlation between embedding and human judgements are reported. We measure correlative significance at $\\alpha = 0.01$.", "Figure FIGREF30 shows the V-measures of the clusters of the most biased words in Wikipedia for each embedding. Gigaword patterns similarly (see appendix). Figure FIGREF31 shows example tSNE projections for the Gigaword embeddings (\u201c$\\mathrm {V}$\u201d refers to their V-measures; these examples were chosen as they represent the best results achieved by BIBREF1's (BIBREF1) method, BIBREF5's (BIBREF5) method, and our new names variant). On both corpora, the new nCDA and nCDS techniques have significantly lower purity of biased-word cluster than all other evaluated mitigation techniques (0.420 for nCDS on Gigaword, which corresponds to a reduction of purity by 58% compared to the unmitigated embedding, and 0.609 (39%) on Wikipedia).", "In our experiments, we test the degree to which the spaces are successful at mitigating direct and indirect bias, as well as the degree to which they can still be used in two NLP tasks standardly performed with embeddings, word similarity and sentiment classification. We also introduce one further, novel task, which is designed to quantify how well the embedding spaces capture an understanding of gender using non-biased analogies. Our evaluation matrix and methodology is expanded below.", "Figure FIGREF30 shows the V-measures of the clusters of the most biased words in Wikipedia for each embedding. Gigaword patterns similarly (see appendix). Figure FIGREF31 shows example tSNE projections for the Gigaword embeddings (\u201c$\\mathrm {V}$\u201d refers to their V-measures; these examples were chosen as they represent the best results achieved by BIBREF1's (BIBREF1) method, BIBREF5's (BIBREF5) method, and our new names variant). On both corpora, the new nCDA and nCDS techniques have significantly lower purity of biased-word cluster than all other evaluated mitigation techniques (0.420 for nCDS on Gigaword, which corresponds to a reduction of purity by 58% compared to the unmitigated embedding, and 0.609 (39%) on Wikipedia). nWED70's V-Measure is significantly higher than either of the other Names variants (reduction of 11% on Gigaword, only 1% on Wikipedia), suggesting that the success of nCDS and nCDA is not merely due to their larger list of gender-words.", "Following BIBREF6, we use a standard sentiment classification task to quantify the downstream performance of the embedding spaces when they are used as a pretrained word embedding input BIBREF18 to Doc2Vec on the Stanford Large Movie Review dataset. The classification is performed by an SVM classifier using the document embeddings as features, trained on 40,000 labelled reviews and tested on the remaining 10,000 documents, reported as error percentage."]}
{"question_id": "3138f916e253abed643d3399aa8a4555b2bd8c0f", "predicted_answer": "The empirical comparison methods used in this study are:\n\n1. WEAT (Word Embedding Association Test) with one-tailed and two-tailed p-values to determine the significance of differences in sample means between targets and attributes.\n2. Spearman correlation to measure the correlation between embedding similarity scores and human judgments of word similarity, with a significance level of \u03b1 = 0.01.\n3. V-measures to evaluate the purity of biased-word clusters in embeddings.\n4. tSNE (t-Distributed Stochastic Neighbor Embedding) projections to visualize the embeddings.\n5. Proportional pair-based analogy test to evaluate the performance of embeddings in completing analogies, with error percentage reported.\n\nThese methods are used to compare the performance of different mitigation methods for debiasing word embeddings, including CDA, gCDA, nCDA, gCDS, nCDS, WED40, WED70, and nWED70.", "predicted_evidence": ["Table TABREF27 presents the $d$ scores and WEAT one-tailed $p$-values, which indicate whether the difference in samples means between targets $X$ and $Y$ and attributes $A$ and $B$ is significant. We also compute a two-tailed $p$-value to determine whether the difference between the various sets is significant.", "The quality of a space is traditionally measured by how well it replicates human judgements of word similarity. The SimLex-999 dataset BIBREF17 provides a ground-truth measure of similarity produced by 500 native English speakers. Similarity scores in an embedding are computed as the cosine angle between word-vector pairs, and Spearman correlation between embedding and human judgements are reported. We measure correlative significance at $\\alpha = 0.01$.", "We compare eight variations of the mitigation methods. CDA is our reimplementation of BIBREF5's (BIBREF5) na\u00efve intervention, gCDA uses their grammar intervention, and nCDA uses our new Names Intervention. gCDS and nCDS are variants of the grammar and Names Intervention using CDS. WED40 is our reimplementation of BIBREF1's (BIBREF1) method, which (like the original) uses a single component to define the gender subspace, accounting for $>40\\%$ of variance. As this is much lower than in the original paper (where it was 60%, reproduced in Figure FIGREF18), we define a second space, WED70, which uses a 2D subspace accounting for $>70\\%$ of variance. To test whether WED profits from additional names, we use the 5000 paired names in the names gazetteer as additional equalise pairs (nWED70).", "Figure FIGREF30 shows the V-measures of the clusters of the most biased words in Wikipedia for each embedding. Gigaword patterns similarly (see appendix). Figure FIGREF31 shows example tSNE projections for the Gigaword embeddings (\u201c$\\mathrm {V}$\u201d refers to their V-measures; these examples were chosen as they represent the best results achieved by BIBREF1's (BIBREF1) method, BIBREF5's (BIBREF5) method, and our new names variant). On both corpora, the new nCDA and nCDS techniques have significantly lower purity of biased-word cluster than all other evaluated mitigation techniques (0.420 for nCDS on Gigaword, which corresponds to a reduction of purity by 58% compared to the unmitigated embedding, and 0.609 (39%) on Wikipedia). nWED70's V-Measure is significantly higher than either of the other Names variants (reduction of 11% on Gigaword, only 1% on Wikipedia), suggesting that the success of nCDS and nCDA is not merely due to their larger list of gender-words.", "When proposing WED, BIBREF1 use human raters to class gender-analogies as either biased (woman:housewife :: man:shopkeeper) or appropriate (woman:grandmother :: man::grandfather), and postulate that whilst biased analogies are undesirable, appropriate ones should remain. Our new analogy test uses the 506 analogies in the family analogy subset of the Google Analogy Test set BIBREF14 to define many such appropriate analogies that should hold even in a debiased environment, such as boy:girl :: nephew:niece. We use a proportional pair-based analogy test, which measures each embedding's performance when drawing a fourth word to complete each analogy, and report error percentage."]}
{"question_id": "810e6d09813486a64e87ef6c1fb9b1e205871632", "predicted_answer": "According to the text, tokens are defined as either \"a word or a word-piece\". This suggests that the authors are using a subword modeling approach, where each token can be either a full word or a part of a word (a word-piece). This is similar to the approach used in BERT, which is mentioned in the text as an inspiration for the proposed method.", "predicted_evidence": ["where $P_{trans\\_lm}$ denotes the sentence generative probability given by a Transformer language model.", "While an external language model may be used to mitigate the weakness of the language modeling power of an attention-based E2E model, by either re-scoring the hypothesis or through shallow or deep fusion BIBREF2, the improvements are usually limited, and it incurs additional computational cost. Inspired by SpecAgument BIBREF3 and BERT BIBREF4, we propose a semantic mask approach to improve the strength of the language modeling power in the attention-based E2E model, which, at the same time, improves the generalization capacity of the model as well. Like SpecAugment, this approach masks out partial of the acoustic features during model training. However, instead of using a random mask as in SpecAugment, our approach masks out the whole patch of the features corresponding to an output token during training, e.g., a word or a word-piece. The motivation is to encourage the model to fill in the missing token (or correct the semantic error) based on the contextual information with less acoustic evidence, and consequently, the model may have a stronger language modeling power and is more robust to acoustic distortions.", "Our masking approach requires the alignment information in order to perform the token-wise masking as shown in Figure FIGREF2. There are multiple speech recognition toolkits available to generate such kind of alignments. In this work, we used the Montreal Forced Alignertrained with the training data to perform forced-alignment between the acoustic signals and the transcriptions to obtain the word-level timing information. During model training, we randomly select a percentage of the tokens and mask the corresponding speech segments in each iteration. Following BIBREF4, in our work, we randomly sample 15% of the tokens and set the masked piece to the mean value of the whole utterance.", "While an external language model may be used to mitigate the weakness of the language modeling power of an attention-based E2E model, by either re-scoring the hypothesis or through shallow or deep fusion BIBREF2, the improvements are usually limited, and it incurs additional computational cost. Inspired by SpecAgument BIBREF3 and BERT BIBREF4, we propose a semantic mask approach to improve the strength of the language modeling power in the attention-based E2E model, which, at the same time, improves the generalization capacity of the model as well. Like SpecAugment, this approach masks out partial of the acoustic features during model training. However, instead of using a random mask as in SpecAugment, our approach masks out the whole patch of the features corresponding to an output token during training, e.g., a word or a word-piece.", "We combine scores of E2E model $P_{s2s}$, CTC score $P_{ctc}$ and a RNN based language model $P_{rnn}$ in the decoding process, which is formulated as"]}
{"question_id": "ab8b0e6912a7ca22cf39afdac5531371cda66514", "predicted_answer": "The answer is not explicitly stated in the text, but it can be inferred that the proposed model achieves a significant word error rate reduction (WER) on top of SpecAugment, and reports the lowest WERs on the test sets of the Librispeech corpus with an E2E model. This implies that the proposed model outperforms existing state-of-the-art models on end-to-end speech recognition tasks, but the exact amount of improvement is not specified.", "predicted_evidence": ["End-to-end (E2E) acoustic models, particularly with the attention-based encoder-decoder framework BIBREF0, have achieved a competitive recognition accuracy in a wide range of speech datasets BIBREF1. This model directly learns the mapping from the input acoustic signals to the output transcriptions without decomposing the problems into several different modules such as lexicon modeling, acoustic modeling and language modeling as in the conventional hybrid architecture. While this kind of E2E approach significantly simplifies the speech recognition pipeline, the weakness is that it is difficult to tune the strength of each component. One particular problem from our observations is that the attention based E2E model tends to make grammatical errors, which indicates that the language modeling power of the model is weak, possibly due to the small amount of training data, or the mismatch between the training and evaluation data.", "End-to-end (E2E) acoustic models, particularly with the attention-based encoder-decoder framework BIBREF0, have achieved a competitive recognition accuracy in a wide range of speech datasets BIBREF1. This model directly learns the mapping from the input acoustic signals to the output transcriptions without decomposing the problems into several different modules such as lexicon modeling, acoustic modeling and language modeling as in the conventional hybrid architecture. While this kind of E2E approach significantly simplifies the speech recognition pipeline, the weakness is that it is difficult to tune the strength of each component. One particular problem from our observations is that the attention based E2E model tends to make grammatical errors, which indicates that the language modeling power of the model is weak, possibly due to the small amount of training data, or the mismatch between the training and evaluation data. However, due to the jointly model approach in the attention model, it is unclear how to improve the strength of the language modeling power, i.e., attributing more weights to the previous output tokens in the decoder, or to improve the strength of the acoustic modeling power, i.e., attributing more weights to the context vector from the encoder.", "In principle, our approach is applicable to the attention-based E2E framework with any type of neural network encoder. To constrain our research scope, we focus on the transformer architecture BIBREF5, which is originally proposed for neural machine translation. Recently, it has been shown that the transformer model can achieve competitive or even higher recognition accuracy compared with the recurrent neural network (RNN) based E2E model for speech recognition BIBREF6. Compared with RNNs, the transformer model can capture the long-term correlations with a computational complexity of $O(1)$, instead of using many steps of back-propagation through time (BPTT) as in RNNs. We evaluate our transformer model with semantic masking on Librispeech and TedLium datasets.", "This paper presents a semantic mask method for E2E speech recognition, which is able to train a model to better consider the whole audio context for the disambiguation. Moreover, we elaborate a new architecture for E2E model, achieving state-of-the-art performance on the Librispeech test set in the scope of E2E models.", "In principle, our approach is applicable to the attention-based E2E framework with any type of neural network encoder. To constrain our research scope, we focus on the transformer architecture BIBREF5, which is originally proposed for neural machine translation. Recently, it has been shown that the transformer model can achieve competitive or even higher recognition accuracy compared with the recurrent neural network (RNN) based E2E model for speech recognition BIBREF6. Compared with RNNs, the transformer model can capture the long-term correlations with a computational complexity of $O(1)$, instead of using many steps of back-propagation through time (BPTT) as in RNNs. We evaluate our transformer model with semantic masking on Librispeech and TedLium datasets. We show that semantic masking can achieve significant word error rate reduction (WER) on top of SpecAugment, and we report the lowest WERs on the test sets of the Librispeech corpus with an E2E model."]}
{"question_id": "89373db8ced1fe420eae0093b2736f06b565616e", "predicted_answer": "No, the authors did not collect new data for evaluation. They used two existing datasets: the Czech Facebook dataset, which was created by collecting posts from popular Facebook pages in Czech, and the Mall dataset, which contains user reviews about household devices purchased at mall.cz.", "predicted_evidence": ["The research was [partially] supported by OP RDE project No. CZ.02.2.69/0.0/0.0/16_027/0008495, International Mobility of Researchers at Charles University.", "The second dataset we use contains user reviews about household devices purchased at mall.cz BIBREF7 . The reviews are evaluative in nature (users apprising items they bought) and were categorized as negative or positive only. Some minor problems they carry are the grammatical or typing errors that frequently appear in their texts BIBREF11 . In Table 1 we present some rounded statistics about the two datasets. As we can see, Mall product reviews are slightly longer (13 vs. 10 tokens) than Czech Facebook posts. We also see that the number of data samples in each sentiment category are unbalanced in both cases. A few samples of Mall reviews are illustrated in Figure 2 .", "The second uses emotionally-labeled text datasets to train supervised (or deep supervised) algorithms and use them to predict emotions in other documents. Naturally, most of sentiment analysis research has been conducted for the English language. Chinese BIBREF1 , BIBREF2 , BIBREF3 and Spanish BIBREF4 , BIBREF5 have also received a considerable extra attention in the last years. Smaller languages like Czech have seen fewer efforts in this aspect. It is thus much easier to find online data resources for English than for other languages BIBREF6 . One of the first attempts to create sentiment annotated resources of Czech texts dates back in 2012 BIBREF7 . Authors released three datasets of news articles, movie reviews, and product reviews. A subsequent work consisted in creating a Czech dataset of information technology product reviews, their aspects and customers' attitudes towards those aspects BIBREF8 . This latter dataset is an essential basis for performing aspect-based sentiment analysis experiments BIBREF9 .", "For lower variance, the average of five measures is reported. The top scores on the two datasets differ a lot. That is because Facebook data classification is a multiclass discrimination problem (negative vs. neutral vs. positive), in contrast with Mall review analysis which is purely binary (negative vs. positive). As we can see, Logistic Regression and SVM are the top performers in Facebook data. NuSVM and Na\u00efve Bayes perform slightly worse. MLP and Random Forest, on the other hand, fall discretely behind. On the Mall dataset, SVM is dominant in both accuracy and F $_1$ . It is followed by Logistic Regression, Na\u00efve Bayes and NuSVM. Maximum Entropy is near whereas MLP and Random Forest are again considerably weaker. Similar results are also reported in other works like BIBREF23 where again, SVM and Na\u00efve Bayes outrun Random Forest on text analysis tasks. From the top three algorithms, Na\u00efve Bayes was the fastest to train, followed by Logistic Regression.", "Czech Facebook dataset was created by collecting posts from popular Facebook pages in Czech BIBREF10 . The ten thousand records were independently revised by two annotators. Two other annotators were involved in cases of disagreement. To estimate inter-annotator agreement, they used Cohen's kappa coefficient which was about 0.66. Each post was labeled as negative, neutral or positive. There were yet a few samples that revealed both negative and positive sentiments and were marked as bipolar. Same as the authors in their paper, we removed the bipolar category from our experimental set to avoid ambiguity and used the remaining 9752 samples. A few data samples are illustrated in Figure 1 ."]}
{"question_id": "74a17eb3bf1d4f36e2db1459a342c529b9785f6e", "predicted_answer": "The evaluation metrics mentioned in the given context are:\n\n1. BLEU score\n2. Faithfulness of translation results (subjective evaluation)\n\nThe faithfulness of translation results is evaluated on a scale of 0 to 5, where a higher score indicates a more faithful translation.", "predicted_evidence": ["In the future, we plan to design more effective methods to calculate accurate bonus values.", "Besides the BLEU score, we also conduct a subjective evaluation to validate the benefit of incorporating a phrase table in NMT. The subjective evaluation is conducted on CH-EN translation. As our method tries to solve the problem that NMT system cannot reflect the true meaning of the source sentence, the criterion of the subjective evaluation is the faithfulness of translation results. Specifically, five human evaluators, who are native Chinese and expert in English, are asked to evaluate the translations of 500 source sentences randomly sampled from the test sets without knowing which system a translation is selected from. The score ranges from 0 to 5. For a translation result, the higher its score is, the more faithful it is. Table 2 shows the average results of five subjective evaluations on CH-EN translation. As shown in Table 2\uff0cthe faithfulness of translation results produced by our method is better than Arthur and baseline NMT system.", "In this section, we describe the experiments to evaluate our proposed methods.", "where INLINEFORM0 is the score produced by NMT: DISPLAYFORM0", "2) How to determine an appropriate bonus value for each recommendable word?"]}
{"question_id": "4b6745982aa64fbafe09f7c88c8d54d520b3f687", "predicted_answer": "The language pairs explored in this research are Chinese (CH) and English (EN).", "predicted_evidence": ["The research work described in this paper has been supported by the National Key Research and Development Program of China under Grant No. 2016QY02D0303 and the Natural Science Foundation of China under Grant No. 61333018 and 61673380. The research work in this paper also has been supported by Beijing Advanced Innovation Center for Language Resources.", "1) Which words are worthy to recommend at each decoding step?", "Our phrase translation table is learned directly from parallel data by Moses BIBREF22 . To ensure the quality of the phrase pair, in all experiments, the phrase translation table is filtered as follows: 1) out-of-vocabulary words in the phrase table are replaced by UNK; 2) we remove the phrase pairs whose words are all punctuations and UNK; 3) for a source phrase, we retain at most 10 target phrases having the highest phrase translation probabilities.", "We also conduct another experiment to find out whether or not our methods are still effective when much more sentence pairs are available. Therefore, the CH-EN experiments on millions of sentence pairs are conducted and Table 3 reports the results. We can conclude from Table 3 that our model can also improve the NMT translation quality on all of the test sets and the average improvement is up to 1.83 BLEU points.", "Fig. 4 shows an illustrative example. In this example, baseline NMT mistranslates \u201cdianli (electricity) anquan (safe)\u201d into \u201ccoal\u201d. Arthur partially fixes this error and it can correctly translate \u201cdianli (electrical)\u201d into \u201celectrical\u201d, but the source word \u201canquan (safe)\u201d is still missed. Fortunately, this mistake is fixed by our proposed method. The reason behind this is that Arthur uses information from translation lexicons, which makes the system only fix the translation mistake of an individual lexicon (in this example, it is \u201cdianli (electrical)\u201d), while our method uses the information from phrases, which makes the system can not only obtain the correct translation of the individual lexicon but also capture local lexicon reordering and fixed collocation etc."]}
{"question_id": "6656a9472499331f4eda45182ea697a4d63e943c", "predicted_answer": "The datasets used are not explicitly mentioned in the given context. However, it can be inferred that the experiments involve parallel data, which is used to learn the phrase translation table using Moses. The specific datasets used are likely to be mentioned in the references (BIBREF22, BIBREF23, BIBREF24, etc.) but are not provided in this context.", "predicted_evidence": ["2) Baseline: It is the baseline attention-based NMT system BIBREF23 , BIBREF24 .", "1) Moses: It is a widely used phrasal SMT system BIBREF22 .", "Our phrase translation table is learned directly from parallel data by Moses BIBREF22 . To ensure the quality of the phrase pair, in all experiments, the phrase translation table is filtered as follows: 1) out-of-vocabulary words in the phrase table are replaced by UNK; 2) we remove the phrase pairs whose words are all punctuations and UNK; 3) for a source phrase, we retain at most 10 target phrases having the highest phrase translation probabilities.", "In this section, we describe the experiments to evaluate our proposed methods.", "3) Arthur: It is the state-of-the-art method which incorporates discrete translation lexicons into NMT model BIBREF6 . We choose automatically learned lexicons and bias method. We implement the method on the base of the baseline attention-based NMT system. Hyper parameter INLINEFORM0 is 0.001, the same as that reported in their work."]}
{"question_id": "430ad71a0fd715a038f3c0fe8d7510e9730fba23", "predicted_answer": "The attention-based NMT method they compared with is the baseline attention-based NMT system, which is based on the work of BIBREF23 and BIBREF24.", "predicted_evidence": ["2) Baseline: It is the baseline attention-based NMT system BIBREF23 , BIBREF24 .", "Table 1 reports the detailed translation results for different methods. Comparing the first two rows in Table 1, it is very obvious that the attention-based NMT system Baseline substantially outperforms the phrase-based SMT system Moses on both CH-EN translation and EN-JA translation. The average improvement for CH-EN and EN-JA translation is up to 3.99 BLEU points (32.71 vs. 28.72) and 3.59 BLEU (25.99 vs. 22.40) points, respectively.", "3) Arthur: It is the state-of-the-art method which incorporates discrete translation lexicons into NMT model BIBREF6 . We choose automatically learned lexicons and bias method. We implement the method on the base of the baseline attention-based NMT system. Hyper parameter INLINEFORM0 is 0.001, the same as that reported in their work.", "We also conduct another experiment to find out whether or not our methods are still effective when much more sentence pairs are available. Therefore, the CH-EN experiments on millions of sentence pairs are conducted and Table 3 reports the results. We can conclude from Table 3 that our model can also improve the NMT translation quality on all of the test sets and the average improvement is up to 1.83 BLEU points.", "NMT contains two parts, encoder and decoder, where encoder transforms the source sentence INLINEFORM0 into context vectors INLINEFORM1 . This context set is constructed by INLINEFORM2 stacked Long Short Term Memory (LSTM) BIBREF20 layers. INLINEFORM3 can be calculated as follows: DISPLAYFORM0"]}
{"question_id": "b79ff0a50bf9f361c5e5fed68525283856662076", "predicted_answer": "According to the text, their system improved by 2.23 BLEU points for CH-EN translation and 1.96 BLEU points for EN-JA translation compared to the baseline NMT system.", "predicted_evidence": ["In the future, we plan to design more effective methods to calculate accurate bonus values.", "Table 1 reports the detailed translation results for different methods. Comparing the first two rows in Table 1, it is very obvious that the attention-based NMT system Baseline substantially outperforms the phrase-based SMT system Moses on both CH-EN translation and EN-JA translation. The average improvement for CH-EN and EN-JA translation is up to 3.99 BLEU points (32.71 vs. 28.72) and 3.59 BLEU (25.99 vs. 22.40) points, respectively.", "We compare our method with other relevant methods as follows:", "A natural question arises that whether it is more beneficial to incorporate a phrase translation table than the translation lexicons. From Table 1, we can conclude that both translation lexicons and phrase translation table can improve NMT system's translation quality. In CH-EN translation, Arthur improves the baseline NMT system with 0.81 BLEU points, while our method improves the baseline NMT system with 2.23 BLEU points. In EN-JA translation, Arthur improves the baseline NMT system with 0.73 BLEU points, while our method improves the baseline NMT system with 1.96 BLEU points. Therefore, it is very obvious that phrase information is more effective than lexicon information when we use them to improve the NMT system.", "In this section, we describe the experiments to evaluate our proposed methods."]}
{"question_id": "d66c31f24f582c499309a435ec3c688dc3a41313", "predicted_answer": "The baseline methods mentioned are:\n\n1. DSSM (Deep Structured Semantic Model) [BIBREF12]\n2. Match Pyramid [BIBREF18]\n3. ARC-II [BIBREF15]\n4. The model with frozen, randomly initialized embeddings.", "predicted_evidence": ["In this section we describe our metrics, training procedure, and the results, including the impact of our method in production.", "We define two evaluation subtasks: matching and ranking.", "Finally, in Table TABREF30 , we compare the results of our model to four baselines: DSSM BIBREF12 , Match Pyramid BIBREF18 , ARC-II BIBREF15 , and our model with frozen, randomly initialized embeddings. We only use word unigrams or character trigrams in our model, as it is not immediately clear how to extend the bag-of-tokens approach to methods that incorporate ordering. We compare the performance of using the 3-part L2 hinge loss to the original loss presented for each model. Across all baselines, matching performance of the model improves using the 3-part L2 hinge loss. ARC-II and Match Pyramid ranking performance is similar or lower when using the 3-part loss. Ranking performance improves for DSSM, possibly because the original approach uses only random negatives to approximate the softmax normalization.", "Finally, in Table TABREF30 , we compare the results of our model to four baselines: DSSM BIBREF12 , Match Pyramid BIBREF18 , ARC-II BIBREF15 , and our model with frozen, randomly initialized embeddings. We only use word unigrams or character trigrams in our model, as it is not immediately clear how to extend the bag-of-tokens approach to methods that incorporate ordering. We compare the performance of using the 3-part L2 hinge loss to the original loss presented for each model. Across all baselines, matching performance of the model improves using the 3-part L2 hinge loss. ARC-II and Match Pyramid ranking performance is similar or lower when using the 3-part loss. Ranking performance improves for DSSM, possibly because the original approach uses only random negatives to approximate the softmax normalization. More complex models, like Match Pyramid and ARC-II, had significantly lower matching and ranking performance while taking significantly longer to train and evaluate.", "where INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 denote indicators signifying if the product INLINEFORM3 was purchased, not impressed and not purchased, and impressed (but not purchased) in response to the query INLINEFORM4 , respectively, and INLINEFORM5 . Based on the 2-part hinge score distribution, INLINEFORM6 was set to INLINEFORM7 with INLINEFORM8 and INLINEFORM9 as before. The effectiveness of this strategy can be seen in Figure FIGREF14 , where one can observe a clear separation in scores between random and impressed negatives vs positives."]}
{"question_id": "c47312f2ca834ee75fa9bfbf912ea04239064117", "predicted_answer": "The dataset used for training is 11 months of search logs, which were sampled to 54 billion query-product training pairs and then preprocessed to 650 million rows by grouping the training data by query-product pairs over the entire time period.", "predicted_evidence": ["We use 11 months of search logs as training data and 1 month as evaluation. We sample 54 billion query-product training pairs. We preprocess these sampled pairs to 650 million rows by grouping the training data by query-product pairs over the entire time period and using the aggregated counts as weights for the pairs. We also decrease the training time by 3X by preprocessing the training data into tokens and using mmap to store the tokens. More details on our best practices for reducing training time can be found in Section SECREF6 .", "In this section we describe our metrics, training procedure, and the results, including the impact of our method in production.", "The embeddings in our model are trained end-to-end. Previous experiments using other methods, including Glove and word2vec, to initialize the embeddings yielded poorer results than end-to-end training. When comparing our model with randomly initialized to one with trained embeddings, we see that end-to-end training results in over a 3X improvement in Recall@100 and MAP.", "In Table TABREF28 , we compare the performance of using different tokenization methods. We use average pooling and the 3-part L2 hinge loss. For each tokenization method, we select the top INLINEFORM0 terms by frequency in the training data. Unless otherwise noted, INLINEFORM1 was set to 125K, 25K, 64K, and 500K for unigrams, bigrams, character trigrams, and out-of-vocabulary (OOV) bins respectively. It is worth noting that using only character trigrams, which was an essential component of DSSM BIBREF12 , has competitive ranking but not matching performance compared to unigrams. Adding bigrams improves matching performance as bigrams capture short phrase-level information that is not captured by averaging unigrams. For example, the unigrams for \u201cchocolate milk\u201d and \u201cmilk chocolate\u201d are the same although these are different products.", "In Table TABREF28 , we compare the performance of using different tokenization methods. We use average pooling and the 3-part L2 hinge loss. For each tokenization method, we select the top INLINEFORM0 terms by frequency in the training data. Unless otherwise noted, INLINEFORM1 was set to 125K, 25K, 64K, and 500K for unigrams, bigrams, character trigrams, and out-of-vocabulary (OOV) bins respectively. It is worth noting that using only character trigrams, which was an essential component of DSSM BIBREF12 , has competitive ranking but not matching performance compared to unigrams. Adding bigrams improves matching performance as bigrams capture short phrase-level information that is not captured by averaging unigrams. For example, the unigrams for \u201cchocolate milk\u201d and \u201cmilk chocolate\u201d are the same although these are different products. Additionally including character trigrams improves the performance further as character trigrams provide generalization and robustness to spelling errors."]}
{"question_id": "5499440674f0e4a9d6912b9ac29fa1f7b7cd5253", "predicted_answer": "The question is: Do they (the results of the current study) compare to previous work?\n\nThe answer is not explicitly stated in the given context, but it can be inferred that the authors are building upon previous work in the field of extracting data elements from publication full texts, as they cite several references (BIBREF0, BIBREF1, BIBREF2) related to tasks such as systematic review preparation, construction of reference databases, and knowledge discovery. The authors' method and results are likely being compared to these previous works, but a direct comparison is not presented in the given context.", "predicted_evidence": ["The only difference between the four models is which sentences from each document are passed to the classifier for training and testing. The intuition is that a classifier utilizing the correct sentences should outperform both other models.", "Due to space limitations, Figures FIGREF11 , FIGREF12 , and FIGREF13 show results generated on abstracts rather than on full text; however, we have observed similarly accurate results when we applied our method to full text. The only difference between the abstracts and the full text version is how many top sentences we retrieved. When working with abstracts only, we observed that if the criteria was discussed in the abstract, it was generally sufficient to retrieve the single most similar sentence. However, as the criteria may be mentioned in multiple places within the document, when working with full text documents we have retrieved and analyzed the top k sentences instead of just a single sentence. In this case we have typically found the correct sentence/sentences among the top 5 sentences. We have also observed that the similar sentences which don't discuss the criteria directly (i.e. the \u201cincorrect\u201d sentences) typically discuss related topics. For example, consider the following three sentences:", "This section provides more details about the specific task and the dataset used in our study which motivated the development of our model.", "These three sentences were extracted from the abstract and the full text of a single document (document 20981862, the abstract of which is shown in Figures FIGREF2 and FIGREF11 - FIGREF21 ). These three sentences were retrieved as the most similar to MC 1, with similarity scores of 70.61, 65.31, and 63.69, respectively. The third sentence contains the \u201canswer\u201d to MC 1 (underlined). However, it can be seen the top two sentences also discuss the animals used in the study (more specifically, the sentences discuss the animals' housing and their origin).", "Extracting data elements such as study descriptors from publication full texts is an essential step in a number of tasks including systematic review preparation BIBREF0 , construction of reference databases BIBREF1 , and knowledge discovery BIBREF2 . These tasks typically involve domain experts identifying relevant literature pertaining to a specific research question or a topic being investigated, identifying passages in the retrieved articles that discuss the sought after information, and extracting structured data from these passages. The extracted data is then analyzed, for example to assess adherence to existing guidelines BIBREF1 . Figure FIGREF2 shows an example text excerpt with information relevant to a specific task (assessment of adherence to existing guidelines BIBREF1 ) highlighted."]}
{"question_id": "de313b5061fc22e8ffef1706445728de298eae31", "predicted_answer": "The source of their data is a database of 670 publications (spanning the years 1938 through 2014) with results from 2,615 uterotrophic bioassays, which was created by following a study protocol design set forth in BIBREF3. The publication PDFs were provided to them by the database creators.", "predicted_evidence": ["This section provides more details about the specific task and the dataset used in our study which motivated the development of our model.", "The creation of the database followed the study protocol design set forth in BIBREF3 , which is composed of six minimum criteria (MC, Table TABREF5 ). An example of information pertaining to the criteria is shown in Figure FIGREF2 . Only studies which met all six minimum criteria were considered guideline-like (GL) and were included in a follow-up detailed study and the final database BIBREF1 . However, of the 670 publications initially considered for inclusion, only 93 ( INLINEFORM0 14%) were found to contain studies which met all six MC and could therefore be included in the final database; the remaining 577 publications could not be used in the final reference set. Therefore, significant time and resources could be saved by automating the identification and extraction of the MC.", "The version of the database which contains both GL and non-GL studies consists of 670 publications (spanning the years 1938 through 2014) with results from 2,615 uterotrophic bioassays. Specifically, each entry in the database describes one study, and studies are linked to publications using PubMed reference numbers (PMIDs). Each study is assigned seven 0/1 labels \u2013 one for each of the minimum criteria and one for the overall GL/non-GL label. The database also contains more detailed subcategories for each label (for example \u201cspecies\u201d label for MC 1) which were not used in this study. The publication PDFs were provided to us by the database creators. We have used the Grobid library to convert the PDF files into structured text. After removing documents with missing PDF files and documents which were not converted successfully, we were left with 624 full text documents.", "For this experiment we did not distinguish between publications describing a single or multiple studies. Instead, our focus was on retrieving all text segments (which may be related to multiple studies) relevant to each of the criteria. For each MC, if a document contained multiple studies with different labels, we discarded that document from our analysis of that criteria; if a document contained multiple studies with the same label, we simply combine all those labels into a single label. Table TABREF8 shows the final size of the dataset.", "There are two main contributions of this work. We present an unsupervised method that employs representation learning to identify text segments from publication full text which are relevant to/contain specific sought after information (such as number of dose groups). In addition, we explore a new dataset which hasn't been previously used in the field of information extraction."]}
{"question_id": "47b7bc232af7bf93338bd3926345e23e9e80c0c1", "predicted_answer": "The binary classifier is a Bernoulli Na\u00efve Bayes classifier that determines whether a publication meets a given criteria or not.", "predicted_evidence": ["The goal of this experiment was to explore empirically whether our approach truly identifies mentions of the minimum criteria in text. As we did not have any fine-grained annotations that could be used to directly evaluate whether our model identifies the correct sequences, we have used a different methodology. We have utilized the existing 0/1 labels which were available in the database (these were discussed in Section SECREF2 ) to train one binary classifier for each MC. The task of each of the classifiers is to determine whether a publication met the given criteria or not. We have then compared a baseline classifier trained on all full text with three other models:", "In this paper we presented a method for unsupervised identification of text segments relevant to specific sought after information being extracted from scientific documents. Our method is entirely unsupervised and only requires the current document itself and the input descriptions instead of corpus linked to this document. The method utilizes short descriptions of the information being extracted from the documents and the ability of word embeddings to capture word context. Consequently, it is domain independent and can potentially be applied to another set of documents and criteria with minimal effort. We have used the method on a corpus of toxicology documents and a set of guideline protocol criteria needed to be extracted from the documents. We have shown the identified text segments are very accurate. Furthermore, a binary classifier trained to identify publications that met the criteria performed better when trained on the candidate sentences than when trained on sentences randomly picked from the text, supporting our intuition that our method is able to accurately identify relevant text segments from full text documents.", "To account for the variations in language that can be used to describe the criteria, we represent words as vectors generated using Word2Vec BIBREF7 . The following two excerpts show two different ways MC 6 was described in text:", "To avoid selecting the same sentences across the three models we removed documents which contained less than INLINEFORM0 sentences (Table TABREF17 , row Number of documents shows how many documents satisfied this condition). In all of the experiments presented in this section, the publication full text was tokenized, lower-cased, stemmed, and stop words were removed. All models used a Bernoulli Na\u00efve Bayes classifier (scikit-learn implementation which used a uniform class prior) trained on binary occurrence matrices created using 1-3-grams extracted from the publications, with n-grams appearing in only one document removed. The complete results obtained from leave-one-out cross validation are shown in Table TABREF17 . In all cases we report classification accuracy. In the case of the random-k sentences model the accuracy was averaged over 10 runs of the model.", "This section provides more details about the specific task and the dataset used in our study which motivated the development of our model."]}
{"question_id": "0b5c599195973c563c4b1a0fe5d8fc77204d71a0", "predicted_answer": "The dataset consists of 670 publications, which contain results from 2,615 uterotrophic bioassays, spanning the years 1938 through 2014. After processing, the final dataset used in the study consists of 624 full-text documents.", "predicted_evidence": ["This section provides more details about the specific task and the dataset used in our study which motivated the development of our model.", "The version of the database which contains both GL and non-GL studies consists of 670 publications (spanning the years 1938 through 2014) with results from 2,615 uterotrophic bioassays. Specifically, each entry in the database describes one study, and studies are linked to publications using PubMed reference numbers (PMIDs). Each study is assigned seven 0/1 labels \u2013 one for each of the minimum criteria and one for the overall GL/non-GL label. The database also contains more detailed subcategories for each label (for example \u201cspecies\u201d label for MC 1) which were not used in this study. The publication PDFs were provided to us by the database creators. We have used the Grobid library to convert the PDF files into structured text. After removing documents with missing PDF files and documents which were not converted successfully, we were left with 624 full text documents.", "For this experiment we did not distinguish between publications describing a single or multiple studies. Instead, our focus was on retrieving all text segments (which may be related to multiple studies) relevant to each of the criteria. For each MC, if a document contained multiple studies with different labels, we discarded that document from our analysis of that criteria; if a document contained multiple studies with the same label, we simply combine all those labels into a single label. Table TABREF8 shows the final size of the dataset.", "The creation of the database followed the study protocol design set forth in BIBREF3 , which is composed of six minimum criteria (MC, Table TABREF5 ). An example of information pertaining to the criteria is shown in Figure FIGREF2 . Only studies which met all six minimum criteria were considered guideline-like (GL) and were included in a follow-up detailed study and the final database BIBREF1 . However, of the 670 publications initially considered for inclusion, only 93 ( INLINEFORM0 14%) were found to contain studies which met all six MC and could therefore be included in the final database; the remaining 577 publications could not be used in the final reference set. Therefore, significant time and resources could be saved by automating the identification and extraction of the MC.", "The remainder of this paper is organized as follows. In the following section we provide more details of the task and the dataset used in this study. In Section SECREF3 we describe our approach. In Section SECREF4 we evaluate our model and discuss our results. In Section SECREF5 we compare our work to existing approaches. Finally, in Section SECREF6 we provide ideas for further study."]}
{"question_id": "1397b1c51f722a4ee2b6c64dc9fc6afc8bd3e880", "predicted_answer": "A study descriptor is a piece of information that describes a study, such as the animals used, housing conditions, origin, etc. In the context of the text, study descriptors are extracted from publication full texts to aid in tasks like systematic review preparation, construction of reference databases, and knowledge discovery.", "predicted_evidence": ["Extracting data elements such as study descriptors from publication full texts is an essential step in a number of tasks including systematic review preparation BIBREF0 , construction of reference databases BIBREF1 , and knowledge discovery BIBREF2 . These tasks typically involve domain experts identifying relevant literature pertaining to a specific research question or a topic being investigated, identifying passages in the retrieved articles that discuss the sought after information, and extracting structured data from these passages. The extracted data is then analyzed, for example to assess adherence to existing guidelines BIBREF1 . Figure FIGREF2 shows an example text excerpt with information relevant to a specific task (assessment of adherence to existing guidelines BIBREF1 ) highlighted.", "For this experiment we did not distinguish between publications describing a single or multiple studies. Instead, our focus was on retrieving all text segments (which may be related to multiple studies) relevant to each of the criteria. For each MC, if a document contained multiple studies with different labels, we discarded that document from our analysis of that criteria; if a document contained multiple studies with the same label, we simply combine all those labels into a single label. Table TABREF8 shows the final size of the dataset.", "These three sentences were extracted from the abstract and the full text of a single document (document 20981862, the abstract of which is shown in Figures FIGREF2 and FIGREF11 - FIGREF21 ). These three sentences were retrieved as the most similar to MC 1, with similarity scores of 70.61, 65.31, and 63.69, respectively. The third sentence contains the \u201canswer\u201d to MC 1 (underlined). However, it can be seen the top two sentences also discuss the animals used in the study (more specifically, the sentences discuss the animals' housing and their origin).", "We have developed and tested our approach on a reference database of rodent uterotropic bioassays BIBREF1 which are labeled according to their adherence to test guidelines set forth in BIBREF3 . Each study in the database is assigned a label determining whether or not it met each of six main criteria defined by the guidelines; however, the database does not contain sentence-level annotations or any information about where the criteria was mentioned in each publication. Due to the lack of fine-grained annotations, supervised learning methods cannot be easily applied to aid annotating new publications or to annotate related but distinct types of studies. This database therefore presents an ideal use-case for unsupervised approaches.", "This section provides more details about the specific task and the dataset used in our study which motivated the development of our model."]}
{"question_id": "230f127e83ac62dd65fccf6b1a4960cf0f7316c7", "predicted_answer": "Experiments are designed to measure the impact on performance by different choices through the following methods:\n\n1. **Varying batch size**: Experiments are conducted with different batch sizes, ranging from 256 to 4,096, to observe how batch size affects performance when using the Cyclical Learning Rate (CLR) policy.\n2. **Comparing learning rate policies**: The study compares the performance of the CLR policy with a decay policy, as shown in BIBREF17, to determine which policy is more effective.\n3. **Isolating the effect of CLR**: By controlling for other factors, the experiment examines the impact of CLR on performance, allowing for a better understanding of its effect.\n4. **Systematic experimentation**: The study conducts a series of experiments, as described in Section SECREF4, to systematically evaluate the performance of CLR with different settings and parameters.\n\nBy using these methods, the study can provide guidance on how to choose the optimal optimizer, learning rate policy, and batch size to improve performance in Neural Machine Translation (NMT) tasks.", "predicted_evidence": ["Raise awareness of how a judicial choice of optimizer with a good learning rate policy can help improve performance;", "The other point of interest is how to deal with batch size when using CLR. Our primary interest is to use a larger batch size without compromising the generalization capability on the test set. Following the lead in BIBREF22, we look at how the NMT tasks perform when varying the batch size on top of the CLR policy. Compared to BIBREF22, we stretch the batch size range, going from batch size as small as 256 to as high as 4,096. Only through examining the extreme behaviors can we better understand the effect of batch size superimposed on CLR.", "Our adopted learning rate decay policy is interesting because experiments in BIBREF17 showed that using a decay rate is detrimental to the resultant accuracy. Our designed experiments in Section SECREF4 reveal how CLR performs with the chosen decay policy.", "The contributions of this study are to:", "Provide guidance on how cyclical learning rate policy can be used for NMT to improve performance."]}
{"question_id": "75c221920bee14a6153bd5f4c1179591b2f48d59", "predicted_answer": "The choice of optimizer and learning rate policy can significantly impact performance. Different optimizers and learning rate policies can lead to varying degrees of convergence and performance improvement. For example, a cyclical learning rate policy, where the learning rate is decayed to a small value to ensure convergence, can be particularly effective for Neural Machine Translation (NMT) tasks. In this study, two learning rate decay policies are adopted: a fixed decay policy where the maximum learning rate is halved after each learning rate cycle, and other decay schemes such as piece-wise constant step function and inverse square root. The optimal choice of learning rate and decay policy can help avoid the extremes of slow training with small learning rates and divergence with overly large learning rates, leading to improved performance.", "predicted_evidence": ["Raise awareness of how a judicial choice of optimizer with a good learning rate policy can help improve performance;", "Provide guidance on how cyclical learning rate policy can be used for NMT to improve performance.", "The other hyperparameter to take care of is the learning rate decay rate, shown in Figure FIGREF8. For the various optimizers, the learning rate is usually decayed to a small value to ensure convergence. There are various commonly used decay schemes such as piece-wise constant step function, inverse (reciprocal) square root. This study adopts two learning rate decay policies:", "Fixed decay (shrinking) policy where the max learning rate is halved after each learning rate cycle;", "There has been many interests in deep learning optimizer research recently BIBREF0, BIBREF1, BIBREF2, BIBREF3. These works attempt to answer the question: what is the best step size to use in each step of the gradient descent? With the first order gradient descent being the de facto standard in deep learning optimization, the question of the optimal step size or learning rate in each step of the gradient descent arises naturally. The difficulty in choosing a good learning rate can be better understood by considering the two extremes: 1) when the learning rate is too small, training takes a long time; 2) while overly large learning rate causes training to diverge instead of converging to a satisfactory solution."]}
{"question_id": "4eb42c5d56d695030dd47ea7f6d65164924c4017", "predicted_answer": "The domain of the audio samples is sound or audio content that is not music, sound effects, or speech. This is inferred from the text where it is mentioned that the audio samples are filtered to exclude those with tags indicating music, sound effects, or speech.", "predicted_evidence": ["We collect the set of audio samples $\\mathbb {X}_{\\text{init}}=\\lbrace \\mathbf {x}_{\\text{init}}^{i}\\rbrace _{i=1}^{N_{\\text{init}}}$, with $N_{\\text{init}}=12000$ and their corresponding metadata (e.g. tags that indicate their content, and a short textual description), from the online platform Freesound BIBREF8.", "tags), and not having tags that indicate music, sound effects, or speech. As tags indicating speech files we consider those like \u201cspeech\u201d, \u201cspeak\u201d, and \u201cwoman\u201d. We normalize $\\mathbf {x}^{i}_{\\text{init}}$ to the range $[-1, 1]$, trim the silence (60 dB below the maximum amplitude) from the beginning and end, and resample to 44.1 kHz. Finally, we keep samples that are longer than 15 s as a result of the processing. This results in $\\mathbb {X}^{\\prime }_{\\text{init}}=\\lbrace \\mathbf {x}_{\\text{init}}^{j}\\rbrace _{j=1}^{N^{\\prime }_{\\text{init}}},\\,N^{\\prime }_{\\text{init}}=9000$.", "For enhancing the diversity of the audio content, we aim to create $\\mathbb {X}_{\\text{med}}\\subset \\mathbb {X}^{\\prime }_{\\text{init}}$ based on the tags of $\\mathbb {X}^{\\prime }_{\\text{init}}$, targeting to the most uniform possible distribution of the tags of the audio samples in $\\mathbb {X}_{\\text{med}}$. We first create the bag of tags $\\mathbb {T}$ by collecting all the tags of sounds in $\\mathbb {X}^{\\prime }_{\\text{init}}$. We omit tags that describe time or recording equipment and process (e.g. \u201cautumn\u201d, \u201cfield-recording\u201d).", "We collect the set of audio samples $\\mathbb {X}_{\\text{init}}=\\lbrace \\mathbf {x}_{\\text{init}}^{i}\\rbrace _{i=1}^{N_{\\text{init}}}$, with $N_{\\text{init}}=12000$ and their corresponding metadata (e.g. tags that indicate their content, and a short textual description), from the online platform Freesound BIBREF8. $\\mathbf {x}_{\\text{init}}$ was obtained by randomly sampling audio files from Freesound fulfilling the following criteria: lossless file type, audio quality at least 44.1 kHz and 16-bit, duration $10\\text{ s}\\le d({\\mathbf {x}_{\\text{init}}^{i}})\\le 300$ s (where $d(\\mathbf {x})$ is the duration of $\\mathbf {x}$), a textual description which first sentence does not have spelling errors according to US and UK English dictionaries (as an indication of the correctness of the metadata, e.g.", "An audio sample should belong to only one split of data (e.g., training, development, testing). This means that if a word appears only at the captions of one $\\mathbf {x}^{o}$, then this word will be appearing only at one of the splits. Having a word appearing only in training split leads to sub-optimal learning procedure, because resources are spend to words unused in validation and testing. If a word is not appearing in the training split, then the evaluation procedure suffers by having to evaluate on words not known during training. For that reason, for each $\\mathbf {x}^{o}$ we construct the set of words $\\mathbb {S}_{a}^{o}$ from $\\mathbb {C}^{\\prime o}$."]}
{"question_id": "eff9192e05d23e9a67d10be0c89a7ab2b873995b", "predicted_answer": "The quality of annotations was evaluated by having three different annotators score each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4 (the higher the better).", "predicted_evidence": ["The annotators have access only to $\\mathbf {x}_{\\text{sam}}^{z}$ and not any other information. In the second step, different annotators are instructed to correct any grammatical errors, typos, and/or rephrase the captions. This process results in $2\\times N_{\\text{cp}}$ captions per $\\mathbf {x}_{\\text{sam}}^{z}$. Finally, three (again different) annotators have access to $\\mathbf {x}_{\\text{sam}}^{z}$ and its $2\\times N_{\\text{cp}}$ captions, and score each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4 (the higher the better). The captions for each $\\mathbf {x}_{\\text{sam}}^{z}$ are sorted (first according to accuracy of description and then according to fluency), and two groups are formed: the top $N_{\\text{cp}}$ and the bottom $N_{\\text{cp}}$ captions.", "In a nutshell, each audio sample $\\mathbf {x}_{\\text{sam}}^{z}$ gets annotated by $N_{\\text{cp}}$ different annotators in the first step of the framework. The annotators have access only to $\\mathbf {x}_{\\text{sam}}^{z}$ and not any other information. In the second step, different annotators are instructed to correct any grammatical errors, typos, and/or rephrase the captions. This process results in $2\\times N_{\\text{cp}}$ captions per $\\mathbf {x}_{\\text{sam}}^{z}$. Finally, three (again different) annotators have access to $\\mathbf {x}_{\\text{sam}}^{z}$ and its $2\\times N_{\\text{cp}}$ captions, and score each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4 (the higher the better).", "Finally, three (again different) annotators have access to $\\mathbf {x}_{\\text{sam}}^{z}$ and its $2\\times N_{\\text{cp}}$ captions, and score each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4 (the higher the better). The captions for each $\\mathbf {x}_{\\text{sam}}^{z}$ are sorted (first according to accuracy of description and then according to fluency), and two groups are formed: the top $N_{\\text{cp}}$ and the bottom $N_{\\text{cp}}$ captions. The top $N_{\\text{cp}}$ captions are selected as $\\mathbb {C}_{\\text{sam}}^{z}$. We manually sanitize further $\\mathbb {C}_{\\text{sam}}^{z}$, e.g.", "Finally, three (again different) annotators have access to $\\mathbf {x}_{\\text{sam}}^{z}$ and its $2\\times N_{\\text{cp}}$ captions, and score each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4 (the higher the better). The captions for each $\\mathbf {x}_{\\text{sam}}^{z}$ are sorted (first according to accuracy of description and then according to fluency), and two groups are formed: the top $N_{\\text{cp}}$ and the bottom $N_{\\text{cp}}$ captions. The top $N_{\\text{cp}}$ captions are selected as $\\mathbb {C}_{\\text{sam}}^{z}$. We manually sanitize further $\\mathbb {C}_{\\text{sam}}^{z}$, e.g. by replacing \u201cit's\u201d with \u201cit is\u201d or \u201cits\u201d, making consistent hyphenation and compound words (e.g.", "In a nutshell, each audio sample $\\mathbf {x}_{\\text{sam}}^{z}$ gets annotated by $N_{\\text{cp}}$ different annotators in the first step of the framework. The annotators have access only to $\\mathbf {x}_{\\text{sam}}^{z}$ and not any other information. In the second step, different annotators are instructed to correct any grammatical errors, typos, and/or rephrase the captions. This process results in $2\\times N_{\\text{cp}}$ captions per $\\mathbf {x}_{\\text{sam}}^{z}$."]}
{"question_id": "87523fb927354ddc8ad1357a81f766b7ea95f53c", "predicted_answer": "According to the text, there are three different groups of annotators:\n\n1. In the first step, $N_{\\text{cp}}$ different annotators are involved.\n2. In the second step, different annotators (not specified how many) are involved.\n3. In the third step, three different annotators are involved.\n\nSo, in total, there are at least $N_{\\text{cp}} + 3$ annotators, but the exact number is not specified.", "predicted_evidence": ["The annotators have access only to $\\mathbf {x}_{\\text{sam}}^{z}$ and not any other information. In the second step, different annotators are instructed to correct any grammatical errors, typos, and/or rephrase the captions. This process results in $2\\times N_{\\text{cp}}$ captions per $\\mathbf {x}_{\\text{sam}}^{z}$. Finally, three (again different) annotators have access to $\\mathbf {x}_{\\text{sam}}^{z}$ and its $2\\times N_{\\text{cp}}$ captions, and score each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4 (the higher the better). The captions for each $\\mathbf {x}_{\\text{sam}}^{z}$ are sorted (first according to accuracy of description and then according to fluency), and two groups are formed: the top $N_{\\text{cp}}$ and the bottom $N_{\\text{cp}}$ captions.", "In a nutshell, each audio sample $\\mathbf {x}_{\\text{sam}}^{z}$ gets annotated by $N_{\\text{cp}}$ different annotators in the first step of the framework. The annotators have access only to $\\mathbf {x}_{\\text{sam}}^{z}$ and not any other information. In the second step, different annotators are instructed to correct any grammatical errors, typos, and/or rephrase the captions. This process results in $2\\times N_{\\text{cp}}$ captions per $\\mathbf {x}_{\\text{sam}}^{z}$. Finally, three (again different) annotators have access to $\\mathbf {x}_{\\text{sam}}^{z}$ and its $2\\times N_{\\text{cp}}$ captions, and score each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4 (the higher the better).", "Finally, three (again different) annotators have access to $\\mathbf {x}_{\\text{sam}}^{z}$ and its $2\\times N_{\\text{cp}}$ captions, and score each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4 (the higher the better). The captions for each $\\mathbf {x}_{\\text{sam}}^{z}$ are sorted (first according to accuracy of description and then according to fluency), and two groups are formed: the top $N_{\\text{cp}}$ and the bottom $N_{\\text{cp}}$ captions. The top $N_{\\text{cp}}$ captions are selected as $\\mathbb {C}_{\\text{sam}}^{z}$. We manually sanitize further $\\mathbb {C}_{\\text{sam}}^{z}$, e.g.", "This process results in $2\\times N_{\\text{cp}}$ captions per $\\mathbf {x}_{\\text{sam}}^{z}$. Finally, three (again different) annotators have access to $\\mathbf {x}_{\\text{sam}}^{z}$ and its $2\\times N_{\\text{cp}}$ captions, and score each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4 (the higher the better). The captions for each $\\mathbf {x}_{\\text{sam}}^{z}$ are sorted (first according to accuracy of description and then according to fluency), and two groups are formed: the top $N_{\\text{cp}}$ and the bottom $N_{\\text{cp}}$ captions. The top $N_{\\text{cp}}$ captions are selected as $\\mathbb {C}_{\\text{sam}}^{z}$.", "Finally, three (again different) annotators have access to $\\mathbf {x}_{\\text{sam}}^{z}$ and its $2\\times N_{\\text{cp}}$ captions, and score each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4 (the higher the better). The captions for each $\\mathbf {x}_{\\text{sam}}^{z}$ are sorted (first according to accuracy of description and then according to fluency), and two groups are formed: the top $N_{\\text{cp}}$ and the bottom $N_{\\text{cp}}$ captions. The top $N_{\\text{cp}}$ captions are selected as $\\mathbb {C}_{\\text{sam}}^{z}$. We manually sanitize further $\\mathbb {C}_{\\text{sam}}^{z}$, e.g. by replacing \u201cit's\u201d with \u201cit is\u201d or \u201cits\u201d, making consistent hyphenation and compound words (e.g."]}
{"question_id": "9e9aa8af4b49e2e1e8cd9995293a7982ea1aba0e", "predicted_answer": "The baseline method is not explicitly mentioned in the given context. The text describes a method for splitting a dataset into development, evaluation, and testing sets, and evaluating the performance of a machine translation method using various metrics (BLEU, METEOR, CIDEr, ROUGEL). However, it does not mention a specific baseline method that is being compared to.", "predicted_evidence": ["In Table TABREF13 are the scores of the employed metrics for the evaluation and testing splits.", "The tolerance means, for example, that we can tolerate a word appearing a total of 3 times in the whole Clotho dataset $\\mathbb {D}$, to appear 2 times in the development split (appearing 0 times in development split results in the rejection of the split set). This will result to this word appearing in either evaluation or testing split, but still this word will not appear only in one split. To pick the best set of splits, we count the amount of words that have a frequency $f^{\\text{d}}_{w}\\notin [f^{\\text{Dev}}_{w}-\\delta _{w},f^{\\text{Dev}}_{w}+\\delta _{w}]$. We score, in an ascending fashion, the sets of splits according to that amount of words and we pick the top 50 ones. For each of the 50 sets of splits, we further separate the 40% split to 20% and 20%, 1000 times.", "The tolerance means, for example, that we can tolerate a word appearing a total of 3 times in the whole Clotho dataset $\\mathbb {D}$, to appear 2 times in the development split (appearing 0 times in development split results in the rejection of the split set). This will result to this word appearing in either evaluation or testing split, but still this word will not appear only in one split. To pick the best set of splits, we count the amount of words that have a frequency $f^{\\text{d}}_{w}\\notin [f^{\\text{Dev}}_{w}-\\delta _{w},f^{\\text{Dev}}_{w}+\\delta _{w}]$. We score, in an ascending fashion, the sets of splits according to that amount of words and we pick the top 50 ones.", "We assess the performance of the method on evaluation and testing splits, using the machine translation metrics BLEU$n$ (with $n=1,\\ldots ,4$), METEOR, CIDEr, and ROUGEL for comparing the output of the method and the reference captions for the input audio sample. In a nutshell, BLEU$n$ measures a modified precision of $n$-grams (e.g. BLEU2 for 2-grams), METEOR measures a harmonic mean-based score of the precision and recall for unigrams, CIDEr measures a weighted cosine similarity of $n$-grams, and ROUGEL is a longest common subsequence-based score.", "This will result to this word appearing in either evaluation or testing split, but still this word will not appear only in one split. To pick the best set of splits, we count the amount of words that have a frequency $f^{\\text{d}}_{w}\\notin [f^{\\text{Dev}}_{w}-\\delta _{w},f^{\\text{Dev}}_{w}+\\delta _{w}]$. We score, in an ascending fashion, the sets of splits according to that amount of words and we pick the top 50 ones. For each of the 50 sets of splits, we further separate the 40% split to 20% and 20%, 1000 times. That is, we end up with 50 000 sets of splits of 60%, 20%, 20%, corresponding to development, evaluation, and testing splits, respectively."]}
{"question_id": "1fa9b6300401530738995f14a37e074c48bc9fd8", "predicted_answer": "The language of the captions is not explicitly stated in the given context. However, based on the content and style of the text, it is likely that the captions are written in English.", "predicted_evidence": ["Experiments confirmed our hypotheses: image representations are useful for caption similarity and they are complementary to textual representations, as results improve significantly when two modalities are combined together.", "The dataset is derived from a subset of the caption pairs already annotated in the Semantic Textual Similarity Task (see below). We selected some caption pairs with their similarity annotations, and added the images corresponding to each caption. While the human annotators had access to only the text, we provide the system with both the caption and corresponding image, to check whether the visual representations can be exploited by the system to solve a text understanding and inference task.", "In addition we show that the dataset allows to explore two hypothesis: H1) whether the image representations alone are able to predict caption similarity; H2) whether a combination of image and text representations allow to improve the text-only results on this similarity task.", "Results Table TABREF4 shows the results of the single and combined models. Among single models, as expected, dam obtains the highest Pearson correlation ( INLINEFORM0 ). Interestingly, the results show that images alone are valid to predict caption similarity (0.61 INLINEFORM1 ). Results also show that image and sentence representations are complementary, with the best results for a combination of DAM and RESNET50 representations. These results confirm our hypotheses, and more generally, show indications that in systems that work with text describing the real world, the representation of the real world helps to better understand the text and do better inferences.", "Score distribution Due to the caption pairs are generated from different images, strong bias towards low scores is expected (see Figure FIGREF3 ). We measured the score distribution in the two subsets separately and jointly, and see that the two subsets follow same distribution. As expected, the most frequent score is 0 (Table TABREF2 ), but the dataset still shows wide range of similarity values, with enough variability."]}
{"question_id": "9d98975ab0b75640b2c83e29e1438c76a959fbde", "predicted_answer": "The answer is not explicitly stated in the given context. However, we can infer that the average length of the captions is not provided. The context discusses the dataset, experiments, and results of caption similarity tasks, but it does not mention the average length of the captions.", "predicted_evidence": ["Experiments confirmed our hypotheses: image representations are useful for caption similarity and they are complementary to textual representations, as results improve significantly when two modalities are combined together.", "Score distribution Due to the caption pairs are generated from different images, strong bias towards low scores is expected (see Figure FIGREF3 ). We measured the score distribution in the two subsets separately and jointly, and see that the two subsets follow same distribution. As expected, the most frequent score is 0 (Table TABREF2 ), but the dataset still shows wide range of similarity values, with enough variability.", "In addition we show that the dataset allows to explore two hypothesis: H1) whether the image representations alone are able to predict caption similarity; H2) whether a combination of image and text representations allow to improve the text-only results on this similarity task.", "Results Table TABREF4 shows the results of the single and combined models. Among single models, as expected, dam obtains the highest Pearson correlation ( INLINEFORM0 ). Interestingly, the results show that images alone are valid to predict caption similarity (0.61 INLINEFORM1 ). Results also show that image and sentence representations are complementary, with the best results for a combination of DAM and RESNET50 representations. These results confirm our hypotheses, and more generally, show indications that in systems that work with text describing the real world, the representation of the real world helps to better understand the text and do better inferences.", "As the original dataset contained captions referring to the same image, and the task would be trivial for pairs of the same image, we filtered those out, that is, we only consider caption pairs that refer to different images. In total, the dataset comprises 829 instances, each instance containing a pair of images and their description, as well as a similarity value that ranges from 0 to 5. The instances are derived from the following datasets:"]}
{"question_id": "cc8bcea4052bf92f249dda276acc5fd16cac6fb4", "predicted_answer": "No, each image does not have one caption. According to the text, each instance in the dataset contains a pair of images and their descriptions, implying that each image has multiple captions.", "predicted_evidence": ["As the original dataset contained captions referring to the same image, and the task would be trivial for pairs of the same image, we filtered those out, that is, we only consider caption pairs that refer to different images. In total, the dataset comprises 829 instances, each instance containing a pair of images and their description, as well as a similarity value that ranges from 0 to 5. The instances are derived from the following datasets:", "Experiments confirmed our hypotheses: image representations are useful for caption similarity and they are complementary to textual representations, as results improve significantly when two modalities are combined together.", "In addition we show that the dataset allows to explore two hypothesis: H1) whether the image representations alone are able to predict caption similarity; H2) whether a combination of image and text representations allow to improve the text-only results on this similarity task.", "Score distribution Due to the caption pairs are generated from different images, strong bias towards low scores is expected (see Figure FIGREF3 ). We measured the score distribution in the two subsets separately and jointly, and see that the two subsets follow same distribution. As expected, the most frequent score is 0 (Table TABREF2 ), but the dataset still shows wide range of similarity values, with enough variability.", "We introduced the vSTS dataset, which contains caption pairs with human similarity annotations, where the systems can also access the actual images. The dataset aims at being a standard dataset to test the contribution of visual information when evaluating the similarity of sentences."]}
{"question_id": "35f48b8f73728fbdeb271b170804190b5448485a", "predicted_answer": "The size of the dataset is 829 instances for Subset 2014 and 445 pairs for Subset 2015.", "predicted_evidence": ["Subset 2014 This subset is derived from the Image Descriptions dataset which is a subset of the PASCAL VOC-2008 dataset BIBREF16 . PASCAL VOC-2008 dataset consists of 1,000 images and has been used by a number of image description systems. In total, we obtained 374 pairs (out of 750 in the original file).", "As the original dataset contained captions referring to the same image, and the task would be trivial for pairs of the same image, we filtered those out, that is, we only consider caption pairs that refer to different images. In total, the dataset comprises 829 instances, each instance containing a pair of images and their description, as well as a similarity value that ranges from 0 to 5. The instances are derived from the following datasets:", "Experimental setting We split the vSTS dataset into development and test partitions, sampling 50% at random, while preserving the overall score distributions. In addition, we used part of the text-only STS benchmark dataset as a training set, discarding the examples that overlap with vSTS.", "Subset 2015 The subset is derived from Image Descriptions dataset, which is a subset of 8k-picture of Flickr. 8k-Flicker is a benchmark collection for sentence-based image description, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We obtained 445 pairs (out of 750 in the original).", "In the future we plan to re-annotate the dataset with scores which are based on both the text and the image, in order to shed light on the interplay of images and text when understanding text."]}
{"question_id": "16edc21a6abc89ee2280dccf1c867c2ac4552524", "predicted_answer": "The source of the images and textual captions is a subset of the caption pairs already annotated in the Semantic Textual Similarity Task.", "predicted_evidence": ["Experiments confirmed our hypotheses: image representations are useful for caption similarity and they are complementary to textual representations, as results improve significantly when two modalities are combined together.", "The dataset is derived from a subset of the caption pairs already annotated in the Semantic Textual Similarity Task (see below). We selected some caption pairs with their similarity annotations, and added the images corresponding to each caption. While the human annotators had access to only the text, we provide the system with both the caption and corresponding image, to check whether the visual representations can be exploited by the system to solve a text understanding and inference task.", "In addition we show that the dataset allows to explore two hypothesis: H1) whether the image representations alone are able to predict caption similarity; H2) whether a combination of image and text representations allow to improve the text-only results on this similarity task.", "As the original dataset contained captions referring to the same image, and the task would be trivial for pairs of the same image, we filtered those out, that is, we only consider caption pairs that refer to different images. In total, the dataset comprises 829 instances, each instance containing a pair of images and their description, as well as a similarity value that ranges from 0 to 5. The instances are derived from the following datasets:", "Results Table TABREF4 shows the results of the single and combined models. Among single models, as expected, dam obtains the highest Pearson correlation ( INLINEFORM0 ). Interestingly, the results show that images alone are valid to predict caption similarity (0.61 INLINEFORM1 ). Results also show that image and sentence representations are complementary, with the best results for a combination of DAM and RESNET50 representations. These results confirm our hypotheses, and more generally, show indications that in systems that work with text describing the real world, the representation of the real world helps to better understand the text and do better inferences."]}
{"question_id": "3b8da74f5b359009d188cec02adfe4b9d46a768f", "predicted_answer": "The evaluation metric used in this experiment is the F1 score.", "predicted_evidence": ["We conducted experiments on Chinese NER datasets to demonstrate the effectiveness of the pre-training models specified in section III. For the dataset, we used the MSRA-2006 published by Microsoft Research Asia.", "The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.", "As mentioned above, the performance of deep learning methods depends on the quality of labeled training sets. Therefore, researchers have proposed pre-training models to improve the performance of the NLP tasks through a large number of unlabeled data. Recent research on pre-training models has mainly focused on BERT. For example, R. Qiao $et$ $al$. and N. Li $et$ $al$. BIBREF13BIBREF14 used BERT and ELMO respectively to improve the performance of entity recognition in chinese clinical records. E. Alsentzer $et$ $al$. , L. Yao $et$ $al$. and K. Huang $et$ $al$. BIBREF15BIBREF16BIBREF17 used domain-specific corpus to train BERT(the model structure and pre-training tasks are unchanged), and used this model for a domain-specific task, obtaining the result of SOTA.", "The purpose of this paper is to introduce the structure and pre-training tasks of four common pre-trained models (BERT, ERNIE, ERNIE2.0-tiny, RoBERTa), and how to apply these models to a NER task by fine-tuning. Moreover, we also conduct experiments on the MSRA-2006 dataset to test the effects of different pre-training models on the NER task, and discuss the reasons for these results from the model architecture and pre-training tasks respectively.", "The experiments were conducted on the AI Studio platform launched by the Baidu. This platform has a build-in deep learning framework PaddlePaddle and is equipped with a V100 GPU. The pre-training models mentioned above were downloaded by PaddleHub, which is a pre-training model management toolkit. It is also launched by the Baidu. For hyper-parameter configuration, we adjusted them according to the performance on development sets. In this article, the number of the epoch is 2, the learning rate is 5e-5, and the batch size is 16."]}
{"question_id": "6bce04570d4745dcfaca5cba64075242308b65cf", "predicted_answer": "The baseline model is the BiGRU+CRF model, which achieved an F1 value of 90.32.", "predicted_evidence": ["The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.", "Secondly, for pre-training models with the same model structure, RoBERTa obtained the result of SOTA. BERT and ERNIE retain the sentence pre-training tasks of NSP and DLM respectively, while RoBERTa removes the sentence-level pre-training task because Liu $et$ $al$. BIBREF9 hypothesizes the model can not learn long-range dependencies. The results confirm the above hypothesis. For the NER task, sentence-level pre-training tasks do not improve performance. In contrast, RoBERTa removes the NSP task and improves the performance of entity recognition. As described by Liu $et$ $al$. BIBREF9, the NSP and the MLP are designed to improve the performance on specific downstream tasks, such as the SQuAD 1.1, which requires reasoning about the relationships between pairs of sentences.", "The purpose of this paper is to introduce the structure and pre-training tasks of four common pre-trained models (BERT, ERNIE, ERNIE2.0-tiny, RoBERTa), and how to apply these models to a NER task by fine-tuning. Moreover, we also conduct experiments on the MSRA-2006 dataset to test the effects of different pre-training models on the NER task, and discuss the reasons for these results from the model architecture and pre-training tasks respectively.", "Secondly, for pre-training models with the same model structure, RoBERTa obtained the result of SOTA. BERT and ERNIE retain the sentence pre-training tasks of NSP and DLM respectively, while RoBERTa removes the sentence-level pre-training task because Liu $et$ $al$. BIBREF9 hypothesizes the model can not learn long-range dependencies. The results confirm the above hypothesis. For the NER task, sentence-level pre-training tasks do not improve performance. In contrast, RoBERTa removes the NSP task and improves the performance of entity recognition. As described by Liu $et$ $al$. BIBREF9, the NSP and the MLP are designed to improve the performance on specific downstream tasks, such as the SQuAD 1.1, which requires reasoning about the relationships between pairs of sentences. However, the results show that the NER task does not rely on sentence-level knowledge, and using sentence-level pre-training tasks hurts performance because the pre-training models may not able to learn long-range dependencies.", "In this section, we first introduce the four pre-trained models (BERT, ERNIE, ERNIE 2.0-tiny, RoBERTa), including their model structures and pre-training tasks. Then we introduce how to use them for the NER task through fine-tuning."]}
{"question_id": "37e6ce5cfc9d311e760dad8967d5085446125408", "predicted_answer": "According to the text, Roberta's results were an F1 value of 94.17, which is the highest among the pre-training models, and 3-4 percentage points higher than the baseline model.", "predicted_evidence": ["RoBERTa is similar to BERT, except that it changes the masking strategy and removes the NSP taskBIBREF9.", "Like ERNIE, RoBERTa has the same model structure as BERT, with 12 Transformer layers, 768 hidden units, and 12 self-attention heads.", "Secondly, for pre-training models with the same model structure, RoBERTa obtained the result of SOTA. BERT and ERNIE retain the sentence pre-training tasks of NSP and DLM respectively, while RoBERTa removes the sentence-level pre-training task because Liu $et$ $al$. BIBREF9 hypothesizes the model can not learn long-range dependencies. The results confirm the above hypothesis. For the NER task, sentence-level pre-training tasks do not improve performance. In contrast, RoBERTa removes the NSP task and improves the performance of entity recognition. As described by Liu $et$ $al$. BIBREF9, the NSP and the MLP are designed to improve the performance on specific downstream tasks, such as the SQuAD 1.1, which requires reasoning about the relationships between pairs of sentences.", "The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.", "RoBERTa removes the NSP task in BERT and changes the masking strategy from static to dynamicBIBREF9. BERT performs masking once during data processing, resulting in a single static mask. However, RoBoERTa changes masking position in every epoch. Therefore, the pre-training model will gradually adapt to different masking strategies and learn different language representations."]}
{"question_id": "6683008e0a8c4583058d38e185e2e2e18ac6cf50", "predicted_answer": "The worst performing model is ERNIE-tiny (also referred to as ERNIE-2.0-tiny) with an F1 value that is even lower than the baseline model (BiGRU+CRF) and 4 percentage points lower than the baseline model.", "predicted_evidence": ["The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.", "In this section, we first introduce the four pre-trained models (BERT, ERNIE, ERNIE 2.0-tiny, RoBERTa), including their model structures and pre-training tasks. Then we introduce how to use them for the NER task through fine-tuning.", "The purpose of this paper is to introduce the structure and pre-training tasks of four common pre-trained models (BERT, ERNIE, ERNIE2.0-tiny, RoBERTa), and how to apply these models to a NER task by fine-tuning. Moreover, we also conduct experiments on the MSRA-2006 dataset to test the effects of different pre-training models on the NER task, and discuss the reasons for these results from the model architecture and pre-training tasks respectively.", "First of all, it is shown that the deeper the layer, the better the performance. All pre-training models have 12 Transformer layers, except ERNIE2.0-tiny. Although Ernie2.0-tiny increases the number of hidden units and improves the pre-training task with continual pre-training, 3 Transformer layers can not extract semantic knowledge well. The F1 value of ERNIE-2.0-tiny is even lower than the baseline model.", "The experiments were conducted on the AI Studio platform launched by the Baidu. This platform has a build-in deep learning framework PaddlePaddle and is equipped with a V100 GPU. The pre-training models mentioned above were downloaded by PaddleHub, which is a pre-training model management toolkit. It is also launched by the Baidu. For hyper-parameter configuration, we adjusted them according to the performance on development sets. In this article, the number of the epoch is 2, the learning rate is 5e-5, and the batch size is 16."]}
{"question_id": "7bd24920163a4801b34d0a50aed957ba8efed0ab", "predicted_answer": "The question is asking about the length of the sentiment analysis dataset, but the provided context does not explicitly mention the length of the dataset. However, it does mention that the statistics of the datasets are shown in Table TABREF15, which is not provided in the given context. Therefore, it is not possible to determine the length of the sentiment analysis dataset from the given information.", "predicted_evidence": ["We use three popular datasets in ABSA task: Restaurant reviews and Laptop reviews from SemEval 2014 Task 4 BIBREF5, and ACL 14 Twitter dataset BIBREF6.", "In order to visualize how BERT-LSTM benefits from sequential representations of intermediate layers, we use principal component analysis (PCA) to visualize the intermediate representations of [CLS] token, shown in figure FIGREF20. There are three classes of the sentiment data, illustrated in blue, green and red, representing positive, neural and negative, respectively. Since the task-specific information is mainly extracted from the last six layers of BERT, we simply illustrate the last six layers. It is easy to draw the conclusion that BERT-LSTM partitions different classes of data faster and more dense than vanilla BERT under the same training epoch.", "Aspect based sentiment analysis (ABSA) is an important task in natural language processing. It aims at collecting and analyzing the opinions toward the targeted aspect in an entire text. In the past decade, ABSA has received great attention due to a wide range of applications BIBREF0, BIBREF1. Aspect-level (also mentioned as \u201ctarget-level\u201d) sentiment classification as a subtask of ABSA BIBREF0 aims at judging the sentiment polarity for a given aspect. For example, given a sentence \u201cI hated their service, but their food was great\u201d, the sentiment polarities for the target \u201cservice\u201d and \u201cfood\u201d are negative and positive respectively.", "This section briefly describes three ABSA datasets and SNLI dataset. Statistics of these datasets are shown in Table TABREF15.", "It is the first to explore the potential of utilizing intermediate layers of BERT and we design two effective information pooling strategies to solve aspect based sentiment analysis task."]}
{"question_id": "df01e98095ba8765d9ab0d40c9e8ef34b64d3700", "predicted_answer": "The NLI dataset used is SNLI (Stanford Natural Language Inference) dataset.", "predicted_evidence": ["This section briefly describes three ABSA datasets and SNLI dataset. Statistics of these datasets are shown in Table TABREF15.", "Additional experiments on a large NLI dataset illustrate that our method has a certain degree of versatility, and can be easily applied to some other NLP tasks.", "We use three popular datasets in ABSA task: Restaurant reviews and Laptop reviews from SemEval 2014 Task 4 BIBREF5, and ACL 14 Twitter dataset BIBREF6.", "The SNLI dataset is quite large, so we simply take the best-performing model on the development set for testing.", "To validate the generality of our method, we conduct experiment on SNLI dataset and apply same pooling strategies to currently state-of-the-art method MT-DNN BIBREF11, which is also a BERT based model, named MT-DNN-Attention and MT-DNN-LSTM."]}
{"question_id": "a7a433de17d0ee4dd7442d7df7de17e508baf169", "predicted_answer": "According to the context, the aspects considered in Aspect-Based Sentiment Analysis (ABSA) are:\n\n1. Sentiment polarity (positive, negative, or neutral)\n2. Targeted aspect (e.g., \"service\" and \"food\" in the example sentence)\n\nThese aspects are considered to analyze the opinions towards a specific aspect in a given text.", "predicted_evidence": ["Aspect based sentiment analysis (ABSA) is an important task in natural language processing. It aims at collecting and analyzing the opinions toward the targeted aspect in an entire text. In the past decade, ABSA has received great attention due to a wide range of applications BIBREF0, BIBREF1. Aspect-level (also mentioned as \u201ctarget-level\u201d) sentiment classification as a subtask of ABSA BIBREF0 aims at judging the sentiment polarity for a given aspect. For example, given a sentence \u201cI hated their service, but their food was great\u201d, the sentiment polarities for the target \u201cservice\u201d and \u201cfood\u201d are negative and positive respectively.", "Main contributions of this paper can be summarized as follows:", "Most of existing methods focus on designing sophisticated deep learning models to mining the relation between context and the targeted aspect. Majumder et al., majumder2018iarm adopt a memory network architecture to incorporate the related information of neighboring aspects. Fan et al., fan2018multi combine the fine-grained and coarse-grained attention to make LSTM treasure the aspect-level interactions. However, the biggest challenge in ABSA task is the shortage of training data, and these complex models did not lead to significant improvements in outcomes.", "It is the first to explore the potential of utilizing intermediate layers of BERT and we design two effective information pooling strategies to solve aspect based sentiment analysis task.", "Given a sentence-apsect pair, ABSA aims at predicting the sentiment polarity (positive, negative or neural) of the sentence over the aspect."]}
{"question_id": "abfa3daaa984dfe51289054f4fb062ce93f31d19", "predicted_answer": "The intermediate layers of BERT gave better results. Specifically, BERT-PT-LSTM and BERT-PT-Attention, which utilize intermediate layers, outperformed BERT-PT with a large margin on Laptop and Restaurant datasets.", "predicted_evidence": ["The gains seem to be small, but the improvements of the method are straightforwardly reasonable and the flexibility of our strategies makes it easier to apply to a variety of other tasks.", "In this work, we explore the potential of utilizing BERT intermediate layers and propose two effective pooling strategies to enhance the performance of fine-tuning of BERT. Experimental results demonstrate the effectiveness and generality of the proposed approach.", "As shown in Table TABREF26, the results were consistent with those on ABSA. From the results, BERT-Attention and BERT-LSTM perform better than vanilla BERT$_{\\tiny \\textsc {BASE}}$. Furthermore, MT-DNN-Attention and MT-DNN-LSTM outperform vanilla MT-DNN on Dev set, and are slightly inferior to vanilla MT-DNN on Test set. As a whole, our pooling strategies generally improve the vanilla BERT-based model, which draws the same conclusion as on ABSA.", "The BERT-PT, BERT-PT-LSTM and BERT-PT-Attention are all initialized with post-trained BERT BIBREF9 weights . We can see that both BERT-PT-LSTM and BERT-PT-Attention outperform BERT-PT with a large margin on Laptop and Restaurant dataset . From the results, the conclusion that utilizing intermediate layers of BERT brings better results is still true.", "Finally, we pass the pooled output $o$ to a fully-connected layer for label prediction:"]}
{"question_id": "1702985a3528e876bb19b8e223399729d778b4e4", "predicted_answer": "The answer is not explicitly stated in the given context. It only mentions \"sentiment labels by human annotators\" and \"sentiment labels by expert Pidgin English speakers\", but it does not provide the exact number of annotators used.", "predicted_evidence": ["The quality of sentiment labels generated by our updated VADER lexicon is better compared to the labels generated by the original VADER English lexicon.TABREF4.Sentiment labels by human annotators was able to capture nuances that the rule based sentiment labelling could not capture.More work can be done to increase the number of instances in the dataset.", "This study uses the original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner) to calculate the compound sentiment scores for about 14,000 Nigerian Pidgin tweets. The updated VADER lexicon (updated with 300 Pidgin tokens and their sentiment scores) performed better than the original VADER lexicon. The labelled sentiments from the updated VADER were then compared with sentiment labels by expert Pidgin English speakers.", "VADER lexicon has about 9000 tokens (built from existing well-established sentiment word-banks (LIWC, ANEW, and GI) incorporated with a full list of Western-style emoticons, sentiment-related acronyms and initialisms (e.g., LOL and WTF)commonly used slang with sentiment value (e.g., nah, meh and giggly) ) with their mean sentiment rating.BIBREF6. Sentiment analysis in code-mixed text has been established in literature both at word and sub-word levels BIBREF7 BIBREF8 BIBREF9. The possibility of improving sentiment detection via label transfer from monolingual to synthetic code-switched text has been well executed with significant improvements in sentiment labelling accuracy (1.5%, 5.11%, 7.20%) for three different language pairs BIBREF5", "But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text. One of the biggest social catalysts is the emerging urban youth subculture and the new growing semi-literate lower class in a chaotic medley of a converging megacity BIBREF4 BIBREF5 VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains. VADER lexicon has about 9000 tokens (built from existing well-established sentiment word-banks (LIWC, ANEW, and GI) incorporated with a full list of Western-style emoticons, sentiment-related acronyms and initialisms (e.g., LOL and WTF)commonly used slang with sentiment value (e.g., nah, meh and giggly) ) with their mean sentiment rating.BIBREF6.", "One of the biggest social catalysts is the emerging urban youth subculture and the new growing semi-literate lower class in a chaotic medley of a converging megacity BIBREF4 BIBREF5 VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains. VADER lexicon has about 9000 tokens (built from existing well-established sentiment word-banks (LIWC, ANEW, and GI) incorporated with a full list of Western-style emoticons, sentiment-related acronyms and initialisms (e.g., LOL and WTF)commonly used slang with sentiment value (e.g., nah, meh and giggly) ) with their mean sentiment rating.BIBREF6. Sentiment analysis in code-mixed text has been established in literature both at word and sub-word levels BIBREF7 BIBREF8 BIBREF9."]}
{"question_id": "f44a9ed166a655df1d54683c91935ab5e566a04f", "predicted_answer": "Data was collected through the following process:\n\n1. Three people who are indigenes or lived in the South South part of Nigeria, where Nigerian Pidgin is a prevalent method of communication, were briefed on the fundamentals of word sentiments.\n2. Each of them labelled data points, which were then verified by at least one other person after initial labelling.\n3. The labelled sentiments were then compared with sentiment labels by expert Pidgin English speakers.\n\nAdditionally, the study used a dataset of about 14,000 Nigerian Pidgin tweets, which were analyzed using the original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner) to calculate the compound sentiment scores.", "predicted_evidence": ["We acknowledge Kessiena Rita David,Patrick Ehizokhale Oseghale and Peter Chimaobi Onuoha for using their mastery of Nigerian Pidgin to translate and label the datasets.", "Three people who are indigenes or lived in the South South part of Nigeria, where Nigerian Pidgin is a prevalent method of communication were briefed on the fundamentals of word sentiments. Each labelled Data point was verified by at least one other person after initial labelling.", "The increased instances of these often lead to changes in the lexical and grammatical context of the language, which are largely motivated by situational and stylistic factors BIBREF1. In addition, the need to communicate effectively to different social classes have further orchestrated this shift in language meaning over a long period of time to serve socio-linguistic functions BIBREF2 Nigeria is estimated to have between three and five million people, who primarily use Pidgin in their day-to-day interactions. But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text. One of the biggest social catalysts is the emerging urban youth subculture and the new growing semi-literate lower class in a chaotic medley of a converging megacity BIBREF4 BIBREF5 VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains.", "But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text. One of the biggest social catalysts is the emerging urban youth subculture and the new growing semi-literate lower class in a chaotic medley of a converging megacity BIBREF4 BIBREF5 VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains. VADER lexicon has about 9000 tokens (built from existing well-established sentiment word-banks (LIWC, ANEW, and GI) incorporated with a full list of Western-style emoticons, sentiment-related acronyms and initialisms (e.g., LOL and WTF)commonly used slang with sentiment value (e.g., nah, meh and giggly) ) with their mean sentiment rating.BIBREF6.", "This study uses the original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner) to calculate the compound sentiment scores for about 14,000 Nigerian Pidgin tweets. The updated VADER lexicon (updated with 300 Pidgin tokens and their sentiment scores) performed better than the original VADER lexicon. The labelled sentiments from the updated VADER were then compared with sentiment labels by expert Pidgin English speakers."]}
{"question_id": "0ba1514fb193c52a15c31ffdcd5c3ddbb2bb2c40", "predicted_answer": "The performance of the updated VADER lexicon, which includes 300 Nigerian Pidgin tokens and their sentiment scores, is better than the original VADER lexicon. The updated lexicon was able to capture the nuances of Nigerian Pidgin English sentiment more accurately, as it was tailored to the specific language and cultural context. The study found that the updated VADER lexicon performed better than the original VADER lexicon, suggesting that incorporating Nigerian English data into the model improves its performance in sentiment classification.", "predicted_evidence": ["This study uses the original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner) to calculate the compound sentiment scores for about 14,000 Nigerian Pidgin tweets. The updated VADER lexicon (updated with 300 Pidgin tokens and their sentiment scores) performed better than the original VADER lexicon. The labelled sentiments from the updated VADER were then compared with sentiment labels by expert Pidgin English speakers.", "During the translation of VADER English lexicon to suitable one-word Nigerian Pidgin translation, a total of 300 Nigerian pidgin tokens were successfully translated from the standard VADER English lexicon. One of the challenges of this translation is that the direct translation of most the sentiment words in the original VADER English Lexicon translates to phrases not single one-word tokens and certain pidgin words translates to many english words.TABREF5.", "But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text. One of the biggest social catalysts is the emerging urban youth subculture and the new growing semi-literate lower class in a chaotic medley of a converging megacity BIBREF4 BIBREF5 VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains. VADER lexicon has about 9000 tokens (built from existing well-established sentiment word-banks (LIWC, ANEW, and GI) incorporated with a full list of Western-style emoticons, sentiment-related acronyms and initialisms (e.g., LOL and WTF)commonly used slang with sentiment value (e.g., nah, meh and giggly) ) with their mean sentiment rating.BIBREF6.", "Three people who are indigenes or lived in the South South part of Nigeria, where Nigerian Pidgin is a prevalent method of communication were briefed on the fundamentals of word sentiments. Each labelled Data point was verified by at least one other person after initial labelling.", "The increased instances of these often lead to changes in the lexical and grammatical context of the language, which are largely motivated by situational and stylistic factors BIBREF1. In addition, the need to communicate effectively to different social classes have further orchestrated this shift in language meaning over a long period of time to serve socio-linguistic functions BIBREF2 Nigeria is estimated to have between three and five million people, who primarily use Pidgin in their day-to-day interactions. But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text. One of the biggest social catalysts is the emerging urban youth subculture and the new growing semi-literate lower class in a chaotic medley of a converging megacity BIBREF4 BIBREF5 VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains."]}
{"question_id": "d14118b18ee94dafe170439291e20cb19ab7a43c", "predicted_answer": "The full English language-based sentiment analysis models that have been tried are:\n\n1. VADER (Valence Aware Dictionary and Sentiment Reasoner)\n2. LIWC (Linguistic Inquiry and Word Count)\n3. ANEW (Affective Norms for English Words)\n4. GI (General Inquirer)\n\nThese models have been used as a basis for building the VADER lexicon, which has been updated with 300 Pidgin tokens and their sentiment scores to improve sentiment analysis in Nigerian Pidgin tweets.", "predicted_evidence": ["VADER lexicon has about 9000 tokens (built from existing well-established sentiment word-banks (LIWC, ANEW, and GI) incorporated with a full list of Western-style emoticons, sentiment-related acronyms and initialisms (e.g., LOL and WTF)commonly used slang with sentiment value (e.g., nah, meh and giggly) ) with their mean sentiment rating.BIBREF6. Sentiment analysis in code-mixed text has been established in literature both at word and sub-word levels BIBREF7 BIBREF8 BIBREF9. The possibility of improving sentiment detection via label transfer from monolingual to synthetic code-switched text has been well executed with significant improvements in sentiment labelling accuracy (1.5%, 5.11%, 7.20%) for three different language pairs BIBREF5", "One of the biggest social catalysts is the emerging urban youth subculture and the new growing semi-literate lower class in a chaotic medley of a converging megacity BIBREF4 BIBREF5 VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains. VADER lexicon has about 9000 tokens (built from existing well-established sentiment word-banks (LIWC, ANEW, and GI) incorporated with a full list of Western-style emoticons, sentiment-related acronyms and initialisms (e.g., LOL and WTF)commonly used slang with sentiment value (e.g., nah, meh and giggly) ) with their mean sentiment rating.BIBREF6. Sentiment analysis in code-mixed text has been established in literature both at word and sub-word levels BIBREF7 BIBREF8 BIBREF9.", "But it is said to be a second language to a much higher number of up to 75 million people in Nigeria alone, about half the population.BIBREF3. It has evolved in meaning compared to Standard English due to intertextuality, the shaping of a text's meaning by another text based on the interconnection and influence of the audience's interpretation of a text. One of the biggest social catalysts is the emerging urban youth subculture and the new growing semi-literate lower class in a chaotic medley of a converging megacity BIBREF4 BIBREF5 VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains. VADER lexicon has about 9000 tokens (built from existing well-established sentiment word-banks (LIWC, ANEW, and GI) incorporated with a full list of Western-style emoticons, sentiment-related acronyms and initialisms (e.g., LOL and WTF)commonly used slang with sentiment value (e.g., nah, meh and giggly) ) with their mean sentiment rating.BIBREF6.", "This study uses the original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner) to calculate the compound sentiment scores for about 14,000 Nigerian Pidgin tweets. The updated VADER lexicon (updated with 300 Pidgin tokens and their sentiment scores) performed better than the original VADER lexicon. The labelled sentiments from the updated VADER were then compared with sentiment labels by expert Pidgin English speakers.", "The quality of sentiment labels generated by our updated VADER lexicon is better compared to the labels generated by the original VADER English lexicon.TABREF4.Sentiment labels by human annotators was able to capture nuances that the rule based sentiment labelling could not capture.More work can be done to increase the number of instances in the dataset."]}
{"question_id": "d922eaa5aa135c1ae211827c6a599b4d69214563", "predicted_answer": "The answer is not explicitly stated in the given context, but we can make an educated inference.\n\nThe context discusses various approaches to modeling features for emotion classification tasks, including the use of different embeddings, hierarchical bi-directional GRU models, and feature engineering techniques such as sentiment and objective information (SOI) and emotion lexicon features.\n\nWhile the context does not directly address how different turns of conversation are treated, we can infer that the models are designed to capture contextual information and relationships between words or sentences. This implies that different turns of conversation might be treated differently when modeling features, as the context and relationships between words or sentences may change across turns.\n\nFor example, the use of hierarchical bi-directional GRU models suggests that the model is designed to capture long-range dependencies and contextual information, which may be affected by different turns of conversation. Similarly, the SOI and emotion lexicon features are designed to capture sentiment and emotion information, which may also be influenced by different turns of conversation.\n\nHowever, without more specific information on how the models are implemented and trained, it is difficult to say exactly how different turns of conversation are treated when modeling features.", "predicted_evidence": ["The results indicates the impact of contextual information using different embeddings, which are different in feature representation. Results of class happy without contextual features has %44.16 by GRU-att-ELMo model, and %49.38 by GRU-att-ELMo+F.", "In future we want to improve our model to be able to distinguish between emotion classes in a more sufficient way. It is possible that hierarchical bi-directional GRU model can be beneficial, since these models compute history and future sequence while training the model.", "Among these works BIBREF16 proposed a Bi-LSTM architecture equipped with a multi-layer self attention mechanism, BIBREF17 their model learned the representation of each tweet using mixture of different embedding. in WASSA 2017 Shared Task on Emotion Intensity BIBREF18 , among the proposed approaches, we can recognize teams who used different word embeddings: GloVe or word2vec BIBREF19 , BIBREF20 and exploit a neural net architecture such as LSTM BIBREF21 , BIBREF22 , LSTM-CNN combinations BIBREF23 , BIBREF24 and bi-directional versions BIBREF19 to predict emotion intensity. Similar approach is developed by BIBREF25 using sentiment and LSTM architecture. Proper word embedding for emotion task is key, choosing the most efficient distance between vectors is crucial, the following studies explore solution sparsity related properties possibly including uniqueness BIBREF26 , BIBREF27 .", "Sentiment and objective Information (SOI)- relativity of subjectivity and sentiment with emotion are well studied in the literature. To craft these features we use SentiwordNet BIBREF5 , we create sentiment and subjective score per word in each sentences. SentiwordNet is the result of the automatic annotation of all the synsets of WORDNET according to the notions of positivity, negativity, and neutrality. Each synset s in WORDNET is associated to three numerical scores Pos(s), Neg(s), and Obj(s) which indicate how positive, negative, and objective (i.e., neutral) the terms contained in the synset are. Different senses of the same term may thus have different opinion-related properties. These scores are presented per sentence and their lengths are equal to the length of each sentence. In case that the score is not available, we used a fixed score 0.001.", "Emotion Lexicon feature (emo)- presence of emotion words is the first flag for a sentence to be emotional. We use NRC Emotion Lexicon BIBREF6 with 8 emotion tags (e.i. joy, trust, anticipation, surprise, anger, fear, sadness, disgust). We demonstrate the presence of emotion words as an 8 dimension feature, presenting all 8 emotion categories of the NRC lexicon. Each feature represent one emotion category, where 0.001 indicates of absent of the emotion and 1 indicates the presence of the emotion. The advantage of this feature is their portability in transferring emotion learning across genres."]}
{"question_id": "ff668c7e890064756cdd2f9621e1cedb91eef1d0", "predicted_answer": "According to the text, they bootstrap with contextual information by using different word embeddings that capture local context, such as ELMo and fasttext, which create better results in emotion detection. They also experiment with different word embedding methods, including word2vec and GloVe, and tune the pre-trained word embeddings with local context during their experiments. Additionally, they use a GRU-att-ELMo model, which incorporates contextual information through the use of ELMo embeddings and attention mechanisms.", "predicted_evidence": ["The results indicates the impact of contextual information using different embeddings, which are different in feature representation. Results of class happy without contextual features has %44.16 by GRU-att-ELMo model, and %49.38 by GRU-att-ELMo+F.", "Using different word embedding or end to end models where word representation learned from local context create different results in emotion detection. We noted that pre-trained word embeddings need to be tuned with local context during our experiments or it causes the model to not converge. We experimented with different word embedding methods such as word2vec, GloVe BIBREF7 , fasttext BIBREF8 , and ELMo. Among these methods fasttext and ELMo create better results.", "Data pre-processing - we tokenize all the data. For tweets we replace all the URLs, image URLs, hashtags, @users with specific anchors. Based on the popularity of each emoticon per each emotion tag, we replace them with the corresponding emotion tag. We normalized all the repeated characters, finally caps words are replaced with lower case but marked as caps words.", "In future we want to improve our model to be able to distinguish between emotion classes in a more sufficient way. It is possible that hierarchical bi-directional GRU model can be beneficial, since these models compute history and future sequence while training the model.", "baseline- in each sentence we tagged every emotional word using NRC emotion lexicon BIBREF6 , if any emotion has majority occurrence we pick that emotion tag as sentence emotion tag, when all emotion tags happen only once we randomly choose among them, when there is no emotional word we tag the sentence as others. We only use the portion of the emotion lexicon that covers the tags in the task (i.e. happy, sad, and angry)."]}
{"question_id": "d3cfbe497a30b750a8de3ea7f2cecf4753a4e1f9", "predicted_answer": "According to the text, they utilize fasttext and ELMo word embeddings for the EmoContext task.", "predicted_evidence": ["Using different word embedding or end to end models where word representation learned from local context create different results in emotion detection. We noted that pre-trained word embeddings need to be tuned with local context during our experiments or it causes the model to not converge. We experimented with different word embedding methods such as word2vec, GloVe BIBREF7 , fasttext BIBREF8 , and ELMo. Among these methods fasttext and ELMo create better results.", "Emotion Lexicon feature (emo)- presence of emotion words is the first flag for a sentence to be emotional. We use NRC Emotion Lexicon BIBREF6 with 8 emotion tags (e.i. joy, trust, anticipation, surprise, anger, fear, sadness, disgust). We demonstrate the presence of emotion words as an 8 dimension feature, presenting all 8 emotion categories of the NRC lexicon. Each feature represent one emotion category, where 0.001 indicates of absent of the emotion and 1 indicates the presence of the emotion. The advantage of this feature is their portability in transferring emotion learning across genres.", "The results indicates the impact of contextual information using different embeddings, which are different in feature representation. Results of class happy without contextual features has %44.16 by GRU-att-ELMo model, and %49.38 by GRU-att-ELMo+F.", "Model Architecture - our model has an embedding layer of 300 dimensions using fasttext embedding, and 1024 dimensions using ELMo BIBREF3 embedding. GRU layer has 70 hidden unites. We have 3 perceptron layers with size 300. Last layer is a softmax layer to predict emotion tags. Textual information layers (explained in section SECREF8 ) are concatenated with GRU layer as auxiliary layer. We utilize a dropout BIBREF4 layer after the first perceptron layer for regularization.", "In semEval 2018 task-1, Affect in Tweets BIBREF13 , 6 team reported results on sub-task E-c (emotion classification), mainly using neural net architectures, features and resources, and emotion lexicons. Among these works BIBREF16 proposed a Bi-LSTM architecture equipped with a multi-layer self attention mechanism, BIBREF17 their model learned the representation of each tweet using mixture of different embedding. in WASSA 2017 Shared Task on Emotion Intensity BIBREF18 , among the proposed approaches, we can recognize teams who used different word embeddings: GloVe or word2vec BIBREF19 , BIBREF20 and exploit a neural net architecture such as LSTM BIBREF21 , BIBREF22 , LSTM-CNN combinations BIBREF23 , BIBREF24 and bi-directional versions BIBREF19 to predict emotion intensity. Similar approach is developed by BIBREF25 using sentiment and LSTM architecture."]}
{"question_id": "73d87f6ead32653a518fbe8cdebd81b4a3ffcac0", "predicted_answer": "The performance results of their network are:\n\n* For Japanese-English, their approach yields a gain of 2.2 BLEU compared to the original single NMT network at about the same decoding speed, or a CPU decoding speed up with only a minor drop in BLEU.\n* For English-German (En-De), the results are not explicitly stated in terms of BLEU score improvement, but the method is applied to this dataset as well.", "predicted_evidence": ["Unfolded NMT networks approximate but do not exactly match the output of the ensemble due to two reasons. First, the unfolded network synchronizes the attentions of the individual models. Each decoding step in the unfolded network computes a single attention weight vector. In contrast, ensemble decoding would compute one attention weight vector for each of the INLINEFORM0 input models. A second difference is that the ensemble decoder first applies the softmax at the output layer, and then averages the prediction probabilities. The unfolded network averages the neuron activities (i.e. the logits) first, and then applies the softmax function. Interestingly, as shown in Sec. SECREF4 , these differences do not have any impact on the BLEU score but yield potential speed advantages of unfolding since the computationally expensive softmax layer is only applied once.", "Junczys-Dowmunt et al. averaging2,averaging1 reported gains from averaging the weight matrices of multiple checkpoints of the same training run. However, our attempts to replicate their approach were not successful. Averaging might work well when the behaviour of corresponding units is similar across networks, but that cannot be guaranteed when networks are trained independently.", "We have described a generic method for improving the decoding speed and BLEU score of single system NMT. Our approach involves unfolding an ensemble of multiple systems into a single large neural network and shrinking this network by removing redundant neurons. Our best results on Japanese-English either yield a gain of 2.2 BLEU compared to the original single NMT network at about the same decoding speed, or a INLINEFORM0 CPU decoding speed up with only a minor drop in BLEU.", "We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for En-De). We choose a widely used, simple ensembling method (prediction averaging) as our baseline. We feel that the prevalence of this method makes it a reasonable baseline for our experiments.", "The standard attention-based NMT network architecture BIBREF19 includes three linear layers: the embedding layer in the encoder, and the output and feedback embedding layers in the decoder. We have found that linear layers are particularly easy to shrink using low-rank matrix approximation. As before we denote the incoming weight matrix as INLINEFORM0 and the outgoing weight matrix as INLINEFORM1 . Since the layer is linear, we could directly connect the previous layer with the next layer using the product of both weight matrices INLINEFORM2 . However, INLINEFORM3 may be very large. Therefore, we approximate INLINEFORM4 as a product of two low rank matrices INLINEFORM5 and INLINEFORM6 ( INLINEFORM7 ) where INLINEFORM8 is the desired layer size. A very common way to find such a matrix factorization is using truncated singular value decomposition (SVD). The layer is eventually shrunk by replacing INLINEFORM9 with INLINEFORM10 and INLINEFORM11 with INLINEFORM12 ."]}
{"question_id": "fda47c68fd5f7b44bd539f83ded5882b96c36dd7", "predicted_answer": "The baseline is a widely used, simple ensembling method, specifically prediction averaging.", "predicted_evidence": ["We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for En-De). We choose a widely used, simple ensembling method (prediction averaging) as our baseline. We feel that the prevalence of this method makes it a reasonable baseline for our experiments.", "sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for En-De). We choose a widely used, simple ensembling method (prediction averaging) as our baseline.", "Unfolded NMT networks approximate but do not exactly match the output of the ensemble due to two reasons. First, the unfolded network synchronizes the attentions of the individual models. Each decoding step in the unfolded network computes a single attention weight vector. In contrast, ensemble decoding would compute one attention weight vector for each of the INLINEFORM0 input models. A second difference is that the ensemble decoder first applies the softmax at the output layer, and then averages the prediction probabilities. The unfolded network averages the neuron activities (i.e. the logits) first, and then applies the softmax function. Interestingly, as shown in Sec. SECREF4 , these differences do not have any impact on the BLEU score but yield potential speed advantages of unfolding since the computationally expensive softmax layer is only applied once.", "Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for En-De).", "Our SGNMT decoder BIBREF25 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De."]}
{"question_id": "643645e02ffe8fde45918615ec92013a035d1b92", "predicted_answer": "The datasets used are:\n\n1. Japanese-English (Ja-En) ASPEC dataset\n2. WMT dataset for English-German (En-De)\n\nSpecifically, the following subsets are used:\n\n* 500K sentence pairs from the Ja-En ASPEC dataset for training\n* news-test2014 as a development set for En-De\n* news-test2015 and news-test2016 as test sets for En-De", "predicted_evidence": ["Neuron INLINEFORM0 is selected for removal if (1) there is another neuron INLINEFORM1 which has a very similar set of incoming weights and if (2) INLINEFORM2 has a small outgoing weight vector. Their criterion is data-free since it does not require any training data. For further details we refer to Srinivas and Babu sparsify-datafree.", "Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for En-De).", "We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for En-De). We choose a widely used, simple ensembling method (prediction averaging) as our baseline. We feel that the prevalence of this method makes it a reasonable baseline for our experiments.", "The second intuition of the criterion used by Srinivas and Babu sparsify-datafree is that neurons with small outgoing weights contribute very little overall. Therefore, they search for a pair of neurons INLINEFORM0 according the following term and remove the INLINEFORM1 -th neuron. DISPLAYFORM0", "In this paper we mimicked the output of the high performing but cumbersome ensemble by constructing a large unfolded network, and shrank this network afterwards. Another approach, known as knowledge distillation, uses the large model (the teacher) to generate soft training labels for the smaller student network BIBREF11 , BIBREF12 . The student network is trained by minimizing the cross-entropy to the teacher. This idea has been applied to sequence modelling tasks such as machine translation and speech recognition BIBREF35 , BIBREF13 , BIBREF14 . Our approach can be computationally more efficient as the training set does not have to be decoded by the large teacher network."]}
{"question_id": "a994cc18046912a8c9328dc572f4e4310736c0e2", "predicted_answer": "Yes, they explore other language pairs, specifically English-German (En-De) in addition to Japanese-English (Ja-En).", "predicted_evidence": ["Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for En-De).", "We follow Sennrich et al. nmt-bpe and use subword units based on byte pair encoding rather than words as modelling units. Our SGNMT decoder BIBREF25 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets.", "Our SGNMT decoder BIBREF25 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De.", "We follow Sennrich et al. nmt-bpe and use subword units based on byte pair encoding rather than words as modelling units. Our SGNMT decoder BIBREF25 with a beam size of 12 is used in all experiments. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation.", "We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation. We set the vocabulary sizes to 30K for Ja-En and 50K for En-De. We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for En-De). We choose a widely used, simple ensembling method (prediction averaging) as our baseline. We feel that the prevalence of this method makes it a reasonable baseline for our experiments."]}
{"question_id": "9baca9bdb8e7d5a750f8cbe3282beb371347c164", "predicted_answer": "To preprocess tweets, the following steps were taken:\n\n1. Removed retweets, as they do not bring additional information to the study.\n2. Removed semantically meaningless expressions, such as:\n\t* URLs\n\t* Emoticons\n\t* Mentions of other users (denoted by the @ symbol)\n\t* Hashtags (denoted by the # symbol)\n3. Downcased the text of every tweet.\n4. Stripped punctuation from the text of every tweet.", "predicted_evidence": ["To obtain meaningful linguistic data we pre-processed the incoming tweet streams in several ways. As our central question here deals with language semantics of individuals, re-tweets do not bring any additional information to our study, thus we removed them by default. We also removed any expressions considered to be semantically meaningless like URLs, emoticons, mentions of other users (denoted by the @ symbol) and hashtags (denoted by the # symbol) to simplify later post-processing. In addition, as a last step of textual pre-processing, we downcased and stripped the punctuation from the text of every tweet.", "Our central dataset was collected from Twitter, an online news and social networking service. Through Twitter, users can post and interact by \u201ctweeting\" messages with restricted length. Tweets may come with several types of metadata including information about the author's profile, the detected language as well as where and when the tweet was posted. Specifically, we recorded 90,369,215 tweets written in French, posted by 1.3 Million users in the timezones GMT and GMT+1 over one year (between August 2014 to July 2015) BIBREF23 . These tweets were obtained via the Twitter Powertrack API provided by Datasift with an access rate of INLINEFORM0 . Using this dataset we built several other corpora:", "The user level features are based on the general user information or aggregated statistics about the tweets BIBREF11 . We therefore include general ordinal values such as the number and rate of retweets, mentions, and coarse-grained information about the social network of users (number of friends, followers, and ratio of friends to followers). Finally we vectorized each user's profile description and tweets and selected the top 450 and 560 1-grams and 2-grams, respectively, observed through their accounts (where the rank of a given 1-gram was estimated via tf-idf BIBREF41 ).", "Applying these filters we obtained 1,000,064 locations from geolocated tweets. By focusing on the geolocated users, we kept those with limited mobility, i.e., with median distance between locations not greater than 30 km, with tweets posted at places and times which did not require travel faster than 130 INLINEFORM1 (maximum speed allowed within France), and with no more than three tweets within a two seconds window. We further filtered out tweets with coordinates corresponding to locations referring to places (such as \u201cParis\" or \u201cFrance\"). Thus, we removed locations that didn't exactly correspond to GPS-tagged tweets and also users which were most likely bots. Home location was estimated by the most frequent location for a user among all coordinates he visited. This way we obtained INLINEFORM2 users, each associated with a unique home location. Finally, we collected the latest INLINEFORM3 tweets from the timeline of all of geolocated users using the Twitter public API BIBREF17 .", "To represent textual information, in addition to word count data, we used topic models to encode coarse-grained information on the content of the tweets of a user, similar to BIBREF9 . This enabled us to easily interpret the relation between semantic and socioeconomic features. Specifically, we started by training a word2vec model BIBREF42 on the whole set of tweets (obtained in the 2014-2015 timeframe) by using the skip-gram model and negative sampling with parameters similar to BIBREF11 , BIBREF10 . To scale up the analysis, the number of dimensions for the embedding was kept at 50. This embedded words in the initial dataset in a INLINEFORM0 vector space."]}
{"question_id": "2cb20bae085b67e357ab1e18ebafeac4bbde5b4a", "predicted_answer": "The authors build a model that associates a single income value to each user by identifying the cell of their estimated home locations and assigning them with the median of the corresponding income distribution. This model is based on the idea that socioeconomic status (SES) can be inferred from individual characteristics and environmental variables, such as income, gender, age, ethnicity, occupation, education level, and home location. The model takes into account the correlations between SES and other individual or network attributes, such as status homophily and social influence, to estimate SES.", "predicted_evidence": ["The precise inference of SES would contribute to overcome several scientific challenges and could potentially have several commercial applications BIBREF7 . Further, robust SES inference would provide unique opportunities to gain deeper insights on socioeconomic inequalities BIBREF8 , social stratification BIBREF2 , and on the driving mechanisms of network evolution, such as status homophily or social segregation.", "To associate a single income value to each user, we identified the cell of their estimated home locations and assigned them with the median of the corresponding income distribution. Thus we obtained an average socioeconomic indicator for each user, which was distributed heterogeneously in accordance with Pareto's law BIBREF28 . This is demonstrated in Fig. FIGREF15 a, where the INLINEFORM0 cumulative income distributions as the function of population fraction INLINEFORM1 appears as a Lorentz-curve with area under the diagonal proportional to socioeconomic inequalities. As an example, Fig. FIGREF7 depicts the spatial distribution of INLINEFORM2 users with inferred home locations in IRIS cells located in central Paris and colored as the median income.", "The quantification and inference of SES of individuals is a long lasting question in the social sciences. It is a rather difficult problem as it may depend on a combination of individual characteristics and environmental variables BIBREF0 . Some of these features can be easier to assess like income, gender, or age whereas others, relying to some degree on self-definition and sometimes entangled with privacy issues, are harder to assign like ethnicity, occupation, education level or home location. Furthermore, individual SES correlates with other individual or network attributes, as users tend to build social links with others of similar SES, a phenomenon known as status homophily BIBREF1 , arguably driving the observed stratification of society BIBREF2 . At the same time, shared social environment, similar education level, and social influence have been shown to jointly lead socioeconomic groups to exhibit stereotypical behavioral patterns, such as shared political opinion BIBREF3 or similar linguistic patterns BIBREF4 . Although these features are entangled and causal relation between them is far from understood, they appear as correlations in the data.", "The quantification and inference of SES of individuals is a long lasting question in the social sciences. It is a rather difficult problem as it may depend on a combination of individual characteristics and environmental variables BIBREF0 . Some of these features can be easier to assess like income, gender, or age whereas others, relying to some degree on self-definition and sometimes entangled with privacy issues, are harder to assign like ethnicity, occupation, education level or home location. Furthermore, individual SES correlates with other individual or network attributes, as users tend to build social links with others of similar SES, a phenomenon known as status homophily BIBREF1 , arguably driving the observed stratification of society BIBREF2 . At the same time, shared social environment, similar education level, and social influence have been shown to jointly lead socioeconomic groups to exhibit stereotypical behavioral patterns, such as shared political opinion BIBREF3 or similar linguistic patterns BIBREF4 .", "Online social networks have become one of the most disruptive communication platforms, as everyday billions of individuals use them to interact with each other. Their penetration in our everyday lives seems ever-growing and has in turn generated a massive volume of publicly available data open to analysis. The digital footprints left across these multiple media platforms provide us with a unique source to study and understand how the linguistic phenotype of a given user is related to social attributes such as socioeconomic status (SES)."]}
{"question_id": "892ee7c2765b3764312c3c2b6f4538322efbed4e", "predicted_answer": "According to the text, they gathered a total of 90,369,215 tweets written in French, posted by 1.3 Million users over one year (between August 2014 to July 2015).", "predicted_evidence": ["In this work we combined multiple datasets collected from various sources. Each of them came with some bias due to the data collection and post-treatment methods or the incomplete set of users. These biases may limit the success of our inference, thus their identification is important for the interpretation and future developments of our framework.", "We thank J-Ph. Magu\u00e9, J-P. Chevrot, D. Seddah, D. Carnino and E. De La Clergerie for constructive discussions and for their advice on data management and analysis. We are grateful to J. Altn\u00e9der and M. Hunyadi for their contributions as expert architects for data annotation.", "Our central dataset was collected from Twitter, an online news and social networking service. Through Twitter, users can post and interact by \u201ctweeting\" messages with restricted length. Tweets may come with several types of metadata including information about the author's profile, the detected language as well as where and when the tweet was posted. Specifically, we recorded 90,369,215 tweets written in French, posted by 1.3 Million users in the timezones GMT and GMT+1 over one year (between August 2014 to July 2015) BIBREF23 . These tweets were obtained via the Twitter Powertrack API provided by Datasift with an access rate of INLINEFORM0 . Using this dataset we built several other corpora:", "INLINEFORM0 Census data: As we already mentioned the census data does not cover the entire French territory as it reports only cells with close to INLINEFORM1 inhabitants. This may introduce biases in two ways: by limiting the number of people in our sample living in rural areas, and by associating income with large variation to each cell. While the former limit had marginal effects on our predictions, as Twitter users mostly live in urban areas, we addressed the latter effect by associating the median income to users located in a given cell.", "This procedure was applied to each of our datasets. The obtained results are shown in Fig. FIGREF21 and in Table TABREF22 ."]}
{"question_id": "c68946ae2e548ec8517c7902585c032b3f3876e6", "predicted_answer": "Yes, they analyze features that help indicate socioeconomic status (SES). Specifically, they use architectural/urban features associated with home locations, such as satellite views and Street View images, to estimate SES. They also use income data from the IRIS census to validate their estimates and categorize users into two socioeconomic classes. Additionally, they analyze linguistic patterns and other individual attributes, such as age, gender, and occupation, that are correlated with SES.", "predicted_evidence": ["Online social networks have become one of the most disruptive communication platforms, as everyday billions of individuals use them to interact with each other. Their penetration in our everyday lives seems ever-growing and has in turn generated a massive volume of publicly available data open to analysis. The digital footprints left across these multiple media platforms provide us with a unique source to study and understand how the linguistic phenotype of a given user is related to social attributes such as socioeconomic status (SES).", "Next we aimed to estimate SES from architectural/urban features associated to the home locations. Thus, for each home location we collected two additional satellite views at different resolutions as well as six Street View images, each with a horizontal view of approximately INLINEFORM0 . We randomly selected a sample of INLINEFORM1 locations and involved architects to assign a SES score (from 1 to 9) to a sample set of selected locations based on the satellite and Street View around it (both samples had 333 overlapping locations). For validation, we took users from each annotated SES class and computed the distribution of their incomes inferred from the IRIS census data (see Section SECREF6 ). Violin plots in Fig. FIGREF12 d show that in expert annotated data, as expected, the inferred income values were positively correlated with the annotated SES classes. Labels were then categorized into two socioeconomic classes for comparison purposes.", "Next we aimed to estimate SES from architectural/urban features associated to the home locations. Thus, for each home location we collected two additional satellite views at different resolutions as well as six Street View images, each with a horizontal view of approximately INLINEFORM0 . We randomly selected a sample of INLINEFORM1 locations and involved architects to assign a SES score (from 1 to 9) to a sample set of selected locations based on the satellite and Street View around it (both samples had 333 overlapping locations). For validation, we took users from each annotated SES class and computed the distribution of their incomes inferred from the IRIS census data (see Section SECREF6 ). Violin plots in Fig. FIGREF12 d show that in expert annotated data, as expected, the inferred income values were positively correlated with the annotated SES classes. Labels were then categorized into two socioeconomic classes for comparison purposes. All in all, both annotators assigned the same label to the overlapping locations in INLINEFORM2 of samples.", "To associate a single income value to each user, we identified the cell of their estimated home locations and assigned them with the median of the corresponding income distribution. Thus we obtained an average socioeconomic indicator for each user, which was distributed heterogeneously in accordance with Pareto's law BIBREF28 . This is demonstrated in Fig. FIGREF15 a, where the INLINEFORM0 cumulative income distributions as the function of population fraction INLINEFORM1 appears as a Lorentz-curve with area under the diagonal proportional to socioeconomic inequalities. As an example, Fig. FIGREF7 depicts the spatial distribution of INLINEFORM2 users with inferred home locations in IRIS cells located in central Paris and colored as the median income.", "The quantification and inference of SES of individuals is a long lasting question in the social sciences. It is a rather difficult problem as it may depend on a combination of individual characteristics and environmental variables BIBREF0 . Some of these features can be easier to assess like income, gender, or age whereas others, relying to some degree on self-definition and sometimes entangled with privacy issues, are harder to assign like ethnicity, occupation, education level or home location. Furthermore, individual SES correlates with other individual or network attributes, as users tend to build social links with others of similar SES, a phenomenon known as status homophily BIBREF1 , arguably driving the observed stratification of society BIBREF2 . At the same time, shared social environment, similar education level, and social influence have been shown to jointly lead socioeconomic groups to exhibit stereotypical behavioral patterns, such as shared political opinion BIBREF3 or similar linguistic patterns BIBREF4 . Although these features are entangled and causal relation between them is far from understood, they appear as correlations in the data."]}
{"question_id": "7557f2c3424ae70e2a79c51f9752adc99a9bdd39", "predicted_answer": "The inference models used in this work are not explicitly stated, but based on the context, it can be inferred that the authors used machine learning models, specifically classification models, to predict the Socio-Economic Status (SES) of Twitter users. The evaluation metric used is the Area Under the Receiver Operating Characteristic Curve (AUC), which is commonly used to evaluate the performance of binary classification models.", "predicted_evidence": ["In this work we combined multiple datasets collected from various sources. Each of them came with some bias due to the data collection and post-treatment methods or the incomplete set of users. These biases may limit the success of our inference, thus their identification is important for the interpretation and future developments of our framework.", "We provide in Section SECREF2 an overview of the related literature to contextualize the novelty of our work. In Section SECREF3 we provide a detailed description of the data collection and combination methods. In Section SECREF4 we introduce the features extracted to solve the SES inference problem, with results summarized in Section SECREF5 . Finally, in Section SECREF6 and SECREF7 we conclude our paper with a brief discussion of the limitations and perspectives of our methods.", "Our first motivation in this study was to overcome earlier limitations by exploring alternative data collection and combination methods. We provide here three ways to estimate the SES of Twitter users by using (a) open census data, (b) crawled and manually annotated data on professional skills and occupation, and (c) expert annotated data on home location Street View images. We provide here a collection of procedures that enable interested researchers to introduce predictive performance and scalability considerations when interested in developing language to SES inference pipelines. In the following we present in detail all of our data collection and combination methods.", "In this work we proposed a novel methodology for the inference of the SES of Twitter users. We built our models combining information obtained from numerous sources, including Twitter, census data, LinkedIn and Google Maps. We developed precise methods of home location inference from geolocation, novel annotation of remotely sensed images of living environments, and effective combination of datasets collected from multiple sources. As new scientific results, we demonstrated that within the French Twitter space, the utilization of words in different topic categories, identified via advanced semantic analysis of tweets, can discriminate between people of different income. More importantly, we presented a proof-of-concept that our methods are competitive in terms of SES inference when compared to other methods relying on domain specific information.", "In terms of prediction score, we followed a standard procedure in the literature BIBREF45 and evaluated the learned models by considering the area under the receiver operating characteristic curve (AUC). This metric can be thought as the probability that a classifier ranks a randomly chosen positive instance higher than a randomly chosen negative one BIBREF44 ."]}
{"question_id": "b03249984c26baffb67e7736458b320148675900", "predicted_answer": "The baseline model is not explicitly mentioned in the given context. However, it can be inferred that the baseline model is the census-based inference model, which is used as a comparison to evaluate the performance of the proposed model. The proposed model is said to have an INLINEFORM1 increase in performance compared to the census-based inference model.", "predicted_evidence": ["For each socioeconomic dataset, we trained our models by using 75% of the available data for training and the remaining 25% for testing. During the training phase, the training data undergoes a INLINEFORM0 -fold inner cross-validation, with INLINEFORM1 , where all splits are computed in a stratified manner to get the same ratio of lower to higher SES users. The four first blocks were used for inner training and the remainder for inner testing. This was repeated ten times for each model so that in the end, each model's performance on the validation set was averaged over 50 samples. For each model, the parameters were fine-tuned by training 500 different models over the aforementioned splits. The selected one was that which gave the best performance on average, which was then applied to the held-out test set. This is then repeated through a 5-fold outer cross-validation.", "In terms of prediction score, we followed a standard procedure in the literature BIBREF45 and evaluated the learned models by considering the area under the receiver operating characteristic curve (AUC). This metric can be thought as the probability that a classifier ranks a randomly chosen positive instance higher than a randomly chosen negative one BIBREF44 .", "In this work we combined multiple datasets collected from various sources. Each of them came with some bias due to the data collection and post-treatment methods or the incomplete set of users. These biases may limit the success of our inference, thus their identification is important for the interpretation and future developments of our framework.", "We hence used it for our predictions in the remainder of this study. We found that the LinkedIn data was the best, with INLINEFORM0 , to train a model to predict SES of people based on their semantic features. It provided a INLINEFORM1 increase in performance as compared to the census based inference with INLINEFORM2 , and INLINEFORM3 relative to expert annotated data with INLINEFORM4 . Thus we can conclude that there seem to be a trade-off between scalability and prediction quality, as while the occupation dataset provided the best results, it seems unlikely to be subject to any upscaling due to the high cost of obtaining a clean dataset. Relying on location to estimate SES seems to be more likely to benefit from such an approach, though at the cost of an increased number of mislabelled users in the dataset. Moreover, the annotator's estimation of SES using Street View at each home location seems to be hindered by the large variability of urban features.", "Next we aimed to estimate SES from architectural/urban features associated to the home locations. Thus, for each home location we collected two additional satellite views at different resolutions as well as six Street View images, each with a horizontal view of approximately INLINEFORM0 . We randomly selected a sample of INLINEFORM1 locations and involved architects to assign a SES score (from 1 to 9) to a sample set of selected locations based on the satellite and Street View around it (both samples had 333 overlapping locations). For validation, we took users from each annotated SES class and computed the distribution of their incomes inferred from the IRIS census data (see Section SECREF6 ). Violin plots in Fig. FIGREF12 d show that in expert annotated data, as expected, the inferred income values were positively correlated with the annotated SES classes. Labels were then categorized into two socioeconomic classes for comparison purposes. All in all, both annotators assigned the same label to the overlapping locations in INLINEFORM2 of samples."]}
{"question_id": "9595fdf7b51251679cd39bc4f6befc81f09c853c", "predicted_answer": "The remotely sensed data was annotated by experts through visual inspection, which introduced some subjectivity and biases into the annotations.", "predicted_evidence": ["INLINEFORM0 Annotated home locations: The remote sensing annotation was done by experts and their evaluation was based on visual inspection and biased by some unavoidable subjectivity. Although their annotations were cross-referenced and found to be consistent, they still contained biases, like over-representative middle classes, which somewhat undermined the prediction task based on this dataset.", "We can identify several future directions and applications of our work. First, further development of data annotation of remotely sensed information is a promising direction. Note that after training, our model requires as input only information, which can be collected exclusively from Twitter, without relying on other data sources. This holds a large potential in terms of SES inference of larger sets of Twitter users, which in turn opens the door for studies to address population level correlations of SES with language, space, time, or the social network. This way our methodology has the merit not only to answer open scientific questions, but also to contribute to the development of new applications in recommendation systems, predicting customer behavior, or in online social services.", "Despite these shortcomings, using all the three datasets we were able to infer SES with performances close to earlier reported results, which were based on more thoroughly annotated datasets. Our results, and our approach of using open, crawlable, or remotely sensed data highlights the potential of the proposed methodologies.", "We thank J-Ph. Magu\u00e9, J-P. Chevrot, D. Seddah, D. Carnino and E. De La Clergerie for constructive discussions and for their advice on data management and analysis. We are grateful to J. Altn\u00e9der and M. Hunyadi for their contributions as expert architects for data annotation.", "In order to filter out inferred home locations not in urban/residential areas, we downloaded via Google Maps Static API BIBREF36 a satellite view in a INLINEFORM0 radius around each coordinate (for a sample see Fig. FIGREF12 a). To discriminate between residential and non-residential areas, we built on land use classifier BIBREF37 using aerial imagery from the UC Merced dataset BIBREF38 . This dataset contains 2100 INLINEFORM1 INLINEFORM2 aerial RGB images over 21 classes of different land use (for a pair of sample images see Fig. FIGREF12 b). To classify land use a CaffeNet architecture was trained which reached an accuracy over INLINEFORM3 . Here, we instantiated a ResNet50 network using keras BIBREF39 pre-trained on ImageNet BIBREF40 where all layers except the last five were frozen."]}
{"question_id": "08c0d4db14773cbed8a63e69381a2265e85f8765", "predicted_answer": "The professional profiles were crawled from LinkedIn using an automatic crawler mainly used in Search Engine Optimization (SEO) tasks.", "predicted_evidence": ["As a second method to estimate SES, we took a sample of Twitter users who mentioned their LinkedIn BIBREF29 profile url in their tweets or Twitter profile. Using these pointers we collected professional profile descriptions from LinkedIn by relying on an automatic crawler mainly used in Search Engine Optimization (SEO) tasks BIBREF30 . We obtained INLINEFORM0 Twitter/LinkedIn users all associated with their job title, professional skills and profile description. Apart from the advantage of working with structured data, professional information extracted from LinkedIn is significantly more reliable than Twitter's due to the high degree of social scrutiny to which each profile is exposed BIBREF31 .", "INLINEFORM0 Occupation data: LinkedIn as a professional online social network is predominantly used by people from IT, business, management, marketing or other expert areas, typically associated with higher education levels and higher salaries. Moreover, we could observe only users who shared their professional profiles on Twitter, which may further biased our training set. In terms of occupational-salary classification, the data in BIBREF32 was collected in 2010 thus may not contain more recent professions. These biases may induce limits in the representativeness of our training data and thus in the predictions' precision. However, results based on this method of SES annotation performed best in our measurements, indicating that professions are among the most predictive features of SES, as has been reported in BIBREF9 .", "To associate income to Twitter users with LinkedIn profiles, we matched them with a given salary based on their reported profession and an occupational salary classification table provided by INSEE BIBREF32 . Due to the ambiguous naming of jobs and to acknowledge permanent/non-permanent, senior/junior contract types we followed three strategies for the matching. In INLINEFORM0 of the cases we directly associated the reported job titles to regular expressions of an occupation. In INLINEFORM1 of the cases we used string sequencing methods borrowed from DNA-sequencing BIBREF33 to associate reported and official names of occupations with at least INLINEFORM2 match. For the remaining INLINEFORM3 of users we directly inspected profiles. The distribution of estimated salaries reflects the expected income heterogeneities as shown in Fig. FIGREF15 . Users were eventually assigned to one of two SES classes based on whether their salary was higher or lower than the average value of the income distribution. Also note, that LinkedIn users may not be representative of the whole population.", "To associate income to Twitter users with LinkedIn profiles, we matched them with a given salary based on their reported profession and an occupational salary classification table provided by INSEE BIBREF32 . Due to the ambiguous naming of jobs and to acknowledge permanent/non-permanent, senior/junior contract types we followed three strategies for the matching. In INLINEFORM0 of the cases we directly associated the reported job titles to regular expressions of an occupation. In INLINEFORM1 of the cases we used string sequencing methods borrowed from DNA-sequencing BIBREF33 to associate reported and official names of occupations with at least INLINEFORM2 match. For the remaining INLINEFORM3 of users we directly inspected profiles. The distribution of estimated salaries reflects the expected income heterogeneities as shown in Fig. FIGREF15 . Users were eventually assigned to one of two SES classes based on whether their salary was higher or lower than the average value of the income distribution.", "Due to the ambiguous naming of jobs and to acknowledge permanent/non-permanent, senior/junior contract types we followed three strategies for the matching. In INLINEFORM0 of the cases we directly associated the reported job titles to regular expressions of an occupation. In INLINEFORM1 of the cases we used string sequencing methods borrowed from DNA-sequencing BIBREF33 to associate reported and official names of occupations with at least INLINEFORM2 match. For the remaining INLINEFORM3 of users we directly inspected profiles. The distribution of estimated salaries reflects the expected income heterogeneities as shown in Fig. FIGREF15 . Users were eventually assigned to one of two SES classes based on whether their salary was higher or lower than the average value of the income distribution. Also note, that LinkedIn users may not be representative of the whole population. We discuss this and other types of poential biases in Section SECREF6 ."]}
{"question_id": "5e29f16d7302f24ab93b7707d115f4265a0d14b0", "predicted_answer": "The size of the extra data used in BIBREF0 is about 10 times larger than the 2-votes data.", "predicted_evidence": ["Finally, we want to provide insight into what kind of instances the system extracts, and why back-translation labels sometimes disagree. We have identified four major cases based on a manual analysis of 100 randomly sampled instances.", "We compare the explicitations obtained from translations into three different languages, and find that instances where at least two back-translations agree yield the best quality, significantly outperforming a version of the model that does not use additional data, or uses data from just one language. A qualitative analysis furthermore shows that the strength of the method partially stems from being able to learn additional discourse cues which are typically translated consistently, and suggests that our method may also be used for identifying multiple relations holding between two arguments.", "Due to the limited size of available training data, several approaches have been proposed for acquiring additional training data using automatic methods BIBREF10 , BIBREF11 . The most promising approach so far, BIBREF0 , exploits the fact that human translators sometimes insert a connective in their translation even when a relation was implicit in the original text. Using a back-translation method, BIBREF0 showed that such instances can be used for acquiring additional labeled text.", "We train three MT systems to back-translate French, German and Czech to English. To have words alignments, better and stable back-translations, we employ a statistical machine translation system Moses BIBREF12 , trained on the same parallel corpora. Source and target sentences are first tokenized, true-cased and then fed into the system for training. In our case, the translation target texts are identical with the training set of the translation systems; this would not be a problem because our only objective in the translation is to back-translate connectives in the translation into English. On the training set, the translation system achieves BLEU scores of 66.20 (French), 65.30 (German) and 69.05 (Czech).", "Table TABREF7 shows that best results are achieved by adding only those samples for which two back-translations agree with one another. This may represent the best trade-off between reliability of the label and the amount of additional data. The setting where the data from all languages is added performs badly despite the large number of samples, because this method contains different labels for the same argument pairs, for all those instances where the back-translations don't yield the same label, introducing noise into the system. The size of the extra data used in BIBREF0 is about 10 times larger than our 2-votes data, as they relied on additional training data (which we could not use in this experiment, as there is no pairing with translations into other languages) and exploited also intra-sentential instances."]}
{"question_id": "26844cec57df6ff0f02245ea862af316b89edffe", "predicted_answer": "Yes, they do train discourse relation models with augmented data. The text mentions several approaches to augmenting training data for implicit discourse relation classification, including:\n\n* Augmenting training data for implicit relations by typical explicits (BIBREF10)\n* Selecting explicit samples where connectives can be omitted without changing the interpretation of the discourse (BIBREF11)\n* Automatically labeling English implicit discourse samples based on explicitation of discourse connectives during human translating in parallel corpora (BIBREF0)\n* Employing document-aligned cross-lingual parallel corpora and majority votes to get more reliable and in-topic annotated implicit discourse relation instances (the authors' own approach)\n\nThese approaches aim to increase the amount of reliable annotated data available for training discourse relation models, which is essential for improving their performance.", "predicted_evidence": ["The difficulty in classifying implicit discourse relations stems from the lack of strong indicative cues. Early work has already shown that implicit relations cannot be learned from explicit ones BIBREF9 , making human-annotated relations the currently only source for training relation classification.", "Discourse relations connect two sentences/clauses to each other. The identification of discourse relations is an important step in natural language understanding and is beneficial to various downstream NLP applications such as text summarization BIBREF1 , BIBREF2 , question answering BIBREF3 , BIBREF4 , machine translation BIBREF5 , BIBREF6 , and so on.", "Recent methods for discourse relation classification have increasingly relied on neural network architectures. However, with the high number of parameters to be trained in more and more complicated deep neural network architectures, the demand of more reliable annotated data has become even more urgent. Data extension has been a longstanding goal in implicit discourse classification. BIBREF10 proposed to differentiate typical and atypical examples for each relation and augment training data for implicit only by typical explicits. BIBREF11 designed criteria for selecting explicit samples in which connectives can be omitted without changing the interpretation of the discourse. More recently, BIBREF0 proposed a pipeline to automatically label English implicit discourse samples based on explicitation of discourse connectives during human translating in parallel corpora, and achieve substantial improvements in classification. Our work here directly extended theirs by employing document-aligned cross-lingual parallel corpora and majority votes to get more reliable and in-topic annotated implicit discourse relation instances.", "The Penn Discourse Treebank (PDTB) It is the largest manually annotated corpus of discourse relations from Wall Street Journal. Each discourse relation has been annotated in three hierarchy levels. In this paper, we follow the previous conventional settings and focus on the second-level 11-ways classification.", "Case 4: Implicit relations can co-occur with marked discourse relations BIBREF17 , and multiple translations help discover these instances, for example:"]}
{"question_id": "d1d59bca40b8b308c0a35fed1b4b7826c85bc9f8", "predicted_answer": "According to the given context, they attempt to use 3 languages to generate discourse relation labelled data.", "predicted_evidence": ["We compare the explicitations obtained from translations into three different languages, and find that instances where at least two back-translations agree yield the best quality, significantly outperforming a version of the model that does not use additional data, or uses data from just one language. A qualitative analysis furthermore shows that the strength of the method partially stems from being able to learn additional discourse cues which are typically translated consistently, and suggests that our method may also be used for identifying multiple relations holding between two arguments.", "Discourse relations connect two sentences/clauses to each other. The identification of discourse relations is an important step in natural language understanding and is beneficial to various downstream NLP applications such as text summarization BIBREF1 , BIBREF2 , question answering BIBREF3 , BIBREF4 , machine translation BIBREF5 , BIBREF6 , and so on.", "The Penn Discourse Treebank (PDTB) It is the largest manually annotated corpus of discourse relations from Wall Street Journal. Each discourse relation has been annotated in three hierarchy levels. In this paper, we follow the previous conventional settings and focus on the second-level 11-ways classification.", "The difficulty in classifying implicit discourse relations stems from the lack of strong indicative cues. Early work has already shown that implicit relations cannot be learned from explicit ones BIBREF9 , making human-annotated relations the currently only source for training relation classification.", "Case 4: Implicit relations can co-occur with marked discourse relations BIBREF17 , and multiple translations help discover these instances, for example:"]}
{"question_id": "4d824b49728649432371ecb08f66ba44e50569e0", "predicted_answer": "The system improved by a reduction in false positives of more than INLINEFORM0.", "predicted_evidence": ["e.g. ts_tick_abs (US_Unemployment, 4.9%), or", "e.g. ts_tick_abs (US_Unemployment, -0.2%).", "We would like to thank my managers Alex Bozic, Tim Phelan and Joshwini Pereira for supporting this project, as well as David Rosenberg from the CTO's office for providing access to GPU infrastructure.", "The full pipeline, deployed in a production setting, resulted in a reduction in false positives of more than INLINEFORM0 in the extractions produced by our pipeline. The drop in recall relative to the production system was smaller than INLINEFORM1 .", "Our system leverages \u201cfree\u201d data to train a deep neural network, and does not require large-scale manual annotation. The network is trained with noisy supervision provided by measures of consistency with existing databases (e.g. an extraction ts_tick_abs (US_Unemployment, 49%) would be implausible given recent US employment history). With slight modifications, our pipeline could be trained with supervision from human interaction, such as clicks on online advertisements. Learning without explicit annotations is critical in applications where large-scale manual annotation would be prohibitively expensive."]}
{"question_id": "02a5acb484bda77ef32a13f5d93d336472cf8cd4", "predicted_answer": "According to the text, the existing databases used are not explicitly named, but they are implied to be financial databases that contain time series data, such as databases for macroeconomic forecasting or financial analysis.", "predicted_evidence": ["There are two sources of noise in our application's database supervision. First, there is a high rate of false positives. It is not rare for the parser to generate an extraction candidate ts_tick_abs (TS symbol, numerical value) in which the numerical value fits into the time series of the time series symbol, but the extraction is nonetheless incorrect. False negatives are also a problem: many financial time series are sparse and are rarely observed. As a result, it is common for differences between reference numerical values and extracted numerical values to be large even for correct extractions.", "We compute a consistency score INLINEFORM0 for the candidate extraction, measuring if the extracted relation is consistent with (noisy) supervision INLINEFORM1 (e.g. an existing database).", "Unstructured textual data is abundant in the financial domain (see e.g. Figure FIGREF2 ). This information is by definition not in a format that lends itself to immediate processing. Hence, information extraction is an essential step in business applications that require fast, accurate, and low-cost information processing. In the financial domain, these applications include the creation of time series databases for macroeconomic forecasting or financial analysis, as well as the real-time extraction of time series data to inform algorithmic trading strategies. Bloomberg has had information extraction systems for financial language text for nearly a decade.", "The information extraction pipeline we developed consists of four stages (see right pane of Figure FIGREF12 ).", "Our system leverages \u201cfree\u201d data to train a deep neural network, and does not require large-scale manual annotation. The network is trained with noisy supervision provided by measures of consistency with existing databases (e.g. an extraction ts_tick_abs (US_Unemployment, 49%) would be implausible given recent US employment history). With slight modifications, our pipeline could be trained with supervision from human interaction, such as clicks on online advertisements. Learning without explicit annotations is critical in applications where large-scale manual annotation would be prohibitively expensive."]}
{"question_id": "863d8d32a1605402e11f0bf63968a14bcfd15337", "predicted_answer": "The existing parser used is not explicitly mentioned in the text. The text only refers to a \"potentially constraint-based parser\" and a \"candidate-generating parser\" without specifying a particular parser or its implementation.", "predicted_evidence": ["The document is parsed using a potentially constraint-based parser, which outputs a set of candidate extractions. Each candidate extraction consists of the character offsets of all extracted constituent entities, as well as a representation of the extracted relation. It may additionally contain auxilliary information that the parser may have generated, such as part of speech tags.", "The vectors INLINEFORM0 are a concatenation of (i) a one-hot encoding of the character and (ii) information about entities the parser identified at the position of INLINEFORM1 . For (i) we use a restricted character set of size 94, including [a-zA-Z0-9] and several whitespace and special characters, plus an indicator to represent characters not present in our restricted character set. For (ii), INLINEFORM2 contains data representing the parser's output. For our application, we include in INLINEFORM3 a vector of indicators specifying whether or not any of the entities appearing in the relations supported by the parser were found in the position of character INLINEFORM4 .", "We present an information extraction architecture that augments a candidate-generating parser with a deep neural network. The candidate-generating parser may leverage constraints. At the same time, the architecture gains the neural networks's ability to leverage large amounts of data to learn complex features that are tuned for the application at hand. Our method assumes the existence of a potentially noisy source of supervision INLINEFORM0 , e.g. via consistency checks of extracted data against existing databases, or via human interaction. This supervision is used to train the neural network.", "We encode the candidate-generating parser's document annotations character-by-character into vectors INLINEFORM0 that also include a one-hot encoding of the character itself. We believe that this encoding makes it easier for the network to learn character-level characteristics of the entities in the semantic relation. Moreover, our encoding lends itself well to processing both by recurrent architectures (processing character-by-character input vectors INLINEFORM1 ) and convolutional architectures (performing 1D convolutions over an input matrix whose columns are vectors INLINEFORM2 ).", "If an extractor for the given application has already been built, the neural network boosts its accuracy without the need to re-engineer or discard the existing solution. Even for new systems, the decoupling of candidate-generation and the neural network offers advantages: the candidate-generating parser can easily enforce contraints that would be difficult to support in an algorithm relying entirely on a neural network. Note that, in particular, a carefully engineered candidate-generating parser enforces constraints intelligently, and can in many instances eliminate the need to evaluate computationally expensive constraints, e.g. API calls."]}
{"question_id": "d4b84f48460517bc0a6d4e0c38f6853c58081166", "predicted_answer": "They combine the socioeconomic maps with Twitter data by assigning each geolocated Twitter user to a patch closest to their estimated home location (within 1 km) on the socioeconomic map. This allows them to obtain socioeconomic indicators (SES) for each user based on their estimated home location.", "predicted_evidence": ["Data collected from Twitter provides a large variety of information about several users including their tweets, which disclose their interests, vocabulary, and linguistic patterns; their direct mentions from which their social interactions can be inferred; and the sequence of their locations, which can be used to infer their representative location. However, no information is directly available regarding their socioeconomic status, which can be pivotal to understand the dynamics and structure of their personal linguistic patterns.", "The overall goal of our study was to explore the dependencies of linguistic variables on the socioeconomic status, location, time varying activity, and social network of users. To do so we constructed a combined dataset from a large Twitter data corpus, including geotagged posts and proxy social interactions of millions of users, as well as a detailed socioeconomic map describing average socioeconomic indicators with a high spatial resolution in France. The combination of these datasets provided us with a large set of Twitter users all assigned to their Twitter timeline over three years, their location, three individual socioeconomic indicators, and a set of meaningful social ties. Three linguistic variables extracted from individual Twitter timelines were then studied as a function of the former, namely, the rate of standard negation, the rate of plural agreement and the size of vocabulary set.", "To overcome this limitation we combined our Twitter data with the socioeconomic maps of INSEE by assigning each geolocated Twitter user to a patch closest to their estimated home location (within 1 km). This way we obtained for all $110,369$ geolocated users their dynamical linguistic data, their egocentric social network as well as a set of SES indicators.", "Many studies have overcome this limitation by taking advantage of the geolocation feature allowing Twitter users to include in their posts the location from which they were tweeted. Based on this metadata, studies have been able to assign home location to geolocated users with varying degrees of accuracy BIBREF15 . Subsequent work has also been devoted to assigning to each user some indicator that might characterize their socioeconomic status based on their estimated home location. These indicators are generally extracted from other datasets used to complete the Twitter one, namely census data BIBREF16 , BIBREF12 , BIBREF17 or real estate online services as Zillow.com BIBREF18 . Other approaches have also relied on sources of socioeconomic information such as the UK Standard Occupation Classification (SOC) hierarchy, to assign socioeconomic status to users with occupation mentions BIBREF19 .", "By measuring the defined linguistic variables in the Twitter timeline of users we were finally set to address the core questions of our study, which dealt with linguistic variation. More precisely, we asked whether the language variants used online depend on the socioeconomic status of the users, on the location or time of usage, and on ones social network. To answer these questions we present here a multidimensional correlation study on a large set of Twitter geolocated users, to which we assigned a representative location, three SES indicators, and a set of meaningful social ties based on the collection of their tweets."]}
{"question_id": "90756bdcd812b7ecc1c5df2298aa7561fd2eb02c", "predicted_answer": "No, the fact that people are active during the day time does not define their socioeconomic status (SEC). However, the study found that people who are active during the day tend to have higher average income and use more standard language patterns, which are correlated with higher socioeconomic status. In other words, the time of day when people are active is not a direct determinant of their SEC, but it is related to other factors that are associated with SEC, such as language use and income.", "predicted_evidence": ["Not surprisingly, these two curves were strongly correlated as indicated by the high Pearson correlation coefficients summarized in the last column of Table 3 which, again, assured us that our geolocated sample of Twitter users was representative of the whole set of users. At the same time, the temporal variability of these curves suggested that people tweeting during the day used a more standard language than those users who are more active during the night. However, after measuring the average income of active users in a given hour over a week, we obtained an even more sophisticated picture. It turned out that people active during the day have higher average income (warmer colors in Fig. 4 ) than people active during the night (colder colors in Fig. 4 ). Thus the variability of standard language patterns was largely explained by the changing overall composition of active Twitter users during different times of day and the positive correlation between socioeconomic status and the usage of higher linguistic standards (that we have seen earlier).", "Not surprisingly, these two curves were strongly correlated as indicated by the high Pearson correlation coefficients summarized in the last column of Table 3 which, again, assured us that our geolocated sample of Twitter users was representative of the whole set of users. At the same time, the temporal variability of these curves suggested that people tweeting during the day used a more standard language than those users who are more active during the night. However, after measuring the average income of active users in a given hour over a week, we obtained an even more sophisticated picture. It turned out that people active during the day have higher average income (warmer colors in Fig. 4 ) than people active during the night (colder colors in Fig. 4 ). Thus the variability of standard language patterns was largely explained by the changing overall composition of active Twitter users during different times of day and the positive correlation between socioeconomic status and the usage of higher linguistic standards (that we have seen earlier). This explanation was supported by the high coefficients (summarized in Table 3 ), which were indicative of strong and significant correlations between the temporal variability of average linguistic variables and average income of the active population on Twitter.", "In Fig. 4 a and b we show the temporal variability of $\\overline{L}^{\\Lambda }_{\\mathrm {cn}}(t)$ and $\\overline{L}^{\\Lambda }_{\\mathrm {cp}}(t)$ (respectively) computed for the whole Twitter user set ( $\\Gamma =all$ , solid line) and for geolocated users ( $\\Gamma =geo$ , dashed lines). Not surprisingly, these two curves were strongly correlated as indicated by the high Pearson correlation coefficients summarized in the last column of Table 3 which, again, assured us that our geolocated sample of Twitter users was representative of the whole set of users. At the same time, the temporal variability of these curves suggested that people tweeting during the day used a more standard language than those users who are more active during the night. However, after measuring the average income of active users in a given hour over a week, we obtained an even more sophisticated picture.", "In Fig. 4 a and b we show the temporal variability of $\\overline{L}^{\\Lambda }_{\\mathrm {cn}}(t)$ and $\\overline{L}^{\\Lambda }_{\\mathrm {cp}}(t)$ (respectively) computed for the whole Twitter user set ( $\\Gamma =all$ , solid line) and for geolocated users ( $\\Gamma =geo$ , dashed lines). Not surprisingly, these two curves were strongly correlated as indicated by the high Pearson correlation coefficients summarized in the last column of Table 3 which, again, assured us that our geolocated sample of Twitter users was representative of the whole set of users. At the same time, the temporal variability of these curves suggested that people tweeting during the day used a more standard language than those users who are more active during the night.", "Via a detailed multidimensional correlation study we concluded that (a) socioeconomic indicators and linguistic variables are significantly correlated. i.e. people with higher socioeconomic status are more prone to use more standard variants of language and a larger vocabulary set, while people on the other end of the socioeconomic spectrum tend to use more non-standard terms and, on average, a smaller vocabulary set; (b) Spatial position was also found to be a key feature of standard language use as, overall, people from the North tended to use more non-standard terms and a smaller vocabulary set compared to people from the South; a more fine-grained analysis reveals that the spatial variability of language is determined to a greater extent locally by the socioeconomic status; (c) In terms of temporal activity, standard language was more likely to be used during the daytime while non-standard variants were predominant during the night. We explained this temporal variability by the turnover of population with different socioeconomic status active during night and day; Finally (d) we showed that the social network and status homophily mattered in terms of linguistic similarity between peers, as connected users with the same socioeconomic status appeared to be the most similar, while disconnected people were found to be the most dissimilar in terms of their individual use of the aforementioned linguistic markers."]}
{"question_id": "028d0d9b7a71133e51a14a32cd09dea1e2f39f05", "predicted_answer": "Based on the text, it appears that the authors defined standard language in terms of three linguistic markers:\n\n1. Negation: Using negation in a standard way.\n2. Pluralization: Using pluralization in a standard way.\n3. Vocabulary: Using a diverse vocabulary.\n\nIn other words, the authors considered language usage to be \"standard\" if it conformed to certain norms or patterns in terms of negation, pluralization, and vocabulary. They then compared the language usage of different regions and socioeconomic groups to see how closely they adhered to these standards.", "predicted_evidence": ["Results shown in Fig. 3 a-c revealed some surprising patterns, which appeared to be consistent for each linguistic variable. By considering latitudinal variability it appeared that, overall, people living in the northern part of the country used a less standard language, i.e., negated and pluralized less standardly, and used a smaller number of words. On the other hand, people from the South used a language which is somewhat closer to the standard (in terms of the aforementioned linguistic markers) and a more diverse vocabulary. The most notable exception is Paris, where in the city center people used more standard language, while the contrary is true for the suburbs. This observation, better shown in Fig. 3 a inset, can be explained by the large differences in average socioeconomic status between districts. Such segregation is known to divide the Eastern and Western sides of suburban Paris, and in turn to induce apparent geographic patterns of standard language usage.", "Results shown in Fig. 3 a-c revealed some surprising patterns, which appeared to be consistent for each linguistic variable. By considering latitudinal variability it appeared that, overall, people living in the northern part of the country used a less standard language, i.e., negated and pluralized less standardly, and used a smaller number of words. On the other hand, people from the South used a language which is somewhat closer to the standard (in terms of the aforementioned linguistic markers) and a more diverse vocabulary. The most notable exception is Paris, where in the city center people used more standard language, while the contrary is true for the suburbs. This observation, better shown in Fig. 3 a inset, can be explained by the large differences in average socioeconomic status between districts. Such segregation is known to divide the Eastern and Western sides of suburban Paris, and in turn to induce apparent geographic patterns of standard language usage. We found less evident longitudinal dependencies of the observed variables.", "3 a-c revealed some surprising patterns, which appeared to be consistent for each linguistic variable. By considering latitudinal variability it appeared that, overall, people living in the northern part of the country used a less standard language, i.e., negated and pluralized less standardly, and used a smaller number of words. On the other hand, people from the South used a language which is somewhat closer to the standard (in terms of the aforementioned linguistic markers) and a more diverse vocabulary. The most notable exception is Paris, where in the city center people used more standard language, while the contrary is true for the suburbs. This observation, better shown in Fig. 3 a inset, can be explained by the large differences in average socioeconomic status between districts. Such segregation is known to divide the Eastern and Western sides of suburban Paris, and in turn to induce apparent geographic patterns of standard language usage. We found less evident longitudinal dependencies of the observed variables. Although each variable shows a somewhat diagonal trend, the most evident longitudinal dependency appeared for the average rate of standard pluralization (see Fig.", "On the other hand, people from the South used a language which is somewhat closer to the standard (in terms of the aforementioned linguistic markers) and a more diverse vocabulary. The most notable exception is Paris, where in the city center people used more standard language, while the contrary is true for the suburbs. This observation, better shown in Fig. 3 a inset, can be explained by the large differences in average socioeconomic status between districts. Such segregation is known to divide the Eastern and Western sides of suburban Paris, and in turn to induce apparent geographic patterns of standard language usage. We found less evident longitudinal dependencies of the observed variables. Although each variable shows a somewhat diagonal trend, the most evident longitudinal dependency appeared for the average rate of standard pluralization (see Fig. 3 b), where users from the Eastern side of the country used the language in less standard ways.", "The most notable exception is Paris, where in the city center people used more standard language, while the contrary is true for the suburbs. This observation, better shown in Fig. 3 a inset, can be explained by the large differences in average socioeconomic status between districts. Such segregation is known to divide the Eastern and Western sides of suburban Paris, and in turn to induce apparent geographic patterns of standard language usage. We found less evident longitudinal dependencies of the observed variables. Although each variable shows a somewhat diagonal trend, the most evident longitudinal dependency appeared for the average rate of standard pluralization (see Fig. 3 b), where users from the Eastern side of the country used the language in less standard ways. Note that we also performed a multivariate regression analysis (not shown here), using the linguistic markers as target and considering as factors both location (in terms of latitude and longitude) as and income as proxy of socioeconomic status. It showed that while location is a strong global determinant of language variability, socioeconomic variability may still be significant locally to determine standard language usage (just as we demonstrated in the case of Paris)."]}
{"question_id": "cfc73e0c82cf1630b923681c450a541a964688b9", "predicted_answer": "According to the text, socioeconomic status is operationalized from Twitter user data by:\n\n1. Assigning a home location to geolocated users based on their tweets' metadata.\n2. Extracting socioeconomic indicators from other datasets, such as census data, to characterize the socioeconomic status of each user based on their estimated home location.\n3. Using real estate online services, such as Zillow.com, to gather socioeconomic information.\n4. Relying on sources of socioeconomic information, such as the UK Standard Occupation Classification (SOC) hierarchy, to assign socioeconomic status to users with occupation mentions.\n\nIn other words, researchers use the user's location information to link them to socioeconomic data from external sources, such as census data or real estate websites, to infer their socioeconomic status.", "predicted_evidence": ["Data collected from Twitter provides a large variety of information about several users including their tweets, which disclose their interests, vocabulary, and linguistic patterns; their direct mentions from which their social interactions can be inferred; and the sequence of their locations, which can be used to infer their representative location. However, no information is directly available regarding their socioeconomic status, which can be pivotal to understand the dynamics and structure of their personal linguistic patterns.", "In addition, home locations inferred for lower activity users may induced some noise in our inference method. Nevertheless, we demonstrated that our selected Twitter users are quite representative in terms of spatial, temporal, and socioeconomic distributions once compared to census data. Other sources of bias include the \"homogenization\" performed by INSEE to ensure privacy rights are upheld as well as the proxies we devised to approximate users' home location and social network. Currently, a sample survey of our set of geolocated users is being conducted so as to bootstrap socioeconomic data to users and definitely validate our inference results. Nonetheless, this INSEE dataset provides still the most comprehensive available information on socioeconomic status over the whole country. For limiting such risk of bias, we analyzed the potential effect of the confounding variables on distribution and cross-correlations of SES indicators. Acknowledging possible limitations of this study, we consider it as a necessary first step in analyzing income through social media using datasets orders of magnitude larger than in previous research efforts.", "Despite these findings, one has to acknowledge the multiple limitations affecting this work: First of all, although Twitter is a broadly adopted service in most technologically enabled societies, it commonly provides a biased sample in terms of age and socioeconomic status as older or poorer people may not have access to this technology. In addition, home locations inferred for lower activity users may induced some noise in our inference method. Nevertheless, we demonstrated that our selected Twitter users are quite representative in terms of spatial, temporal, and socioeconomic distributions once compared to census data. Other sources of bias include the \"homogenization\" performed by INSEE to ensure privacy rights are upheld as well as the proxies we devised to approximate users' home location and social network. Currently, a sample survey of our set of geolocated users is being conducted so as to bootstrap socioeconomic data to users and definitely validate our inference results. Nonetheless, this INSEE dataset provides still the most comprehensive available information on socioeconomic status over the whole country.", "In addition, home locations inferred for lower activity users may induced some noise in our inference method. Nevertheless, we demonstrated that our selected Twitter users are quite representative in terms of spatial, temporal, and socioeconomic distributions once compared to census data. Other sources of bias include the \"homogenization\" performed by INSEE to ensure privacy rights are upheld as well as the proxies we devised to approximate users' home location and social network. Currently, a sample survey of our set of geolocated users is being conducted so as to bootstrap socioeconomic data to users and definitely validate our inference results. Nonetheless, this INSEE dataset provides still the most comprehensive available information on socioeconomic status over the whole country. For limiting such risk of bias, we analyzed the potential effect of the confounding variables on distribution and cross-correlations of SES indicators.", "Many studies have overcome this limitation by taking advantage of the geolocation feature allowing Twitter users to include in their posts the location from which they were tweeted. Based on this metadata, studies have been able to assign home location to geolocated users with varying degrees of accuracy BIBREF15 . Subsequent work has also been devoted to assigning to each user some indicator that might characterize their socioeconomic status based on their estimated home location. These indicators are generally extracted from other datasets used to complete the Twitter one, namely census data BIBREF16 , BIBREF12 , BIBREF17 or real estate online services as Zillow.com BIBREF18 . Other approaches have also relied on sources of socioeconomic information such as the UK Standard Occupation Classification (SOC) hierarchy, to assign socioeconomic status to users with occupation mentions BIBREF19 ."]}
{"question_id": "3746aaa1a81d9c725bc7a4a67086634c11998d39", "predicted_answer": "No, the authors do not provide any specific benchmark tasks in this new environment. They mention that HoME can facilitate various tasks, such as exploration, object interaction, and multi-agent scenarios, but they do not define specific benchmark tasks or metrics for evaluating performance in these tasks.", "predicted_evidence": ["SUNCG provides over 45,000 hand-designed house layouts containing over 750,000 hand-designed rooms and sometimes multiple floors. Within these rooms, of which there are 24 kinds, there are objects from among 84 categories and on average over 14 objects per room. As shown in Figure 3 , HoME consists of several, distinct components built on SUNCG that can be utilized individually. The platform runs faster than real-time on a single-core CPU, enables GPU acceleration, and allows users to run multiple environment instances in parallel. These features facilitate faster algorithmic development and learning with more data. HoME provides an OpenAI Gym-compatible environment which loads agents into randomly selected houses and lets it explore via actions such as moving, looking, and interacting with objects (i.e. pick up, drop, push). HoME also enables multiple agents to be spawned at once. The following sections detail HoME's core components.", "Overviewed in Figure 1 , HoME is an interactive extension of the SUNCG dataset BIBREF23 . SUNCG provides over 45,000 hand-designed house layouts containing over 750,000 hand-designed rooms and sometimes multiple floors. Within these rooms, of which there are 24 kinds, there are objects from among 84 categories and on average over 14 objects per room. As shown in Figure 3 , HoME consists of several, distinct components built on SUNCG that can be utilized individually. The platform runs faster than real-time on a single-core CPU, enables GPU acceleration, and allows users to run multiple environment instances in parallel. These features facilitate faster algorithmic development and learning with more data. HoME provides an OpenAI Gym-compatible environment which loads agents into randomly selected houses and lets it explore via actions such as moving, looking, and interacting with objects (i.e. pick up, drop, push). HoME also enables multiple agents to be spawned at once.", "Other 3D house datasets could also be turned into interactive platforms, but these datasets are not as large-scale as SUNCG, which consists of 45622 house layouts. These datasets include Stanford Scenes (1723 layouts) BIBREF33 , Matterport3D BIBREF34 (90 layouts), sceneNN (100 layouts) BIBREF35 , SceneNet (57 layouts) BIBREF36 , and SceneNet RGB-D (57 layouts) BIBREF37 . We used SUNCG, as scale and diversity in data have proven critical for machine learning algorithms to generalize BIBREF20 , BIBREF21 and transfer, such as from simulation to real BIBREF22 . SUNCG's simpler graphics also allow for faster rendering.", "Overviewed in Figure 1 , HoME is an interactive extension of the SUNCG dataset BIBREF23 . SUNCG provides over 45,000 hand-designed house layouts containing over 750,000 hand-designed rooms and sometimes multiple floors. Within these rooms, of which there are 24 kinds, there are objects from among 84 categories and on average over 14 objects per room. As shown in Figure 3 , HoME consists of several, distinct components built on SUNCG that can be utilized individually. The platform runs faster than real-time on a single-core CPU, enables GPU acceleration, and allows users to run multiple environment instances in parallel. These features facilitate faster algorithmic development and learning with more data. HoME provides an OpenAI Gym-compatible environment which loads agents into randomly selected houses and lets it explore via actions such as moving, looking, and interacting with objects (i.e. pick up, drop, push).", "Using these engines and/or external data collection, HoME can facilitate tasks such as:"]}
{"question_id": "143409d16125790c8db9ed38590a0796e0b2b2e2", "predicted_answer": "The dimensions of the considered embeddings are:\n\n* 5-dimensional (binary digits)\n* INLINEFORM1-dimensional (projected vector)\n* The dimensionality of the word embeddings (e.g. GloVe, word2vec) is not explicitly stated, but it is typically high-dimensional (e.g. 100-300 dimensions).", "predicted_evidence": ["Here, we consider how this global structure in the learned embeddings is related to a linearity in the training objective. In particular, linear functions have the property that INLINEFORM0 , imposing a systematic relation between the predictions we make for INLINEFORM1 , INLINEFORM2 and INLINEFORM3 . In fact, we could think of this as a form of translational symmetry where adding INLINEFORM4 to the input has the same effect on the output throughout the space.", "Finally, in our fifth model (proj) we employ another change of representation, this time a dimensionality reduction technique. Specifically, we project the 5-dimensional binary digits INLINEFORM0 onto an INLINEFORM1 dimensional vector INLINEFORM2 and carry out the learning using an INLINEFORM3 -to- INLINEFORM4 layer in this smaller space. DISPLAYFORM0", "Word embeddings, such as GloVe BIBREF12 and word2vec BIBREF13 , have been enormously effective as input representations for downstream tasks such as question answering or natural language inference. One well known application is the INLINEFORM0 example, which represents an impressive extrapolation from word co-occurrence statistics to linguistic analogies BIBREF14 . To some extent, we can see this prediction as exploiting a global structure in which the differences between analogical pairs, such as INLINEFORM1 , INLINEFORM2 and INLINEFORM3 , are approximately equal.", "In the next section, we present four ways to solve this problem and discuss the role of global symmetry in effective extrapolation to the unseen digit. Following that we present practical examples of global structure in the representation of sentences and words. Global, in these examples, means a model form that introduces dependencies between distant regions of the input space.", "Language is a very complex phenomenon, and many of its quirks and idioms need to be treated as local phenomena. However, we have also shown here examples in the representation of words and sentences where global structure supports extrapolation outside the training data."]}
{"question_id": "8ba582939823faae6822a27448ea011ab6b90ed7", "predicted_answer": "Global structures are considered as models that introduce dependencies between distant regions of the input space, allowing for extrapolation outside the training data and supporting regularities that hold for every instance, not just the known set.", "predicted_evidence": ["We suggest that NLP will benefit from incorporating more global structure into its models. Existing background knowledge is one possible source for such additional structure BIBREF17 , BIBREF18 . But it will also be necessary to uncover novel global relations, following the example of the other natural sciences.", "Language is a very complex phenomenon, and many of its quirks and idioms need to be treated as local phenomena. However, we have also shown here examples in the representation of words and sentences where global structure supports extrapolation outside the training data.", "We have used the development of the scientific understanding of planetary motion as a repeated example of the possibility of uncovering global structures that support extrapolation, throughout our discussion. Kepler and Newton found laws that went beyond simply maximising the fit to the known set of planetary bodies to describe regularities that held for every body, terrestrial and heavenly.", "In the next section, we present four ways to solve this problem and discuss the role of global symmetry in effective extrapolation to the unseen digit. Following that we present practical examples of global structure in the representation of sentences and words. Global, in these examples, means a model form that introduces dependencies between distant regions of the input space.", "We hypothesise that breaking this linearity, and allowing a more local fit to the training data will undermine the global structure that the analogy predictions exploit."]}
{"question_id": "65c7a2b734dab51c4c81f722527424ff33b023f8", "predicted_answer": "The translation model employed is a Statistical Machine Translation (SMT) system.", "predicted_evidence": ["We trained translation systems over the following basic units: character, morpheme, word, orthographic syllable and BPE unit. In this section, we summarize the languages and writing systems chosen for our experiments, the datasets used and the experimental configuration of our translation systems, and the evaluation methodology.", "Since we are concerned with low resource scenarios, a desirable property of subword units is robustness of the translation models to change of translation domain. kunchukuttan2016orthographic have shown that OS level models are robust to domain change. Since BPE units are learnt from a specific corpus, it is not guaranteed that they would also be robust to domain changes. To study the behaviour of BPE unit trained models, we also tested the translation models trained on tourism & health domains on an agriculture domain test set of 1000 sentences (see Table TABREF27 for results). In this cross-domain translation scenario, the BPE level model outperforms the OS-level and word-level models for most language pairs. The Konkani-Marathi pair alone shows a degradation using the OS level model. The BPE model is almost on par with the OS level model for Telugu-Malayalam and Hindi-Malayalam.", "There is substantial government, commercial and cultural communication among people speaking related languages (Europe, India and South-East Asia being prominent examples and linguistic regions in Africa possibly in the future). As these regions integrate more closely and move to a digital society, translation between related languages is becoming an important requirement. In addition, translation to/from related languages to a lingua franca like English is also very important. However, despite significant communication between people speaking related languages, most of these languages have few parallel corpora resources. It is therefore important to leverage the relatedness of these languages to build good-quality statistical machine translation (SMT) systems given the lack of parallel corpora.", "The improved performance of BPE units compared to word-level and morpheme-level representations is easy to explain: with a limited vocabulary they address the problem of data sparsity. But character level models also have a limited vocabulary, yet they do not improve translation performance except for very close languages. Character level models learn character mappings effectively, which is sufficient for translating related languages which are very close to each other (translation is akin to transliteration in these cases). But they are not sufficient for translating related languages that are more divergent. In this case, translating cognates, morphological affixes, non-cognates etc. require a larger context. So, BPE and OS units \u2014 which provide more context \u2014 outperform character units.", "The paper is organized as follows. Section SECREF2 discusses related work. Section SECREF3 discusses why BPE is a promising method for learning subword units and describes how we train BPE unit level translation models. Section SECREF4 describes our experimental set-up. Section SECREF5 reports the results of our experiments and analyses the results. Based on experimental results, we analyse why BPE units outperform other units in Section SECREF6 . Section SECREF7 concludes the paper by summarizing our work and discussing further research directions."]}
{"question_id": "11ef46187a5bf15e89d63220fdeaecbeb92d818e", "predicted_answer": "The datasets used for the experiments are not explicitly mentioned in the given context. However, it is mentioned that the experiments involved 16 language pairs, 17 languages, and 10 writing systems, and that the authors trained SMT systems with various representation units for different training set sizes.", "predicted_evidence": ["Our major observations are described below (based on BLEU scores):", "Our experiments spanned a diverse set of languages: 16 language pairs, 17 languages and 10 writing systems. Table TABREF11 summarizes the key aspects of the languages involved in the experiments.", "The LeBLEU scores also show the same trends as the BLEU scores.", "We trained translation systems over the following basic units: character, morpheme, word, orthographic syllable and BPE unit. In this section, we summarize the languages and writing systems chosen for our experiments, the datasets used and the experimental configuration of our translation systems, and the evaluation methodology.", "For different training set sizes, we trained SMT systems with various representation units (Figure FIGREF28 shows the learning curves for two language pairs). BPE level models are better than OS, morpheme and word level across a range of dataset sizes. Especially when the training data is very small, the OS and BPE level models perform significantly better than the word and morpheme level models. For Malayalam-Hindi, the BPE level model is better than the OS level model at utilizing more training data."]}
{"question_id": "45aab23790161cbc55f78e16fdf5678a3f5b4b92", "predicted_answer": "Apart from BPE and orthographic syllables, the authors experiment with the following units of text:\n\n1. Character\n2. Morpheme\n3. Word", "predicted_evidence": ["Both OS and BPE units are variable length units which provide longer and more relevant context for translation compared to character n-grams. In contrast to orthographic syllables, the BPE units are highly frequent character sequences reflecting the underlying statistical properties of the text. Some of the character sequences discovered by the BPE algorithm may be different linguistic units like syllables, morphemes and affixes. Moreover, BPE can be applied to text in any writing system.", "We primarily compare BPE units with orthographic syllables (OS) BIBREF7 , which are good translation units for related languages. The orthographic syllable is a sequence of one or more consonants followed by a vowel, i.e. a C INLINEFORM0 V unit, which approximates a linguistic syllable (e.g. spacious would be segmented as spa ciou s). Orthographic syllabification is rule based and applies to writing systems which represent vowels (alphabets and abugidas).", "We trained translation systems over the following basic units: character, morpheme, word, orthographic syllable and BPE unit. In this section, we summarize the languages and writing systems chosen for our experiments, the datasets used and the experimental configuration of our translation systems, and the evaluation methodology.", "Why does BPE performs better than OS which also provides a larger contextual window for translation? While orthographic syllables represent just approximate syllables, we observe that BPE units also represent higher level semantic units like frequent morphemes, suffixes and entire words. Table TABREF33 shows a few examples for some Indian languages. So, BPE level models can learn semantically similar translation mappings in addition to lexically similar mappings. In this way, BPE units enable the translation models to balance the use of lexical similarity with semantic similarity. This further decouples the translation quality from lexical similarity as seen from the table. BPE units also have an additional degree of freedom (choice of vocabulary size), which allows tuning for best translation performance. This could be important when larger parallel corpora are available, allowing larger vocabulary sizes.", "INLINEFORM0 BPE units also show modest improvement over the recently proposed orthographic syllables over most language pairs (average improvement of 2.6% and maximum improvement of up to 11%). The improvements are not statistically significant for most language pairs. The only exceptions are Bengali-Hindi, Punjabi-Hindi and Malay-Indonesian - all these languages pairs have relatively less morphological affixing (Bengali-Hindi, Punjabi-Hindi) or are registers of the same language (Malay-Indonesian). For Bengali-Hindi and Punjabi-Hindi, the BPE unit translation accuracies are quite close to OS level accuracies. Since OS level models have been shown to be better than character level models BIBREF7 , BPE units are better than character level models by transitivity."]}
{"question_id": "bf5e80f1ab4eae2254b4f4d7651969a3cf945fb4", "predicted_answer": "According to the text, they experimented with a range of 1000 to 4000 merge operations for BPE.", "predicted_evidence": ["The above mentioned results for BPE units do not explore optimal values of the number of merge operations. This is the only hyper-parameter that has to be selected for BPE. We experimented with number of merge operations ranging from 1000 to 4000 and the translation results for these are shown in Table TABREF25 . Selecting the optimal value of merge operations lead to a modest, average increase of 1.6% and maximum increase of 3.5% in the translation accuracy over B INLINEFORM0 across different language pairs .", "INLINEFORM0 It is worth mentioning that BPE units provide a substantial benefit over OS units when translation involves a morphologically rich language. In translations involving Malayalam, Tamil and Telugu, average accuracy improvement of 6.25% were observed.", "Both OS and BPE units are variable length units which provide longer and more relevant context for translation compared to character n-grams. In contrast to orthographic syllables, the BPE units are highly frequent character sequences reflecting the underlying statistical properties of the text. Some of the character sequences discovered by the BPE algorithm may be different linguistic units like syllables, morphemes and affixes. Moreover, BPE can be applied to text in any writing system.", "Table TABREF22 shows translation accuracies of all the language pairs under experimentation for different translation units, in terms of BLEU as well as LeBLEU scores. The number of BPE merge operations was chosen such that the resultant vocabulary size would be equivalent to the vocabulary size of the orthographic syllable encoded corpus. Since we could not do orthographic syllabification for Urdu, Korean and Japanese, we selected the merge operations as follows: For Urdu, number of merge operations were selected based on Hindi OS vocabulary since Hindi and Urdu are registers of the same language. For Korean and Japanese, the number of BPE merge operations was set to 3000, discovered by tuning on a separate validation set.", "Table TABREF31 shows the results of the joint BPE model for language pairs where such a model is built. We do not see any major improvement over the monolingual BPE model due to the joint BPE model."]}
{"question_id": "0a70af6ba334dfd3574991b1dd06f54fc6a700f2", "predicted_answer": "The nuances between fake news and satire discovered in this study include:\n\n* Satire articles are more sophisticated, or less easy to read, than fake news articles.\n\nThese nuances were identified through the evaluation of the linguistic-based method, which achieved a significantly better performance than the baseline language-based method in classifying fake news and satire.", "predicted_evidence": ["In the following subsections, we investigate the semantic and linguistic differences of satire and fake news articles.", "In the following sub sections, we evaluate our classification model and share insights on the nuances between fake news and satire, while addressing our two research questions.", "Our main research questions are therefore, RQ1) are there semantic and linguistic differences between fake news and satire stories that can help to tell them apart?; and RQ2) can these semantic and linguistic differences contribute to the understanding of nuances between fake news and satire beyond differences in the language being used?", "We addressed the challenge of identifying nuances between fake news and satire. Inspired by the humor and social message aspects of satire articles, we tested two classification approaches based on a state-of-the-art contextual language model, and linguistic features of textual coherence. Evaluation of our methods pointed to the existence of semantic and linguistic differences between fake news and satire. In particular, both methods achieved a significantly better performance than the baseline language-based method. Lastly, we studied the feature importance of our linguistic-based method to help shed light on the nuances between fake news and satire. For instance, we observed that satire articles are more sophisticated, or less easy to read, than fake news articles.", "Overall, our contributions, with the improved classification accuracy and towards the understanding of nuances between fake news and satire, carry great implications with regard to the delicate balance of fighting misinformation while protecting free speech."]}
{"question_id": "98b97d24f31e9c535997e9b6cb126eb99fc72a90", "predicted_answer": "The empirical evaluation used in this study is a combination of:\n\n1. Ten-fold cross-validation\n2. F1 score\n3. Principal Component Analysis (PCA)\n4. Logistic regression model\n5. Step-wise backward elimination regression\n6. Two-tailed paired t-test with a 0.05 significance level.", "predicted_evidence": ["We evaluate the performance of our method based on the dataset of fake news and satire articles and using the F1 score with a ten-fold cross-validation as in the baseline work BIBREF1.", "Observing the significant features, in bold in Table TABREF3, we see a combination of surface level related features, such as sentence length and average word frequency, as well as semantic features including LSA (Latent Semantic Analysis) overlaps between verbs and between adjacent sentences. Semantic features which are associated with the gist representation of content are particularly interesting to see among the predictors since based on Fuzzy-trace theory BIBREF13, a well-known theory of decision making under risk, gist representation of content drives individual's decision to spread misinformation online. Also among the significant features, we observe the causal connectives, that are proven to be important in text comprehension, and two indices related to the text easability and readability, both suggesting that satire articles are more sophisticated, or less easy to read, than fake news articles.", "As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable. Significant features of the logistic regression model are shown in Table TABREF3 with the respective significance levels. We also run a step-wise backward elimination regression. Those components that are also significant in the step-wise model appear in bold.", "Table TABREF7 provides a summary of the results. We compare the results of our methods of the pre-trained BERT, using both the headline and text body, and the Coh-Mertix approach, to the language-based baseline with Multinomial Naive Bayes from BIBREF1. Both the semantic cues with BERT and the linguistic cues with Coh-Metrix significantly outperform the baseline on the F1 score. The two-tailed paired t-test with a 0.05 significance level was used for testing statistical significance of performance differences. The best result is given by the BERT model. Overall, these results provide an answer to research question RQ1 regarding the existence of semantic and linguistic difference between fake news and satire.", "Consequently, we use the set of text coherence metrics as implemented by Coh-Metrix BIBREF12. Coh-Metrix is a tool for producing linguistic and discourse representations of a text. As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable."]}
{"question_id": "71b07d08fb6ac8732aa4060ae94ec7c0657bb1db", "predicted_answer": "The baseline is the work referred to as BIBREF1, which is a previous study that evaluated the performance of a method on a dataset of fake news and satire articles using the F1 score with a ten-fold cross-validation.", "predicted_evidence": ["We evaluate the performance of our method based on the dataset of fake news and satire articles and using the F1 score with a ten-fold cross-validation as in the baseline work BIBREF1.", "As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable. Significant features of the logistic regression model are shown in Table TABREF3 with the respective significance levels. We also run a step-wise backward elimination regression. Those components that are also significant in the step-wise model appear in bold.", "Coh-Metrix is a tool for producing linguistic and discourse representations of a text. As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable. Significant features of the logistic regression model are shown in Table TABREF3 with the respective significance levels. We also run a step-wise backward elimination regression.", "First, we consider the semantic representation with BERT. Our experiments included multiple pre-trained models of BERT with different sizes and cases sensitivity, among which the large uncased model, bert_uncased_L-24_H-1024_A-16, gave the best results. We use the recommended settings of hyper-parameters in BERT's Github repository and use the fake news and satire data to fine-tune the model. Furthermore, we tested separate models based on the headline and body text of a story, and in combination. Results are shown in Table TABREF6. The models based on the headline and text body give a similar F1 score. However, while the headline model performs poorly on precision, perhaps due to the short text, the model based on the text body performs poorly on recall. The model based on the full text of headline and body gives the best performance.", "Consequently, we use the set of text coherence metrics as implemented by Coh-Metrix BIBREF12. Coh-Metrix is a tool for producing linguistic and discourse representations of a text. As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable."]}
{"question_id": "812c974311747f74c3aad23999bfef50539953c8", "predicted_answer": "The linguistic features used in this context are:\n\n1. Text statistics:\n\t* Number of words\n\t* Number of sentences\n2. Referential cohesion:\n\t* Overlap in content words between sentences\n3. Text readability formulas:\n\t* Various formulas (not specified)\n4. Connective words:\n\t* Different types of connective words (not specified)\n\nThese features are extracted using the Coh-Metrix tool, which produces linguistic and discourse representations of a text.", "predicted_evidence": ["Consequently, we use the set of text coherence metrics as implemented by Coh-Metrix BIBREF12. Coh-Metrix is a tool for producing linguistic and discourse representations of a text. As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable.", "Consequently, we use the set of text coherence metrics as implemented by Coh-Metrix BIBREF12. Coh-Metrix is a tool for producing linguistic and discourse representations of a text. As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable. Significant features of the logistic regression model are shown in Table TABREF3 with the respective significance levels.", "Coh-Metrix is a tool for producing linguistic and discourse representations of a text. As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable. Significant features of the logistic regression model are shown in Table TABREF3 with the respective significance levels. We also run a step-wise backward elimination regression.", "To investigate the predictive power of the linguistic cues, we use those Coh-Metrix indices that were significant in both the logistic and step-wise backward elimination regression models, and train a classifier on fake news and satire articles. We tested a few classification models, including Naive Bayes, Support Vector Machine (SVM), logistic regression, and gradient boosting - among which the SVM classifier gave the best results.", "As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable. Significant features of the logistic regression model are shown in Table TABREF3 with the respective significance levels. We also run a step-wise backward elimination regression. Those components that are also significant in the step-wise model appear in bold."]}
{"question_id": "180c7bea8caf05ca97d9962b90eb454be4176425", "predicted_answer": "The contextual language model used is BERT (Bidirectional Encoder Representations from Transformers).", "predicted_evidence": ["To study the semantic nuances between fake news and satire, we use BERT BIBREF8, which stands for Bidirectional Encoder Representations from Transformers, and represents a state-of-the-art contextual language model. BERT is a method for pre-training language representations, meaning that it is pre-trained on a large text corpus and then used for downstream NLP tasks. Word2Vec BIBREF9 showed that we can use vectors to properly represent words in a way that captures semantic or meaning-related relationships. While Word2Vec is a context-free model that generates a single word-embedding for each word in the vocabulary, BERT generates a representation of each word that is based on the other words in the sentence. It was built upon recent work in pre-training contextual representations, such as ELMo BIBREF10 and ULMFit BIBREF11, and is deeply bidirectional, representing each word using both its left and right context.", "To study the semantic nuances between fake news and satire, we use BERT BIBREF8, which stands for Bidirectional Encoder Representations from Transformers, and represents a state-of-the-art contextual language model. BERT is a method for pre-training language representations, meaning that it is pre-trained on a large text corpus and then used for downstream NLP tasks. Word2Vec BIBREF9 showed that we can use vectors to properly represent words in a way that captures semantic or meaning-related relationships. While Word2Vec is a context-free model that generates a single word-embedding for each word in the vocabulary, BERT generates a representation of each word that is based on the other words in the sentence. It was built upon recent work in pre-training contextual representations, such as ELMo BIBREF10 and ULMFit BIBREF11, and is deeply bidirectional, representing each word using both its left and right context. We use the pre-trained models of BERT and fine-tune it on the dataset of fake news and satire articles using Adam optimizer with 3 types of decay and 0.01 decay rate.", "BERT is a method for pre-training language representations, meaning that it is pre-trained on a large text corpus and then used for downstream NLP tasks. Word2Vec BIBREF9 showed that we can use vectors to properly represent words in a way that captures semantic or meaning-related relationships. While Word2Vec is a context-free model that generates a single word-embedding for each word in the vocabulary, BERT generates a representation of each word that is based on the other words in the sentence. It was built upon recent work in pre-training contextual representations, such as ELMo BIBREF10 and ULMFit BIBREF11, and is deeply bidirectional, representing each word using both its left and right context. We use the pre-trained models of BERT and fine-tune it on the dataset of fake news and satire articles using Adam optimizer with 3 types of decay and 0.01 decay rate. Our BERT-based binary classifier is created by adding a single new layer in BERT's neural network architecture that will be trained to fine-tune BERT to our task of classifying fake news and satire articles.", "Consequently, we use the set of text coherence metrics as implemented by Coh-Metrix BIBREF12. Coh-Metrix is a tool for producing linguistic and discourse representations of a text. As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable.", "Consequently, we use the set of text coherence metrics as implemented by Coh-Metrix BIBREF12. Coh-Metrix is a tool for producing linguistic and discourse representations of a text. As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable. Significant features of the logistic regression model are shown in Table TABREF3 with the respective significance levels."]}
{"question_id": "95083d486769b9b5e8c57fe2ef1b452fc3ea5012", "predicted_answer": "The state-of-the-art models being compared to are not explicitly mentioned in the given context. However, it can be inferred that the Recurrent Neural Network Grammars (RNNGs) model is being compared to other phrase-structure parsing models, as it achieves a new state of the art in phrase-structure parsing for both languages (PTB \u00a723 and CTB 5.1 test sets).", "predicted_evidence": ["We thank Brendan O'Connor, Swabha Swayamdipta, and Brian Roark for feedback on drafts of this paper, and Jan Buys, Phil Blunsom, and Yue Zhang for help with data preparation. This work was sponsored in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under Contract No. HR0011-15-C-0114; it was also supported in part by Contract No. W911NF-15-1-0543 with the DARPA and the Army Research Office (ARO). Approved for public release, distribution unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. Miguel Ballesteros was supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENSOR) and H2020-RIA-645012 (project KRISTINA).", "2pt Chris Dyer INLINEFORM0 Adhiguna Kuncoro INLINEFORM1 Miguel Ballesteros INLINEFORM2 Noah A. Smith INLINEFORM3 INLINEFORM4 School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA INLINEFORM5 NLP Group, Pompeu Fabra University, Barcelona, Spain INLINEFORM6 Google DeepMind, London, UK INLINEFORM7 Computer Science & Engineering, University of Washington, Seattle, WA, USA {cdyer,akuncoro}@cs.cmu.edu miguel.ballesteros@upf.edu, nasmith@cs.washington.edu [ Corrigendum to Recurrent Neural Network Grammars ] Due to an implentation bug in the RNNG's recursive composition function, the results reported in Dyer et al. (2016) did not correspond to the model as it was presented. This corrigendum describes the buggy implementation and reports results with a corrected implementation. After correction, on the PTB \u00a723 and CTB 5.1 test sets, respectively, the generative model achieves language modeling perplexities of 105.2 and 148.5, and phrase-structure parsing F1 of 93.3 and 86.9, a new state of the art in phrase-structure parsing for both languages.", "Finally, although we considered only the supervised learning scenario, RNNGs are joint models that could be trained without trees, for example, using expectation maximization.", "The parameters in the model are learned to maximize the likelihood of a corpus of trees.", "To obtain an estimate of the MAP tree INLINEFORM0 , we choose the sampled tree with the highest probability under the joint model INLINEFORM1 ."]}
{"question_id": "4c7ec282697f4f6646eb1c19f46bbaf8670b0de6", "predicted_answer": "The weak supervision signal used in Baidu Baike corpus is the title of each sample, which is considered as a pseudo label for NER pre-training.", "predicted_evidence": ["(2) A large-scale Baidu Baike corpus is introduced for entity recognition pre-training, which is of weekly supervised learning since there is no actual named entity label.", "Previous works show that introducing extra data for distant supervised learning usually boost the model performance. For this task, we collect a large-scale Baidu Baike corpus (about 6 million sentences) for NER pre-training. As shown in figure FIGREF12, each sample contains the content and its title. These samples are auto-crawled so there is no actual entity label. We consider the title of each sample as a pseudo label and conduct NER pre-training using these data. Experimental results show that it improves performance.", "NER (Named Entity Recognition) is the first task in the joint multi-head selection model. It is usually formulated as a sequence labeling problem using the BIO (Beginning, Inside, Outside) encoding scheme. Since there are different entity types, the tags are extended to B-type, I-type and O. Linear-chain CRF BIBREF15 is widely used for sequence labeling in deep models. In our method, CRF is built on the top of BERT. Supposed $y\\in {\\left\\lbrace B-type,I-type,O \\right\\rbrace }$ is the label, score function $ s(X,i)_{y_{i}} $ is the output of BERT at $ i_{th}$ character and $ b_{y_{i-1}y_{i}} $ is trainable parameters, the probability of a possible label sequence is formalized as:", "We evaluate our method on the SKE dataset used in this competition, which is the largest schema-based Chinese information extraction dataset in the industry, containing more than 430,000 SPO triples in over 210,000 real-world Chinese sentences, bounded by a pre-specified schema with 50 types of predicates. All sentences in SKE Dataset are extracted from Baidu Baike and Baidu News Feeds. The dataset is divided into a training set (170k sentences), a development set (20k sentences) and a testing set (20k sentences). The training set and the development set are to be used for training and are available for free download. The test set is divided into two parts, the test set 1 is available for self-verification, the test set 2 is released one week before the end of the competition and used for the final evaluation.", "We formulated the relation classification task as a multi-head selection problem, since each token in the sentence has multiple heads, i.e., multiple relations with other tokens. Soft label embedding of the $ i_{th}$ token $ h_{i}$ is feed into two separate fully connected layers to get the subject representation $ h_{i}^{s}$ and object representation $ h_{i}^{o}$. Given the $ i_{th}$ token ($ h_{i}^{s}$, $ h_{i}^{o}$) and the $ j_{th}$ token ($ h_{j}^{s}$, $ h_{j}^{o}$) , our task is to predict their relation:"]}
{"question_id": "07104dd36a0e7fdd2c211ad710de9a605495b697", "predicted_answer": "BERT is optimized for this task by introducing a semantic-enhanced task in the pre-training process, which combines masked language model (MLM), next sentence prediction (NSP), previous sentence prediction, and document level prediction. This enhances the performance of BERT by capturing more contextual and semantic features.", "predicted_evidence": ["Original google BERT is pre-trained using two unsupervised tasks, masked language model (MLM) and next sentence prediction (NSP). MLM task enables the model to capture the discriminative contextual feature. NSP task makes it possible to understand the relationship between sentence pairs, which is not directly captured by language modeling. We further design a semantic-enhanced task to enhance the performance of BERT. It incorporate previous sentence prediction and document level prediction. We pre-train BERT by combining MLM, NSP and the semantic-enhanced task together.", "(1) BERT BIBREF14 is introduced as a feature extraction layer in place of BiLSTM. We also optimize the pre-training process of BERT by introducing a semantic-enhanced task.", "BERT (Bidirectional Encoder Representations from Transformers) BIBREF14 is a new language representation model, which uses bidirectional transformers to pre-train a large unlabeled corpus, and fine-tunes the pre-trained model on other tasks. BERT has been widely used and shows great improvement on various natural language processing tasks, e.g., word segmentation, named entity recognition, sentiment analysis, and question answering. We use BERT to extract contextual feature for each character instead of BiLSTM in the original work BIBREF13. To further improve the performance, we optimize the pre-training process of BERT by introducing a semantic-enhanced task.", "$\\cdot $ Enhance BERT + Soft Label Embedding + Global Relation Prediction + NER Pretraining", "$\\cdot $ Google BERT + Soft Label Embedding + Global Relation Prediction + NER Pretraining"]}
{"question_id": "3e88fcc94d0f451e87b65658751834f6103b2030", "predicted_answer": "A soft label is a vector of probabilities representing the likelihood of each entity type for a token, rather than a hard label which is a single discrete entity type. In the context of entity recognition and relation extraction, soft label embedding takes the logits (output probabilities) from the entity recognition model as input, preserving the uncertainty of each entity type, rather than using the hard labels obtained through CRF decoding. This approach helps to mitigate the propagation of errors from entity recognition to relation classification and allows for differentiable computation.", "predicted_evidence": ["(3) Soft label embedding is proposed to effectively transmit information between entity recognition and relation extraction.", "We formulated the relation classification task as a multi-head selection problem, since each token in the sentence has multiple heads, i.e., multiple relations with other tokens. Soft label embedding of the $ i_{th}$ token $ h_{i}$ is feed into two separate fully connected layers to get the subject representation $ h_{i}^{s}$ and object representation $ h_{i}^{o}$. Given the $ i_{th}$ token ($ h_{i}^{s}$, $ h_{i}^{o}$) and the $ j_{th}$ token ($ h_{j}^{s}$, $ h_{j}^{o}$) , our task is to predict their relation:", "$\\cdot $ Enhanced BERT + Soft Label Embedding + Global Relation Prediction", "$\\cdot $ Google BERT + Soft Label Embedding + Global Relation Prediction", "Miwa et al. (2016) BIBREF16 and Bekoulis et al. (2018) BIBREF13 use the entity tags as input to relation classification layer by learning label embeddings. As reported in their experiments, an improvement of 1$\\sim $2% F1 is achieved with the use of label embeddings. Their mechanism is hard label embedding because they use the CRF decoding results, which have two disadvantages. On one hand, the entity recognition results are not absolutely correct since they are predicted by the model during inference. The error from the entity tags may propagate to the relation classification branch and hurt the performance. On the other hand, CRF decoding process is based on the Viterbi Algorithm, which contains an argmax operation which is not differentiable. To solve this problem, we proposed soft label embedding, which takes the logits as input to preserve probability of each entity type. Suppose $N$ is the logits dimension, i.e., the number of entity type, M is the label embedding matrix, then soft label embedding for $ i_{th}$ character can be formalized as Eq DISPLAY_FORM15:"]}
{"question_id": "c8cf20afd75eb583aef70fcb508c4f7e37f234e1", "predicted_answer": "No, the authors do not examine the real-world distribution of female workers in the country/countries where the gender-neutral languages (Hungarian and Chinese) are spoken. They compare the translation outputs with the U.S. Bureau of Labor Statistics data on the frequency of women's participation in each job position, which is specific to the United States.", "predicted_evidence": ["Although implicit, these biases possibly stem from the real world data which is used to train them, and in this context possibly provide a window into the way our society talks (and writes) about women in the workplace. In this paper, we suggest that and test the hypothesis that statistical translation tools can be probed to yield insights about stereotypical gender roles in our society \u2013 or at least in their training data. By translating professional-related sentences such as \u201cHe/She is an engineer\u201d from gender neutral languages such as Hungarian and Chinese into English, we were able to collect statistics about the asymmetry between female and male pronominal genders in the translation outputs. Our results show that male defaults are not only prominent, but exaggerated in fields suggested to be troubled with gender stereotypes, such as STEM (Science, Technology, Engineering and Mathematics) occupations. And because Google Translate typically uses English as a lingua franca to translate between other languages (e.g. Chinese INLINEFORM0 English INLINEFORM1 Portuguese) BIBREF38 , BIBREF39 , our findings possibly extend to translations between gender neutral languages and non-gender neutral languages (apart from English) in general, although we have not tested this hypothesis.", "In this paper, we have provided evidence that statistical translation tools such as Google Translate can exhibit gender biases and a strong tendency towards male defaults. Although implicit, these biases possibly stem from the real world data which is used to train them, and in this context possibly provide a window into the way our society talks (and writes) about women in the workplace. In this paper, we suggest that and test the hypothesis that statistical translation tools can be probed to yield insights about stereotypical gender roles in our society \u2013 or at least in their training data. By translating professional-related sentences such as \u201cHe/She is an engineer\u201d from gender neutral languages such as Hungarian and Chinese into English, we were able to collect statistics about the asymmetry between female and male pronominal genders in the translation outputs.", "In this paper, we have provided evidence that statistical translation tools such as Google Translate can exhibit gender biases and a strong tendency towards male defaults. Although implicit, these biases possibly stem from the real world data which is used to train them, and in this context possibly provide a window into the way our society talks (and writes) about women in the workplace. In this paper, we suggest that and test the hypothesis that statistical translation tools can be probed to yield insights about stereotypical gender roles in our society \u2013 or at least in their training data. By translating professional-related sentences such as \u201cHe/She is an engineer\u201d from gender neutral languages such as Hungarian and Chinese into English, we were able to collect statistics about the asymmetry between female and male pronominal genders in the translation outputs. Our results show that male defaults are not only prominent, but exaggerated in fields suggested to be troubled with gender stereotypes, such as STEM (Science, Technology, Engineering and Mathematics) occupations.", "We will thus assume throughout this paper that although the distribution of translated gender pronouns may deviate from 50:50, it should not deviate to the extent of misrepresenting the demographics of job positions. That is to say we shall assume that Google Translate incorporates a negative gender bias if the frequency of male defaults overestimates the (possibly unequal) distribution of male employees per female employee in a given occupation.", "In order to strengthen our results, we ran pronominal gender translation statistics against the U.S. Bureau of Labor Statistics data on the frequency of women participation for each job position. Although Google Translate exhibits male defaults, this phenomenon may merely reflect the unequal distribution of male and female workers in some job positions. To test this hypothesis, we compared the distribution of female workers with the frequency of female translations, finding no correlation between said variables. Our data shows that Google Translate outputs fail to reflect the real-world distribution of female workers, under-estimating the expected frequency. That is to say that even if we do not expect a 50:50 distribution of translated gender pronouns, Google Translate exhibits male defaults in a greater frequency that job occupation data alone would suggest. The prominence of male defaults in Google Translate is therefore to the best of our knowledge yet lacking a clear justification."]}
{"question_id": "3567241b3fafef281d213f49f241071f1c60a303", "predicted_answer": "The text does not explicitly state which of the 12 languages showed the strongest tendency towards male defaults. However, it does mention that Hungarian exhibits a better equilibrium between male and female pronouns than Chinese, suggesting that Chinese may have a stronger tendency towards male defaults. Additionally, the text notes that Basque is an exception, exhibiting a strong tendency towards neutral pronouns, rather than male defaults.", "predicted_evidence": ["Although there is a noticeable level of variation among languages and categories, the null hypothesis that male pronouns are not significantly more frequent than female ones was consistently rejected for all languages and all categories examined. The same is true for the null hypothesis that male pronouns are not significantly more frequent than gender neutral pronouns, with the one exception of the Basque language (which exhibits a rather strong tendency towards neutral pronouns). The null hypothesis that neutral pronouns are not significantly more frequent than female ones is accepted with much more frequency, namely for the languages Malay, Estonian, Finnish, Hungarian, Armenian and for the categories Farming & Fishing & Forestry, Healthcare, Legal, Arts & Entertainment, Education. In all three cases, the null hypothesis corresponding to the aggregate for all languages and categories is rejected.", "Once again the data points towards male defaults, but some variation can be observed throughout different adjectives. Sentences containing the words Shy, Attractive, Happy, Kind and Ashamed are predominantly female translated (Attractive is translated as female and gender-neutral in equal parts), while Arrogant, Cruel and Guilty are disproportionately translated with male pronouns (Guilty is in fact never translated with female or neutral pronouns).", "Although there is a noticeable level of variation among languages and categories, the null hypothesis that male pronouns are not significantly more frequent than female ones was consistently rejected for all languages and all categories examined. The same is true for the null hypothesis that male pronouns are not significantly more frequent than gender neutral pronouns, with the one exception of the Basque language (which exhibits a rather strong tendency towards neutral pronouns). The null hypothesis that neutral pronouns are not significantly more frequent than female ones is accepted with much more frequency, namely for the languages Malay, Estonian, Finnish, Hungarian, Armenian and for the categories Farming & Fishing & Forestry, Healthcare, Legal, Arts & Entertainment, Education. In all three cases, the null hypothesis corresponding to the aggregate for all languages and categories is rejected. We can learn from this, in summary, that Google Translate translates male pronouns more frequently than both female and gender neutral ones, either in general for Language-Category pairs or consistently among languages and among categories (with the notable exception of the Basque idiom).", "Our results seem to suggest that this phenomenon extends beyond the scope of the workplace, with the proportion of female pronouns varying significantly according to adjectives used to describe a person. Adjectives such as Shy and Desirable are translated with a larger proportion of female pronouns, while Guilty and Cruel are almost exclusively translated with male ones. Different languages also seemingly have a significant impact in machine gender bias, with Hungarian exhibiting a better equilibrium between male and female pronouns than, for instance, Chinese. Some languages such as Yoruba and Basque were found to translate sentences with gender neutral pronouns very often, although this is the exception rather than the rule and Basque also exhibits a high frequency of phrases for which we could not automatically extract a gender pronoun.", "We have taken the care of experimenting with a fair amount of different gender neutral languages. Because of that, another sensible way of coalescing our data is by language groups, as shown in Table TABREF25 . This can help us visualize the effect of different cultures in the genesis \u2013 or lack thereof \u2013 of gender bias. Nevertheless, the barplots in Figure FIGREF26 are perhaps most useful to identifying the difficulty of extracting a gender pronoun when translating from certain languages. Basque is a good example of this difficulty, although the quality of Bengali, Yoruba, Chinese and Turkish translations are also compromised."]}
{"question_id": "d5d48b812576470edbf978fc18c00bd24930a7b7", "predicted_answer": "Based on the provided context, it appears that the authors are using a single sentence construction template that can be ported to multiple gender-neutral languages. The template involves using a gender-neutral pronoun (such as \"\u0151\" in Hungarian) with a profession or adjective (such as \"nurse\" or \"engineer\"), and then translating the sentence into English using an automated translation tool like Google Translate.\n\nThe authors do not explicitly state the number of different sentence constructions they are using, but based on the examples provided, it seems that they are using a single template with varying adjectives or professions. This template is then applied to multiple gender-neutral languages, as listed in Table TABREF4.\n\nTherefore, the answer to the question is that the authors are using at least one sentence construction template, which can be adapted to multiple languages and professions/adjectives.", "predicted_evidence": ["Our results seem to suggest that this phenomenon extends beyond the scope of the workplace, with the proportion of female pronouns varying significantly according to adjectives used to describe a person. Adjectives such as Shy and Desirable are translated with a larger proportion of female pronouns, while Guilty and Cruel are almost exclusively translated with male ones. Different languages also seemingly have a significant impact in machine gender bias, with Hungarian exhibiting a better equilibrium between male and female pronouns than, for instance, Chinese. Some languages such as Yoruba and Basque were found to translate sentences with gender neutral pronouns very often, although this is the exception rather than the rule and Basque also exhibits a high frequency of phrases for which we could not automatically extract a gender pronoun.", "With this in mind, we propose a quantitative analysis of the phenomenon of gender bias in machine translation. We illustrate how this can be done by simply exploiting Google Translate to map sentences from a gender neutral language into English. As Figure FIGREF1 exemplifies, this approach produces results consistent with the hypothesis that sentences about stereotypical gender roles are translated accordingly with high probability: nurse and baker are translated with female pronouns while engineer and CEO are translated with male ones.", "With this in mind, one can argue that a move towards gender neutrality in language and communication should be striven as a means to promote improved gender equality. Thus, in languages where gender neutrality can be achieved \u2013 such as English \u2013 it would be a valid aim to create translation tools that keep the gender-neutrality of texts translated into such a language, instead of defaulting to male or female variants.", "We shall assume and then show that the phenomenon of gender bias in machine translation can be assessed by mapping sentences constructed in gender neutral languages to English by the means of an automated translation tool. Specifically, we can translate sentences such as the Hungarian \u201c\u0151 egy \u00e1pol\u00f3n\u0151\u201d, where \u201c\u00e1pol\u00f3n\u0151\u201d translates to \u201cnurse\u201d and \u201c\u0151\u201d is a gender-neutral pronoun meaning either he, she or it, to English, yielding in this example the result \u201cshe's a nurse\u201d on Google Translate. As Figure FIGREF1 clearly shows, the same template yields a male pronoun when \u201cnurse\u201d is replaced by \u201cengineer\u201d. The same basic template can be ported to all other gender neutral languages, as depicted in Table TABREF4 . Given the success of Google Translate, which amounts to 200 million users daily, we have chosen to exploit its API to obtain the desired thermometer of gender bias. Also, in order to solidify our results, we have decided to work with a fair amount of gender neutral languages, forming a list of these with help from the World Atlas of Language Structures (WALS) BIBREF30 and other sources.", "We shall assume and then show that the phenomenon of gender bias in machine translation can be assessed by mapping sentences constructed in gender neutral languages to English by the means of an automated translation tool. Specifically, we can translate sentences such as the Hungarian \u201c\u0151 egy \u00e1pol\u00f3n\u0151\u201d, where \u201c\u00e1pol\u00f3n\u0151\u201d translates to \u201cnurse\u201d and \u201c\u0151\u201d is a gender-neutral pronoun meaning either he, she or it, to English, yielding in this example the result \u201cshe's a nurse\u201d on Google Translate. As Figure FIGREF1 clearly shows, the same template yields a male pronoun when \u201cnurse\u201d is replaced by \u201cengineer\u201d. The same basic template can be ported to all other gender neutral languages, as depicted in Table TABREF4 . Given the success of Google Translate, which amounts to 200 million users daily, we have chosen to exploit its API to obtain the desired thermometer of gender bias."]}
{"question_id": "643527e94e8eed1e2229915fcf8cd74d769173fc", "predicted_answer": "The evaluation metrics used are:\n\n1. Area Under the ROC Curve (AUC)\n2. F\u2081 score, which is calculated using precision and recall rates.", "predicted_evidence": ["SLA modeling is actually the word level classification task, so we use area under the ROC curve (AUC) BIBREF32 and $F_{1}$ score BIBREF33 as evaluation metric.", "We compare our method with the following state-of-the-art baselines:", "LR Here, we use the official baseline provided by Duolingo BIBREF29. It is a simple logistic regression using all the meta information and context information provided by datasets.", "where $precision$ and $recall$ are the precision rate and recall rate of the trained model.", "The experiments above show that our method has a huge advantage over the existing methods in low-resource scenarios. In this section, we will observe the performance of our method in the non-low-resource scenario."]}
{"question_id": "bfd55ae9630a08a9e287074fff3691dfbffc3258", "predicted_answer": "The baseline is a simple logistic regression (LR) model that uses all the meta information and context information provided by the datasets.", "predicted_evidence": ["We compare our method with the following state-of-the-art baselines:", "LR Here, we use the official baseline provided by Duolingo BIBREF29. It is a simple logistic regression using all the meta information and context information provided by datasets.", "Our task is to build a model based on users' exercises, and further to predict word-level label sequence of future exercises.", "where $precision$ and $recall$ are the precision rate and recall rate of the trained model.", "Specifically, it can be found that user RWDt7srk and t6nj6nr/ are all English speakers and learn both Spanish and French, so they have data both in the dataset es_en and fr_en. The statistics are shown in Table TABREF32. For baseline methods, we remove the data of these two users on the training set as well as development set of es_en, and then train a model. At last, we use the trained model to directly predict the data of this two users on the es_en test set. Similarly, we use our multi-task method to do the same experiment, and the training data of these two users is also removed from the es_en data set, but fr_en and en_es are unchanged."]}
{"question_id": "3a06d40a4bf5ba6e26d9138434e9139a014deb40", "predicted_answer": "The language learning datasets used are:\n\n1. en_es (English students who can speak Spanish)\n2. es_en (Spanish students who can speak English)\n3. fr_en (French students who can speak English)\n\nThese datasets are from Duolingo SLA modeling shared datasets.", "predicted_evidence": ["We first verify the advantages of our method in cases where the training data of the whole language-learning dataset is insufficient.", "ours-MTL It is our encoder-decoder model without multi-task learning. Thus, we will separately train a model for each language-learning dataset.", "Suppose there are $N$ second language-learning datasets $\\lbrace D^{1},D^{2},..,D^{N}\\rbrace $, and the $k^{th}$ dataset $D^{k}$ is composed of $M^{k}$ exercises $\\lbrace e^{k}_{1},e^{k}_{2},...,e^{k}_{M^{k}}\\rbrace $, where $e^{k}_{j}$ is the $j^{th}$ exercise in the $k^{th}$ dataset.", "We conduct experiments on Duolingo SLA modeling shared datasets, which have three datasets and are collected from English students who can speak Spanish (en_es), Spanish students who can speak English (es_en), and French students who can speak English (fr_en) BIBREF29. Table TABREF19 shows basic statistics of each dataset.", "Specifically, we gradually decrease the size of training data from 400K ( 300K for fr_en ) to 1K and keep the development set and test set. For all baseline methods, since they only use the single language dataset for training, we hence only reduce the data of corresponding language data. For our multi-task learning method, we reduce the training data of one language dataset and keep the remaining other two datasets unchanged."]}
{"question_id": "641fe5dc93611411582e6a4a0ea2d5773eaf0310", "predicted_answer": "In this context, \"lexically overlapping\" sentences are sentences that share words or phrases with each other. In other words, they have some common vocabulary or lexical elements. This is in contrast to sentences that may be semantically related or conceptually similar, but do not share specific words or phrases.\n\nIn the context of explanation graphs, lexical overlap is important because it allows sentences to be explicitly connected and form a coherent explanation. The authors require that each sentence in an explanation be lexically connected to the question, answer, or other sentences in the explanation, which means they must share words or phrases.\n\nThe concept of lexical overlap is also relevant to the idea of using overlap in explanations as a method of inference to generate new explanations. If questions have similar explanations with overlapping sentences, it may be possible to edit, merge, or expand known explanations to create new ones.", "predicted_evidence": ["Figure 5 shows the proportion of questions in the corpus that have 1 or more, 2 or more, 3 or more, etc., overlapping rows in their explanations with at least one other question in the corpus. Similarly, to ground this, Figure 4 shows a visualization of questions whose explanations have 2 or more overlapping rows. For a given level of overlapping explanation sentences, Figure 5 shows that the proportion of questions with that level of overlap increases logarithmically with the number of questions.", "Lexical glue: Sentences that lexically link two concepts, such as \u201cto add means to increase\u201d, or \u201cheating means adding heat\u201d. This is an artificial category in our corpus, brought about by the need for explanation graphs to be explicitly lexically linked.", "All of the above systems share the commonality that they work to connect (or aggregate) multiple pieces of knowledge that, through a variety of inference methods, move towards the goal of answering questions. Fried et al. fried2015higher report that information aggregation for QA is currently very challenging, with few methods able to combine more than two pieces of knowledge before succumbing to semantic drift, or the phenomenon of two pieces of knowledge being erroneously connected due to shared lexical overlap, incomplete word-sense disambiguation, or other noisy signals (e.g. erroneously aggregating a sentence about Apple computers to an inference when working to determine whether apples are a kind of fruit). In a generating a corpus of natural-language explanations for 432 elementary science questions, Jansen et al. jansen2016:COLING found that the average question requires aggregating 4 separate pieces of knowledge to explainably answer, with some questions requiring much longer explanations.", "Explanations for a given question here take the form of a list of sentences, where each sentence is a reference to a specific table row in the table store. To increase their utility for knowledge and inference analyses, we require that each sentence in an explanation be explicitly lexically connected (i.e. share words) with either the question, answer, or other sentences in the explanation. We call this lexically-connected set of sentences an explanation graph.", "One might hypothesize that questions that require similar inferences to correctly answer may also contain some of the same knowledge in their explanations, with the amount of knowledge overlap dependent upon the similarity of the questions. We plan to explore using this overlap as a method of inference that can generate new explanations by editing, merging, or expanding known explanations from similar, known questions (see Jansen jansen:akbc2017 for an initial study). For this to be possible, an explanation corpus must reach a sufficient size that a large majority of questions have substantial overlap in their explanations."]}
{"question_id": "7d34cdd9cb1c988e218ce0fd59ba6a3b5de2024a", "predicted_answer": "According to the text, there are 62 tables in the tablestore.", "predicted_evidence": ["Fine-grained column structure: In tabular representations, columns represent specific roles or arguments to a specific relation (such as X is when Y changes from A to B using mechanism C). In our tablestore we attempt to minimize the amount of information per cell, instead favouring tables with many columns that explicitly identify common roles, conditions, or other relations. This finer-grained structure eases the annotator's cognitive load when authoring new rows, while also better compartmentalizing the relational knowledge in each row for inference algorithms. The tables in our tablestore contain between 2 and 16 content columns, as compared to 2 to 5 columns for the Ariso tablestore BIBREF5 .", "Finally, we examine the growth of the tablestore as it relates to the number of questions in the corpus. Figure 6 shows a monte-carlo simulation of the number of unique tablestore rows required to author explanations for specific corpus sizes. This relationship is strongly correlated (R=0.99) with an exponential proportional decrease. For this elementary science corpus, this asymptotes at approximately 6,000 unique table rows, and 10,000 questions, providing an estimate of the upper-bound of knowledge required in this domain, and the number of unique questions that can be generated within the scope of the elementary science curriculum.", "Khashabi et al. Khashabi:2016TableILP provide the largest elementary science table store to date, containing approximately 5,000 manually-authored rows across 65 tables based on science curriculum topics obtained from study guides and a small corpus of questions. Khashabi et al. also augment their tablestore with 4 tables containing 2,600 automatically generated table rows using OpenIE triples. Reasoning is accomplished using an integer-linear programming algorithm to chain table rows, with Khashabi et al. reporting that an average of 2 table rows are used to answer each question. Evaluation on a small set of 129 science questions achieved passing performance (61%), with an ablation study showing that the bulk of their model's performance was from the manually authored tables.", "Each explanation sentence is represented as a single row from a semi-structured table defined around a particular relation. Our tablestore includes 62 such tables, each centered around a particular relation such as taxonomy, meronymy, causality, changes, actions, requirements, or affordances, and a number of tables specified around specific properties, such as average lifespans of living things, the magnetic properties of materials, or the nominal durations of certain processes (like the Earth orbiting the Sun). The initial selection of table relations was drawn from a list of 21 common relations required for science explanations identified by Jansen et al. jansen2016:COLING on a smaller corpus, and expanded as new knowledge types were identified. Subsets of example tables are included in Figure 2 . Each explanation in this corpus contains an average of 6.3 rows.", "At the collection end of the spectrum, Pasupat and Liang pasupat:2015 extract 2,108 HTML tables from Wikipedia, and propose a method of answering these questions by reasoning over the tables using formal logic. They also introduce the WikiTableQuestions dataset, a set of 22,033 question-answer pairs (such as \u201cGreece held its last Summer Olympics during which year?\u201d) that can be answered using these tables. Demonstrating the ability for collection at scale, Sun et al. sun:2016table extract a total of 104 million tables from Wikipedia and the web, and develop a model that constructs relational chains between table rows using a deep-learning framework. Using their system and table store, Sun et al."]}
{"question_id": "83db51da819adf6faeb950fe04b4df942a887fb5", "predicted_answer": "The dataset used is a collection of over 1.11 million essay responses, which include various types of essays, topics, age ranges, writing styles, and alert types. After preprocessing and removing duplicates, the training sample consists of 866,137 unique responses.", "predicted_evidence": ["To examine the efficacy of each model, our methodology consisted of constructing three sets of data:", "In section SECREF2 we outline the nature of the data we have collected, a precise definition of an alert and how we processed the data for the neural network. In section SECREF3 we outline the definition of the models we evaluate and how they are defined. In section SECREF4 we outline our methodology in determining which models perform best given representative sensitivities of the engine. We attempt to give an approximation of the importance of each feature of the final model.", "Each of the models had 512 recurrent units (the attention mechanisms were not recurrent), hence, in stacking and using bidirectional variants, the number of units were halved. We predominantly trained on using Keras with Tensorflow serving the back-end. The machines we used had NVIDIA Tesla K80s. Each epoch took approximately two to three hours, however, the rate of convergence was such that we could restrict our attention to the models formed in the first 20 epochs as it was clear that the metrics we assessed had converged fairly quickly given the volume of data we had. The total amount of GPU time spent on developing these models was in excess of 4000 hours.", "The idea is that we use the generic test responses to determine how each model would score the types of responses the engine would typically see. While the number of alerts in any set can vary wildly, it is assumed that the set includes both normal and alert responses in the proportions we expect in production. Our baseline model is logistic regression applied to a TF-IDF model with latent semantic analysis used to reduce the representations of words to three hundred dimensions. This baseline model performs poorly at lower thresholds and fairly well at higher thresholds.", "Since the programs inception, we have greatly expanded our collection of training data, which is summarized below in Table TABREF3 . While we have accumulated over 1.11 million essay responses, which include many types of essays over a range of essay topics, student age ranges, styles of writing as well as a multitude of types of alerts, we find that many of them are mapped to the same set of words after applying our preprocessing steps. When we disregard duplicate responses after preprocessing, our training sample consists of only 866,137 unique responses."]}
{"question_id": "7e7471bc24970c6f23baff570be385fd3534926c", "predicted_answer": "The neural network models used in this study are:\n\n1. Recurrent Neural Networks (RNN)\n2. Gated Recurrent Units (GRU)\n3. Long-Short-Term-Memory (LSTM) units\n4. Stacked versions of RNN, GRU, and LSTM\n5. Bidirectional variants of RNN, GRU, and LSTM\n6. Attention mechanism-based models\n7. Convolutional Neural Networks (CNN) (mentioned as a dominant type of neural network in NLP, but not used in this study for alert classification due to the arbitrary length of responses)\n\nThese models are used for classifying alerts and modeling semantically complex features of text.", "predicted_evidence": ["In section SECREF2 we outline the nature of the data we have collected, a precise definition of an alert and how we processed the data for the neural network. In section SECREF3 we outline the definition of the models we evaluate and how they are defined. In section SECREF4 we outline our methodology in determining which models perform best given representative sensitivities of the engine. We attempt to give an approximation of the importance of each feature of the final model.", "Since natural languages contain so many rules, it is inconceivable that we could simply list all possible combinations of words that would constitute an alert. This means that the only feasible models we create are statistical in nature. Just as mathematicians use elementary functions like polynomials or periodic functions to approximate smooth functions, recurrent neural networks are used to fit classes of sequences. Character-level language models are typically useful in predicting text BIBREF27 , speech recognition BIBREF28 and correcting spelling, in contrast it is generally accepted that semantic details are encoded by word-embedding based language models BIBREF29 .", "When it comes to neural network design, there are two dominant types of neural networks in NLP; convolutional neural networks (CNN) and recurrent neural networks (RNN) BIBREF15 . Since responses may be of an arbitrary length different recurrent neural networks are more appropriate tools for classifying alerts BIBREF16 . The most common types of cells used in the design of recurrent neural networks are Gated Recurrent Units (GRU)s BIBREF17 and Long-Short-Term-Memory (LSTM) units BIBREF18 . The latter were originally designed to overcome the vanishing gradient problem BIBREF19 . The GRU has some interesting properties which simplify the LSTM unit and the two types of units can give very similar results BIBREF20 . We also consider stacked versions, bidirectional variants BIBREF21 and the effect of an attention mechanism BIBREF22 . This study has been designed to guide the creation of our desired final production model, which may include higher stacking, dropouts (both regular and recurrent) and may be an ensemble of various networks tuned to different types of responses BIBREF23 .", "When it comes to neural network design, there are two dominant types of neural networks in NLP; convolutional neural networks (CNN) and recurrent neural networks (RNN) BIBREF15 . Since responses may be of an arbitrary length different recurrent neural networks are more appropriate tools for classifying alerts BIBREF16 . The most common types of cells used in the design of recurrent neural networks are Gated Recurrent Units (GRU)s BIBREF17 and Long-Short-Term-Memory (LSTM) units BIBREF18 . The latter were originally designed to overcome the vanishing gradient problem BIBREF19 . The GRU has some interesting properties which simplify the LSTM unit and the two types of units can give very similar results BIBREF20 . We also consider stacked versions, bidirectional variants BIBREF21 and the effect of an attention mechanism BIBREF22 .", "A layer of these recurrent units is a collection of independent units, each of which may pick up a different aspect of the series. A recurrent layer, consisting of INLINEFORM0 independent recurrent units, has the ability to take the most important/prevalent features and summarize those features in a vector of length INLINEFORM1 . When we feed the sequence of outputs of one recurrent layer into another recurrent layer, we call this a stacked recurrent layer. Analogous to the types of features observed in stacking convolutional and dense layers in convolutional neural networks BIBREF30 , it is suspected that stacking recurrent layers allows a neural network to model more semantically complex features of a text BIBREF31 , BIBREF32 ."]}
{"question_id": "ec5e84a1d1b12f7185183d165cbb5eae66d9833e", "predicted_answer": "Based on the context, it appears that the results are reported on data from America, which suggests that the results are likely reported only on English data, as the testing is conducted across America.", "predicted_evidence": ["The American Institutes for Research tests up to 1.8 million students a day during peak testing periods. Over the 2016\u20132017 period AIR delivered 48 million online tests across America. Each test could involve a number of comments, notes and long answer free-form text responses that are considered to be a possible alerts as well as equations or other interactive items that are not considered to be possible alerts. In a single year we evaluate approximately 90 million free-form text responses which range anywhere from a single word or number to ten thousand word essays. These responses are recorded in html and embedded within an xml file along with additional information that allows our clients to identify which student wrote the response. The first step in processing such a response is to remove tags, html code and any non-text using regular expressions.", "Automated Essay Scoring (AES) and Automated Short Answer Scoring (ASAS) has become more prevalent among testing agencies BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . These systems are often designed to address one task and one task alone; to determine whether a written piece of text addresses a question or not. These engines were originally based on either hand-crafted features or term frequency\u2013inverse document frequency (TF-IDF) approaches BIBREF4 . More recently, these techniques have been superseded by the combination of word-embeddings and neural networks BIBREF5 , BIBREF6 , BIBREF7 . For semantically simple responses, the accuracy of these approaches can often be greater than accuracy of human raters, however, these systems are not trained to appropriately deal with the anomalous cases in which a student writes something that elicits concern for the writer or those around them, which we simply call an `alert'.", "Automated Essay Scoring (AES) and Automated Short Answer Scoring (ASAS) has become more prevalent among testing agencies BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . These systems are often designed to address one task and one task alone; to determine whether a written piece of text addresses a question or not. These engines were originally based on either hand-crafted features or term frequency\u2013inverse document frequency (TF-IDF) approaches BIBREF4 . More recently, these techniques have been superseded by the combination of word-embeddings and neural networks BIBREF5 , BIBREF6 , BIBREF7 .", "These systems are often designed to address one task and one task alone; to determine whether a written piece of text addresses a question or not. These engines were originally based on either hand-crafted features or term frequency\u2013inverse document frequency (TF-IDF) approaches BIBREF4 . More recently, these techniques have been superseded by the combination of word-embeddings and neural networks BIBREF5 , BIBREF6 , BIBREF7 . For semantically simple responses, the accuracy of these approaches can often be greater than accuracy of human raters, however, these systems are not trained to appropriately deal with the anomalous cases in which a student writes something that elicits concern for the writer or those around them, which we simply call an `alert'. Typically essay scoring systems do not handle alerts, but rather, separate systems must be designed to process these types of responses before they are sent to the essay scoring system.", "These systems are often designed to address one task and one task alone; to determine whether a written piece of text addresses a question or not. These engines were originally based on either hand-crafted features or term frequency\u2013inverse document frequency (TF-IDF) approaches BIBREF4 . More recently, these techniques have been superseded by the combination of word-embeddings and neural networks BIBREF5 , BIBREF6 , BIBREF7 . For semantically simple responses, the accuracy of these approaches can often be greater than accuracy of human raters, however, these systems are not trained to appropriately deal with the anomalous cases in which a student writes something that elicits concern for the writer or those around them, which we simply call an `alert'. Typically essay scoring systems do not handle alerts, but rather, separate systems must be designed to process these types of responses before they are sent to the essay scoring system. Our goal is not to produce a classification, but rather to use the same methods developed in AES, ASAS and sentiment analysis BIBREF8 , BIBREF9 to identify some percentage of responses that fit patterns seen in known alerts and send them to be assessed by a team of reviewers."]}
{"question_id": "7f958017cbb08962c80e625c2fd7a1e2375f27a3", "predicted_answer": "The baseline model used is logistic regression applied to a TF-IDF model with latent semantic analysis used to reduce the representations of words to three hundred dimensions.", "predicted_evidence": ["To examine the efficacy of each model, our methodology consisted of constructing three sets of data:", "To give an approximation of the effect of each of the attributes we endowed our models with, we can average over the effectiveness of each model with and without each attribute in question. It is clear that that stacking two layers of recurrent units, each with half as many cells, offers the greatest boost in effectiveness, followed by the difference in recurrent structures followed by the use of attention. Using bidirectional units seems to give the smallest increase, but given the circumstances, any positive increase could potentially save lives.", "The idea is that we use the generic test responses to determine how each model would score the types of responses the engine would typically see. While the number of alerts in any set can vary wildly, it is assumed that the set includes both normal and alert responses in the proportions we expect in production. Our baseline model is logistic regression applied to a TF-IDF model with latent semantic analysis used to reduce the representations of words to three hundred dimensions. This baseline model performs poorly at lower thresholds and fairly well at higher thresholds.", "Each of the models had 512 recurrent units (the attention mechanisms were not recurrent), hence, in stacking and using bidirectional variants, the number of units were halved. We predominantly trained on using Keras with Tensorflow serving the back-end. The machines we used had NVIDIA Tesla K80s. Each epoch took approximately two to three hours, however, the rate of convergence was such that we could restrict our attention to the models formed in the first 20 epochs as it was clear that the metrics we assessed had converged fairly quickly given the volume of data we had. The total amount of GPU time spent on developing these models was in excess of 4000 hours.", "In section SECREF2 we outline the nature of the data we have collected, a precise definition of an alert and how we processed the data for the neural network. In section SECREF3 we outline the definition of the models we evaluate and how they are defined. In section SECREF4 we outline our methodology in determining which models perform best given representative sensitivities of the engine. We attempt to give an approximation of the importance of each feature of the final model."]}
{"question_id": "4130651509403becc468bdbe973e63d3716beade", "predicted_answer": "The type of neural network models used are Recurrent Neural Networks (RNNs), specifically:\n\n1. Gated Recurrent Units (GRU)\n2. Long-Short-Term-Memory (LSTM) units\n3. Stacked versions of RNNs\n4. Bidirectional variants of RNNs\n5. RNNs with an attention mechanism", "predicted_evidence": ["Since natural languages contain so many rules, it is inconceivable that we could simply list all possible combinations of words that would constitute an alert. This means that the only feasible models we create are statistical in nature. Just as mathematicians use elementary functions like polynomials or periodic functions to approximate smooth functions, recurrent neural networks are used to fit classes of sequences. Character-level language models are typically useful in predicting text BIBREF27 , speech recognition BIBREF28 and correcting spelling, in contrast it is generally accepted that semantic details are encoded by word-embedding based language models BIBREF29 .", "In section SECREF2 we outline the nature of the data we have collected, a precise definition of an alert and how we processed the data for the neural network. In section SECREF3 we outline the definition of the models we evaluate and how they are defined. In section SECREF4 we outline our methodology in determining which models perform best given representative sensitivities of the engine. We attempt to give an approximation of the importance of each feature of the final model.", "When it comes to neural network design, there are two dominant types of neural networks in NLP; convolutional neural networks (CNN) and recurrent neural networks (RNN) BIBREF15 . Since responses may be of an arbitrary length different recurrent neural networks are more appropriate tools for classifying alerts BIBREF16 . The most common types of cells used in the design of recurrent neural networks are Gated Recurrent Units (GRU)s BIBREF17 and Long-Short-Term-Memory (LSTM) units BIBREF18 . The latter were originally designed to overcome the vanishing gradient problem BIBREF19 . The GRU has some interesting properties which simplify the LSTM unit and the two types of units can give very similar results BIBREF20 . We also consider stacked versions, bidirectional variants BIBREF21 and the effect of an attention mechanism BIBREF22 . This study has been designed to guide the creation of our desired final production model, which may include higher stacking, dropouts (both regular and recurrent) and may be an ensemble of various networks tuned to different types of responses BIBREF23 .", "When it comes to neural network design, there are two dominant types of neural networks in NLP; convolutional neural networks (CNN) and recurrent neural networks (RNN) BIBREF15 . Since responses may be of an arbitrary length different recurrent neural networks are more appropriate tools for classifying alerts BIBREF16 . The most common types of cells used in the design of recurrent neural networks are Gated Recurrent Units (GRU)s BIBREF17 and Long-Short-Term-Memory (LSTM) units BIBREF18 . The latter were originally designed to overcome the vanishing gradient problem BIBREF19 . The GRU has some interesting properties which simplify the LSTM unit and the two types of units can give very similar results BIBREF20 . We also consider stacked versions, bidirectional variants BIBREF21 and the effect of an attention mechanism BIBREF22 .", "A layer of these recurrent units is a collection of independent units, each of which may pick up a different aspect of the series. A recurrent layer, consisting of INLINEFORM0 independent recurrent units, has the ability to take the most important/prevalent features and summarize those features in a vector of length INLINEFORM1 . When we feed the sequence of outputs of one recurrent layer into another recurrent layer, we call this a stacked recurrent layer. Analogous to the types of features observed in stacking convolutional and dense layers in convolutional neural networks BIBREF30 , it is suspected that stacking recurrent layers allows a neural network to model more semantically complex features of a text BIBREF31 , BIBREF32 ."]}
{"question_id": "6edef748370e63357a57610b5784204c9715c0b4", "predicted_answer": "Based on the given context, validity is identified through the use of hand-scorers to evaluate the responses that looked most like alerts. The metric used to quantify validity is the percentage of true alerts found in the sample of 200 responses, which is 14/200 = 7%. This suggests that the model has a validity of around 7% in identifying true alerts.", "predicted_evidence": ["To examine the efficacy of each model, our methodology consisted of constructing three sets of data:", "we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set. We aggregated the estimates at various levels of sensitivity in combination with the efficacy of our best model to estimate that the rate of alerts is approximately 77 to 90 alerts per million responses. Further study is required to approximate what percentage are Tier A and Tier B.", "To evaluate our models, we did a 5-fold validation on a withheld set of 1000 alerts. That is to say we split our set into 5 partitions of 200 alerts, each of which was used as a validation sample for a neural network trained on all remaining data. This produced five very similar models whose performance is given by the percentage of 1000 alerts that were flagged. The percentage of 1000 alerts flagged was computed for each level of sensitivity considered, as measured by the percentage of the total population flagged for potentially being an alert.", "This method also lends itself to a method of approximating the number of alerts in a typical population. we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set. We aggregated the estimates at various levels of sensitivity in combination with the efficacy of our best model to estimate that the rate of alerts is approximately 77 to 90 alerts per million responses.", "This method also lends itself to a method of approximating the number of alerts in a typical population. we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set."]}
{"question_id": "6b302280522c350c4d1527d8c6ebc5b470f9314c", "predicted_answer": "Based on the provided context, severity is identified through a classification system with two tiers of alerts: Tier A and Tier B. Tier A consists of true responses that are alarming and require urgent attention, while Tier B consists of responses that are concerning in nature but require further review.\n\nThe metric used to quantify severity is not explicitly stated, but it can be inferred that the severity of an alert is determined by its classification into either Tier A or Tier B. The system treats both types of responses as alerts, indicating that the classification is binary.\n\nIt's worth noting that the context does not provide a clear metric for quantifying severity, such as a numerical score or rating. Instead, the severity of an alert is determined by its classification into one of the two tiers, which are defined based on the level of urgency and concern required.", "predicted_evidence": ["This method also lends itself to a method of approximating the number of alerts in a typical population. we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set. We aggregated the estimates at various levels of sensitivity in combination with the efficacy of our best model to estimate that the rate of alerts is approximately 77 to 90 alerts per million responses.", "we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set. We aggregated the estimates at various levels of sensitivity in combination with the efficacy of our best model to estimate that the rate of alerts is approximately 77 to 90 alerts per million responses. Further study is required to approximate what percentage are Tier A and Tier B.", "This method also lends itself to a method of approximating the number of alerts in a typical population. we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set.", "Assessment organizations typically perform some sort of alert detection as part of doing business. In among hundreds of millions of long and short responses we find cases of alerts in which students have outlined cases of physical abuse, drug abuse, depression, anxiety, threats to others or plans to harm themselves BIBREF10 . Such cases are interesting from a linguistic, educational, statistical and psychological viewpoint BIBREF11 . While some of these responses require urgent attention, given the volume of responses many testing agencies deal with, it is not feasible to systematically review every single student response within a reasonable time-frame. The benefits of an automated system for alert detection is that we can prioritize a small percentage which can be reviewed quickly so that clients can receive alerts within some fixed time period, which is typically 24 hours. Given the prevalence of school shootings and similarly urgent situations, reducing the number of false positives can effectively speed up the review process and hence optimize our clients ability to intervene when necessary.", "In our classification of alerts, with respect to how they are identified by the team of reviewers, we have two tiers of alerts, Tier A and Tier B. Tier A consists of true responses that are alarming and require urgent attention while Tier B consists of responses that are concerning in nature but require further review. For simplification, both types of responses are flagged as alerts are treated equivalently by the system. This means the classification we seek is binary. Table TABREF1 and Table TABREF2 outline certain subcategories of this classification in addition to some example responses."]}
{"question_id": "7da138ec43a88ea75374c40e8491f7975db29480", "predicted_answer": "Urgency is identified through a classification system that categorizes alerts into two tiers: Tier A and Tier B. Tier A consists of true responses that are alarming and require urgent attention, while Tier B consists of responses that are concerning in nature but require further review.\n\nThe metric used to quantify urgency is not explicitly stated, but it can be inferred that the threshold value used to flag responses for review is a proxy for urgency. The threshold value is set based on the percentage of responses that are desired to be flagged for review, and responses with scores above this threshold are considered alerts. The rate of alerts is then estimated to be approximately 77 to 90 alerts per million responses.", "predicted_evidence": ["This method also lends itself to a method of approximating the number of alerts in a typical population. we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set. We aggregated the estimates at various levels of sensitivity in combination with the efficacy of our best model to estimate that the rate of alerts is approximately 77 to 90 alerts per million responses.", "we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set. We aggregated the estimates at various levels of sensitivity in combination with the efficacy of our best model to estimate that the rate of alerts is approximately 77 to 90 alerts per million responses. Further study is required to approximate what percentage are Tier A and Tier B.", "Assessment organizations typically perform some sort of alert detection as part of doing business. In among hundreds of millions of long and short responses we find cases of alerts in which students have outlined cases of physical abuse, drug abuse, depression, anxiety, threats to others or plans to harm themselves BIBREF10 . Such cases are interesting from a linguistic, educational, statistical and psychological viewpoint BIBREF11 . While some of these responses require urgent attention, given the volume of responses many testing agencies deal with, it is not feasible to systematically review every single student response within a reasonable time-frame. The benefits of an automated system for alert detection is that we can prioritize a small percentage which can be reviewed quickly so that clients can receive alerts within some fixed time period, which is typically 24 hours. Given the prevalence of school shootings and similarly urgent situations, reducing the number of false positives can effectively speed up the review process and hence optimize our clients ability to intervene when necessary.", "This method also lends itself to a method of approximating the number of alerts in a typical population. we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set.", "In our classification of alerts, with respect to how they are identified by the team of reviewers, we have two tiers of alerts, Tier A and Tier B. Tier A consists of true responses that are alarming and require urgent attention while Tier B consists of responses that are concerning in nature but require further review. For simplification, both types of responses are flagged as alerts are treated equivalently by the system. This means the classification we seek is binary. Table TABREF1 and Table TABREF2 outline certain subcategories of this classification in addition to some example responses."]}
{"question_id": "d5d4504f419862275a532b8e53d0ece16e0ae8d1", "predicted_answer": "None of the attribute-value pairs are found in video. According to the given context, the evidence provided for each product item includes a textual product description and a collection of images, but there is no mention of video.", "predicted_evidence": ["In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs.", "We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ).", "In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs. By releasing such a large dataset, we hope to drive progress on this task similar to how the Penn Treebank BIBREF5 , SQuAD BIBREF6 , and Imagenet BIBREF7 have driven progress on syntactic parsing, question answering, and object recognition, respectively.", "Since a multimodal attribute extractor needs to be able to return values for attributes which occur in images as well as text, we cannot treat the problem as a labeling problem as is done in the existing approaches to attribute extraction. We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e.", "We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ). In general, the products at query time will not be in $a$2 , and we do not assume any fixed ontology for products, attributes, or values."]}
{"question_id": "f1e70b63c45ab0fc35dc63de089c802543e30c8f", "predicted_answer": "None of the attribute-value pairs are found in audio. The dataset contains textual product descriptions, collections of images, and open-schema tables of attribute-value pairs, but no audio data.", "predicted_evidence": ["In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs.", "Since a multimodal attribute extractor needs to be able to return values for attributes which occur in images as well as text, we cannot treat the problem as a labeling problem as is done in the existing approaches to attribute extraction. We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e.", "For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ). In general, the products at query time will not be in $a$2 , and we do not assume any fixed ontology for products, attributes, or values. We evaluate the performance on this task as the accuracy of the predicted value with the observed value, however since there may be multiple correct values, we also include hits@ $a$3 evaluation.", "We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ).", "In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs. By releasing such a large dataset, we hope to drive progress on this task similar to how the Penn Treebank BIBREF5 , SQuAD BIBREF6 , and Imagenet BIBREF7 have driven progress on syntactic parsing, question answering, and object recognition, respectively."]}
{"question_id": "39d20b396f12f0432770c15b80dc0d740202f98d", "predicted_answer": "The answer is not explicitly stated in the text, but we can infer that not all attribute-value pairs are found in images. The text mentions that the provided attribute-value pairs only provide a very weak source of supervision, and it is not even guaranteed that the value can be extracted from the provided evidence. This suggests that some attribute-value pairs may not be present in the images, and may only be present in the textual descriptions.", "predicted_evidence": ["In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs.", "We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ).", "Since a multimodal attribute extractor needs to be able to return values for attributes which occur in images as well as text, we cannot treat the problem as a labeling problem as is done in the existing approaches to attribute extraction. We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e.", "We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ). In general, the products at query time will not be in $a$2 , and we do not assume any fixed ontology for products, attributes, or values.", "Since a multimodal attribute extractor needs to be able to return values for attributes which occur in images as well as text, we cannot treat the problem as a labeling problem as is done in the existing approaches to attribute extraction. We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest."]}
{"question_id": "4e0df856b39055a9ba801cc9c8e56d5b069bda11", "predicted_answer": "According to the text, 7.6 million attribute-value pairs are found in semi-structured text.", "predicted_evidence": ["In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs. By releasing such a large dataset, we hope to drive progress on this task similar to how the Penn Treebank BIBREF5 , SQuAD BIBREF6 , and Imagenet BIBREF7 have driven progress on syntactic parsing, question answering, and object recognition, respectively.", "Motivated by this goal, we introduce the task of multimodal attribute extraction. Provided contextual information about an entity, in the form of any of the modes described above, along with an attribute query, the goal is to extract the corresponding value for that attribute. While attribute extraction on the domain of text has been well-studied BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , to our knowledge this is the first time attribute extraction using a combination of multiple modes of data has been considered. This introduces additional challenges to the problem, since a multimodal attribute extractor needs to be able to return values provided any kind of evidence, whereas modern attribute extractors treat attribute extraction as a tagging problem and thus only work when attributes occur as a substring of text.", "In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs.", "We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ).", "Since a multimodal attribute extractor needs to be able to return values for attributes which occur in images as well as text, we cannot treat the problem as a labeling problem as is done in the existing approaches to attribute extraction. We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e."]}
{"question_id": "bbc6d0402cae16084261f8558cebb4aa6d5b1ea5", "predicted_answer": "The answer is not explicitly stated in the text, but it can be inferred that not all attribute-value pairs are found in unstructured text. The text mentions that the provided attribute-value pairs only provide a very weak source of supervision, and it is not even guaranteed that the value can be extracted from the provided evidence. This suggests that some attribute-value pairs may not be present in the textual product description, but rather in the images or other modes of data.", "predicted_evidence": ["In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs.", "In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs. By releasing such a large dataset, we hope to drive progress on this task similar to how the Penn Treebank BIBREF5 , SQuAD BIBREF6 , and Imagenet BIBREF7 have driven progress on syntactic parsing, question answering, and object recognition, respectively.", "Motivated by this goal, we introduce the task of multimodal attribute extraction. Provided contextual information about an entity, in the form of any of the modes described above, along with an attribute query, the goal is to extract the corresponding value for that attribute. While attribute extraction on the domain of text has been well-studied BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , to our knowledge this is the first time attribute extraction using a combination of multiple modes of data has been considered. This introduces additional challenges to the problem, since a multimodal attribute extractor needs to be able to return values provided any kind of evidence, whereas modern attribute extractors treat attribute extraction as a tagging problem and thus only work when attributes occur as a substring of text.", "We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ).", "Since a multimodal attribute extractor needs to be able to return values for attributes which occur in images as well as text, we cannot treat the problem as a labeling problem as is done in the existing approaches to attribute extraction. We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e."]}
{"question_id": "a7e03d24549961b38e15b5386d9df267900ef4c8", "predicted_answer": "The answer is 1. There is only one semi-structured template represented in the data, which is the open-schema table of attribute-value pairs.", "predicted_evidence": ["Given the large collections of unstructured and semi-structured data available on the web, there is a crucial need to enable quick and efficient access to the knowledge content within them. Traditionally, the field of information extraction has focused on extracting such knowledge from unstructured text documents, such as job postings, scientific papers, news articles, and emails. However, the content on the web increasingly contains more varied types of data, including semi-structured web pages, tables that do not adhere to any schema, photographs, videos, and audio. Given a query by a user, the appropriate information may appear in any of these different modes, and thus there's a crucial need for methods to construct knowledge bases from different types of data, and more importantly, combine the evidence in order to extract the correct answer.", "In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs.", "In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs. By releasing such a large dataset, we hope to drive progress on this task similar to how the Penn Treebank BIBREF5 , SQuAD BIBREF6 , and Imagenet BIBREF7 have driven progress on syntactic parsing, question answering, and object recognition, respectively.", "We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ).", "We evaluate on a subset of the MAE dataset consisting of the 100 most common attributes, covering roughly 50% of the examples in the overall MAE dataset. To determine the relative effectiveness of the different modes of information, we train image and text only versions of the model described above. Following the suggestions in BIBREF15 we use a 600 unit single layer in our text convolutions, and a 5 word window size. We apply dropout to the output of both the image and text CNNs before feeding the output through fully connected layers to obtain the image and text embeddings. Employing a coarse grid search, we found models performed best using a large embedding dimension of $k=1024$ . Lastly, we explore multimodal models using both the Concat and the GMU strategies. To evaluate models we use the hits@ $k$ metric on the values."]}
{"question_id": "036c400424357457e42b22df477b7c3cdc2eefe9", "predicted_answer": "No, the datapoints are not from the same website. According to the text, the Multimodal Attribute Extraction (MAE) dataset was collected from \"a large number of e-commerce sites\" using the Diffbot Product API.", "predicted_evidence": ["Given the large collections of unstructured and semi-structured data available on the web, there is a crucial need to enable quick and efficient access to the knowledge content within them. Traditionally, the field of information extraction has focused on extracting such knowledge from unstructured text documents, such as job postings, scientific papers, news articles, and emails. However, the content on the web increasingly contains more varied types of data, including semi-structured web pages, tables that do not adhere to any schema, photographs, videos, and audio. Given a query by a user, the appropriate information may appear in any of these different modes, and thus there's a crucial need for methods to construct knowledge bases from different types of data, and more importantly, combine the evidence in order to extract the correct answer.", "In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs.", "In order to kick start research on multimodal information extraction problems, we introduce the multimodal attribute extraction dataset, an attribute extraction dataset derived from a large number of e-commerce websites. MAE features images, textual descriptions, and attribute-value pairs for a diverse set of products. Preliminary data from an Amazon Mechanical Turk study demonstrates that both modes of information are beneficial to attribute extraction. We measure the performance of a collection of baseline models, and observe that reasonably high accuracy can be obtained using only text. However, we are unable to train off-the-shelf methods to effectively leverage image data.", "For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ). In general, the products at query time will not be in $a$2 , and we do not assume any fixed ontology for products, attributes, or values. We evaluate the performance on this task as the accuracy of the predicted value with the observed value, however since there may be multiple correct values, we also include hits@ $a$3 evaluation.", "In order to support research on this task, we release the Multimodal Attribute Extraction (MAE) dataset, a large dataset containing mixed-media data for over 2.2 million commercial product items, collected from a large number of e-commerce sites using the Diffbot Product API. The collection of items is diverse and includes categories such as electronic products, jewelry, clothing, vehicles, and real estate. For each item, we provide a textual product description, collection of images, and open-schema table of attribute-value pairs (see Figure 1 for an example). The provided attribute-value pairs only provide a very weak source of supervision; where the value might appear in the context is not known, and further, it is not even guaranteed that the value can be extracted from the provided evidence. In all, there are over 4 million images and 7.6 million attribute-value pairs. By releasing such a large dataset, we hope to drive progress on this task similar to how the Penn Treebank BIBREF5 , SQuAD BIBREF6 , and Imagenet BIBREF7 have driven progress on syntactic parsing, question answering, and object recognition, respectively."]}
{"question_id": "63eda2af88c35a507fbbfda0ec1082f58091883a", "predicted_answer": "Yes, they do consider semi-structured webpages. In the given context, it is mentioned that \"the content on the web increasingly contains more varied types of data, including semi-structured web pages...\" Additionally, BIBREF4 proposes to extract attribute-value pairs from structured data on product pages, such as HTML tables and lists, to construct the knowledge base, which implies that semi-structured web pages are being considered.", "predicted_evidence": ["Given the large collections of unstructured and semi-structured data available on the web, there is a crucial need to enable quick and efficient access to the knowledge content within them. Traditionally, the field of information extraction has focused on extracting such knowledge from unstructured text documents, such as job postings, scientific papers, news articles, and emails. However, the content on the web increasingly contains more varied types of data, including semi-structured web pages, tables that do not adhere to any schema, photographs, videos, and audio. Given a query by a user, the appropriate information may appear in any of these different modes, and thus there's a crucial need for methods to construct knowledge bases from different types of data, and more importantly, combine the evidence in order to extract the correct answer.", "BIBREF1 treat attribute extraction of retail products as a form of named entity recognition. They predefine a list of attributes to extract and train a Na\u00efve Bayes model on a manually labeled seed dataset to extract the corresponding values. BIBREF3 build on this work by bootstrapping to expand the seed list, and evaluate more complicated models such as HMMs, MaxEnt, SVMs, and CRFs. To mitigate the introduction noisy labels when using semi-supervised techniques, BIBREF2 incorporates crowdsourcing to manually accept or reject the newly introduced labels. One major drawback of these approaches is that they require manually labelled seed data to construct the knowledge base of attribute-value pairs, which can be quite expensive for a large number of attributes. BIBREF0 address this problem by using an unsupervised, LDA-based approach to generate word classes from reviews, followed by aligning them to the product description. BIBREF4 propose to extract attribute-value pairs from structured data on product pages, such as HTML tables, and lists, to construct the KB.", "BIBREF3 build on this work by bootstrapping to expand the seed list, and evaluate more complicated models such as HMMs, MaxEnt, SVMs, and CRFs. To mitigate the introduction noisy labels when using semi-supervised techniques, BIBREF2 incorporates crowdsourcing to manually accept or reject the newly introduced labels. One major drawback of these approaches is that they require manually labelled seed data to construct the knowledge base of attribute-value pairs, which can be quite expensive for a large number of attributes. BIBREF0 address this problem by using an unsupervised, LDA-based approach to generate word classes from reviews, followed by aligning them to the product description. BIBREF4 propose to extract attribute-value pairs from structured data on product pages, such as HTML tables, and lists, to construct the KB. This is essentially the approach used to construct the knowledge base of attribute-value pairs used in our work, which is automatically performed by Diffbot's Product API.", "Our work is related to, and builds upon, a number of existing approaches.", "In order to kick start research on multimodal information extraction problems, we introduce the multimodal attribute extraction dataset, an attribute extraction dataset derived from a large number of e-commerce websites. MAE features images, textual descriptions, and attribute-value pairs for a diverse set of products. Preliminary data from an Amazon Mechanical Turk study demonstrates that both modes of information are beneficial to attribute extraction. We measure the performance of a collection of baseline models, and observe that reasonably high accuracy can be obtained using only text. However, we are unable to train off-the-shelf methods to effectively leverage image data."]}
{"question_id": "fe6181ab0aecf5bc8c3def843f82e530347d918b", "predicted_answer": "The baseline models are:\n\n1. Baseline: An MLE model trained on the Conceptual Captions training split alone.\n2. Baseline$+(t)$: A finetuned version of the Baseline model that utilizes positively-rated captions from the Caption-Quality training split, with a rating threshold of $t \\in [0,1]$. There are two variants of this model, with $t=0.5$ and $t=0.7$.", "predicted_evidence": ["Figure FIGREF46 presents some qualitative example outputs for our models, showcasing the effectiveness of the OffPG method. We observe that the OffPG model is often successful at correcting arbitrary qualifiers present in the baseline outputs (e.g., `half marathon' and `most beautiful' in the second and third examples, respectively).", "In the other type of evaluation, we measure the relative improvement of a model against the Baseline model; Three professional raters are shown the input image and two captions (anonymized and randomly shuffled with respect to their left/right position) side-by-side. One of the captions is from a candidate model and the other always from Baseline. We ask for relative judgments on three dimensions \u2013 Informativeness, Correctness and Fluency, using their corresponding questions shown in Table TABREF32. Each of these dimensions allows a 5-way choice, shown below together with their corresponding scores:", "We first train an MLE model as our baseline, trained on the Conceptual Captions training split alone. We referred to this model as Baseline. For a baseline approach that utilizes (some of) the Caption-Quality data, we merge positively-rated captions from the Caption-Quality training split with the Conceptual Captions examples and finetune the baseline model. We call this model Baseline$+(t)$, where $t \\in [0,1]$ is the rating threshold for the included positive captions. We train models for two variants, $t\\in \\lbrace 0.5, 0.7\\rbrace $, which results in $\\sim $72K and $\\sim $51K additional (pseudo-)ground-truth captions, respectively. Note that the Baseline$+(t)$ approaches attempt to make use of the same additional dataset as our two reinforced models, OnPG and OffPG, but they need to exclude below-threshold captions due to the constraints in MLE.", "Each model is evaluated by the average rating scores from 3 distinct raters. As a result, we obtain 3 values for each model in the range $[-1, 1]$, where a negative score means a performance degradation in the given dimension with respect to Baseline. For every human evaluation, we report confidence intervals based on bootstrap resampling BIBREF35.", "Figure FIGREF44 highlights the way in which the OffPG method achieves its superiority over the Baseline model, compared to the other alternative models (using the 'Corectness' scores). For instance, over 75% of the captions for both Baseline$+(t)$ models receive a 0.0 score (equal quality), and more than half of them are exactly identical to their corresponding Baseline captions. In contrast, OffPG makes a strong impact by explicitly penalizing the captions with negative feedback: less than 16% captions are identical to the corresponding Baseline captions. Moreover, we observe a large portion of captions with scores of 1.0 in favor of OffPG, indicating that many captions are significantly enhanced. We observe similar trends in all the three metrics."]}
{"question_id": "0b1b8e1b583242e5be9b7be73160630a0d4a96b2", "predicted_answer": "The image caption datasets used in this work are:\n\n1. Conceptual Captions (BIBREF0)\n2. Open Images Dataset (BIBREF29)\n3. Caption-Quality dataset (sampled from Open Images Dataset, but with a disjoint split)\n4. T2 test dataset (used in the CVPR 2019 Conceptual Captions Challenge, contains 1K images sampled from Open Images Dataset)", "predicted_evidence": ["In the experiments, we use Conceptual Captions BIBREF0, a large-scale captioning dataset that consists of images crawled from the Internet, with captions derived from corresponding Alt-text labels on the webpages. The training and validation splits have approximately 3.3M and 16K samples, respectively.", "To evaluate our models, we run human evaluation studies on the T2 test dataset used in the CVPR 2019 Conceptual Captions Challenge. The dataset contains 1K images sampled from the Open Images Dataset BIBREF29. Note that the images in the Caption-Quality dataset are also sampled from the Open Images Dataset, but using a disjoint split. So there is no overlap between the caption ratings dataset $\\mathcal {D}_\\mathrm {CR}$ we use for training, and the T2 test set we use for evaluations.", "We run two sets of human evaluation studies to evaluate the performance of our models and baselines, using the T2 dataset (1K images). For every evaluation, we generate captions using beam search (beam size of 5).", "This technique allows to obtain rating estimates for any image-caption pairs, including ones that are not present in the dataset $\\mathcal {D}$. The training objective with Eq. (DISPLAY_FORM14) is now maximizing the expected rating estimate of captions. This approach is effective only if the trained rating estimator generalizes well to unseen images and captions, and it is expected to be effective only to the extent to which the rating estimator performs well over the sampled search space. In our work, we have observed artifacts of the ratings estimator that negatively impact the performance of this method, e.g., severely ill-formed captions for which the caption estimator had no training signal but assigned high ratings. We report results for this method in Section SECREF4.", "In this paper, we describe how to train an improved captioning model by using a caption ratings dataset, which is often a natural by-product in the development process of image captioning models. We show that an off-policy RL technique with an alternative sampling distribution successfully deals with the sparsity of information about the rating function, while an on-policy method has difficulties in obtaining an improved model, due to generalization issues of the ratings estimator. While this conclusion may not be definitive, it is definitely an important result, and it also opens up additional lines of inquiry along the relative merits of these RL techniques."]}
{"question_id": "830f9f9499b06fb4ac3ce2f2cf035127b4f0ec63", "predicted_answer": "The time it takes to train the model is not explicitly stated in the text, but we can estimate it based on the number of iterations and batch size.\n\nFor the Baseline model, it is trained for 3M iterations with a batch size of 4,096. Assuming a constant training time per iteration, we can estimate the total training time as:\n\n3,000,000 iterations / 4,096 batch size = approximately 732 hours or around 30 days\n\nFor the Baseline+(t) models, they are fine-tuned for 1M iterations with the same batch size, so the estimated training time is:\n\n1,000,000 iterations / 4,096 batch size = approximately 244 hours or around 10 days\n\nFor the OnPG model, the batch size is reduced by a factor of 0.25, so the estimated training time would be longer than the Baseline model.\n\nNote that these estimates are rough and do not take into account other factors that can affect training time, such as the complexity of the model, the hardware used, and the implementation details.", "predicted_evidence": ["We train Baseline using the Adam optimizer BIBREF34 on the training split of the Conceptual dataset for 3M iterations with the batch size of 4,096 and the learning rate of $3.2\\times 10^{-5}$. The learning rate is warmed up for 20 epochs and exponentially decayed by a factor of 0.95 every 25 epochs. Baseline$+(t)$ are obtained by fine-tuning Baseline on the merged dataset for 1M iterations, with the learning rate of $3.2\\times 10^{-7}$ and the same decaying factor. For OnPG, because its memory footprint is increased significantly due to the additional parameters for the rating estimator, we reduce the batch size for training this model by a 0.25 factor; the value of $b$ in Eq. (DISPLAY_FORM12) is set to the moving average of the rating estimates. During OffPG training, for each batch, we sample half of the examples from the Conceptual dataset and the other half from Caption-Quality dataset; $b$ is set to the average of the ratings in the dataset.", "We train Baseline using the Adam optimizer BIBREF34 on the training split of the Conceptual dataset for 3M iterations with the batch size of 4,096 and the learning rate of $3.2\\times 10^{-5}$. The learning rate is warmed up for 20 epochs and exponentially decayed by a factor of 0.95 every 25 epochs. Baseline$+(t)$ are obtained by fine-tuning Baseline on the merged dataset for 1M iterations, with the learning rate of $3.2\\times 10^{-7}$ and the same decaying factor. For OnPG, because its memory footprint is increased significantly due to the additional parameters for the rating estimator, we reduce the batch size for training this model by a 0.25 factor; the value of $b$ in Eq. (DISPLAY_FORM12) is set to the moving average of the rating estimates.", "As our training conditions, we assume the access to both a captioning dataset and a caption ratings dataset. Under a curriculum learning procedure, we first train a model by MLE on the captioning dataset, and then fine-tune the model with the above methods using the caption ratings dataset. To avoid overfitting during fine-tuning, we add the MLE loss on the captioning dataset as a regularization term. Given the caption labeled dataset $\\mathcal {D}_\\mathrm {IC}$ and the caption ratings dataset $\\mathcal {D}_\\mathrm {CR}$, the final gradients w.r.t. the parameters are therefore computed as follows:", "We run two sets of human evaluation studies to evaluate the performance of our models and baselines, using the T2 dataset (1K images). For every evaluation, we generate captions using beam search (beam size of 5).", "MLE with ground-truth captions has been widely adopted as the standard surrogate objective for training BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7. Aside from this main thrust, an additional line of research is concerned with optimizing models that maximize some automatic evaluation metric(s) using RL, in an attempt to bridge the mismatch between the training objective and the evaluation criteria BIBREF9, BIBREF10, BIBREF11, BIBREF12. To our knowledge, this is the first study that proposes to optimize test-time scores of human judgment using a dataset generated by a human evaluation process."]}
{"question_id": "a606bffed3bfeebd1b66125be580f908244e5d92", "predicted_answer": "The human ratings dataset, also referred to as the caption ratings dataset, contains approximately 130,000 rated captions for training, 7,000 for validation, and 7,000 for testing.", "predicted_evidence": ["In our experiments, we use the Caption-Quality dataset BIBREF13, recently introduced for the purpose of training quality-estimation models for image captions. We re-purpose this data as our caption ratings dataset $\\mathcal {D}_\\mathrm {CR}$. The dataset is divided into training, validation and test splits containing approximately 130K, 7K and 7K rated captions, respectively. Each image has an average of 4.5 captions (generated by different models that underwent evaluation evaluation). The captions are individually rated by asking raters the question \u201cIs this a good caption for the image?\u201d, with the answers \u201cNO\u201d or \u201cYES\u201d mapped to a 0 or 1 score, respectively. Each image/caption pair is evaluated by 10 different human raters, and an average rating score per-caption is obtained by quantizing the resulting averages into a total of nine bins $\\lbrace 0, \\frac{1}{8} \\dots \\frac{7}{8}, 1\\rbrace $.", "A sample in a caption ratings dataset is comprised of an image $I$, a machine-generated caption $c$, and a human judgment for the caption quality $r(c|I) \\in \\mathbb {R}$. For each image, multiple captions from several candidate models are available, some of which might be rated higher than others. In the setup used in this paper, the low-rated captions serve as negative examples, because human annotators judged them as bad captions (see examples in Figure FIGREF1). $r(c|I)$ is possibly an aggregate of multiple ratings from different raters. Section SECREF23 provides more details of the caption ratings dataset that we employ.", "One approach to address the sparsity of the rating function is to construct a caption quality estimator, while keeping the sampling process on-policy; this is the method adopted in, e.g., BIBREF21, BIBREF27, BIBREF28. Incidentally, it is also the expressed goal for the effort behind the caption ratings dataset in BIBREF13 that we use in this work.", "We make a few observations that apply not only to image captioning, but more generally to the principle of generating annotations. Although a human-ratings dataset is usually just a by-product of human evaluations for past models, such a dataset can be valuable for improving models (as we show in this paper). There are several advantageous properties of a ratings dataset over traditional supervised-learning datasets. First, obtaining ratings for automatically generated outputs is significantly cheaper than collecting ground-truth labels, because it requires less rater training and less time spent annotating. Moreover, if human evaluation is performed anyway during a model's development cycle, there is no additional cost associated to using these annotations for further improving the model. In addition to that, it is easy to capture consensus between multiple raters to reduce noise, e.g., by averaging their scores; it is completely non-trivial to achieve a similar effect from multiple ground-truth labels.", "In addition to an aggregate model performance, such human evaluation studies also produce a valuable by-product: a dataset of model-generated image captions with human annotated quality labels, as shown in Figure FIGREF1. We argue that such a by-product, henceforth called a caption ratings dataset, can be successfully used to improve the quality of image captioning models, for several reasons. First, optimizing based on instance-level human judgments of caption quality represent a closer-to-truth objective for image captioning: generating more captions judged as good but fewer ones rated as poor by human raters. Second, while having highly-rated captions as positive examples (i.e., how good captions may look like), a caption ratings dataset also contains captions that are highly-scored by a model but annotated as negative examples (i.e., how model-favored yet bad captions look like), which intuitively should be a useful signal for correcting common model biases. To the best of our knowledge, our work is the first to propose using human caption ratings directly for training captioning models."]}
{"question_id": "f8fe4049bea86d0518d1881f32049e60526d0f34", "predicted_answer": "The authors compare MERF against the following existing techniques:\n\n1. ANGE (for narrator chain extraction)\n2. ATEEMA (for temporal entity extraction)\n3. GENTREE (for genealogy extraction)\n4. NUMNORM (for number normalization)", "predicted_evidence": ["Table TABREF35 shows that MERF has a clear advantage over task specific techniques in the effort required to develop the application at a reasonable cost in terms of accuracy and run time. Developers needed three hours, three hours, four hours, and one hour to develop the narrator chain, temporal entity, genealogy, and number normalization case studies using MERF, respectively. However, the developers of ANGE, ATEEMA, GENTREE, and NUMNORM needed two months, one and a half months, three weeks, and one week, respectively. MERF needed eight MBFs and four MREs for narrator chain, three MBFs and two MREs for temporal entity, three MBFs and three MREs for genealogy, and three MBFs, one MRE, and 57 lines of code actions for the number normalization tasks.", "Table TABREF35 shows that MERF has a clear advantage over task specific techniques in the effort required to develop the application at a reasonable cost in terms of accuracy and run time. Developers needed three hours, three hours, four hours, and one hour to develop the narrator chain, temporal entity, genealogy, and number normalization case studies using MERF, respectively. However, the developers of ANGE, ATEEMA, GENTREE, and NUMNORM needed two months, one and a half months, three weeks, and one week, respectively. MERF needed eight MBFs and four MREs for narrator chain, three MBFs and two MREs for temporal entity, three MBFs and three MREs for genealogy, and three MBFs, one MRE, and 57 lines of code actions for the number normalization tasks. However, ANGE, ATEEMA, GENTREE, and NUMNORM required 3,000+, 1,000+, 3,000+, and 500 lines of code, respectively.", "We evaluated ANGE with Musnad Ahmad, a hadith book, where we constructed an annotated golden reference containing 1,865 words. We evaluated ATEEMA with articles from issues of the Lebanese Al-Akhbar newspaper where we constructed an annotated golden reference containing 1,677 words. For the genealogical tree extraction we used an extract from the Genesis biblical text with 1,227 words. Finally, we used an annotated article from the Lebanese Assafir newspaper with 1,399 words to evaluate the NUMNORM case study. In the online appendix, we report on eight additional MERF case studies. Manual annotators inspected the outcome and provided corrections where tools made mistakes. The corrections form the manual gold annotation that we compared against.", "In this section we evaluate MERF with four case studies. We perform a survey-like evaluation where developers manually built task specific information extraction tools for the case studies and other developers built equivalent MERF tools. The aim of the comparison is to showcase that MERF enables fast development of linguistic applications with similar accuracy and a reasonable affordable overhead in computational time. We report development time, size of developed code versus size of grammar, running time, and precision-recall as metrics of cost, complexity, overhead, and accuracy, respectively.", "The first row in Table TABREF36 is an example narrator chain, the second is the transliteration, the third shows the MBF tags. Rows 4, 5, and 6 show the matches for name, nar, and nchain, respectively. MERF assigns the symbols INLINEFORM0 and INLINEFORM1 for the MRE subexpressions TOLD and nar, respectively. We define the relation INLINEFORM2 to relate sequences of narrators with edges labeled by the tags of TOLD where INLINEFORM3 denotes the next match of nar in the one or more MRE subexpression. Table TABREF38 shows that MERF detected almost all the MBF matches with 99% recall and 85% precision and extracted user-defined relations with 98% recall and 99% precision."]}
{"question_id": "a9eb8039431e2cb885cfcf96eb58c0675b36b3bd", "predicted_answer": "No, the dataset is not completely automatically generated. While the methodology for automatic generation is presented, the validation and test sets are human-annotated, indicating that human supervision is still involved in the creation of the dataset.", "predicted_evidence": ["The methodology to automatically generate our dataset is presented in Section SECREF3. Data preprocessing and linking, along with details on the generated dataset, are given in Section SECREF4. Section SECREF5 presents a baseline using deep neural networks.", "The dataset is characterized by lines corresponding to words extracted from the preprocessing steps described previously, following the BIO annotations methodology.", "The vast amounts of data available from public sources such as Wikipedia can be readily used to pre-train machine learning models in an unsupervised fashion \u2013 for example, learning word embeddings BIBREF0. However, large labeled datasets are still often required to successfully train complex models such as deep neural networks, collecting them remain an obstacle for many tasks.", "Unfortunately, such datasets are a scarce resource whose construction is costly due to the required human-made, word-level annotations. In this work we propose a method to construct labeled datasets without human supervision for NER, using public data sources structured according to Semantic Web principles, namely, DBpedia and Wikipedia.", "In order to have a fair evaluation of our model, we use human-annotated datasets as validation and test sets."]}
{"question_id": "998fa38634000f2d7b52d16518b9e18e898ce933", "predicted_answer": "The text does not explicitly mention whether the SESAME dataset includes discontiguous entities or not.", "predicted_evidence": ["The HAREM corpus follows a different format than the one of SESAME: it uses a markup structure, without a proper tokenization of sentences and words. To circumvent this, we convert it to BIO format by applying the same tokenization process used for generating our dataset.", "SESAME consists of 87,769,158 tokens in total. The count and proportion of each entity tag (not a named entity, organization, person, location) is given in TABREF45.", "Additionally, the baseline was developed on a balanced re-sample of SESAME with a total of 1,216,976 sentences. The model also receives additional categorical features for each word, signalizing whether it: (1) starts with a capital letter, (2) has capitalized letters only, (3) has lowercase letters only, (4) contains digits, (5) has mostly digits ($>$ 50%) and (6) has digits only.", "Table TABREF47 shows a size comparison between SESAME and popular datasets for Portuguese NER.", "SESAME consists of 3,650,909 sentences, with lengths (in terms of number of tokens) following the distribution shown in Figure FIGREF42. A breakdown of relevant statistics, such as the mean and standard deviation of sentences' lengths, is given in Table TABREF43."]}
{"question_id": "a82686c054b96f214521e468b17f0435e6cdf7cf", "predicted_answer": "The resulting SESAME dataset consists of 87,769,158 tokens in total, divided into 3,650,909 sentences.", "predicted_evidence": ["Table TABREF47 shows a size comparison between SESAME and popular datasets for Portuguese NER.", "SESAME consists of 87,769,158 tokens in total. The count and proportion of each entity tag (not a named entity, organization, person, location) is given in TABREF45.", "SESAME consists of 3,650,909 sentences, with lengths (in terms of number of tokens) following the distribution shown in Figure FIGREF42. A breakdown of relevant statistics, such as the mean and standard deviation of sentences' lengths, is given in Table TABREF43.", "Additionally, the baseline was developed on a balanced re-sample of SESAME with a total of 1,216,976 sentences. The model also receives additional categorical features for each word, signalizing whether it: (1) starts with a capital letter, (2) has capitalized letters only, (3) has lowercase letters only, (4) contains digits, (5) has mostly digits ($>$ 50%) and (6) has digits only.", "The HAREM corpus follows a different format than the one of SESAME: it uses a markup structure, without a proper tokenization of sentences and words. To circumvent this, we convert it to BIO format by applying the same tokenization process used for generating our dataset."]}
{"question_id": "80d425258d027e3ca3750375d170debb9d92fbc6", "predicted_answer": "Yes, their method can be transferred to other Q&A platforms in other languages. The studies mentioned in the text, such as Shah et al., Wang et al., Paul et al., and Patil et al., have developed methods to predict answer quality, detect experts, and evaluate knowledge sharing on platforms like Yahoo! Answers and Quora. These methods are based on data-driven approaches, machine learning algorithms, and feature engineering, which are language-agnostic and can be applied to other Q&A platforms with similar characteristics.\n\nIn particular, the method proposed by Patil et al. to detect experts on Quora based on activity, quality of answers, linguistic characteristics, and temporal behaviors can be adapted to other platforms like Zhihu, which also allows users to follow topics, up-vote or down-vote answers, and write their own answers. The features used in their method, such as user activity, answer quality, and linguistic characteristics, are not specific to Quora and can be extracted from other Q&A platforms.\n\nHowever, it's worth noting that the effectiveness of these methods may vary depending on the specific characteristics of the platform, the language, and the user behavior. Therefore, some modifications and adaptations may be necessary to ensure the method works well on other platforms.", "predicted_evidence": ["Knowledge sharing platforms such as Quora and Zhihu emerge as very convenient tools for acquiring knowledge. These question and answer (Q&A) platforms are newly emerged communities about knowledge acquisition, experience sharing and social networks services (SNS).", "Recently, studies on social Q&A platforms and knowledge sharing are rising and have achieved many promising results. Shah et al. BIBREF0 propose a data-driven approach with logistic regression and carefully designed hand-crafted features to predict the answer quality on Yahoo! Answers. Wang et al. BIBREF1 illustrate that heterogeneity in the user and question graphs are important contributors to the quality of Quora's knowledge base. Paul et al. BIBREF2 explore reputation mechanism in quora through detailed data analysis, their experiments indicate that social voting helps users identify and promote good content but is prone to preferential attachment. Patil et al. BIBREF3 propose a method to detect experts on Quora by their activity, quality of answers, linguistic characteristics and temporal behaviors, and achieves 97% accuracy and 0.987 AUC. Rughinis et al.", "Recently, studies on social Q&A platforms and knowledge sharing are rising and have achieved many promising results. Shah et al. BIBREF0 propose a data-driven approach with logistic regression and carefully designed hand-crafted features to predict the answer quality on Yahoo! Answers. Wang et al. BIBREF1 illustrate that heterogeneity in the user and question graphs are important contributors to the quality of Quora's knowledge base. Paul et al. BIBREF2 explore reputation mechanism in quora through detailed data analysis, their experiments indicate that social voting helps users identify and promote good content but is prone to preferential attachment. Patil et al. BIBREF3 propose a method to detect experts on Quora by their activity, quality of answers, linguistic characteristics and temporal behaviors, and achieves 97% accuracy and 0.987 AUC. Rughinis et al. BIBREF4 indicate that there are different regimes of engagement at the intersection of the technological infrastructure and users' participation in Quora.", "However, due to the lack of efficient filter mechanism and evaluation schemes, many users suffer from lots of low-quality contents, which affects the service negatively. Recently, studies on social Q&A platforms and knowledge sharing are rising and have achieved many promising results. Shah et al. BIBREF0 propose a data-driven approach with logistic regression and carefully designed hand-crafted features to predict the answer quality on Yahoo! Answers. Wang et al. BIBREF1 illustrate that heterogeneity in the user and question graphs are important contributors to the quality of Quora's knowledge base. Paul et al. BIBREF2 explore reputation mechanism in quora through detailed data analysis, their experiments indicate that social voting helps users identify and promote good content but is prone to preferential attachment. Patil et al.", "Unlike many other Q&A platforms, Zhihu platform resembles a social network community. Users can follow other people, post ideas, up-vote or down-vote answers, and write their own answers. Zhihu allows users to keep track of specific fields by following related topics, such as \u201cEducation\u201d, \u201cMovie\u201d, \u201cTechnology\u201d and \u201cMusic\u201d. Once a Zhihu user starts to follow a specific topic or a person, the related updates are automatically pushed to the user's feed timeline."]}
{"question_id": "2ae66798333b905172e2c0954e9808662ab7f221", "predicted_answer": "Based on the given context, the measures of quality used for a Q&A platform include:\n\n1. Rating scores (within a range of 0 to 1)\n2. Review count (min, Q1, median, Q3, max, mean, and mode)\n3. Logistic regression with hand-crafted features to predict answer quality\n4. Heterogeneity in user and question graphs\n5. Reputation mechanism through social voting\n6. Detection of experts based on activity, quality of answers, linguistic characteristics, and temporal behaviors\n7. Accuracy and AUC (Area Under the ROC Curve) metrics to evaluate the performance of quality evaluation methods.\n\nThese measures are used to evaluate the quality of questions, answers, and users on social Q&A platforms, and to identify and promote high-quality content.", "predicted_evidence": ["Although these platforms have exploded in popularity, they face some potential problems. The key problem is that as the number of users grows, a large volume of low-quality questions and answers emerge and overwhelm users, which make users hard to find relevant and helpful information.", "However, due to the lack of efficient filter mechanism and evaluation schemes, many users suffer from lots of low-quality contents, which affects the service negatively. Recently, studies on social Q&A platforms and knowledge sharing are rising and have achieved many promising results. Shah et al. BIBREF0 propose a data-driven approach with logistic regression and carefully designed hand-crafted features to predict the answer quality on Yahoo! Answers. Wang et al. BIBREF1 illustrate that heterogeneity in the user and question graphs are important contributors to the quality of Quora's knowledge base. Paul et al. BIBREF2 explore reputation mechanism in quora through detailed data analysis, their experiments indicate that social voting helps users identify and promote good content but is prone to preferential attachment. Patil et al.", "The rating scores are within a range of INLINEFORM0 . We calculate min, Q1, median, Q3, max, mean, and mode about review count (see Table TABREF8 ). Because the number of received review may greatly influence the reliability of the review score. From Table TABREF8 we can see that many responses on Zhihu Live receive no review at all, which are useless for quality evaluation.", "Recently, studies on social Q&A platforms and knowledge sharing are rising and have achieved many promising results. Shah et al. BIBREF0 propose a data-driven approach with logistic regression and carefully designed hand-crafted features to predict the answer quality on Yahoo! Answers. Wang et al. BIBREF1 illustrate that heterogeneity in the user and question graphs are important contributors to the quality of Quora's knowledge base. Paul et al. BIBREF2 explore reputation mechanism in quora through detailed data analysis, their experiments indicate that social voting helps users identify and promote good content but is prone to preferential attachment. Patil et al. BIBREF3 propose a method to detect experts on Quora by their activity, quality of answers, linguistic characteristics and temporal behaviors, and achieves 97% accuracy and 0.987 AUC. Rughinis et al. BIBREF4 indicate that there are different regimes of engagement at the intersection of the technological infrastructure and users' participation in Quora.", "However, due to the lack of efficient filter mechanism and evaluation schemes, many users suffer from lots of low-quality contents, which affects the service negatively. Recently, studies on social Q&A platforms and knowledge sharing are rising and have achieved many promising results. Shah et al. BIBREF0 propose a data-driven approach with logistic regression and carefully designed hand-crafted features to predict the answer quality on Yahoo! Answers. Wang et al. BIBREF1 illustrate that heterogeneity in the user and question graphs are important contributors to the quality of Quora's knowledge base. Paul et al. BIBREF2 explore reputation mechanism in quora through detailed data analysis, their experiments indicate that social voting helps users identify and promote good content but is prone to preferential attachment. Patil et al. BIBREF3 propose a method to detect experts on Quora by their activity, quality of answers, linguistic characteristics and temporal behaviors, and achieves 97% accuracy and 0.987 AUC."]}
{"question_id": "9d80ad8cf4d5941a32d33273dc5678195ad1e0d2", "predicted_answer": "No, the given context does not evaluate whether local or global context proves more important. It describes the components of the CEQE model, including the embedding layers, convolution encoding of local context, and encoding of global context by a recurrent neural network. However, it does not provide a comparison or evaluation of the importance of local versus global context.", "predicted_evidence": ["For German-English, English Czech, and English-German (SMT), adding the baseline features can significantly improve the F1-BAD scores.", "Inspired by BIBREF6 , the first embedding layer is a vector representing each target word INLINEFORM0 obtained by concatenating the embedding of that word with those of the aligned words INLINEFORM1 in the source. If a target word is aligned to multiple source words, we average the embedding of all the source words, and concatenate the target word embedding with its average source embedding. The immediate left and right contexts for source and target words are also concatenated, enriching the local context information of the embedding of target word INLINEFORM2 . Thus, the embedding of target word INLINEFORM3 , denoted as INLINEFORM4 , is a INLINEFORM5 dimensional vector, where INLINEFORM6 is the dimension of the word embeddings. The source and target words use the same embedding parameters, and thus identical words in both languages, such as digits and proper nouns, have the same embedding vectors. This allows the model to easily identify identical words in both languages.", "Inspired by BIBREF6 , the first embedding layer is a vector representing each target word INLINEFORM0 obtained by concatenating the embedding of that word with those of the aligned words INLINEFORM1 in the source. If a target word is aligned to multiple source words, we average the embedding of all the source words, and concatenate the target word embedding with its average source embedding. The immediate left and right contexts for source and target words are also concatenated, enriching the local context information of the embedding of target word INLINEFORM2 . Thus, the embedding of target word INLINEFORM3 , denoted as INLINEFORM4 , is a INLINEFORM5 dimensional vector, where INLINEFORM6 is the dimension of the word embeddings. The source and target words use the same embedding parameters, and thus identical words in both languages, such as digits and proper nouns, have the same embedding vectors. This allows the model to easily identify identical words in both languages. Similarly, the POS tags in both languages share the same embedding parameters.", "If a target word is aligned to multiple source words, we average the embedding of all the source words, and concatenate the target word embedding with its average source embedding. The immediate left and right contexts for source and target words are also concatenated, enriching the local context information of the embedding of target word INLINEFORM2 . Thus, the embedding of target word INLINEFORM3 , denoted as INLINEFORM4 , is a INLINEFORM5 dimensional vector, where INLINEFORM6 is the dimension of the word embeddings. The source and target words use the same embedding parameters, and thus identical words in both languages, such as digits and proper nouns, have the same embedding vectors. This allows the model to easily identify identical words in both languages. Similarly, the POS tags in both languages share the same embedding parameters. Table TABREF4 shows the statistics of the set of POS tags over all language pairs.", "CEQE consists of three major components: (1) embedding layers for words and part-of-speech (POS) tags in both languages, (2) convolution encoding of the local context for each target word, and (3) encoding the global context by the recurrent neural network."]}
{"question_id": "bd817a520a62ddd77e65e74e5a7e9006cdfb19b3", "predicted_answer": "According to the text, they use one bidirectional gated recurrent unit (BiGRU) layer, which means they use one layer of recurrent neural networks for encoding the global context.", "predicted_evidence": ["In this paper, we propose a neural architecture, Context Encoding Quality Estimation (CEQE), for better encoding of context in word-level QE. Specifically, we leverage the power of both (1) convolution modules that automatically learn local patterns of surrounding words, and (2) hand-crafted features that allow the model to make more robust predictions in the face of a paucity of labeled data. Moreover, we further utilize stacked recurrent neural networks to capture the long-term dependencies and global context information from the whole sentence.", "CEQE consists of three major components: (1) embedding layers for words and part-of-speech (POS) tags in both languages, (2) convolution encoding of the local context for each target word, and (3) encoding the global context by the recurrent neural network.", "The output of the one-dimensional convolution layer, INLINEFORM0 , is then concatenated with the embedding of POS tags of the target words, as well as its aligned source words, to provide a more direct signal to the following recurrent layers.", "In this paper, we propose a deep neural architecture for word-level QE. Our framework leverages a one-dimensional convolution on the concatenated word embeddings of target and its aligned source words to extract salient local feature maps. In additions, bidirectional RNNs are applied to capture temporal dependencies for better sequence prediction. We conduct thorough experiments on four language pairs in the WMT2018 shared task. The proposed framework achieves highly competitive results, outperforms all other participants on English-Czech and English-Latvian word-level, and is second place on English-German, and German-English language pairs.", "One bi-directional gated recurrent unit (BiGRU; BIBREF10 ) layer with hidden size 200, where the forward and backward hidden states are concatenated and further normalized by layer normalization BIBREF11 ."]}
{"question_id": "c635295c2b77aaab28faecca3b5767b0c4ab3728", "predicted_answer": "According to the text, their model ranked second on English-German (NMT) and German-English word-level QE tasks.", "predicted_evidence": ["We tested our model on the official benchmark of the WMT18 word-level QE task. On this task, it achieved highly competitive results, with the best performance over other competitors on English-Czech, English-Latvian (NMT) and English-Latvian (SMT) word-level QE task, and ranking second place on English-German (NMT) and German-English word-level QE task.", "We evaluate our CEQE model on the WMT2018 Quality Estimation Shared Task for word-level English-German, German-English, English-Czech, and English-Latvian QE. Words in all languages are lowercased. The evaluation metric is the multiplication of F1-scores for the \u201cOK\u201d and \u201cBAD\u201d classes against the true labels. F1-score is the harmonic mean of precision and recall. In Table TABREF15 , our model achieves the best performance on three out of six test sets in the WMT 2018 word-level QE shared task.", "Table TABREF22 shows two examples of quality prediction on the validation data of WMT2018 QE task for English-Czech. In the first example, the model without POS tags and baseline features is biased towards predicting \u201cOK\u201d tags, while the model with full features can detect the reordering error. In the second example, the target word \u201cpanelu\u201d is a variant of the reference word \u201cpanel\u201d. The target word \u201cznaky\u201d is the plural noun of the reference \u201cznak\u201d. Thus, their POS tags have some subtle differences. Note the target word \u201czmnit\u201d and its aligned source word \u201cchange\u201d are both verbs. We can observe that POS tags can help the model capture such syntactic variants.", "In Table TABREF21 , we show the ablation study of the features used in our model on English-German, German-English, and English-Czech. For each language pair, we show the performance of CEQE without adding the corresponding components specified in the second column respectively. The last row shows the performance of the complete CEQE with all the components. As the baseline features released in the WMT2018 QE Shared Task for English-Latvian are incomplete, we train our CEQE model without using such features. We can glean several observations from this data:", "Because the number of \u201cOK\u201d tags is much larger than the number of \u201cBAD\u201d tags, the model is easily biased towards predicting the \u201cOK\u201d tag for each target word. The F1-OK scores are higher than the F1-BAD scores across all the language pairs."]}
{"question_id": "7f8fc3c7d59aba80a3e7c839db6892a1fc329210", "predicted_answer": "No, they do not evaluate only on English datasets. The text mentions a German supermarket \"kaufland\" and a dataset for microblogs (Ritter dataset), which suggests that the algorithm is being tested on datasets that may include non-English languages.", "predicted_evidence": ["As an example, the sentence \u201cparis hilton was once the toast of the town\u201d can show the potential of the proposed approach. The token \u201cparis\u201d with a LOC bias (0.6) and \u201chilton\u201d (global brand of hotels and resorts) with indicators leading to LOC (0.7) or ORG (0.1, less likely though). Furthermore, \u201ctown\u201d being correctly biased to LOC (0.7). The algorithm also suggests that the compound \u201cparis hilton\u201d is more likely to be a PER instead (0.7) and updates (correctly) the previous predictions. As a downside in this example, the algorithm misclassified \u201ctoast\u201d as LOC. However, in this same example, Stanford NER annotates (mistakenly) only \u201cparis\u201d as LOC. It is worth noting also the ability of the algorithm to take advantage of search engine capabilities.", "As an example, the sentence \u201cparis hilton was once the toast of the town\u201d can show the potential of the proposed approach. The token \u201cparis\u201d with a LOC bias (0.6) and \u201chilton\u201d (global brand of hotels and resorts) with indicators leading to LOC (0.7) or ORG (0.1, less likely though). Furthermore, \u201ctown\u201d being correctly biased to LOC (0.7). The algorithm also suggests that the compound \u201cparis hilton\u201d is more likely to be a PER instead (0.7) and updates (correctly) the previous predictions. As a downside in this example, the algorithm misclassified \u201ctoast\u201d as LOC. However, in this same example, Stanford NER annotates (mistakenly) only \u201cparis\u201d as LOC. It is worth noting also the ability of the algorithm to take advantage of search engine capabilities. When searching for \u201cmiCRs0ft\u201d, the returned values strongly indicate a bias for ORG, as expected ( INLINEFORM0 = 0.2, INLINEFORM1 = 0.8, INLINEFORM2 = 0.0, INLINEFORM3 = 6, INLINEFORM4 = -56, INLINEFORM5 = 0.0, INLINEFORM6 = 0.5, INLINEFORM7 = 0.0, INLINEFORM8 = 5).", "The algorithm also suggests that the compound \u201cparis hilton\u201d is more likely to be a PER instead (0.7) and updates (correctly) the previous predictions. As a downside in this example, the algorithm misclassified \u201ctoast\u201d as LOC. However, in this same example, Stanford NER annotates (mistakenly) only \u201cparis\u201d as LOC. It is worth noting also the ability of the algorithm to take advantage of search engine capabilities. When searching for \u201cmiCRs0ft\u201d, the returned values strongly indicate a bias for ORG, as expected ( INLINEFORM0 = 0.2, INLINEFORM1 = 0.8, INLINEFORM2 = 0.0, INLINEFORM3 = 6, INLINEFORM4 = -56, INLINEFORM5 = 0.0, INLINEFORM6 = 0.5, INLINEFORM7 = 0.0, INLINEFORM8 = 5). More local organisations are also recognized correctly, such as \u201ckaufland\u201d (German supermarket), which returns the following metadata: INLINEFORM9 = 0.2, INLINEFORM10 = 0.4, INLINEFORM11 = 0.0, INLINEFORM12 = 2, INLINEFORM13 = -50, INLINEFORM14 = 0.1, INLINEFORM15 = 0.4, INLINEFORM16 = 0.0, INLINEFORM17 = 3.", "In order to check the overall performance of the proposed technique, we ran our algorithm without any further rule or apriori knowledge using a gold standard for NER in microblogs (Ritter dataset BIBREF2 ), achieving INLINEFORM0 F1. tab:performance details the performance measures per class. tab:relatedwork presents current state-of-the-art results for the same dataset. The best model achieves INLINEFORM1 F1-measure, but uses encoded rules. Models which are not rule-based, achieve INLINEFORM2 and INLINEFORM3 . We argue that in combination with existing techniques (such as linguistic patterns), we can potentially achieve even better results.", "Named Entity Recognition (NER) is an important step in most of the natural language processing (NLP) pipelines. It is designed to robustly handle proper names, which is essential for many applications. Although a seemingly simple task, it faces a number of challenges in noisy datasets and it is still considered an emerging research area BIBREF0 , BIBREF1 . Despite recent efforts, we still face limitations at identifying entities and (consequently) correctly classifying them. Current state-of-the-art NER systems typically have about 85-90% accuracy on news text - such as articles (CoNLL03 shared task data set) - but they still perform poorly (about 30-50% accuracy) on short texts, which do not have implicit linguistic formalism (e.g."]}
{"question_id": "2d92ae6b36567e7edb6afdd72f97b06ac144fbdf", "predicted_answer": "The Ritter dataset is a dataset used for part-of-speech (POS) tagging, specifically for classifying tokens as NOUNs. It is mentioned in the text as having an error propagation of 0.09 (107 tokens that should be classified as NOUN) using NLTK 3.0, with an accuracy of 91%.", "predicted_evidence": ["Training (D.1): we used SIFT (Scale Invariant Feature Transform) features BIBREF12 for extracting image descriptors and BoF (Bag of Features) BIBREF13 , BIBREF14 for clustering the histograms of extracted features. The clustering is possible by constructing a large vocabulary of many visual words and representing each image as a histogram of the frequency words that are in the image. We use k-means BIBREF15 to cluster the set of descriptors to INLINEFORM0 clusters. The resulting clusters are compact and separated by similar characteristics. An empirical analysis shows that some image groups are often related to certain named entities (NE) classes when using search engines, as described in tab:tbempirical. For training purposes, we used the Scene 13 dataset BIBREF16 to train our classifiers for location (LOC), \u201cfaces\u201d from Caltech 101 Object Categories BIBREF17 to train our person (PER) and logos from METU dataset BIBREF18 for organisation ORG object detection.", "where INLINEFORM0 and INLINEFORM1 represent the INLINEFORM2 and INLINEFORM3 position of INLINEFORM4 and INLINEFORM5 , respectively. INLINEFORM6 represents the n-gram of POS tag. INLINEFORM7 and INLINEFORM8 ( INLINEFORM9 ) represent the total objects found by a classifier INLINEFORM10 for a given class INLINEFORM11 ( INLINEFORM12 ) (where N is the total of retrieved images INLINEFORM15 ). INLINEFORM16 and INLINEFORM17 represent the distance between the two higher predictions ( INLINEFORM18 ), i.e. INLINEFORM19 . Finally, INLINEFORM20 represents the sum of all predictions made by all INLINEFORM21 classifiers INLINEFORM22 ( INLINEFORM23 ). - Training (E): the outcomes of D.1 and D.2 ( INLINEFORM26 ) are used as input features to our final classifier.", "This can be circumvented by indexing and searching on other large sources of information, such as Common Crawl and Flickr. However, maintaining a large source of images would be an issue, e.g. the Flickr dataset may not be comprehensive enough (i.e. tokens may not return results). This will be a subject of future work. Besides, an important step in the pre-processing is the classification of part-of-speech tags. In the Ritter dataset our current error propagation is 0.09 (107 tokens which should be classified as NOUN) using NLTK 3.0. Despite good performance (91% accuracy), we plan to benchmark this component. In terms of processing time, the bottleneck of the current implementation is the time required to extract features from images, as expected. Currently we achieve a performance of 3~5 seconds per sentence and plan to also optimize this component. The major advantages of this approach are: 1) the fact that there are no hand-crafted rules encoded; 2) the ability to handle misspelled words (because the search engine alleviates that and returns relevant or related information) and incomplete sentences; 3) the generic design of its components, allowing multilingual processing with little effort (the only dependency is the POS tagger) and straightforward extension to support more NER classes (requiring a corpus of images and text associated to each desired NER class, which can be obtained from a Knowledge Base, such as DBpedia, and an image dataset, such as METU dataset).", "A disadvantage when using web search engines is that they are not open and free. This can be circumvented by indexing and searching on other large sources of information, such as Common Crawl and Flickr. However, maintaining a large source of images would be an issue, e.g. the Flickr dataset may not be comprehensive enough (i.e. tokens may not return results). This will be a subject of future work. Besides, an important step in the pre-processing is the classification of part-of-speech tags. In the Ritter dataset our current error propagation is 0.09 (107 tokens which should be classified as NOUN) using NLTK 3.0. Despite good performance (91% accuracy), we plan to benchmark this component. In terms of processing time, the bottleneck of the current implementation is the time required to extract features from images, as expected.", "A disadvantage when using web search engines is that they are not open and free. This can be circumvented by indexing and searching on other large sources of information, such as Common Crawl and Flickr. However, maintaining a large source of images would be an issue, e.g. the Flickr dataset may not be comprehensive enough (i.e. tokens may not return results). This will be a subject of future work. Besides, an important step in the pre-processing is the classification of part-of-speech tags. In the Ritter dataset our current error propagation is 0.09 (107 tokens which should be classified as NOUN) using NLTK 3.0. Despite good performance (91% accuracy), we plan to benchmark this component. In terms of processing time, the bottleneck of the current implementation is the time required to extract features from images, as expected. Currently we achieve a performance of 3~5 seconds per sentence and plan to also optimize this component."]}
{"question_id": "a5df7361ae37b9512fb57cb93efbece9ded8cab1", "predicted_answer": "No, the model does not perform better than the state of the art. According to the text, the best model achieves an F1-measure of INLINEFORM1, but uses encoded rules. The proposed model achieves an F1-measure of INLINEFORM0, which is lower than INLINEFORM1. However, the proposed model does not use encoded rules, which is an advantage.", "predicted_evidence": ["In order to check the overall performance of the proposed technique, we ran our algorithm without any further rule or apriori knowledge using a gold standard for NER in microblogs (Ritter dataset BIBREF2 ), achieving INLINEFORM0 F1. tab:performance details the performance measures per class. tab:relatedwork presents current state-of-the-art results for the same dataset. The best model achieves INLINEFORM1 F1-measure, but uses encoded rules. Models which are not rule-based, achieve INLINEFORM2 and INLINEFORM3 . We argue that in combination with existing techniques (such as linguistic patterns), we can potentially achieve even better results.", "As an example, the sentence \u201cparis hilton was once the toast of the town\u201d can show the potential of the proposed approach. The token \u201cparis\u201d with a LOC bias (0.6) and \u201chilton\u201d (global brand of hotels and resorts) with indicators leading to LOC (0.7) or ORG (0.1, less likely though). Furthermore, \u201ctown\u201d being correctly biased to LOC (0.7). The algorithm also suggests that the compound \u201cparis hilton\u201d is more likely to be a PER instead (0.7) and updates (correctly) the previous predictions. As a downside in this example, the algorithm misclassified \u201ctoast\u201d as LOC. However, in this same example, Stanford NER annotates (mistakenly) only \u201cparis\u201d as LOC. It is worth noting also the ability of the algorithm to take advantage of search engine capabilities.", "Despite good performance (91% accuracy), we plan to benchmark this component. In terms of processing time, the bottleneck of the current implementation is the time required to extract features from images, as expected. Currently we achieve a performance of 3~5 seconds per sentence and plan to also optimize this component. The major advantages of this approach are: 1) the fact that there are no hand-crafted rules encoded; 2) the ability to handle misspelled words (because the search engine alleviates that and returns relevant or related information) and incomplete sentences; 3) the generic design of its components, allowing multilingual processing with little effort (the only dependency is the POS tagger) and straightforward extension to support more NER classes (requiring a corpus of images and text associated to each desired NER class, which can be obtained from a Knowledge Base, such as DBpedia, and an image dataset, such as METU dataset). While initial results in a gold standard dataset showed the potential of the approach, we also plan to integrate these outcomes into a Sequence Labeling (SL) system, including neural architectures such as LSTM, which are more suitable for such tasks as NER or POS.", "As an example, the sentence \u201cparis hilton was once the toast of the town\u201d can show the potential of the proposed approach. The token \u201cparis\u201d with a LOC bias (0.6) and \u201chilton\u201d (global brand of hotels and resorts) with indicators leading to LOC (0.7) or ORG (0.1, less likely though). Furthermore, \u201ctown\u201d being correctly biased to LOC (0.7). The algorithm also suggests that the compound \u201cparis hilton\u201d is more likely to be a PER instead (0.7) and updates (correctly) the previous predictions. As a downside in this example, the algorithm misclassified \u201ctoast\u201d as LOC. However, in this same example, Stanford NER annotates (mistakenly) only \u201cparis\u201d as LOC. It is worth noting also the ability of the algorithm to take advantage of search engine capabilities. When searching for \u201cmiCRs0ft\u201d, the returned values strongly indicate a bias for ORG, as expected ( INLINEFORM0 = 0.2, INLINEFORM1 = 0.8, INLINEFORM2 = 0.0, INLINEFORM3 = 6, INLINEFORM4 = -56, INLINEFORM5 = 0.0, INLINEFORM6 = 0.5, INLINEFORM7 = 0.0, INLINEFORM8 = 5).", "The algorithm also suggests that the compound \u201cparis hilton\u201d is more likely to be a PER instead (0.7) and updates (correctly) the previous predictions. As a downside in this example, the algorithm misclassified \u201ctoast\u201d as LOC. However, in this same example, Stanford NER annotates (mistakenly) only \u201cparis\u201d as LOC. It is worth noting also the ability of the algorithm to take advantage of search engine capabilities. When searching for \u201cmiCRs0ft\u201d, the returned values strongly indicate a bias for ORG, as expected ( INLINEFORM0 = 0.2, INLINEFORM1 = 0.8, INLINEFORM2 = 0.0, INLINEFORM3 = 6, INLINEFORM4 = -56, INLINEFORM5 = 0.0, INLINEFORM6 = 0.5, INLINEFORM7 = 0.0, INLINEFORM8 = 5). More local organisations are also recognized correctly, such as \u201ckaufland\u201d (German supermarket), which returns the following metadata: INLINEFORM9 = 0.2, INLINEFORM10 = 0.4, INLINEFORM11 = 0.0, INLINEFORM12 = 2, INLINEFORM13 = -50, INLINEFORM14 = 0.1, INLINEFORM15 = 0.4, INLINEFORM16 = 0.0, INLINEFORM17 = 3."]}
{"question_id": "915e4d0b3cb03789a20380ead961d473cb95bfc3", "predicted_answer": "The features extracted from text in this context are:\n\n* Part-of-speech (POS) tags\n* Shallow parsing information\n* Tokens tagged as INLINEFORM0, INLINEFORM1, and INLINEFORM2 (local context)\n\nThese features are extracted in the pre-processing step (A) to filter out tokens and prepare the data for further processing in the Computer Vision and Text Analytics components.", "predicted_evidence": ["In the first step (A), we simply apply POS Tagging and Shallow Parsing to filter out tokens except for those tagged as INLINEFORM0 or INLINEFORM1 and their INLINEFORM2 (local context). Afterwards, we use the search engine (B) to query and cache (C) the top INLINEFORM3 texts and images associated to each term INLINEFORM4 , where INLINEFORM5 is the set resulting of the pre-processing step (A) for each (partial or complete) sentence INLINEFORM6 . This resulting data (composed of excerpts of texts and images from web pages) is used to predict a possible class for a given term. These outcomes are then used in the first two levels (D.1 and D.2) of our approach: the Computer Vision and Text Analytics components, respectively, which we introduce as follows:", "This can be circumvented by indexing and searching on other large sources of information, such as Common Crawl and Flickr. However, maintaining a large source of images would be an issue, e.g. the Flickr dataset may not be comprehensive enough (i.e. tokens may not return results). This will be a subject of future work. Besides, an important step in the pre-processing is the classification of part-of-speech tags. In the Ritter dataset our current error propagation is 0.09 (107 tokens which should be classified as NOUN) using NLTK 3.0. Despite good performance (91% accuracy), we plan to benchmark this component. In terms of processing time, the bottleneck of the current implementation is the time required to extract features from images, as expected. Currently we achieve a performance of 3~5 seconds per sentence and plan to also optimize this component. The major advantages of this approach are: 1) the fact that there are no hand-crafted rules encoded; 2) the ability to handle misspelled words (because the search engine alleviates that and returns relevant or related information) and incomplete sentences; 3) the generic design of its components, allowing multilingual processing with little effort (the only dependency is the POS tagger) and straightforward extension to support more NER classes (requiring a corpus of images and text associated to each desired NER class, which can be obtained from a Knowledge Base, such as DBpedia, and an image dataset, such as METU dataset).", "Training (D.1): we used SIFT (Scale Invariant Feature Transform) features BIBREF12 for extracting image descriptors and BoF (Bag of Features) BIBREF13 , BIBREF14 for clustering the histograms of extracted features. The clustering is possible by constructing a large vocabulary of many visual words and representing each image as a histogram of the frequency words that are in the image. We use k-means BIBREF15 to cluster the set of descriptors to INLINEFORM0 clusters. The resulting clusters are compact and separated by similar characteristics. An empirical analysis shows that some image groups are often related to certain named entities (NE) classes when using search engines, as described in tab:tbempirical.", "The main insight underlying this work is that we can produce a NER model which performs similarly to state-of-the-art approaches but without relying on any specific resource or encoded rule. To this aim, we propose a multi-level architecture which intends to produce biased indicators to a certain class (LOC, PER or ORG). These outcomes are then used as input features for our final classifier. We perform clustering on images and texts associated to a given term INLINEFORM0 existing in complete or partial sentences INLINEFORM1 (e.g., \u201cnew york\u201d or \u201ceinstein\u201d), leveraging the global context obtained from the Web providing valuable insights apart from standard local features and hand-coded information. fig:architecture gives an overview of the proposed architecture.", "Existing approaches basically adopt look-up strategies and use standard local features, such as part-of-speech tags, previous and next words, substrings, shapes and regex expressions, for instance. The main drawback is the performance of those models with noisy data, such as Tweets. A major reason is that they rely heavily on hand-crafted features and domain-specific knowledge. In terms of architecture, NER algorithms may also be designed based on generative (e.g., Naive Bayes) or discriminative (e.g., MaxEnt) models. Furthermore, sequence models (HMMs, CMM, MEMM and CRF) are a natural choice to design such systems. A more recent study proposed by Lample et al., 2016 BIBREF9 used neural architectures to solve this problem. Similarly in terms of architecture, Al-Rfou et al., 2015 BIBREF10 had also proposed a model (without dependency) that learns distributed word representations (word embeddings) which encode semantic and syntactic features of words in each language."]}
{"question_id": "c01a8b42fd27b0a3bec717ededd98b6d085a0f5c", "predicted_answer": "The features extracted from images are:\n\n* SIFT (Scale Invariant Feature Transform) features, which are used to extract image descriptors.\n* Histograms of extracted features, which are used for clustering using the Bag of Features (BoF) approach.\n* Visual words, which are used to represent each image as a histogram of the frequency of words that are in the image.\n\nThese features are used to detect specific objects in images, such as logos, faces, and locations, and to train classifiers for named entity recognition (NER) tasks.", "predicted_evidence": ["Computer Vision (CV): Detecting Objects: Function Description (D.1): given a set of images INLINEFORM0 , the basic idea behind this component is to detect a specific object (denoted by a class INLINEFORM1 ) in each image. Thus, we query the web for a given term INLINEFORM2 and then extract the features from each image and try to detect a specific object (e.g., logos for ORG) for the top INLINEFORM3 images retrieved as source candidates. The mapping between objects and NER classes is detailed in tab:tbempirical.", "Training (D.1): we used SIFT (Scale Invariant Feature Transform) features BIBREF12 for extracting image descriptors and BoF (Bag of Features) BIBREF13 , BIBREF14 for clustering the histograms of extracted features. The clustering is possible by constructing a large vocabulary of many visual words and representing each image as a histogram of the frequency words that are in the image. We use k-means BIBREF15 to cluster the set of descriptors to INLINEFORM0 clusters. The resulting clusters are compact and separated by similar characteristics. An empirical analysis shows that some image groups are often related to certain named entities (NE) classes when using search engines, as described in tab:tbempirical.", "Training (D.1): we used SIFT (Scale Invariant Feature Transform) features BIBREF12 for extracting image descriptors and BoF (Bag of Features) BIBREF13 , BIBREF14 for clustering the histograms of extracted features. The clustering is possible by constructing a large vocabulary of many visual words and representing each image as a histogram of the frequency words that are in the image. We use k-means BIBREF15 to cluster the set of descriptors to INLINEFORM0 clusters. The resulting clusters are compact and separated by similar characteristics. An empirical analysis shows that some image groups are often related to certain named entities (NE) classes when using search engines, as described in tab:tbempirical. For training purposes, we used the Scene 13 dataset BIBREF16 to train our classifiers for location (LOC), \u201cfaces\u201d from Caltech 101 Object Categories BIBREF17 to train our person (PER) and logos from METU dataset BIBREF18 for organisation ORG object detection.", "To the best of our knowledge, this is the first report of a NER architecture which aims to provide a priori information based on clusters of images and text features.", "The clustering is possible by constructing a large vocabulary of many visual words and representing each image as a histogram of the frequency words that are in the image. We use k-means BIBREF15 to cluster the set of descriptors to INLINEFORM0 clusters. The resulting clusters are compact and separated by similar characteristics. An empirical analysis shows that some image groups are often related to certain named entities (NE) classes when using search engines, as described in tab:tbempirical. For training purposes, we used the Scene 13 dataset BIBREF16 to train our classifiers for location (LOC), \u201cfaces\u201d from Caltech 101 Object Categories BIBREF17 to train our person (PER) and logos from METU dataset BIBREF18 for organisation ORG object detection. These datasets produces the training data for our set of supervised classifiers (1 for ORG, 1 for PER and 10 for LOC)."]}
{"question_id": "8e113fd9661bc8af97e30c75a20712f01fc4520a", "predicted_answer": "The baseline models mentioned in the text are:\n\n1. Base BERT model with 12 encoder layers, feed-forward networks with 768 hidden units, and 12 attention heads.\n2. Large BERT model with 24 encoder layers, 1024 feed-forward networks, and 12 attention heads.\n3. FastText system with pre-trained embeddings, global max-pooling, and a 64-unit fully connected layer.\n4. XLNet model with 12 layers, 768 hidden units, and 12 attention heads.\n5. BERT-base models (both cased and uncased) with 12 layers, 768 hidden units, and 12 attention heads.\n6. RoBERTa model with similar architecture to BERT-base model.", "predicted_evidence": ["In the pre-training phase the model is trained with a large amount of unlabeled data from Wikipedia, BookCorpus BIBREF85 and WordPiece BIBREF86 embeddings. In this training part, the model was tested on two tasks; on the first task, the model randomly masks 15% of the input tokens aiming to capture conceptual representations of word sequences by predicting masked words inside the corpus, whereas in the second task the model is given two sentences and tries to predict whether the second sentence is the next sentence of the first. In the second phase, BERT is extended with a task-related classifier model that is trained on a supervised manner. During this supervised phase, the pre-trained BERT model receives minimal changes, with the classifier\u2019s parameters trained in order to minimize the loss function. Two models presented in BIBREF18, a \u201cBase Bert\u201d model with 12 encoder layers (i.e.", "In literature we encounter some major drawbacks of previous studies and we aim to resolve with our proposed method:", "In the pre-training phase the model is trained with a large amount of unlabeled data from Wikipedia, BookCorpus BIBREF85 and WordPiece BIBREF86 embeddings. In this training part, the model was tested on two tasks; on the first task, the model randomly masks 15% of the input tokens aiming to capture conceptual representations of word sequences by predicting masked words inside the corpus, whereas in the second task the model is given two sentences and tries to predict whether the second sentence is the next sentence of the first. In the second phase, BERT is extended with a task-related classifier model that is trained on a supervised manner. During this supervised phase, the pre-trained BERT model receives minimal changes, with the classifier\u2019s parameters trained in order to minimize the loss function. Two models presented in BIBREF18, a \u201cBase Bert\u201d model with 12 encoder layers (i.e. transformer blocks), feed-forward networks with 768 hidden units and 12 attention heads, and a \u201cLarge Bert\u201d model with 24 encoder layers 1024 feed-the pre-trained Bert model, an architecture almost identical with the aforementioned Transformer network.", "This is similar to relative positional encodings introduced in TrasformerXL BIBREF87, and extents the ability of XLnet to cope with tasks that encompass arbitrary input segments. Recently, a replication study, BIBREF18, suggested several modifications in the training procedure of BERT which, outperforms the original XLNet architecture on several NLP tasks. The optimized model, called Robustly Optimized BERT Approach (RoBERTa), used 10 times more data (160GB compared with the 16GB originally exploited), and is trained with far more epochs than the BERT model (500K vs 100K), using also 8-times larger batch sizes, and a byte-level BPE vocabulary instead of the character-level vocabulary that was previously utilized. Another significant modification, was the dynamic masking technique instead of the single static mask used in BERT. In addition, RoBERTa model removes the next sentence prediction objective used in BERT, following advises by several other studies that question the NSP loss term BIBREF89, BIBREF90, BIBREF91.", "FastText system was implemented by utilizing pre-trained embeddings BIBREF94 passed through a global max-pooling and a 64 unit fully connected layer. System was trained with Adam optimizer with learning rate ${0.1}$ for 3 epochs. XLnet model implemented using the base-cased model with 12 layers, 768 hidden units and 12 attention heads. Model trained with learning rate ${4 \\times 10^{-5}}$ using ${10^{-5}}$ weight decay for 3 epochs. We exploited both cased and uncased BERT-base models containing 12 layers, 768 hidden units and 12 attention heads. We trained models for 3 epochs with learning rate ${2 \\times 10^{-5}}$ using ${10^{-5}}$ weight decay. We trained RoBERTa model following the setting of BERT model. RoBERTa, XLnet and BERT models implemented using pytorch-transformers library ."]}
{"question_id": "35e0e6f89b010f34cfb69309b85db524a419c862", "predicted_answer": "The three different forms of figurative language (FL) expressions mentioned in this work are:\n\n1. Irony\n2. Sarcasm\n3. Metaphor", "predicted_evidence": ["We may identify three common FL expression forms namely, irony, sarcasm and metaphor. In this paper, figurative expressions, and especially ironic or sarcastic ones, are considered as a way of indirect denial. From this point of view, the interpretation and ultimately identification of the indirect meaning involved in a passage does not entail the cancellation of the indirectly rejected message and its replacement with the intentionally implied message (as advocated in BIBREF26, BIBREF27). On the contrary ironic/sarcastic expressions presupposes the processing of both the indirectly rejected and the implied message so that the difference between them can be identified. This view differs from the assumption that irony and sarcasm involve only one interpretation BIBREF28, BIBREF29. Holding that irony activates both grammatical / explicit as well as ironic / involved notions provides that irony will be more difficult to grasp than a non-ironic use of the same expression.", "In the second phase, BERT is extended with a task-related classifier model that is trained on a supervised manner. During this supervised phase, the pre-trained BERT model receives minimal changes, with the classifier\u2019s parameters trained in order to minimize the loss function. Two models presented in BIBREF18, a \u201cBase Bert\u201d model with 12 encoder layers (i.e. transformer blocks), feed-forward networks with 768 hidden units and 12 attention heads, and a \u201cLarge Bert\u201d model with 24 encoder layers 1024 feed-the pre-trained Bert model, an architecture almost identical with the aforementioned Transformer network. A [CLS] token is supplied in the input as the first token, the final hidden state of which is aggregated for classification tasks. Despite the achieved breakthroughs, the BERT model suffers from several drawbacks.", "To address the cons of BERT, a permutation language model was introduced, so-called XLnet, trained to predict masked tokens in a non-sequential random order, factorizing likelihood in an autoregressive manner without the independence assumption and without relying on any input corruption BIBREF88. In particular, a query stream is used that extends embedding representations to incorporate positional information about the masked words. The original representation set (content stream), including both token and positional embeddings, is then used as input to the query stream following a scheme called \u201cTwo-Stream SelfAttention\u201d. To overcome the problem of slow convergence the authors propose the prediction of the last token in the permutation phase, instead of predicting the entire sequence. Finally, XLnet uses also a special token for the classification and separation of the input sequence, [CLS] and [SEP] respectively, however it also learns an embedding that denotes whether the two words are from the same segment. This is similar to relative positional encodings introduced in TrasformerXL BIBREF87, and extents the ability of XLnet to cope with tasks that encompass arbitrary input segments.", "In the second phase, BERT is extended with a task-related classifier model that is trained on a supervised manner. During this supervised phase, the pre-trained BERT model receives minimal changes, with the classifier\u2019s parameters trained in order to minimize the loss function. Two models presented in BIBREF18, a \u201cBase Bert\u201d model with 12 encoder layers (i.e. transformer blocks), feed-forward networks with 768 hidden units and 12 attention heads, and a \u201cLarge Bert\u201d model with 24 encoder layers 1024 feed-the pre-trained Bert model, an architecture almost identical with the aforementioned Transformer network. A [CLS] token is supplied in the input as the first token, the final hidden state of which is aggregated for classification tasks. Despite the achieved breakthroughs, the BERT model suffers from several drawbacks. Firstly, BERT, as all language models using Transformers, assumes (and pre-supposes) independence between the masked words from the input sequence, and neglects all the positional and dependency information between words.", "Besides the philosophical considerations, theories and debates about the exact nature of FL, findings from the neuroscience research domain present clear evidence on the presence of differentiating FL processing patterns in the human brain BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF14, even for woman-man attraction situations! BIBREF24. A fact that makes FL processing even more challenging and difficult to tackle. Indeed, this is the case of pragmatic FL phenomena like irony and sarcasm that main intention of in most of the cases, are characterized by an oppositeness to the literal language context. It is crucial to distinguish between the literal meaning of an expression considered as a whole from its constituents\u2019 words and phrases. As literal meaning is assumed to be invariant in all context at least in its classical conceptualization BIBREF25, it is exactly this separation of an expression from its context that permits and opens the road to computational approaches in detecting and characterizing FL utterance."]}
{"question_id": "992e67f706c728bc0e534f974c1656da10e7a724", "predicted_answer": "The datasets used for training and testing are:\n\n1. \"Semantic Evaluation Workshop Task 3\" (SemEval-2018) dataset, which contains ironic tweets.\n2. Riloff's high-quality sarcastic unbalanced dataset.\n3. A large dataset containing political comments from Reddit.\n4. \"SemEval-2015 Task 11\" dataset, which contains tweets with various figurative language (FL) forms and is used for sentiment analysis (SA) task.\n\nThese datasets are used in a binary classification manner (i.e., irony/sarcasm vs. literal), except for the \"SemEval-2015 Task 11\" dataset, which is used for predicting a sentiment integer score (from -5 to 5) for each tweet.", "predicted_evidence": ["All datasets are used in a binary classification manner (i.e., irony/sarcasm vs. literal), except from the \u201cSemEval-2015 Task 11\u201d dataset where the task is to predict a sentiment integer score (from -5 to 5) for each tweet (refer to BIBREF0 for more details). The evaluation was made across standard five metrics namely, Accuracy (Acc), Precision (Pre), Recall (Rec), F1-score (F1), and Area Under the Receiver Operating Characteristics Curve (AUC). For the SA task the cosine similarity metric (Cos) and mean squared error (MSE) metrics are used, as proposed in the original study BIBREF66.", "Many approaches attempt to create datasets using social media API\u2019s to automatically collect data rather than exploiting their system on benchmark datasets, with proven quality. To this end, it is impossible to be compared and evaluated BIBREF35, BIBREF37, BIBREF36.", "Many studies exhausting preprocess the input texts, including stemming, tagging, emoji processing etc. that tend to be time consuming especially in large datasets BIBREF35, BIBREF36.", "Due to the limitations of annotated datasets and the high cost of data collection, unsupervised learning approaches tend to be an easier way towards training networks. Recently, transfer learning approaches, i.e., the transfer of already acquired knowledge to new conditions, are gaining attention in several domain adaptation problems BIBREF72. In fact, pre-trained embeddings representations, such as GloVe, ElMo and USE, coupled with transfer learning architectures were introduced and managed to achieve state-of-the-art results on various NLP tasks BIBREF73. In this chapter we review on these methodologies in order to introduce our approach. In this chapter we will summarize those methods and introduce our proposed transfer learning system. Model specifications used for the state-of-the-art models compared can be found in Appendix SECREF6.", "The published results were acquired from the respective original publication (the reference publication is indicated in the respective tables). For the comparison we utilized benchmark datasets that include ironic, sarcastic and metaphoric expressions. Namely, we used the dataset provided in \u201cSemantic Evaluation Workshop Task 3\u201d (SemEval-2018) that contains ironic tweets BIBREF95; Riloff\u2019s high quality sarcastic unbalanced dataset BIBREF96; a large dataset containing political comments from Reddit BIBREF97; and a SA dataset that contains tweets with various FL forms from \u201cSemEval-2015 Task 11\u201d BIBREF66. All datasets are used in a binary classification manner (i.e., irony/sarcasm vs. literal), except from the \u201cSemEval-2015 Task 11\u201d dataset where the task is to predict a sentiment integer score (from -5 to 5) for each tweet (refer to BIBREF0 for more details)."]}
{"question_id": "61e96abdc924c34c6b82a587168ea3d14fe792d1", "predicted_answer": "The approach described in the paper does handle overlapping forms of figurative language (FL), such as metaphor and irony. The authors acknowledge that FL is rich in various linguistic phenomena, including metonymy, synonymy, and metaphors, which can overlap or co-occur with irony and sarcasm. The paper's focus on pragmatic FL phenomena like irony and sarcasm, which often involve oppositeness to the literal language context, implies that the approach is designed to handle complex and nuanced cases where multiple forms of FL may be present.", "predicted_evidence": ["We may identify three common FL expression forms namely, irony, sarcasm and metaphor. In this paper, figurative expressions, and especially ironic or sarcastic ones, are considered as a way of indirect denial. From this point of view, the interpretation and ultimately identification of the indirect meaning involved in a passage does not entail the cancellation of the indirectly rejected message and its replacement with the intentionally implied message (as advocated in BIBREF26, BIBREF27). On the contrary ironic/sarcastic expressions presupposes the processing of both the indirectly rejected and the implied message so that the difference between them can be identified. This view differs from the assumption that irony and sarcasm involve only one interpretation BIBREF28, BIBREF29. Holding that irony activates both grammatical / explicit as well as ironic / involved notions provides that irony will be more difficult to grasp than a non-ironic use of the same expression.", "In literature we encounter some major drawbacks of previous studies and we aim to resolve with our proposed method:", "FL is rich of various linguistic phenomena like \u2018metonymy\u2019 reference to an entity stands for another of the same domain, a more general case of \u2018synonymy\u2019; and \u2018metaphors\u2019 systematic interchange between entities from different abstract domains BIBREF19. Besides the philosophical considerations, theories and debates about the exact nature of FL, findings from the neuroscience research domain present clear evidence on the presence of differentiating FL processing patterns in the human brain BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF14, even for woman-man attraction situations! BIBREF24. A fact that makes FL processing even more challenging and difficult to tackle. Indeed, this is the case of pragmatic FL phenomena like irony and sarcasm that main intention of in most of the cases, are characterized by an oppositeness to the literal language context. It is crucial to distinguish between the literal meaning of an expression considered as a whole from its constituents\u2019 words and phrases.", "Recently, the detection of ironic and sarcastic meanings from respective literal ones have raised scientific interest due to the intrinsic difficulties to differentiate between them. Apart from English language, irony and sarcasm detection have been widely explored on other languages as well, such as Italian BIBREF38, Japanese BIBREF39, Spanish BIBREF40, Greek BIBREF41 etc. In the review analysis that follows we group related approaches according to the their adopted key concepts to handle FL.", "Besides the philosophical considerations, theories and debates about the exact nature of FL, findings from the neuroscience research domain present clear evidence on the presence of differentiating FL processing patterns in the human brain BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF14, even for woman-man attraction situations! BIBREF24. A fact that makes FL processing even more challenging and difficult to tackle. Indeed, this is the case of pragmatic FL phenomena like irony and sarcasm that main intention of in most of the cases, are characterized by an oppositeness to the literal language context. It is crucial to distinguish between the literal meaning of an expression considered as a whole from its constituents\u2019 words and phrases. As literal meaning is assumed to be invariant in all context at least in its classical conceptualization BIBREF25, it is exactly this separation of an expression from its context that permits and opens the road to computational approaches in detecting and characterizing FL utterance."]}
{"question_id": "ee8a77cddbe492c686f5af3923ad09d401a741b5", "predicted_answer": "Yes, this work differentiates metaphor (technique) from irony and sarcasm (purpose).\n\nThe text identifies three common forms of figurative language (FL) expressions: irony, sarcasm, and metaphor. While irony and sarcasm are considered as ways of indirect denial, metaphor is not explicitly defined as having a specific purpose or function. Instead, it is listed as a separate category of FL expression.\n\nThe paper focuses on the detection of ironic and sarcastic meanings, and how they differ from literal meanings. The authors discuss the challenges of differentiating between irony and sarcasm, and how they are often confused with each other. Metaphor, on the other hand, is not discussed in terms of its purpose or function, but rather as a distinct category of FL expression that is included in the benchmark datasets used for evaluation.\n\nThe distinction between metaphor and irony/sarcasm is further reinforced by the fact that the paper uses datasets that contain all three forms of FL expressions, and evaluates the performance of the proposed method on these datasets. This suggests that the authors recognize metaphor as a distinct technique that serves a different purpose than irony and sarcasm.", "predicted_evidence": ["We may identify three common FL expression forms namely, irony, sarcasm and metaphor. In this paper, figurative expressions, and especially ironic or sarcastic ones, are considered as a way of indirect denial. From this point of view, the interpretation and ultimately identification of the indirect meaning involved in a passage does not entail the cancellation of the indirectly rejected message and its replacement with the intentionally implied message (as advocated in BIBREF26, BIBREF27). On the contrary ironic/sarcastic expressions presupposes the processing of both the indirectly rejected and the implied message so that the difference between them can be identified. This view differs from the assumption that irony and sarcasm involve only one interpretation BIBREF28, BIBREF29. Holding that irony activates both grammatical / explicit as well as ironic / involved notions provides that irony will be more difficult to grasp than a non-ironic use of the same expression.", "Recently, the detection of ironic and sarcastic meanings from respective literal ones have raised scientific interest due to the intrinsic difficulties to differentiate between them. Apart from English language, irony and sarcasm detection have been widely explored on other languages as well, such as Italian BIBREF38, Japanese BIBREF39, Spanish BIBREF40, Greek BIBREF41 etc. In the review analysis that follows we group related approaches according to the their adopted key concepts to handle FL.", "To assess the performance of the proposed method we performed an exhaustive comparison with several advanced state-of-the-art methodologies along with published results. The used methodologies were appropriately implemented using the available codes and guidelines, and include: ELMo BIBREF64, USE BIBREF79, NBSVM BIBREF93, FastText BIBREF94, XLnet base cased model (XLnet) BIBREF88, BERT BIBREF18 in two setups: BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model. The published results were acquired from the respective original publication (the reference publication is indicated in the respective tables). For the comparison we utilized benchmark datasets that include ironic, sarcastic and metaphoric expressions.", "The published results were acquired from the respective original publication (the reference publication is indicated in the respective tables). For the comparison we utilized benchmark datasets that include ironic, sarcastic and metaphoric expressions. Namely, we used the dataset provided in \u201cSemantic Evaluation Workshop Task 3\u201d (SemEval-2018) that contains ironic tweets BIBREF95; Riloff\u2019s high quality sarcastic unbalanced dataset BIBREF96; a large dataset containing political comments from Reddit BIBREF97; and a SA dataset that contains tweets with various FL forms from \u201cSemEval-2015 Task 11\u201d BIBREF66. All datasets are used in a binary classification manner (i.e., irony/sarcasm vs. literal), except from the \u201cSemEval-2015 Task 11\u201d dataset where the task is to predict a sentiment integer score (from -5 to 5) for each tweet (refer to BIBREF0 for more details).", "The published results were acquired from the respective original publication (the reference publication is indicated in the respective tables). For the comparison we utilized benchmark datasets that include ironic, sarcastic and metaphoric expressions. Namely, we used the dataset provided in \u201cSemantic Evaluation Workshop Task 3\u201d (SemEval-2018) that contains ironic tweets BIBREF95; Riloff\u2019s high quality sarcastic unbalanced dataset BIBREF96; a large dataset containing political comments from Reddit BIBREF97; and a SA dataset that contains tweets with various FL forms from \u201cSemEval-2015 Task 11\u201d BIBREF66. All datasets are used in a binary classification manner (i.e., irony/sarcasm vs. literal), except from the \u201cSemEval-2015 Task 11\u201d dataset where the task is to predict a sentiment integer score (from -5 to 5) for each tweet (refer to BIBREF0 for more details). The evaluation was made across standard five metrics namely, Accuracy (Acc), Precision (Pre), Recall (Rec), F1-score (F1), and Area Under the Receiver Operating Characteristics Curve (AUC)."]}
{"question_id": "552b1c813f25bf39ace6cd5eefa56f4e4dd70c84", "predicted_answer": "The classification tasks experimented on are:\n\n1. 2-way classification: fake or true\n2. 3-way classification: completely true, fake news with true text, or fake news with false text\n3. 5-way classification: true and four other labels defined within the seven types of fake news.", "predicted_evidence": ["Each data sample consists of multiple labels, allowing users to utilize the dataset for 2-way, 3-way, and 5-way classification. This enables both high-level and fine-grained fake news classification.", "Having this hierarchy of labels will enable researchers to train for fake news detection at a high level or a more fine-grained one. The 2-way classification determines whether a sample is fake or true. The 3-way classification determines whether a sample is completely true, the sample is fake news with true text (text that is true in the real world), or the sample is fake news with false text. Our final 5-way classification was created to categorize different types of fake news rather than just doing a simple binary or trinary classification. This can help in pinpointing the degree and variation of fake news for applications that require this type of fine-grained detection. The first label is true and the other four are defined within the seven types of fake news BIBREF3. We provide examples from each class for 5-way classification in Figure SECREF3. The 5-way classification labels are explained below:", "We provide three labels for each sample, allowing us to train for 2-way, 3-way, and 5-way classification. Having this hierarchy of labels will enable researchers to train for fake news detection at a high level or a more fine-grained one. The 2-way classification determines whether a sample is fake or true. The 3-way classification determines whether a sample is completely true, the sample is fake news with true text (text that is true in the real world), or the sample is fake news with false text. Our final 5-way classification was created to categorize different types of fake news rather than just doing a simple binary or trinary classification. This can help in pinpointing the degree and variation of fake news for applications that require this type of fine-grained detection. The first label is true and the other four are defined within the seven types of fake news BIBREF3. We provide examples from each class for 5-way classification in Figure SECREF3.", "We provide three labels for each sample, allowing us to train for 2-way, 3-way, and 5-way classification. Having this hierarchy of labels will enable researchers to train for fake news detection at a high level or a more fine-grained one. The 2-way classification determines whether a sample is fake or true. The 3-way classification determines whether a sample is completely true, the sample is fake news with true text (text that is true in the real world), or the sample is fake news with false text. Our final 5-way classification was created to categorize different types of fake news rather than just doing a simple binary or trinary classification. This can help in pinpointing the degree and variation of fake news for applications that require this type of fine-grained detection. The first label is true and the other four are defined within the seven types of fake news BIBREF3.", "For our experiments, we excluded submissions that did not have an image associated with them and solely used submission image and title data. We performed 2-way, 3-way, and 5-way classification for each of the three types of inputs: image only, text only, and multimodal (text and image)."]}
{"question_id": "1100e442e00c9914538a32aca7af994ce42e1b66", "predicted_answer": "According to the text, the categories of fake news in the dataset are:\n\n* 2-way classification: fake vs. true\n* 3-way classification: (not specified, but presumably a more fine-grained categorization)\n* 5-way classification: (not specified, but presumably an even more fine-grained categorization)\n\nAdditionally, some datasets provide more fine-grained labels, such as:\n\n* NELA-GT-2018\n* LIAR\n* FakeNewsCorpus\n\nThese datasets may include categories such as politics, celebrity gossip, and other specific areas.", "predicted_evidence": ["A variety of datasets for fake news detection have been published in recent years. These are listed in Table TABREF1, along with their specific characteristics. When comparing these datasets, a few trends can be seen. Most of the datasets are small in size, which can be ineffective for current machine learning models that require large quantities of training data. Only four contain over half a million samples, with CREDBANK and FakeNewsCorpus being the largest with millions of samples BIBREF2. In addition, many of the datasets separate their data into a small number of classes, such as fake vs. true. However, fake news can be categorized into many different types BIBREF3. Datasets such as NELA-GT-2018, LIAR, and FakeNewsCorpus provide more fine-grained labels BIBREF4, BIBREF5. While some datasets include data from a variety of categories BIBREF6, BIBREF7, many contain data from specific areas, such as politics and celebrity gossip BIBREF8, BIBREF9, BIBREF10, BIBREF11.", "In addition, many of the datasets separate their data into a small number of classes, such as fake vs. true. However, fake news can be categorized into many different types BIBREF3. Datasets such as NELA-GT-2018, LIAR, and FakeNewsCorpus provide more fine-grained labels BIBREF4, BIBREF5. While some datasets include data from a variety of categories BIBREF6, BIBREF7, many contain data from specific areas, such as politics and celebrity gossip BIBREF8, BIBREF9, BIBREF10, BIBREF11. These data samples may contain limited styles of writing due to this categorization. Finally, most of the existing fake news datasets collect only text data, which is not the only mode that fake news can appear in. Datasets such as image-verification-corpus, Image Manipulation, BUZZFEEDNEWS, and BUZZFACE can be utilized for fake image detection, but contain small sample sizesBIBREF12, BIBREF13, BIBREF14.", "Only four contain over half a million samples, with CREDBANK and FakeNewsCorpus being the largest with millions of samples BIBREF2. In addition, many of the datasets separate their data into a small number of classes, such as fake vs. true. However, fake news can be categorized into many different types BIBREF3. Datasets such as NELA-GT-2018, LIAR, and FakeNewsCorpus provide more fine-grained labels BIBREF4, BIBREF5. While some datasets include data from a variety of categories BIBREF6, BIBREF7, many contain data from specific areas, such as politics and celebrity gossip BIBREF8, BIBREF9, BIBREF10, BIBREF11. These data samples may contain limited styles of writing due to this categorization. Finally, most of the existing fake news datasets collect only text data, which is not the only mode that fake news can appear in.", "Datasets such as NELA-GT-2018, LIAR, and FakeNewsCorpus provide more fine-grained labels BIBREF4, BIBREF5. While some datasets include data from a variety of categories BIBREF6, BIBREF7, many contain data from specific areas, such as politics and celebrity gossip BIBREF8, BIBREF9, BIBREF10, BIBREF11. These data samples may contain limited styles of writing due to this categorization. Finally, most of the existing fake news datasets collect only text data, which is not the only mode that fake news can appear in. Datasets such as image-verification-corpus, Image Manipulation, BUZZFEEDNEWS, and BUZZFACE can be utilized for fake image detection, but contain small sample sizesBIBREF12, BIBREF13, BIBREF14. It can be seen from the table that compared to other existing datasets, Fakeddit contains a large quantity of data, while also annotating for three different types of classification labels (2-way, 3-way, and 5-way) and comparing both text and image data.", "Each data sample consists of multiple labels, allowing users to utilize the dataset for 2-way, 3-way, and 5-way classification. This enables both high-level and fine-grained fake news classification."]}
{"question_id": "82b93ecd2397e417e1e80f93b7cf49c7bd9aeec3", "predicted_answer": "The gain in performance obtained with user embeddings is not explicitly stated in the text. However, it is mentioned that user embeddings (uernn) perform better than user-specific biases (ubrnn), which suggests that user embeddings capture more information than just a user-specific rejection rate bias. Additionally, it is stated that user embeddings improve the performance of the original rnn-based method, but the exact amount of improvement is not provided.", "predicted_evidence": ["Three of the user types (Red, Yellow, Green) in effect also measure INLINEFORM0 , but in discretized form (three bins), which also explains why user type embeddings (ternn) also perform well (third best method). The performance of tbrnn is close to that of ternn, suggesting again that most of the information captured by user type embeddings can also be captured by simpler scalar user-type-specific biases. The user type biases INLINEFORM1 learned by tbrnn are shown in Table TABREF18 . The bias of the Red type is the largest, the bias of the Green type is the smallest, and the biases of the Unknown and Yellow types are in between, as expected (Section SECREF3 ). The same observations hold for the average user-specific biases INLINEFORM2 learned by ubrnn (Table TABREF18 ).", "User-specific information always improves our original rnn-based method (Table TABREF15 ), but the best results are obtained by adding user embeddings (uernn). Figure FIGREF16 visualizes the user embeddings learned by uernn. The two dimensions of Fig. FIGREF16 correspond to the two principal components of the user embeddings, obtained via pca.The colors and numeric labels reflect the rejection rates INLINEFORM0 of the corresponding users. Moving from left to right in Fig. FIGREF16 , the rejection rate increases, indicating that the user embeddings of uernn capture mostly the rejection rate INLINEFORM1 . This rate (a single scalar value per user) can also be captured by the simpler user-specific biases of ubrnn, which explains why ubrnn also performs well (second best results in Table TABREF15 ). Nevertheless, uernn performs better than ubrnn, suggesting that user embeddings capture more information than just a user-specific rejection rate bias.", "In all methods above, we use 300-dimensional word embeddings, user and user type embeddings with INLINEFORM0 dimensions, and INLINEFORM1 hidden units in the gru cells, as in our previous experiments BIBREF0 , where we tuned all hyper-parameters on 2% held-out training comments. Early stopping evaluates on the same held-out subset. User and user type embeddings are randomly initialized and updated by backpropagation. Word embeddings are initialized to the word2vec embeddings of our previous work BIBREF0 , which were pretrained on 5.2M Gazzetta comments. Out of vocabulary words, meaning words not encountered or encountered only once in the training set and/or words with no initial embeddings, are mapped (during both training and testing) to a single randomly initialized word embedding, updated by backpropagation. We use Glorot initialization BIBREF5 for other parameters, cross-entropy loss, and Adam BIBREF6 .", "When experimenting with user type embeddings or biases, we group the users into the following types. INLINEFORM0 is the number of training comments posted by user (id) INLINEFORM1 . INLINEFORM2 is the ratio of training comments posted by INLINEFORM3 that were rejected.", "Experimenting on that dataset and the datasets of Wulczyn et al. Wulczyn2017, which contain moderated English Wikipedia comments, we showed that a method based on a Recurrent Neural Network (rnn) outperforms detox BIBREF1 , the previous state of the art in automatic user content moderation. Our previous work, however, considered only the texts of the comments, ignoring user-specific information (e.g., number of previously accepted or rejected comments of each user). Here we add user embeddings or user type embeddings to our rnn-based method, i.e., dense vectors that represent individual users or user types, similarly to word embeddings that represent words BIBREF2 , BIBREF3 . Experiments on Gazzetta comments show that both user embeddings and user type embeddings improve the performance of our rnn-based method, with user embeddings helping more. User-specific or user-type-specific scalar biases also help to a lesser extent."]}
{"question_id": "2973fe3f5b4bf70ada02ac4a9087dd156cc3016e", "predicted_answer": "According to the text, the similarity measure outperforms BM25 by 19% on TREC 2006 and 6% on TREC 2007 in terms of average precision.", "predicted_evidence": ["As shown in Table TABREF17 , BM25 performs better than TFIDF and CENTROID. CENTROID maps each query and document to a vector by taking a centroid of word embedding vectors, and the cosine similarity between two vectors is used for scoring and ranking documents. As mentioned earlier, this approach is not effective when multiple topics exist in a document. From the table, the embedding approach boosts the average precision of BM25 by 19% and 6% on TREC 2006 and 2007, respectively. However, CENTROID provides scores lower than BM25 and SEM approaches.", "Table TABREF19 shows normalized discounted cumulative gain (NDCG) scores for top 5, 10 and 20 ranked documents for each approach. NDCG BIBREF45 is a measure for ranking quality and it penalizes relevant documents appearing in lower ranks by adding a rank-based discount factor. In the table, reranking documents by learning to rank performs better than BM25 overall, however the larger gain is obtained from using titles (BM25 + SEMTitle) by increasing NDCG@20 by 23%. NDCG@5 and NDCG@10 also perform better than BM25 by 23% and 25%, respectively. It is not surprising that SEMTitle produces better performance than SEMAbstract. The current PubMed search interface does not allow users to see abstracts on the results page, hence users click documents mostly based on titles. Nevertheless, it is clear that the abstract-based semantic distance helps achieve better performance.", "In our experiments, the TREC set was used for evaluating BM25 and the semantic measure separately and the PubMed set was used for evaluating the learning to rank approach. We did not use the TREC set for learning to rank due to the small number of queries. Only 62 queries and 162,259 documents are available in TREC, whereas the PubMed set consists of many more queries and documents.", "We argue that the single scores (or features) produced by BM25 and our proposed semantic measure complement each other, thus merging these two has a synergistic effect. To confirm this, we measured the impact on document ranking by combining BM25 and semantic scores using the learning to rank approach, LamdaMART BIBREF25 , BIBREF26 . Trained on PubMed user queries and their click-through data, we evaluated the search performance based on the most highly ranked 20 documents. As a result, we found that using our semantic measure further improved the performance of BM25.", "Here we present a query-document similarity measure using a neural word embedding approach. This work is particularly motivated by the Word Mover's Distance BIBREF19 . Unlike the common similarity measure taking query/document centroids of word embeddings, the proposed method evaluates a distance between individual words from a query and a document. Our first experiment was performed on the TREC 2006 and 2007 Genomics benchmark sets BIBREF20 , BIBREF21 , and the experimental results showed that our approach was better than BM25 ranking. This was solely based on matching queries and documents by the semantic measure and no other feature was used for ranking documents."]}
{"question_id": "42269ed04e986ec5dc4164bf57ef306aec4a1ae1", "predicted_answer": "When using their proposed similarity measure, they represent documents by evaluating a distance between individual words from a query and a document, rather than taking a centroid of word embeddings.", "predicted_evidence": ["A common approach to computing similarity between texts (e.g. phrases, sentences or documents) is to take a centroid of word embeddings, and evaluate an inner product or cosine similarity between centroids BIBREF14 , BIBREF27 . This has found use in classification and clustering because they seek an overall topic of each document. However, taking a simple centroid is not a good approximator for calculating a distance between a query and a document BIBREF19 . This is mostly because queries tend to be short and finding the actual query words in documents is feasible and more accurate than comparing lossy centroids. Consistent with this, our approach here is to measure the distance between individual words, not the average distance between a query and a document.", "Here we present a query-document similarity measure using a neural word embedding approach. This work is particularly motivated by the Word Mover's Distance BIBREF19 . Unlike the common similarity measure taking query/document centroids of word embeddings, the proposed method evaluates a distance between individual words from a query and a document. Our first experiment was performed on the TREC 2006 and 2007 Genomics benchmark sets BIBREF20 , BIBREF21 , and the experimental results showed that our approach was better than BM25 ranking. This was solely based on matching queries and documents by the semantic measure and no other feature was used for ranking documents.", "In information retrieval (IR), queries and documents are typically represented by term vectors where each term is a content word and weighted by tf-idf, i.e. the product of the term frequency and the inverse document frequency, or other weighting schemes BIBREF0 . The similarity of a query and a document is then determined as a dot product or cosine similarity. Although this works reasonably, the traditional IR scheme often fails to find relevant documents when synonymous or polysemous words are used in a dataset, e.g. a document including only \u201cneoplasm\" cannot be found when the word \u201ccancer\" is used in a query. One solution of this problem is to use query expansion BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 or dictionaries, but these alternatives still depend on the same philosophy, i.e. queries and documents should share exactly the same words.", "We presented a word embedding approach for measuring similarity between a query and a document. Starting from the Word Mover's Distance, we reinterpreted the model for a query-document search problem. Even with the INLINEFORM0 flow only, the word embedding approach is already efficient and effective. In this setup, the proposed approach cannot distinguish documents when they include all query words, but surprisingly, the word embedding approach shows remarkable performance on the TREC Genomics datasets. Moreover, applied to PubMed user queries and click-through data, our semantic measure allows to further improves BM25 ranking performance. This demonstrates that the semantic measure is an important feature for IR and is closely related to user clicks.", "While the prior work gives a hint that the Word Mover's Distance is a reasonable choice for evaluating a similarity between documents, it is uncertain how the same measure could be used for searching documents to satisfy a query. First, it is expensive to compute the Word Mover's Distance. The time complexity of solving the distance problem is INLINEFORM0 BIBREF28 . Second, the semantic space of queries is not the same as those of documents. A query consists of a small number of words in general, hence words in a query tend to be more ambiguous because of the restricted context. On the contrary, a text document is longer and more informational. Having this in mind, we realize that ideally two distinctive components could be employed for query-document search: 1) mapping queries to documents using a word embedding model trained on a document set and 2) mapping documents to queries using a word embedding model obtained from a query set."]}
{"question_id": "31a3ec8d550054465e55a26b0136f4d50d72d354", "predicted_answer": "The text does not explicitly propose a method to combine BM25 and word embedding similarity. However, it does compare the performance of BM25 with a word embedding-based approach, specifically the Word Mover's Distance, and shows that the word embedding approach outperforms BM25 on the TREC and PubMed datasets. It also mentions that the CENTROID approach, which uses word embeddings, does not perform well, suggesting that a simple combination of BM25 and word embedding similarity may not be effective.", "predicted_evidence": ["Taken together, we make the following important contributions in this work. First, to the best of our knowledge, this work represents the first investigation of query-document similarity for information retrieval using the recently proposed Word Mover's Distance. Second, we modify the original Word Mover's Distance algorithm so that it is computationally less expensive and thus more practical and scalable for real-world search scenarios (e.g. biomedical literature search). Third, we measure the actual impact of neural word embeddings in PubMed by utilizing user queries and relevance information derived from click-through data. Finally, on TREC and PubMed datasets, our proposed method achieves stronger performance than BM25.", "Neural word embedding BIBREF15 , BIBREF16 is similar to the semantic analysis methods described above. It learns low-dimensional word vectors from text, but while LSA and LDA utilize co-occurrences of words, neural word embedding learns word vectors to predict context words BIBREF10 . Moreover, training of semantic vectors is derived from neural networks. Both co-occurrence and neural word embedding approaches have been used for lexical semantic tasks such as semantic relatedness (e.g. king and queen), synonym detection (e.g. cancer and carcinoma) and concept categorization (e.g. banana and pineapple belong to fruits) BIBREF10 , BIBREF17 . But, Baroni et al. Baroni2014 showed that neural word embedding approaches generally performed better on such tasks with less effort required for parameter optimization. The neural word embedding models have also gained popularity in recent years due to their high performance in NLP tasks BIBREF18 .", "Here we present a query-document similarity measure using a neural word embedding approach. This work is particularly motivated by the Word Mover's Distance BIBREF19 . Unlike the common similarity measure taking query/document centroids of word embeddings, the proposed method evaluates a distance between individual words from a query and a document. Our first experiment was performed on the TREC 2006 and 2007 Genomics benchmark sets BIBREF20 , BIBREF21 , and the experimental results showed that our approach was better than BM25 ranking. This was solely based on matching queries and documents by the semantic measure and no other feature was used for ranking documents.", "We used the skip-gram model of word2vec BIBREF16 to obtain word embeddings. The alternative models such as GloVe BIBREF11 and FastText BIBREF37 are available, but their performance varies depending on tasks and is comparable to word2vec overall BIBREF38 , BIBREF39 . word2vec was trained on titles and abstracts from over 25 million PubMed documents. Word vector size and window size were set to 100 and 10, respectively. These parameters were optimized to produce high recall for synonyms BIBREF40 . Note that an independent set (i.e. synonyms) was used for tuning word2vec parameters, and the trained model is available online (https://www.ncbi.nlm.nih.gov/IRET/DATASET).", "As shown in Table TABREF17 , BM25 performs better than TFIDF and CENTROID. CENTROID maps each query and document to a vector by taking a centroid of word embedding vectors, and the cosine similarity between two vectors is used for scoring and ranking documents. As mentioned earlier, this approach is not effective when multiple topics exist in a document. From the table, the embedding approach boosts the average precision of BM25 by 19% and 6% on TREC 2006 and 2007, respectively. However, CENTROID provides scores lower than BM25 and SEM approaches."]}
{"question_id": "a7e1b13cc42bfe78d37b9c943de6288e5f00f01b", "predicted_answer": "No, they do not use pre-trained word embeddings to calculate Word Mover's distance. Instead, they use word embeddings (specifically, the skip-gram model of word2vec) to map queries to documents and documents to queries, which is a separate step from calculating the Word Mover's distance. The Word Mover's distance is calculated based on the word importance and relatedness of words, as described in the original algorithm.", "predicted_evidence": ["The Word Mover's Distance makes use of word importance and the relatedness of words as we now describe.", "Our work is based on the Word Mover's Distance between text documents BIBREF19 , which calculates the minimum cumulative distance that words from a document need to travel to match words from a second document. In this subsection, we outline the original Word Mover's Distance algorithm, and our adapted model is described in Section 2.2.", "While the prior work gives a hint that the Word Mover's Distance is a reasonable choice for evaluating a similarity between documents, it is uncertain how the same measure could be used for searching documents to satisfy a query. First, it is expensive to compute the Word Mover's Distance. The time complexity of solving the distance problem is INLINEFORM0 BIBREF28 . Second, the semantic space of queries is not the same as those of documents. A query consists of a small number of words in general, hence words in a query tend to be more ambiguous because of the restricted context. On the contrary, a text document is longer and more informational. Having this in mind, we realize that ideally two distinctive components could be employed for query-document search: 1) mapping queries to documents using a word embedding model trained on a document set and 2) mapping documents to queries using a word embedding model obtained from a query set.", "While the prior work gives a hint that the Word Mover's Distance is a reasonable choice for evaluating a similarity between documents, it is uncertain how the same measure could be used for searching documents to satisfy a query. First, it is expensive to compute the Word Mover's Distance. The time complexity of solving the distance problem is INLINEFORM0 BIBREF28 . Second, the semantic space of queries is not the same as those of documents. A query consists of a small number of words in general, hence words in a query tend to be more ambiguous because of the restricted context. On the contrary, a text document is longer and more informational. Having this in mind, we realize that ideally two distinctive components could be employed for query-document search: 1) mapping queries to documents using a word embedding model trained on a document set and 2) mapping documents to queries using a word embedding model obtained from a query set. In this work, however, we aim to address the former, and the mapping of documents to queries remains as future work.", "We used the skip-gram model of word2vec BIBREF16 to obtain word embeddings. The alternative models such as GloVe BIBREF11 and FastText BIBREF37 are available, but their performance varies depending on tasks and is comparable to word2vec overall BIBREF38 , BIBREF39 . word2vec was trained on titles and abstracts from over 25 million PubMed documents. Word vector size and window size were set to 100 and 10, respectively. These parameters were optimized to produce high recall for synonyms BIBREF40 . Note that an independent set (i.e. synonyms) was used for tuning word2vec parameters, and the trained model is available online (https://www.ncbi.nlm.nih.gov/IRET/DATASET)."]}
{"question_id": "49cd18448101da146c3187a44412628f8c722d7b", "predicted_answer": "The Twitter sentiment treebank used is not the Stanford sentiment treebank, but rather a custom-created treebank with 20,000 manually annotated tweets on a 5-label basis.", "predicted_evidence": ["Our training data for this part was created by taking a random sample from Twitter and having it manually annotated on a 5-label basis to produce fully sentiment-labeled parse-trees, much like the Stanford sentiment treebank. The sample contains twenty thousand tweets with sentiment distribution as following:", "The algorithm processes each tweet according to its parse tree, starting at the leaves and going up combining words into expressions; this is different than other LSTM algorithms since the parsing data is used explicitly. As an example, figure FIGREF36 presents the simple sentence \u201cAmobee is awesome\u201d with its parsing tree. The leaves are given by INLINEFORM0 -dimensional word vectors together with their POS tagging, syntactic categories (if defined for the leaf) and an entity indicator bit. The computation takes place in the inner nodes; \u201cis\u201d and \u201cawesome\u201d are combined in a node marked by \u201cVP\u201d which is the phrase category. In terms of our terminology, \u201cis\u201d and \u201cawesome\u201d are the INLINEFORM1 nodes, respectively for \u201cVP\u201d node calculation. We define INLINEFORM2 as the cell's state for the left child, in this case the \u201cis\u201d node.", "Overall, we have had some unique advantages and disadvantages in this competition. On the one hand, we enjoyed an additional twenty thousand tweets, where every node of the parse tree was labeled for its sentiment, and also had the manpower to manually prune our dictionaries, as well as the opportunity to get feedback from our clients. On the other hand, we did not use any user information and/or metadata from Twitter, nor did we use the SemEval data for training the RNTN models. In addition, we did not ensemble our models with any commercially or freely available pre-trained sentiment analysis packages.", "The algorithm processes each tweet according to its parse tree, starting at the leaves and going up combining words into expressions; this is different than other LSTM algorithms since the parsing data is used explicitly. As an example, figure FIGREF36 presents the simple sentence \u201cAmobee is awesome\u201d with its parsing tree. The leaves are given by INLINEFORM0 -dimensional word vectors together with their POS tagging, syntactic categories (if defined for the leaf) and an entity indicator bit. The computation takes place in the inner nodes; \u201cis\u201d and \u201cawesome\u201d are combined in a node marked by \u201cVP\u201d which is the phrase category. In terms of our terminology, \u201cis\u201d and \u201cawesome\u201d are the INLINEFORM1 nodes, respectively for \u201cVP\u201d node calculation. We define INLINEFORM2 as the cell's state for the left child, in this case the \u201cis\u201d node. Left and right are concatenated as input INLINEFORM3 and the metadata INLINEFORM4 is from the right child while INLINEFORM5 is the metadata from the left child.", "In this paper we described our system of sentiment analysis adapted to participate in SemEval task 4. The highest ranking we reached was third place on the 5-label classification task. Compared with classification with 2 and 3 labels, in which we scored lower, and the fact we used similar workflow for tasks A, B, C, we speculate that the relative success is due to our sentiment treebank ranking on a 5-label basis. This can also explain the relatively superior results in quantification of 5 categories as opposed to quantification of 2 categories."]}
{"question_id": "e9260f6419c35cbd74143f658dbde887ef263886", "predicted_answer": "The answer is not explicitly stated in the given context. The text mentions that \"Results for all sub-tasks are summarized in table\", but the table is not provided. Therefore, it is not possible to determine where the system placed in the other sub-tasks.", "predicted_evidence": ["The five models output is concatenated and used as input for the various tasks, as described in SECREF27 .", "The experiments were developed by using Scikit-learn machine learning library and Keras deep learning library with TensorFlow backend BIBREF7 . Results for all sub-tasks are summarized in table", "This paper describes our system and participation in all sub-tasks of SemEval 2017 task 4. Our system consists of two parts: a recurrent neural network trained on a private Twitter dataset, followed by a task-specific combination of model stacking and logistic regression classifiers.", "As an example, figure FIGREF36 presents the simple sentence \u201cAmobee is awesome\u201d with its parsing tree. The leaves are given by INLINEFORM0 -dimensional word vectors together with their POS tagging, syntactic categories (if defined for the leaf) and an entity indicator bit. The computation takes place in the inner nodes; \u201cis\u201d and \u201cawesome\u201d are combined in a node marked by \u201cVP\u201d which is the phrase category. In terms of our terminology, \u201cis\u201d and \u201cawesome\u201d are the INLINEFORM1 nodes, respectively for \u201cVP\u201d node calculation. We define INLINEFORM2 as the cell's state for the left child, in this case the \u201cis\u201d node. Left and right are concatenated as input INLINEFORM3 and the metadata INLINEFORM4 is from the right child while INLINEFORM5 is the metadata from the left child. The second calculation takes place at the root \u201cS\u201d; the input INLINEFORM6 is now a concatenation of \u201cAmobee\u201d word vector, the input INLINEFORM7 holds the INLINEFORM8 output of the previous step in node \u201cVP\u201d; the cell state INLINEFORM9 comes from the \u201cAmobee\u201d node.", "SemEval is a yearly event in which teams compete in natural language processing tasks. Task 4 is concerned with sentiment analysis in Twitter; it contains five sub-tasks which include classification of tweets according to 2, 3 or 5 labels and quantification of sentiment distribution regarding topics mentioned in tweets; for a complete description of task 4 see BIBREF0 ."]}
{"question_id": "2834a340116026d5995e537d474a47d6a74c3745", "predicted_answer": "The five labels to be predicted in sub-task C are not explicitly mentioned in the given context. However, based on the information provided, it can be inferred that the task involves 5-label classification, as mentioned in the sentence \"The highest ranking we reached was third place on the 5-label classification task.\"", "predicted_evidence": ["where INLINEFORM0 are the current tweet and label, INLINEFORM1 is the sentiment prediction of the logistic regression model for an entity, INLINEFORM2 is the set of all tweets and INLINEFORM3 is the set of labels. We trained a logistic regression on the new distribution and the predictions were submitted as task C. We obtained a macro-averaged MAE score of INLINEFORM4 .", "The five models output is concatenated and used as input for the various tasks, as described in SECREF27 .", "In this paper we described our system of sentiment analysis adapted to participate in SemEval task 4. The highest ranking we reached was third place on the 5-label classification task. Compared with classification with 2 and 3 labels, in which we scored lower, and the fact we used similar workflow for tasks A, B, C, we speculate that the relative success is due to our sentiment treebank ranking on a 5-label basis. This can also explain the relatively superior results in quantification of 5 categories as opposed to quantification of 2 categories.", "where the probabilities are the predicted probabilities after the previous logistic regression step. Finally we predicted a label for each tweet according to the lowest loss, and calculated the mean sentiment for each entity. These were submitted as task E. We obtained a score of INLINEFORM0 .", "We used the SemEval 2017 task A data in the following way: using SemEval 2016 TEST as our TEST, partitioning the rest into TRAIN and DEV datasets. The test dataset went through the previously mentioned pipeline, getting a 5-label probability distribution."]}
{"question_id": "bd53399be8ff59060792da4c8e42a7fc1e6cbd85", "predicted_answer": "The previous state-of-the-art is PEGASUS$_\\mathrm {HugeNews}$, which achieved the best results on the XSum dataset.", "predicted_evidence": ["In this study, we investigate several combinations of models for these two tasks.", "On the other hand, on the XSum dataset, PEGASUS$_\\mathrm {HugeNews}$ improved the ROUGE scores and achieved the best results. In the XSum dataset, summaries often include the expressions that are not written in the source text. Therefore, increasing the pre-training data and learning more patterns were effective. However, by improving the quality of the pseudo saliency labels, we should be able to improve the accuracy of the CIT model.", "This section describes nine combinations of the pre-trained seq-to-seq model and saliency models.", "The first type uses the shared encoder (\u00a7SECREF26). These models consist of the shared encoder and the decoder, where the shared encoder module blackplays two roles: saliency detection and the encoding of the seq-to-seq model. blackThe saliency scores are used to bias the representation of the seq-to-seq model for several models in this type.", "Several studies have proposed the combination of a token-level saliency model and a seq-to-seq model, blackwhich is not pre-trained, and reported its effectiveness BIBREF3, BIBREF10. We also use a simple token-level saliency model blackas a basic model in this study."]}
{"question_id": "a7313c29b154e84b571322532f5cab08e9d49e51", "predicted_answer": "The architecture of the decoder consists of $M$ layer decoder blocks, each of which contains:\n\n1. A self-attention module\n2. A context-attention module\n3. A two-layer feed-forward network\n\nThe input to the decoder is the output of the encoder $H_e^M$ and the output of the previous step of the decoder $\\lbrace y_1,...,y_{t-1} \\rbrace$. The output of the decoder is projected to the vocabulary space and the highest probability token is selected as the next token.", "predicted_evidence": ["In this work, we define the Transformer-based encoder-decoder model as follows.", "The decoder consists of $M$ layer decoder blocks. The inputs of the decoder are the output of the encoder $H_e^M$ and the output of the previous step of the decoder $\\lbrace y_1,...,y_{t-1} \\rbrace $. The output through the $M$ layer Transformer decoder blocks is defined as", "The encoder and decoder blocks use multi-head attention, which consists of a combination of $K$ attention heads and is denoted as $\\mathrm {Multihead}(Q, K, V) = \\mathrm {Concat}(\\mathrm {head}_1, ...,\\mathrm {head}_k)W^o$, where each head is $\\mathrm {head}_i = \\mathrm {Attention}(QW_i^Q, KW_i^K , VW_i^V)$.", "In each step $t$, the $h_{dt}^M$ is projected to blackthe vocabulary space and the decoder outputs the highest probability token as the next token. The Transformer decoder block consists of a self-attention module, a context-attention module, and a two-layer feed-forward network.", "The encoder consists of $M$ layer encoder blocks. The input of the encoder is $X = \\lbrace x_i, x_2, ... x_L \\rbrace $. The output through the $M$ layer encoder blocks is defined as"]}
{"question_id": "cfe21b979a6c851bdafb2e414622f61e62b1d98c", "predicted_answer": "The architecture of the encoder consists of $M$ layer encoder blocks. Each encoder block consists of two components: \n\n1. A self-attention module, which uses multi-head attention. The multi-head attention is a combination of $K$ attention heads, where each head computes attention using query, key, and value matrices. The output of the multi-head attention is the concatenation of the outputs of each attention head, linearly transformed using a learned weight matrix.\n\n2. A two-layer feed-forward network.", "predicted_evidence": ["The encoder consists of $M$ layer encoder blocks. The input of the encoder is $X = \\lbrace x_i, x_2, ... x_L \\rbrace $. The output through the $M$ layer encoder blocks is defined as", "In this work, we define the Transformer-based encoder-decoder model as follows.", "The encoder block consists of a self-attention module and a two-layer feed-forward network.", "The encoder and decoder blocks use multi-head attention, which consists of a combination of $K$ attention heads and is denoted as $\\mathrm {Multihead}(Q, K, V) = \\mathrm {Concat}(\\mathrm {head}_1, ...,\\mathrm {head}_k)W^o$, where each head is $\\mathrm {head}_i = \\mathrm {Attention}(QW_i^Q, KW_i^K , VW_i^V)$.", "The third type uses both the shared encoder and the extractor (\u00a7SECREF39). These models consist of the extractor, shared encoder, and decoder and also follow two steps: first, blackthe extractor extracts the important tokens from the source text, and second, blackthe shared encoder uses them as an input of the seq-to-seq model."]}
{"question_id": "3e3d123960e40bcb1618e11999bd2031ccc1d155", "predicted_answer": "The languages of the datasets are not explicitly mentioned in the given context. However, based on the names of the datasets, \"CNN/DM\" and \"XSum\", it can be inferred that the languages are likely English, as these datasets are commonly used for news summarization tasks in English.", "predicted_evidence": ["We evaluated the combination models on the CNN/DM BIBREF5 and XSum BIBREF6 datasets. Our CIT model outperformed a simple fine-tuned model in terms of ROUGE scores on both datasets.", "We roughly categorize the combinations into three types. Figure FIGREF23 shows an image of each combination.", "Rouge scores of the combined models on the XSum dataset are shown in Table TABREF52. The CIT model performed the best, although its improvement was smaller than on the CNN/DM dataset. Moreover, the accuracy of the MT, SE + MT, and SEG models decreased on the XSum dataset. These results were very different from those on the CNN/DM dataset.", "We used the CNN/DM dataset BIBREF5 and the XSum dataset BIBREF6, which are both standard datasets for news summarization. The details of the two datasets are listed in Table TABREF48. The CNN/DM is a highly extractive summarization dataset and the XSum is a highly abstractive summarization dataset.", "blackWe used BART$_{\\mathrm {LARGE}}$ BIBREF1, which is one of the state-of-the-art models, as the pre-trained seq-to-seq model and RoBERTa$_\\mathrm {BASE}$ BIBREF11 as the initial model of the extractor. In the extractor of CIT, stop words and duplicate tokens are ignored for the XSum dataset."]}
{"question_id": "2e37eb2a2a9ad80391e57acb53616eab048ab640", "predicted_answer": "According to the text, a basic saliency model consists of:\n\n1. $M$-layer Transformer encoder blocks ($\\mathrm {Encoder}_\\mathrm {sal}$)\n2. A single-layer feed-forward network", "predicted_evidence": ["In this study, we use two types of saliency model for combination: a shared encoder and an extractor. Each model structure is based on the basic saliency model. We describe them below.", "This model first extracts the saliency sentences on the basis of a sentence-level saliency score $S_j$. $S_j$ is calculated by using the token level saliency score of the extractor, $S_l$, as", "A basic saliency model consists of $M$-layer Transformer encoder blocks ($\\mathrm {Encoder}_\\mathrm {sal}$) and a single-layer feed-forward network. We define the saliency score of the $l$-th token ($1 \\le l \\le L$) in the source text as", "This section describes nine combinations of the pre-trained seq-to-seq model and saliency models.", "We investigated nine combinations of pre-trained seq-to-seq and token-level saliency models, where the saliency models share the parameters with the encoder of the seq-to-seq model or extract important tokens independently of the encoder."]}
{"question_id": "54002c15493d4082d352a66fb9465d65bfe9ddca", "predicted_answer": "The special architectures this review focuses on that are related to multimodal fusion are:\n\n1. Simple operation-based methods\n2. Attention-based methods\n3. Tensor-based methods", "predicted_evidence": ["Fusion is a key research problem in multimodal studies, which integrates information extracted from different unimodal data into one compact multimodal representation. There is a clear connection between fusion and multimodal representation. We classify an approach into the fusion category if its focus is the architectures for integrating unimodal representations for particular a task.", "The fusion of the features or representations of the single modalities is undoubtedly a centric problem of any multimodal task. Different from previous studies that often categorise the related work into early, middle and late stage methods based on the stage that fusion happens in the procedure, we classify them according to the actual operation used in the fusion, such as attention and bilinear pooling, since it becomes difficult to classify some recent complex approaches into stages.", "Traditionally, fusion methods are divided based on the stage it appears in the procedure. Early fusion, or feature-level fusion, directly combines the features extracted from each type of unimodal data to stress the intra-modality interactions and can cause the inter-modality interactions to be suppressed. Late fusion, on the other hand, refers to model-level fusion that builds a separate model for each modality and combines their output BIBREF115, BIBREF116, BIBREF117, BIBREF118, BIBREF119. The late fusion methods are strong in modelling intra-modality interactions with the modality-specific models but may suffer from the limited power of simple output value combination since the inter-modality interactions are rather complex. Recent studies focus on the intermediate or middle-level methods that allows fusion to happen at multiple layers of a deep model.", "In this section, a review on intermediate fusion is focused \u2013 not only as it is more flexible, but also because the boundaries between stages are less clear due to the use of unimodal features derived from pre-trained backbone models. Three types of methods mostly used to fuse text with image features are included: simple operation-based, attention-based, as well as tensor-based methods.", "A recent study BIBREF125 employs neural architecture search with progressive exploration BIBREF126, BIBREF127, BIBREF128 to find suitable settings for a number of fusion functions. Each fusion function is configured by which layers to fuse and whether to use concatenation or weighted sum as the fusion operation. Other weak functions can also be used to fuse multiple layers from each modality BIBREF129."]}
{"question_id": "7caeb5ef6f2985b2cf383cd01765d247c936605f", "predicted_answer": "According to the text, authors have explored the following model inference optimization schemes:\n\n1. Batching\n2. Lazy evaluation\n3. Running inference at a decreased frame rate\n4. Frame-skipping\n5. Multi-threaded lazy computation", "predicted_evidence": ["Removing SC and BN simplifies the model structure and speeds up both training and inference. Removing BN also solves the sequence level training problem discussed in Section SECREF3. More importantly, we always observe as good or better accuracy (WER) with the proposed simplified model structure.", "Very deep convolutional neural network acoustic models are computationally expensive when used for speech recognition. Several techniques have been explored to improve inference speed on commodity server CPUs. Batching and lazy evaluation have been shown to improve inference speed on CPUs BIBREF12 for neural networks of all types. Specifically for speech recognition, running inference at a decreased frame rate BIBREF13 has also been shown to reduce computation cost without affecting accuracy.", "We already achieve significant inference speedup by removing BN and SC from ResNet-50 as discussed in Section SECREF25. Further inference optimization for SNDCNN-50 was investigated, particularly frame-skipping and multi-threaded lazy computation.", "The key ResNet innovation is the shortcut connections shown in Figure FIGREF1. Figure FIGREF1 is a typical building block of ResNet. The input to the block, $x$, will go through both the original mapping $F(x)$ (weight layers, RELU activations and batch normalization BIBREF3) and the identity shortcut connection. The output, $y$, will be $F(x)+x$. The authors of BIBREF8 hypothesize that the so-called residual mapping of $y=F(x)+x$ should be easier to optimize than the original mapping of $y=F(x)$. The design of the special building block is motivated by the observation in BIBREF6, BIBREF7 that accuracy degrades when more layers are stacked onto an already very deep CNN model. If the added layers can be constructed as identity mappings, the deeper model should not have worse training error than the original shallower model without these added layers. The degradation actually suggests that the optimizer has difficulties in approximating identity mappings.", "In mini-batch based stochastic optimization, the mean $\\mathbf {E}[x^{(k)}]$ and variance $\\mathbf {Var}[x^{(k)}]$ are estimated within each mini-batch."]}
{"question_id": "1fcd25e9a63a53451cac9ad2b8a1b529aff44a97", "predicted_answer": "The model is trained and tested on internal Siri datasets, specifically:\n\n* en_US dataset with 300 hours of speech for training and 7 hours of speech for testing\n* zh_CN dataset with 4000 hours of speech for training and 30 hours of speech for testing\n* Another en_US dataset with 10000 hours of speech for training and 7 hours of speech for testing (used in Table TABREF23)", "predicted_evidence": ["Table TABREF21 compares WER of different model topologies for en_US. The training data contains 300 hours of speech and the testing data has 7 hours of speech. From Table TABREF21, we have the following observations:", "All the data used in this paper comes from Siri internal datasets (en_US and zh_CN). All the models are trained with Blockwise Model-Update Filtering (BMUF) BIBREF18 with 32 GPUs. A 4-gram language model is used in all experiments. 40 dimensional filter bank feature is extracted with 25ms window and 10ms step size. All the models use a context window of 41 frames (20-1-20) as the visible states BIBREF19.", "Table TABREF23 compares en_US WER of ResNet-50 and SNDCNN-50 with 10000 hours of training data and 7 hours of testing data. In this experiment, the proposed SNDCNN has much better WER than ResNet.", "We verify the Self-Normalizing property by observing the trend of mean and variance in the SELU activation outputs during training. The model topology is a 50-layer CNN obtained by removing SC and BN from ResNet-50. We call this topology SNDCNN-50. Model parameters are initialized as instructed in BIBREF0. In Figures FIGREF14 and FIGREF15, we plot the mean and variance trend of the 1st, 23rd, 46th, 47th, and 49th layers of SNDCNN-50 and the 23rd layer of SNDCNN-24. The mean and variance are computed across frames within a mini-batch (256 frames). Each data point is obtained by averaging all the units in the same layer. The x-axis is training time, and we collect statistics from 33k mini-batches to draw each curve.", "Table TABREF22 compares character error rate (CER) of different model topologies for zh_CN. The training data contains 4000 hours of speech and the testing data consists of 30 hours of speech. From Table TABREF22, we find that in order to make the training of very deep CNNs feasible, we must use at least one of the following three techniques: batch normalization, shortcut connection, and SELU activation. The WERs of different topologies with the same depth are actually very similar. This phenomenon suggests that depth could be the key to better accuracy. The proposed SNDCNN has slightly better WER than ResNet."]}
{"question_id": "049415676f8323f4af16d349f36fbcaafd7367ae", "predicted_answer": "The answer is not explicitly stated in the text, but it can be inferred that the proposed approach significantly improves the performance of domain classification, especially when hypothesis reranking is used. However, the exact amount of improvement is not provided.", "predicted_evidence": ["Domain classification is a task that predicts the most relevant domain given an input utterance BIBREF0. It is becoming more challenging since recent conversational interaction systems such as Amazon Alexa, Google Assistant, and Microsoft Cortana support more than thousands of domains developed by external developers BIBREF3, BIBREF2, BIBREF4. As they are independently and rapidly developed without a centralized ontology, multiple domains have overlapped capabilities that can process the same utterances. For example, \u201cmake an elephant sound\u201d can be processed by AnimalSounds, AnimalNoises, and ZooKeeper domains.", "We take a hypothesis reranking approach, which is widely used in large-scale domain classification for higher scalability BIBREF13, BIBREF4. Within the approach, a shortlister, which is a light-weighted domain classifier, suggests the most promising $k$ domains as the hypotheses. We train the shortlister along with the added pseudo labels, leveraging negative system responses, and self-distillation, which are described in Section SECREF3. Then a hypothesis reranker selects the final prediction from the $k$ hypotheses enriched with additional input features, which is described in Section SECREF4.", "Since there are a large number of domains, which are even frequently added or removed, it is infeasible to obtain all the ground-truth domains of the training utterances, and domain classifiers for conversational interaction systems are usually trained given only a small number (usually one) of ground-truths in the training utterances. This setting corresponds to multi-label positive and unlabeled (PU) learning, where assigned labels are positive, unassigned labels are not necessarily negative, and one or more labels are assigned for an instance BIBREF5, BIBREF6.", "We have proposed deriving pseudo labels along with leveraging utterances with negative system responses and self-distillation to improve the performance of domain classification when multiple domains are ground-truths even if only one ground-truth is known in large-scale domain classification. Evaluating on the test utterances with multiple ground-truths from an intelligent conversational system, we have showed that the proposed approach significantly improves the performance of domain classification with hypothesis reranking.", "Evaluating on an annotated dataset from the user logs of a large-scale conversation interaction system, we show that the proposed approach significantly improves the domain classification especially when hypothesis reranking is used BIBREF13, BIBREF4."]}
{"question_id": "fee498457774d9617068890ff29528e9fa05a2ac", "predicted_answer": "The dataset used for evaluation is a set of 10K random utterances extracted from user log data, which were labeled by independent annotators.", "predicted_evidence": ["In this section, we show training and evaluation sets, and experiment results.", "For the experiments in Section SECREF5, we use $p$=2 and $r$=4, which show the best dev set performance. Those derived pseudo labels are used in the model training as follows:", "where $\\alpha ^t=1-0.95^t$ and $t$ is the current epoch so that the baseline loss is mainly used in the earlier epochs while the pseudo labels and self-distillation are more contributing in the later epochs following BIBREF23. $\\beta $ is a hyperparameter for utilizing negative ground-truths, which is set to 0.00025 showing the best dev set performance.", "For the evaluation, we have extracted 10K random utterances from the user log data and independent annotators labeled the top three predictions of all the evaluated models for each utterance so that we can correctly compute nDCG at rank position 3.", "Table TABREF21 shows the evaluation results of the shortlister and the hypothesis reranker with the proposed approaches. For the shortlisters, we show nDCG$_3$ scores, which are highly correlated with the F1 scores of the rerankers than other metrics since the second and third top shortlister predictions contribute the metric. We find that just using the pseudo labels as the additional targets degrades the performance (2). However, when both the pseudo labels and the negative ground-truths are utilized, we observe significant improvements for both precision and recall (5). In addition, recall is increased when self-distillation is used, which achieves the best F1 score (6)."]}
{"question_id": "c626637ed14dee3049b87171ddf326115e59d9ee", "predicted_answer": "The approach may not work well for domains with few overlapping utterances because the method relies on deriving pseudo labels and leveraging utterances with negative system responses. If there are few overlapping utterances, there may not be enough data to derive accurate pseudo labels, and the negative system responses may not provide sufficient information to distinguish between domains. Additionally, the approach is designed to handle multiple domains with overlapped capabilities, so if there are few overlapping utterances, the approach may not be able to take advantage of this overlap to improve domain classification.", "predicted_evidence": ["Domain classification is a task that predicts the most relevant domain given an input utterance BIBREF0. It is becoming more challenging since recent conversational interaction systems such as Amazon Alexa, Google Assistant, and Microsoft Cortana support more than thousands of domains developed by external developers BIBREF3, BIBREF2, BIBREF4. As they are independently and rapidly developed without a centralized ontology, multiple domains have overlapped capabilities that can process the same utterances. For example, \u201cmake an elephant sound\u201d can be processed by AnimalSounds, AnimalNoises, and ZooKeeper domains.", "Since there are a large number of domains, which are even frequently added or removed, it is infeasible to obtain all the ground-truth domains of the training utterances, and domain classifiers for conversational interaction systems are usually trained given only a small number (usually one) of ground-truths in the training utterances. This setting corresponds to multi-label positive and unlabeled (PU) learning, where assigned labels are positive, unassigned labels are not necessarily negative, and one or more labels are assigned for an instance BIBREF5, BIBREF6.", "We have proposed deriving pseudo labels along with leveraging utterances with negative system responses and self-distillation to improve the performance of domain classification when multiple domains are ground-truths even if only one ground-truth is known in large-scale domain classification. Evaluating on the test utterances with multiple ground-truths from an intelligent conversational system, we have showed that the proposed approach significantly improves the performance of domain classification with hypothesis reranking.", "Sounds as the ground-truth. One difference from the previous work is that we utilize utterances with positive system responses as the positive train set and the dev set, and use those with the negative responses as the negative train set as described in Section SECREF11. We have extracted 3M positive train, 400K negative train, and 600K dev sets from 4M log data with 2,500 most frequent domains as the ground-truths. Pseudo labels are added to 53K out of 3M in the positive train set as described in Section SECREF7.", "We obtain training utterances from log data, where utterances with positive system responses are used as the positive train set in Equation DISPLAY_FORM6 and DISPLAY_FORM10 while the utterances with negative responses are used as the negative train set in Equation DISPLAY_FORM14. For example, AnimalSounds is a (positive) ground-truth domain for \u201ca monkey sound\u201d because the system response to the utterance is \u201cHere comes a monkey sound\u201d while it is a negative ground-truth for \u201ca dragon sound\u201d as the response is \u201cI don't know what sound a dragon makes\u201d."]}
{"question_id": "b160bfb341f24ae42a268aa18641237a4b3a6457", "predicted_answer": "The decision on how much to decrease the confidences of incorrectly predicted domains is not explicitly stated in the given context. However, it is implied that the decrease in confidence is done through a loss function, which is formulated to demote the prediction confidences of the domains that resulted in negative responses if they are top-ranked. The exact mechanism or formula for decreasing the confidences is not provided.", "predicted_evidence": ["Maximally $p$ domains predicted with the highest confidences that are higher than the confidence of the known ground-truth.", "Domains predicted with the highest confidences for $r$ times consecutively so that consistent top predictions are used as pseudo labels.", "Pseudo labels can be wrongly derived when irrelevant domains are top predicted, which can lead the model training with wrong supervision. To mitigate this issue, we leverage utterances with negative system responses to lower the prediction confidences of the failing domains. For example, if a system response of a domain for an input utterance is \u201cI don't know that one\u201d, the domain is regarded as a negative ground-truth since it fails to handle the utterance.", "Previous work BIBREF21, BIBREF22 excludes such negative utterances from the training set. We find that it is more effective to explicitly demote the prediction confidences of the domains resulted in negative responses if they are top ranked. It is formulated as a loss function:", "During the model training, irrelevant domains could be top predicted, and regarding them as additional target labels results in wrong confirmation bias BIBREF19, which causes incorrect model training. To reduce the side effect, we leverage utterances with negative responses in order to discourage the utterances' incorrect predictions. This setting can be considered as a multi-label variant of Positive, Unlabeled, and Biased Negative Data (PUbN) learning BIBREF20."]}
{"question_id": "c0120d339fcdb3833884622e532e7513d1b2c7dd", "predicted_answer": "No, the baseline method is not trained on a new dataset. The text does not mention training a baseline method on a new dataset. Instead, it discusses creating a new dataset for the task at hand and using a pre-trained language model to compensate for the limitations of the training data.", "predicted_evidence": ["Specific constraints when creating a Korean dataset are discussed in the two following sections.", "Built on these preliminary results, we aim to make up a more reliable extracting system, of which the main feature is the utilization of a pre-trained language model that can compensate for the deficit of the training data and appearance of OOVs. Also, content-preserving and controllable sentence generation are to be great strategies that fit the core of our task.", "Here, it is also assumed that the loanword such as sale (\uc138\uc77c, seyil), which is expected to be OOV in the test phase, might have caused the failure in (6c). The gold standard for (6) is `\ubc31\ud654\uc810 \uc138\uc77c\uc740 \ubbf8\ub9ac \uac00\uc11c \ub300\uae30\ud558\uae30, to go to the department store earlier and wait for the sale event', which is identical to (6b) if the decomposed morphemes are accurately merged. This suggests that the self attention-based model architecture and the supplement of the dataset are both the solution for the stable inference. Here are more samples that came from the Transformer model, especially some tricky input sentences (7-8) and wh- questions (9-10). Note that all the input sentences are removed with the punctuation marks, and the output phrases were not polished to deliver the original shape.", "The composition of the entire dataset and data created by augmenting the original data is shown in Table 3. We ensured the ratio between the utterance types is balanced so that common utterances which were not statistically well-represented in the corpus had enough training samples. Additionally, we increased the absolute count of utterances for wh-questions where our approach can be proven most effective. As a result, the class imbalance which was problematic for at the initial point, has been partially resolved.", "Besides, comparing (6b) and (6c), where the tails of the clauses (regarding speech act) were correctly inferred, the latter one fails to choose the lexicon regarding wait, instead picking up help that may trained in a large correlation with the terms such as go earlier in the training phase. Here, it is also assumed that the loanword such as sale (\uc138\uc77c, seyil), which is expected to be OOV in the test phase, might have caused the failure in (6c). The gold standard for (6) is `\ubc31\ud654\uc810 \uc138\uc77c\uc740 \ubbf8\ub9ac \uac00\uc11c \ub300\uae30\ud558\uae30, to go to the department store earlier and wait for the sale event', which is identical to (6b) if the decomposed morphemes are accurately merged. This suggests that the self attention-based model architecture and the supplement of the dataset are both the solution for the stable inference."]}
{"question_id": "f52c9744a371104eb2677c181a7004f7a77d9dd3", "predicted_answer": "What potential applications are demonstrated by the proposal?\n\nThe potential applications demonstrated by the proposal are in the area of task-oriented systems, specifically in intent classification and slot-filling. The proposal suggests representing intent categories in a structured format, which can be beneficial in generating question sets or to-do-lists with multiple possible utterance permutations. This can have applications in human-computer interaction, dialogue systems, and natural language processing.", "predicted_evidence": ["$\\rightarrow $ which concept is the most important in algebra", "$\\rightarrow $ just pick me one the most important concept in algebra", "(3) e. why we always gotta do this", "The foundation of this proposal is based on the studies of intent classification and slot-filling BIBREF8. The theoretical background builds on literature from speech act BIBREF9 and formal semantics BIBREF10. Although many task-oriented systems identify the intents as a specific action that the agent should take BIBREF11, to make such intent categories generic in the aspect of sentence semantics, we hypothesized that it would be beneficial to represent them in a structured format. We believe that the closest problem we have to this task is formulating a question set (QS) or to-do-list (TDL) with multiple possible utterance permutations (Table 1) BIBREF10. While these concepts have stronger relations with the domain of syntactic properties, we extend on this to speech act level to reflect common patterns in a human dialog form.", "Topic: Free, Type: wh- question"]}
{"question_id": "867b1bb1e6a38de525be7757d49928a132d0dbd8", "predicted_answer": "The method proposed to mitigate class imbalance in the final dataset is data augmentation, specifically increasing the absolute count of utterances for wh-questions, which are under-represented in the original corpus.", "predicted_evidence": ["The composition of the entire dataset and data created by augmenting the original data is shown in Table 3. We ensured the ratio between the utterance types is balanced so that common utterances which were not statistically well-represented in the corpus had enough training samples. Additionally, we increased the absolute count of utterances for wh-questions where our approach can be proven most effective. As a result, the class imbalance which was problematic for at the initial point, has been partially resolved.", "In this paper, we explore these aspects in the context of Korean, a less explored, low-resource language with various non-canonical expressions. From there on, we propose a structured sentence annotation scheme which can help enrich the human-like conversation with artificial intelligence (AI). For the automation, we annotate an existing corpus and then augment the dataset to mitigate class imbalance, demonstrating the flexibility, practicality, and extensibility of the proposed methods. To further prove that the scheme is not limited to Korean, we demonstrate the methodology using English examples and supplement specific cases with Korean. To begin with, in section 2, we present the theoretical background of this study. We then discuss the detailed procedure with examples, along with an explanation of how it fits with modern natural language understanding (NLU) systems and an evaluation framework.", "Here, it is also assumed that the loanword such as sale (\uc138\uc77c, seyil), which is expected to be OOV in the test phase, might have caused the failure in (6c). The gold standard for (6) is `\ubc31\ud654\uc810 \uc138\uc77c\uc740 \ubbf8\ub9ac \uac00\uc11c \ub300\uae30\ud558\uae30, to go to the department store earlier and wait for the sale event', which is identical to (6b) if the decomposed morphemes are accurately merged. This suggests that the self attention-based model architecture and the supplement of the dataset are both the solution for the stable inference. Here are more samples that came from the Transformer model, especially some tricky input sentences (7-8) and wh- questions (9-10). Note that all the input sentences are removed with the punctuation marks, and the output phrases were not polished to deliver the original shape.", "Specific constraints when creating a Korean dataset are discussed in the two following sections.", "The result shows the advantage coming from (a) adopting the Transformer BIBREF27 and (b) setting aside a larger volume of data for the training phase. (a) is evident here, comparing both ROUGE-1 and BERTScore, where the Transformer model has better performance with the same split model, and even with the 7:3 split model and less iteration. (b) is observed within the two Transformer models, The main reason for the difference is assumed to be the existence of out-of-vocabulary (OOV) terms in the test set, which confuse the system in the inference phase and that brings decoding of non-related terms."]}
{"question_id": "6167618e0c53964f3a706758bdf5e807bc5d7760", "predicted_answer": "Despite the progress made in VQA, there are still several remaining challenges in this field. Some of these challenges include:\n\n1. **Linguistic bias**: VQA models often rely on linguistic biases in the questions and answers, rather than truly understanding the visual content. This can lead to models performing well on the validation set but poorly on real-world scenarios.\n\n2. **Limited generalizability**: Current VQA models are often trained on specific datasets and may not generalize well to new, unseen datasets or scenarios.\n\n3. **Lack of common sense**: VQA models often lack common sense and real-world knowledge, which can lead to incorrect answers in certain situations.\n\n4. **Ambiguity and uncertainty**: VQA models often struggle with ambiguous or uncertain questions, and may not be able to provide accurate answers in such cases.\n\n5. **Explainability and transparency**: VQA models are often complex and lack transparency, making it difficult to understand why a particular answer was chosen.\n\n6. **Robustness to adversarial attacks**: VQA models may be vulnerable to adversarial attacks, which can manipulate the input images or questions to produce incorrect answers.\n\n7. **Scalability**: VQA models can be computationally expensive and may not be scalable to large datasets or real-time applications.\n\n8. **Multimodal understanding**: VQA models often struggle to understand the relationships between different modalities, such as images, text, and audio.\n\n9. **Real-world applications**: VQA models are often evaluated on benchmark datasets, but may not be directly applicable to real-world scenarios, such as robotics, healthcare, or education.\n\n10. **Evaluation metrics**: The evaluation metrics used for VQA may not be comprehensive or accurate, which can lead to models being optimized for the wrong objectives.\n\nThese challenges highlight the need for further research and development in the field of VQA to create more robust, generalizable, and accurate models that can be applied to real-world scenarios.", "predicted_evidence": ["VQA Dataset: The Visual Question Answering (VQA) dataset BIBREF0 is one of the largest datasets collected from the MS-COCO BIBREF18 dataset. The VQA dataset contains at least 3 questions per image with 10 answers per question. The dataset contains 614,163 questions in the form of open-ended and multiple choice. In multiple choice questions, the answers can be classified as: 1) Correct Answer, 2) Plausible Answer, 3) Popular Answers and 4) Random Answers. Recently, VQA V2 dataset BIBREF0 is released with additional confusing images. The VQA sample images and questions are shown in Fig. SECREF2 in 1st row and 1st column.", "Teney et al. Model BIBREF13: Teney et al. introduced the use of object detection on VQA models and won the VQA Challenge 2017. The model helps in narrowing down the features and apply better attention to images. The model employs the use of R-CNN architecture and showed significant performance in accuracy over other architectures.", "Pythia v1.0 BIBREF27: Pythia v1.0 is the award winning architecture for VQA Challenge 2018. The architecture is similar to Teney et al. BIBREF13 with reduced computations with element-wise multiplication, use of GloVe vectors BIBREF22, and ensemble of 30 models.", "The emergence of deep-learning architectures have led to the development of the VQA systems. We discuss the state-of-the-art methods with an overview in Table TABREF6.", "The major VQA datasets are summarized in Table TABREF2. We present the datasets below."]}
{"question_id": "78a0c25b83cdeaeaf0a4781f502105a514b2af0e", "predicted_answer": "According to the text, each model is trained for 100 epochs for each dataset. The text does not provide a specific time duration for training the hybrid model, but it mentions the number of epochs used for training.", "predicted_evidence": ["The experimental results are presented in Table TABREF7 in terms of the accuracy for three models over two datasets. In the experiments, we found that the Teney et al. BIBREF13 is the best performing model on both VQA and Visual7W Dataset. The accuracies obtained over the Teney et al. model are 67.23% and 65.82% over VQA and Visual7W datasets for the open-ended question-answering task, respectively. The above results re-affirmed that the Teney et al. model is the best performing model till 2018 which has been pushed by Pythia v1.0 BIBREF12, recently, where they have utilized the same model with more layers to boost the performance.", "As part of this survey, we also implemented different methods over different datasets and performed the experiments. We considered the following three models for our experiments, 1) the baseline Vanilla VQA model BIBREF0 which uses the VGG16 CNN architecture BIBREF2 and LSTMs BIBREF6, 2) the Stacked Attention Networks BIBREF11 architecture, and 3) the 2017 VQA challenge winner Teney et al. model BIBREF13. We considered the widely adapted datasets such as standard VQA dataset BIBREF0 and Visual7W dataset BIBREF8 for the experiments. We used the Adam Optimizer for all models with Cross-Entropy loss function. Each model is trained for 100 epochs for each dataset.", "The emergence of deep-learning architectures have led to the development of the VQA systems. We discuss the state-of-the-art methods with an overview in Table TABREF6.", "Vanilla VQA BIBREF0: Considered as a benchmark for deep learning methods, the vanilla VQA model uses CNN for feature extraction and LSTM or Recurrent networks for language processing. These features are combined using element-wise operations to a common feature, which is used to classify to one of the answers as shown in Fig. FIGREF4.", "Pythia v1.0 BIBREF27: Pythia v1.0 is the award winning architecture for VQA Challenge 2018. The architecture is similar to Teney et al. BIBREF13 with reduced computations with element-wise multiplication, use of GloVe vectors BIBREF22, and ensemble of 30 models."]}
{"question_id": "08202b800a946b8283c2684e23b51c0ec1e8b2ac", "predicted_answer": "According to the paper, the new deep learning models discussed are:\n\n1. Stacked Attention Networks (BIBREF11)\n2. The 2017 VQA challenge winner Teney et al. model (BIBREF13)\n\nThese models are compared with the baseline Vanilla VQA model (BIBREF0) in the experiments.", "predicted_evidence": ["The emergence of deep-learning architectures have led to the development of the VQA systems. We discuss the state-of-the-art methods with an overview in Table TABREF6.", "Vanilla VQA BIBREF0: Considered as a benchmark for deep learning methods, the vanilla VQA model uses CNN for feature extraction and LSTM or Recurrent networks for language processing. These features are combined using element-wise operations to a common feature, which is used to classify to one of the answers as shown in Fig. FIGREF4.", "The Visual Question Answering has recently witnessed a great interest and development by the group of researchers and scientists from all around the world. The recent trends are observed in the area of developing more and more real life looking datasets by incorporating the real world type questions and answers. The recent trends are also seen in the area of development of sophisticated deep learning models by better utilizing the visual cues as well as textual cues by different means. The performance of even best model is still lagging and around 60-70% only. Thus, it is still an open problem to develop better deep learning models as well as more challenging datasets for VQA. Different strategies like object level details, segmentation masks, deeper models, sentiment of the question, etc. can be considered to develop the next generation VQA models.", "As part of this survey, we also implemented different methods over different datasets and performed the experiments. We considered the following three models for our experiments, 1) the baseline Vanilla VQA model BIBREF0 which uses the VGG16 CNN architecture BIBREF2 and LSTMs BIBREF6, 2) the Stacked Attention Networks BIBREF11 architecture, and 3) the 2017 VQA challenge winner Teney et al. model BIBREF13. We considered the widely adapted datasets such as standard VQA dataset BIBREF0 and Visual7W dataset BIBREF8 for the experiments. We used the Adam Optimizer for all models with Cross-Entropy loss function. Each model is trained for 100 epochs for each dataset.", "Krizhevsky et al BIBREF1 proposed the AlexNet model, which created a revolution in the computer vision domain. The paper introduced the concept of Convolution Neural Networks (CNN) to the mainstream computer vision application. Later many authors have worked on CNN, which has resulted in robust, deep learning models like VGGNet BIBREF2, Inception BIBREF3, ResNet BIBREF4, and etc. Similarly, the recent advancements in natural language processing area based on deep learning have improved the text understanding prforance as well. The first major algorithm in the context of text processing is considered to be the Recurrent Neural Networks (RNN) BIBREF5 which introduced the concept of prior context for time series based data. This architecture helped the growth of machine text understanding which gave new boundaries to machine translation, text classification and contextual understanding. Another major breakthrough in the domain was the introduction of Long-Short Term Memory (LSTM) architecture BIBREF6 which improvised over the RNN by introducing a context cell which stores the prior relevant information."]}
{"question_id": "00aea97f69290b496ed11eb45a201ad28d741460", "predicted_answer": "The architecture of the 2017 Challenge Winner model is the Teney et al. model, which employs the use of R-CNN architecture and showed significant performance in accuracy over other architectures.", "predicted_evidence": ["Pythia v1.0 BIBREF27: Pythia v1.0 is the award winning architecture for VQA Challenge 2018. The architecture is similar to Teney et al. BIBREF13 with reduced computations with element-wise multiplication, use of GloVe vectors BIBREF22, and ensemble of 30 models.", "Teney et al. Model BIBREF13: Teney et al. introduced the use of object detection on VQA models and won the VQA Challenge 2017. The model helps in narrowing down the features and apply better attention to images. The model employs the use of R-CNN architecture and showed significant performance in accuracy over other architectures.", "The experimental results are presented in Table TABREF7 in terms of the accuracy for three models over two datasets. In the experiments, we found that the Teney et al. BIBREF13 is the best performing model on both VQA and Visual7W Dataset. The accuracies obtained over the Teney et al. model are 67.23% and 65.82% over VQA and Visual7W datasets for the open-ended question-answering task, respectively. The above results re-affirmed that the Teney et al. model is the best performing model till 2018 which has been pushed by Pythia v1.0 BIBREF12, recently, where they have utilized the same model with more layers to boost the performance.", "VQA Dataset: The Visual Question Answering (VQA) dataset BIBREF0 is one of the largest datasets collected from the MS-COCO BIBREF18 dataset. The VQA dataset contains at least 3 questions per image with 10 answers per question. The dataset contains 614,163 questions in the form of open-ended and multiple choice. In multiple choice questions, the answers can be classified as: 1) Correct Answer, 2) Plausible Answer, 3) Popular Answers and 4) Random Answers. Recently, VQA V2 dataset BIBREF0 is released with additional confusing images. The VQA sample images and questions are shown in Fig. SECREF2 in 1st row and 1st column.", "The Visual Question Answering has recently witnessed a great interest and development by the group of researchers and scientists from all around the world. The recent trends are observed in the area of developing more and more real life looking datasets by incorporating the real world type questions and answers. The recent trends are also seen in the area of development of sophisticated deep learning models by better utilizing the visual cues as well as textual cues by different means. The performance of even best model is still lagging and around 60-70% only. Thus, it is still an open problem to develop better deep learning models as well as more challenging datasets for VQA. Different strategies like object level details, segmentation masks, deeper models, sentiment of the question, etc. can be considered to develop the next generation VQA models."]}
{"question_id": "4e1293592e41646a6f5f0cb00c75ee8de14eb668", "predicted_answer": "A common sense question is a question that requires the use of general knowledge and reasoning to arrive at an answer. An example of a common sense question from the KVQA dataset could be:\n\n\"What is the purpose of a fire hydrant?\"\n\nThis question requires the use of world knowledge to understand the function of a fire hydrant, and cannot be answered simply by looking at an image. It requires multi-entity, multi-relation, and multi-hop reasoning over a large Knowledge Graph (KG) to arrive at an answer.", "predicted_evidence": ["KVQA: The recent interest in common-sense questions has led to the development of world Knowledge based VQA dataset BIBREF10. The dataset contains questions targeting various categories of nouns and also require world knowledge to arrive at a solution. Questions in this dataset require multi-entity, multi-relation, and multi- hop reasoning over large Knowledge Graphs (KG) to arrive at an answer. The dataset contains 24,000 images with 183,100 question-answer pairs employing around 18K proper nouns. The KVQA samples are shown in Fig. SECREF2 in 2nd row and 2nd column.", "Visual7W: The Visual7W dataset BIBREF8 is also based on the MS-COCO dataset. It contains 47,300 COCO images with 327,939 question-answer pairs. The dataset also consists of 1,311,756 multiple choice questions and answers with 561,459 groundings. The dataset mainly deals with seven forms of questions (from where it derives its name): What, Where, When, Who, Why, How, and Which. It is majorly formed by two types of questions. The \u2018telling\u2019 questions are the ones which are text-based, giving a sort of description. The \u2018pointing\u2019 questions are the ones that begin with \u2018Which,\u2019 and have to be correctly identified by the bounding boxes among the group of plausible answers.", "Neural-Symbolic VQA BIBREF23: Specifically made for CLEVR dataset, this model leverages the question formation and image generation strategy of CLEVR. The images are converted to structured features and the question features are converted to their original root question strategy. This feature is used to filter out the required answer.", "Visual Question Answering (VQA) refers to a challenging task which lies at the intersection of image understanding and language processing. The VQA task has witnessed a significant progress in the recent years by the machine intelligence community. The aim of VQA is to develop a system to answer specific questions about an input image. The answer could be in any of the following forms: a word, a phrase, binary answer, multiple choice answer, or a fill in the blank answer. Agarwal et al. BIBREF0 presented a novel way of combining computer vision and natural language processing concepts of to achieve Visual Grounded Dialogue, a system mimicking the human understanding of the environment with the use of visual observation and language understanding.", "Visual Madlibs: The Visual Madlibs dataset BIBREF15 presents a different form of template for the Image Question Answering task. One of the forms is the fill in the blanks type, where the system needs to supplement the words to complete the sentence and it mostly targets people, objects, appearances, activities and interactions. The Visual Madlibs samples are shown in Fig. SECREF2 in 1st row and 2nd column."]}
{"question_id": "15aeda407ae3912419fd89211cdb98989d9cde58", "predicted_answer": "The pretrained language representations used in this approach are:\n\n1. GloVe\n2. ELMo\n3. Generative Pretrained Transformer (GPT)\n4. Bidirectional Encoder Representations from Transformers (BERT)", "predicted_evidence": ["To analyze the contributions and effects of language representation pretraining in our approach, we perform ablation tests. GloVe is the method with pretrained GloVe BIBREF22 word embeddings; w/o pretrain is our method without pre-trained embeddings (random initialization). From the evaluation results in Table TABREF11 , we observe the performance drop significantly without pretraining, which proves the effectiveness of explicit common linguistic features learning. We also notice that our model with GloVe does not achieve good performance even compared with the random initialization, which indicates that the poor generalization capability for few-shot text classification.", "The training procedure of our approach consists of two parts. Language Representation Pretraining. Given all the training samples, we first utilize pretraining strategies to learn task-agnostic contextualized features that capture linguistic properties to benefit downstream few-shot text classification tasks.", "Recently, a variety of techniques were proposed for training general-purpose language representation models using an enormous amount of unannotated text, such as ELMo BIBREF10 and generative pretrained transformer (GPT) BIBREF11 . Pretrained models can be fine-tuned on natural language processing (NLP) tasks and have achieved significant improvements over training on task-specific annotated data. More recently, a pretraining technique named bidirectional encoder representations from transformers (BERT) BIBREF12 was proposed and has enabled the creation of state-of-the-art models for a wide variety of NLP tasks, including question answering (SQuAD v1.1) and natural language inference, among others.", "Episode-based Meta Training. Given the pretrained language representations, we construct episodes to compute gradients and update the model in each training iteration with MAML.", "It should be noted that human beings are intelligent to leverage learned knowledge about the world in understanding language. BIBREF23 think human beings have a universal grammar, and our daily language system is only a formal expression of this universal grammar. In other words, there are deep structures related to concepts and superficial structures related to speech and symbols in a language. Moreover, neuroscience research has proposed a prominent idea that language processing may offer such a principle that the brain contains partially separate systems for processing syntax and semantics. The part of the prefrontal cortex responsible for language production, called Broca\u2019s area, is thought to be important for parsing syntactic information and applying selective attention to help a separate comprehension system interpret the semantics BIBREF24 . Our idea for few-shot learning in NLP is somewhat similar to this assumption as the pretraining stage may learn common syntax information across tasks, and the meta-learning stage may learn semantic knowledge, which is task specific."]}
{"question_id": "c8b2fb9e0d5fb9014a25b88d559d93b6dceffbc0", "predicted_answer": "The answer is 12. In the few-shot experiments, 12 target tasks are evaluated, each with a 5-shot learning model.", "predicted_evidence": ["Few-shot learning generally resolves the data deficiency problem by recognizing novel classes from very few labeled examples. This limitation in the size of samples (only one or very few examples) challenges the standard fine-tuning method in DL. Early studies in this field BIBREF3 applied data augmentation and regularization techniques to alleviate the overfitting problem caused by data scarcity but only to a limited extent. Instead, researchers have been inspired by human learning to explore meta-learning BIBREF4 to leverage the distribution over similar tasks. Contemporary approaches to few-shot learning often decompose the training procedure into an auxiliary meta-learning phase, which includes many sub-tasks, following the principle that the testing and training conditions must match. They extract some transferable knowledge by switching the task from one mini-batch to the next. Moreover, the few-shot model is able to classify data into new classes with just a small labeled support set.", "Existing approaches for few-shot learning are still plagued by problems, including imposed strong priors BIBREF5 , complex gradient transfer between tasks BIBREF6 , and fine-tuning of the target problem BIBREF7 . The approaches proposed by BIBREF8 and BIBREF9 , which combine non-parametric methods and metric learning, may provide possible solutions to these problems. The non-parametric methods allow novel examples to be rapidly assimilated without suffering from the effects of catastrophic overfitting. Such non-parametric models only need to learn the representation of the samples and the metric measure.", "Few-shot classification is a task in which a classifier must adapt and accommodate new classes that are not seen in training, given only a few examples of each of these new classes. We have a large labeled training set with a set of defined classes INLINEFORM0 . However, after training, our ultimate goal is to produce classifiers on the testing set with a disjoint set of new classes INLINEFORM1 for which only a small labeled support set will be available. If the support set contains INLINEFORM2 labeled examples for each of the INLINEFORM3 unique classes, the target few-shot problem is called a INLINEFORM4 -way INLINEFORM5 -shot problem. Usually, INLINEFORM6 is a too small sample set to train a supervised classification model. Therefore, we aim to perform meta-learning on the training set in order to extract transferrable knowledge that will allow us to perform better few-shot learning on the support set to classify the test set more successfully.", "We thus create 5-shot learning models on this dataset. We evaluate the performance by few-shot classification accuracy following previous studies in few-shot learning BIBREF8 , BIBREF9 . To evaluate the proposed model objectively with the baselines, note that for ARSC, the support set for testing is fixed by BIBREF20 ; therefore, we need to run the test episode once for each of the target tasks. The mean accuracy from the 12 target tasks is compared to those of the baseline models in accordance with BIBREF20 . We use pretrained BERT-Base for the ARSC dataset. All model parameters are updated by backpropagation using Adam with a learning rate of 0.01. We regularize our network using dropout with a rate of 0.3 tuned using the development set.", "These buckets then form 23 INLINEFORM0 3 = 69 tasks in total. Following BIBREF20 , we select 12 (4 INLINEFORM1 3) tasks from four domains (i.e., Books, DVDs, Electronics, and Kitchen) as the test set, with only five examples as support set for each label in the test set. We thus create 5-shot learning models on this dataset. We evaluate the performance by few-shot classification accuracy following previous studies in few-shot learning BIBREF8 , BIBREF9 . To evaluate the proposed model objectively with the baselines, note that for ARSC, the support set for testing is fixed by BIBREF20 ; therefore, we need to run the test episode once for each of the target tasks. The mean accuracy from the 12 target tasks is compared to those of the baseline models in accordance with BIBREF20 . We use pretrained BERT-Base for the ARSC dataset."]}
{"question_id": "c24f7c030010ad11e71ef4912fd79093503f3a8d", "predicted_answer": "The tasks explored in this context are:\n\n1. Language Representation Pretraining\n2. Few-shot text classification tasks\n3. Meta-learning\n4. Few-shot learning\n5. Text classification\n6. Masked language model\n7. Next sentence prediction objectives", "predicted_evidence": ["It should be noted that human beings are intelligent to leverage learned knowledge about the world in understanding language. BIBREF23 think human beings have a universal grammar, and our daily language system is only a formal expression of this universal grammar. In other words, there are deep structures related to concepts and superficial structures related to speech and symbols in a language. Moreover, neuroscience research has proposed a prominent idea that language processing may offer such a principle that the brain contains partially separate systems for processing syntax and semantics. The part of the prefrontal cortex responsible for language production, called Broca\u2019s area, is thought to be important for parsing syntactic information and applying selective attention to help a separate comprehension system interpret the semantics BIBREF24 . Our idea for few-shot learning in NLP is somewhat similar to this assumption as the pretraining stage may learn common syntax information across tasks, and the meta-learning stage may learn semantic knowledge, which is task specific.", "The training procedure of our approach consists of two parts. Language Representation Pretraining. Given all the training samples, we first utilize pretraining strategies to learn task-agnostic contextualized features that capture linguistic properties to benefit downstream few-shot text classification tasks.", "Few-shot learning generally resolves the data deficiency problem by recognizing novel classes from very few labeled examples. This limitation in the size of samples (only one or very few examples) challenges the standard fine-tuning method in DL. Early studies in this field BIBREF3 applied data augmentation and regularization techniques to alleviate the overfitting problem caused by data scarcity but only to a limited extent. Instead, researchers have been inspired by human learning to explore meta-learning BIBREF4 to leverage the distribution over similar tasks. Contemporary approaches to few-shot learning often decompose the training procedure into an auxiliary meta-learning phase, which includes many sub-tasks, following the principle that the testing and training conditions must match. They extract some transferable knowledge by switching the task from one mini-batch to the next. Moreover, the few-shot model is able to classify data into new classes with just a small labeled support set.", "Existing approaches for few-shot learning are still plagued by problems, including imposed strong priors BIBREF5 , complex gradient transfer between tasks BIBREF6 , and fine-tuning of the target problem BIBREF7 . The approaches proposed by BIBREF8 and BIBREF9 , which combine non-parametric methods and metric learning, may provide possible solutions to these problems. The non-parametric methods allow novel examples to be rapidly assimilated without suffering from the effects of catastrophic overfitting. Such non-parametric models only need to learn the representation of the samples and the metric measure.", "While the pretraining tasks have been designed with particular downstream tasks in mind BIBREF16 , we focus on those training tasks that seek to induce universal representations suitable for downstream few-shot learning tasks. We utilize BERT BIBREF12 as a recent study BIBREF17 has shown its potential to achieve state-of-the-art performance when fine-tuned in NLP tasks. BERT combines both word and sentence representations (via masked language model and next sentence prediction objectives) in a single very large pretrained transformer BIBREF18 . It is adapted to both word- and sentence-level tasks with task-specific layers. We feed the sentence representation into a softmax layer for text classification based on BIBREF12 ."]}
{"question_id": "1d7b99646a1bc05beec633d7a3beb083ad1e8734", "predicted_answer": "The training time is not explicitly compared to the original position encoding in the given context. The text only discusses the experimental setup, results, and comparisons between different models and configurations, but it does not provide a direct comparison of the training time between the original position encoding and the relative position encoding approach.", "predicted_evidence": ["Position encodings based on sinusoids of varying frequency are added to encoder and decoder input elements prior to the first layer. In contrast to learned, absolute position representations, the authors hypothesized that sinusoidal position encodings would help the model to generalize to sequence lengths unseen during training by allowing it to learn to attend also by relative position. This property is shared by our relative position representations which, in contrast to absolute position representations, are invariant to the total sequence length.", "We evaluated the effect of varying the clipping distance, $k$ , of the maximum absolute relative position difference. Notably, for $k \\ge 2$ , there does not appear to be much variation in BLEU scores. However, as we use multiple encoder layers, precise relative position information may be able to propagate beyond the clipping distance. The results are shown in Table 2 .", "For our big model, we used 6 encoder and decoder layers, $d_x = 1024$ , $d_z = 64$ , 16 attention heads, 4096 feed forward inner-layer dimensions, and $P_{dropout} = 0.3$ for EN-DE and $P_{dropout} = 0.1$ for EN-FR. When using relative position encodings, we used $k = 8$ , and used unique edge representations per layer. We trained for 300,000 steps on 8 P100 GPUs, and averaged the last 20 checkpoints, saved at 10 minute intervals.", "For English-to-German our approach improved performance over our baseline by 0.3 and 1.3 BLEU for the base and big configurations, respectively. For English-to-French it improved by 0.5 and 0.3 BLEU for the base and big configurations, respectively. In our experiments we did not observe any benefit from including sinusoidal position encodings in addition to relative position representations. The results are shown in Table 1 .", "For the Transformer, which employs neither convolution nor recurrence, incorporating explicit representations of position information is an especially important consideration since the model is otherwise entirely invariant to sequence ordering. Attention-based models have therefore used position encodings or biased attention weights based on distance BIBREF9 ."]}
{"question_id": "4d887ce7dc43528098e7a3d9cd13c6c36f158c53", "predicted_answer": "No, the new relative position encoder does not require more parameters. The text does not mention any additional parameters required for the relative position encoder. In fact, it suggests that the relative position representations are invariant to the total sequence length, which implies that they do not require any additional parameters to handle varying sequence lengths.", "predicted_evidence": ["We evaluated the effect of varying the clipping distance, $k$ , of the maximum absolute relative position difference. Notably, for $k \\ge 2$ , there does not appear to be much variation in BLEU scores. However, as we use multiple encoder layers, precise relative position information may be able to propagate beyond the clipping distance. The results are shown in Table 2 .", "Position encodings based on sinusoids of varying frequency are added to encoder and decoder input elements prior to the first layer. In contrast to learned, absolute position representations, the authors hypothesized that sinusoidal position encodings would help the model to generalize to sequence lengths unseen during training by allowing it to learn to attend also by relative position. This property is shared by our relative position representations which, in contrast to absolute position representations, are invariant to the total sequence length.", "We compared our model using only relative position representations to the baseline Transformer BIBREF3 with sinusoidal position encodings. We generated baseline results to isolate the impact of relative position representations from any other changes to the underlying library and experimental configuration.", "We also evaluated the impact of ablating each of the two relative position representations defined in section \"Conclusions\" , $a^V_{ij}$ in eq. ( 6 ) and $a^K_{ij}$ in eq. ( 7 ). Including relative position representations solely when determining compatibility between elements may be sufficient, but further work is needed to determine whether this is true for other tasks. The results are shown in Table 3 .", "For the Transformer, which employs neither convolution nor recurrence, incorporating explicit representations of position information is an especially important consideration since the model is otherwise entirely invariant to sequence ordering. Attention-based models have therefore used position encodings or biased attention weights based on distance BIBREF9 ."]}
{"question_id": "d48b5e4a7cf1f96c5b939ba9b46350887c5e5268", "predicted_answer": "According to the text, the answer is: \"further work is needed to determine whether this is true for other tasks.\" The new position representation has been evaluated in the context of the current task, but its generalizability to other tasks remains to be explored.", "predicted_evidence": ["We also evaluated the impact of ablating each of the two relative position representations defined in section \"Conclusions\" , $a^V_{ij}$ in eq. ( 6 ) and $a^K_{ij}$ in eq. ( 7 ). Including relative position representations solely when determining compatibility between elements may be sufficient, but further work is needed to determine whether this is true for other tasks. The results are shown in Table 3 .", "One common approach is to use position encodings which are combined with input elements to expose position information to the model. These position encodings can be a deterministic function of position BIBREF8 , BIBREF3 or learned representations. Convolutional neural networks inherently capture relative positions within the kernel size of each convolution. They have been shown to still benefit from position encodings BIBREF1 , however.", "For the Transformer, which employs neither convolution nor recurrence, incorporating explicit representations of position information is an especially important consideration since the model is otherwise entirely invariant to sequence ordering. Attention-based models have therefore used position encodings or biased attention weights based on distance BIBREF9 .", "Position encodings based on sinusoids of varying frequency are added to encoder and decoder input elements prior to the first layer. In contrast to learned, absolute position representations, the authors hypothesized that sinusoidal position encodings would help the model to generalize to sequence lengths unseen during training by allowing it to learn to attend also by relative position. This property is shared by our relative position representations which, in contrast to absolute position representations, are invariant to the total sequence length.", "We then learn relative position representations $w^K = (w^K_{-k}, \\ldots , w^K_k)$ and $w^V = (w^V_{-k}, \\ldots , w^V_k)$ where $w^K_i, w^V_i \\in \\mathbb {R}^{d_a}$ ."]}
{"question_id": "de344aeb089affebd15a8c370ae9ab5734e99203", "predicted_answer": "The data was collected from Twitter.", "predicted_evidence": ["Data collection is a time consuming and tedious task in terms of human resource. Two code-mixed data pairs HI-EN and BN-EN are provided for developing sentiment analysis systems. The Twitter4j API was used to collect both Bengali and Hindi code-mixed data from Twitter. Initially, common Bengali and Hindi words were collected and then searched using the above API. The collected words are mostly sentiment words in Romanized format. Plenty of tweets had noisy words such as words from other languages and words in utf-8 format. After collection of code-mixed tweets, some were rejected. There are three reasons for which a tweet was rejected.", "The past decade witnessed rapid growth and widespread usage of social media platforms by generating a significant amount of user-generated text. The user-generated texts contain high information content in the form of news, expression, or knowledge. Automatically mining information from user-generated data is unraveling a new field of research in Natural Language Processing (NLP) and has been a difficult task due to unstructured and noisy nature. In spite of the existing challenges, much research has been conducted on user-generated data in the field of information extraction, sentiment analysis, event extraction, user profiling and many more.", "With the rise of social media and user-generated data, information extraction from user-generated text became an important research area. Social media has become the voice of many people over decades and it has special relations with real time events. The multilingual user have tendency to mix two or more languages while expressing their opinion in social media, this phenomenon leads to generate a new code-mixed language. So far, many studies have been conducted on why the code-mixing phenomena occurs and can be found in Kim kim2006reasons. Several experiments have been performed on social media texts including code-mixed data. The first step toward information gathering from these texts is to identify the languages present.", "With the rise of social media and user-generated data, information extraction from user-generated text became an important research area. Social media has become the voice of many people over decades and it has special relations with real time events. The multilingual user have tendency to mix two or more languages while expressing their opinion in social media, this phenomenon leads to generate a new code-mixed language. So far, many studies have been conducted on why the code-mixing phenomena occurs and can be found in Kim kim2006reasons. Several experiments have been performed on social media texts including code-mixed data. The first step toward information gathering from these texts is to identify the languages present. Till date, several language identification experiments or tasks have been performed on several code-mixed language pairs such as Spanish-English BIBREF5 , BIBREF6 , French-English BIBREF7 , Hindi-English BIBREF0 , BIBREF1 , Hindi-English-Bengali BIBREF8 , Bengali-English BIBREF1 .", "a tweet is spam, advertisement or slang."]}
{"question_id": "84327a0a9321bf266e22d155dfa94828784595ce", "predicted_answer": "The text does not explicitly mention the number of data pairs for each dataset. It only mentions that two code-mixed data pairs, HI-EN and BN-EN, were provided for developing sentiment analysis systems.", "predicted_evidence": ["This section describes statistics of the dataset and the evaluation procedure. Preparing a gold standard dataset is the first step towards achieving good accuracy. Several tasks in the field of NLP suffer from lack of gold standard dataset. In the case of Indian languages, there is no such code-mixed dataset available for research purpose. Thus, we developed the dataset and the details are provided below.", "Data collection is a time consuming and tedious task in terms of human resource. Two code-mixed data pairs HI-EN and BN-EN are provided for developing sentiment analysis systems. The Twitter4j API was used to collect both Bengali and Hindi code-mixed data from Twitter. Initially, common Bengali and Hindi words were collected and then searched using the above API. The collected words are mostly sentiment words in Romanized format. Plenty of tweets had noisy words such as words from other languages and words in utf-8 format. After collection of code-mixed tweets, some were rejected. There are three reasons for which a tweet was rejected.", "The baseline systems achieved better scores compared to CEN@AMRIT and SVNIT teams for HI-EN dataset; and AMRITA_CEN, CEN@Amrita and SVNIT teams for BN-EN dataset. IIIT-NBP team has achieved the maximum macro average f-score of 0.569 across all the sentiment classes for HI-EN dataset. IIIT-NBP also achieved the maximum macro average f-score of 0.526 for BN-EN dataset. Two way classification of HI-EN dataset achieved the maximum macro average f-score of 0.707, 0.666, and 0.663 for positive, negative, and neutral, respectively. Similarly, the two way classification of BN-EN dataset achieved the maximum average f-score of 0.641, 0.677, and 0.621 for positive, negative, and neutral, respectively.", "The baseline systems achieved better scores compared to CEN@AMRIT and SVNIT teams for HI-EN dataset; and AMRITA_CEN, CEN@Amrita and SVNIT teams for BN-EN dataset. IIIT-NBP team has achieved the maximum macro average f-score of 0.569 across all the sentiment classes for HI-EN dataset. IIIT-NBP also achieved the maximum macro average f-score of 0.526 for BN-EN dataset. Two way classification of HI-EN dataset achieved the maximum macro average f-score of 0.707, 0.666, and 0.663 for positive, negative, and neutral, respectively. Similarly, the two way classification of BN-EN dataset achieved the maximum average f-score of 0.641, 0.677, and 0.621 for positive, negative, and neutral, respectively. Again, the f-measure achieved using HI-EN dataset is better than BN-EN.", "Several experiments have been performed on social media texts including code-mixed data. The first step toward information gathering from these texts is to identify the languages present. Till date, several language identification experiments or tasks have been performed on several code-mixed language pairs such as Spanish-English BIBREF5 , BIBREF6 , French-English BIBREF7 , Hindi-English BIBREF0 , BIBREF1 , Hindi-English-Bengali BIBREF8 , Bengali-English BIBREF1 . Many shared tasks have also been organized for language identification of code-mixed texts. Language Identification in Code-Switched Data was one of the shared tasks which covered four language pairs such as Spanish-English, Modern Standard Arabic and Arabic dialects, Chinese-English, and Nepalese-English. In the case of Indian languages, Mixed Script Information Retrieval BIBREF9 shared task at FIRE-2015 was organized for eight code-mixed Indian languages such as Bangla, Gujarati, Hindi, Kannada, Malayalam, Marathi, Tamil, and Telugu mixed with English."]}
{"question_id": "c2037887945abbdf959389dc839a86bc82594505", "predicted_answer": "There were 6 systems submitted by different teams.", "predicted_evidence": ["This subsection describes the details of systems submitted for the shared task. Six teams have submitted their system details and those are described below in order of decreasing f-score.", "The precision, recall and f-score are calculated using the sklearn package of scikit-learn BIBREF15 . The macro average f-score is used to rank the submitted systems, because it independently calculates the metric for each classes and then takes the average hence treating all classes equally. Two different types of evaluation are considered and these are described below.", "The baseline systems are developed by randomly assigning any of the sentiment values to each of the test instances. Then, similar evaluation techniques are applied to the baseline system and the results are presented in Table TABREF29 .", "NLP_CEN_AMRITA team have used different distributional and distributed representation. They used Document Term Matrix with N-gram varying from 1 to 5 for the representation and Support Vector Machines (SVM) as a classifier to make the final prediction. Their system performed well for n-grams range 5 and minimum document frequency 2 using the linear kernel.", "The baseline systems achieved better scores compared to CEN@AMRIT and SVNIT teams for HI-EN dataset; and AMRITA_CEN, CEN@Amrita and SVNIT teams for BN-EN dataset. IIIT-NBP team has achieved the maximum macro average f-score of 0.569 across all the sentiment classes for HI-EN dataset. IIIT-NBP also achieved the maximum macro average f-score of 0.526 for BN-EN dataset. Two way classification of HI-EN dataset achieved the maximum macro average f-score of 0.707, 0.666, and 0.663 for positive, negative, and neutral, respectively. Similarly, the two way classification of BN-EN dataset achieved the maximum average f-score of 0.641, 0.677, and 0.621 for positive, negative, and neutral, respectively."]}
{"question_id": "e9a0a69eacd554141f56b60ab2d1912cc33f526a", "predicted_answer": "The baseline was developed by randomly assigning any of the sentiment values to each of the test instances.", "predicted_evidence": ["The baseline systems are developed by randomly assigning any of the sentiment values to each of the test instances. Then, similar evaluation techniques are applied to the baseline system and the results are presented in Table TABREF29 .", "Overall: The macro average precision, recall, and f-score are calculated for all submitted runs.", "The baseline systems achieved better scores compared to CEN@AMRIT and SVNIT teams for HI-EN dataset; and AMRITA_CEN, CEN@Amrita and SVNIT teams for BN-EN dataset. IIIT-NBP team has achieved the maximum macro average f-score of 0.569 across all the sentiment classes for HI-EN dataset. IIIT-NBP also achieved the maximum macro average f-score of 0.526 for BN-EN dataset. Two way classification of HI-EN dataset achieved the maximum macro average f-score of 0.707, 0.666, and 0.663 for positive, negative, and neutral, respectively. Similarly, the two way classification of BN-EN dataset achieved the maximum average f-score of 0.641, 0.677, and 0.621 for positive, negative, and neutral, respectively.", "The baseline systems achieved better scores compared to CEN@AMRIT and SVNIT teams for HI-EN dataset; and AMRITA_CEN, CEN@Amrita and SVNIT teams for BN-EN dataset. IIIT-NBP team has achieved the maximum macro average f-score of 0.569 across all the sentiment classes for HI-EN dataset. IIIT-NBP also achieved the maximum macro average f-score of 0.526 for BN-EN dataset. Two way classification of HI-EN dataset achieved the maximum macro average f-score of 0.707, 0.666, and 0.663 for positive, negative, and neutral, respectively. Similarly, the two way classification of BN-EN dataset achieved the maximum average f-score of 0.641, 0.677, and 0.621 for positive, negative, and neutral, respectively. Again, the f-measure achieved using HI-EN dataset is better than BN-EN.", "This subsection describes the details of systems submitted for the shared task. Six teams have submitted their system details and those are described below in order of decreasing f-score."]}
{"question_id": "5b2839bef513e5d441f0bb8352807f673f4b2070", "predicted_answer": "The metrics used for evaluation are:\n\n1. Precision\n2. Recall\n3. F-score (macro average f-score is used to rank the submitted systems)\n\nThese metrics are calculated using the sklearn package of scikit-learn.", "predicted_evidence": ["Overall: The macro average precision, recall, and f-score are calculated for all submitted runs.", "The precision, recall and f-score are calculated using the sklearn package of scikit-learn BIBREF15 . The macro average f-score is used to rank the submitted systems, because it independently calculates the metric for each classes and then takes the average hence treating all classes equally. Two different types of evaluation are considered and these are described below.", "The baseline systems achieved better scores compared to CEN@AMRIT and SVNIT teams for HI-EN dataset; and AMRITA_CEN, CEN@Amrita and SVNIT teams for BN-EN dataset. IIIT-NBP team has achieved the maximum macro average f-score of 0.569 across all the sentiment classes for HI-EN dataset. IIIT-NBP also achieved the maximum macro average f-score of 0.526 for BN-EN dataset. Two way classification of HI-EN dataset achieved the maximum macro average f-score of 0.707, 0.666, and 0.663 for positive, negative, and neutral, respectively. Similarly, the two way classification of BN-EN dataset achieved the maximum average f-score of 0.641, 0.677, and 0.621 for positive, negative, and neutral, respectively. Again, the f-measure achieved using HI-EN dataset is better than BN-EN.", "The baseline systems achieved better scores compared to CEN@AMRIT and SVNIT teams for HI-EN dataset; and AMRITA_CEN, CEN@Amrita and SVNIT teams for BN-EN dataset. IIIT-NBP team has achieved the maximum macro average f-score of 0.569 across all the sentiment classes for HI-EN dataset. IIIT-NBP also achieved the maximum macro average f-score of 0.526 for BN-EN dataset. Two way classification of HI-EN dataset achieved the maximum macro average f-score of 0.707, 0.666, and 0.663 for positive, negative, and neutral, respectively. Similarly, the two way classification of BN-EN dataset achieved the maximum average f-score of 0.641, 0.677, and 0.621 for positive, negative, and neutral, respectively.", "The baseline systems are developed by randomly assigning any of the sentiment values to each of the test instances. Then, similar evaluation techniques are applied to the baseline system and the results are presented in Table TABREF29 ."]}
{"question_id": "2abf916bc03222d3b2a3d66851d87921ff35c0d2", "predicted_answer": "The datasets used are:\n\n1. HI-EN (Hindi-English code-mixed dataset)\n2. BN-EN (Bengali-English code-mixed dataset)\n\nBoth datasets were collected from Twitter using the Twitter4j API.", "predicted_evidence": ["This section describes statistics of the dataset and the evaluation procedure. Preparing a gold standard dataset is the first step towards achieving good accuracy. Several tasks in the field of NLP suffer from lack of gold standard dataset. In the case of Indian languages, there is no such code-mixed dataset available for research purpose. Thus, we developed the dataset and the details are provided below.", "Data collection is a time consuming and tedious task in terms of human resource. Two code-mixed data pairs HI-EN and BN-EN are provided for developing sentiment analysis systems. The Twitter4j API was used to collect both Bengali and Hindi code-mixed data from Twitter. Initially, common Bengali and Hindi words were collected and then searched using the above API. The collected words are mostly sentiment words in Romanized format. Plenty of tweets had noisy words such as words from other languages and words in utf-8 format. After collection of code-mixed tweets, some were rejected. There are three reasons for which a tweet was rejected.", "Most of the teams used the n-gram based features and it resulted in better macro average f-score. Most teams used the sklearn for identifying n-grams. IIITH-NBP team is only team to use character n-grams. Word embeddings is another important feature used by several teams. For word embeddings, Gensim and fastText are used. JU_KS team has used sentiment lexicon based features for BN-EN dataset only. BITMesra team has used emoji dictionary annotated with sentiment. Hashtags are considered to be one of the most important features for sentiment analysis BIBREF16 , however they removed hashtags during sentiment identification.", "BIT Mesra team submitted systems for only HI-EN dataset. During preprocessing, they removed words having UN language tags, URLs, hashtags and user mentions. An Emoji dictionary was prepared with sentiment tags. Finally, they used SVM and Na\u00efve Bayes classifiers on uni-gram and bi-gram features to classify sentiment of the code-mixed HI-EN dataset only.", "The baseline systems achieved better scores compared to CEN@AMRIT and SVNIT teams for HI-EN dataset; and AMRITA_CEN, CEN@Amrita and SVNIT teams for BN-EN dataset. IIIT-NBP team has achieved the maximum macro average f-score of 0.569 across all the sentiment classes for HI-EN dataset. IIIT-NBP also achieved the maximum macro average f-score of 0.526 for BN-EN dataset. Two way classification of HI-EN dataset achieved the maximum macro average f-score of 0.707, 0.666, and 0.663 for positive, negative, and neutral, respectively. Similarly, the two way classification of BN-EN dataset achieved the maximum average f-score of 0.641, 0.677, and 0.621 for positive, negative, and neutral, respectively."]}
{"question_id": "a222dc5d804a7b453a0f7fbc1d6c1b165a3ccdd6", "predicted_answer": "The Semantic Web (SW) is a concept that refers to the extension of the World Wide Web to enable machines to understand and process the meaning of information on the web. It aims to create a web of data that can be easily shared, accessed, and processed by machines, allowing for more intelligent and automated processing of information. The Semantic Web is built on top of existing web technologies, such as HTML and XML, and uses additional technologies like RDF (Resource Description Framework), OWL (Web Ontology Language), and SPARQL (SPARQL Protocol and RDF Query Language) to provide a common framework for data to be shared and reused across different applications and systems.", "predicted_evidence": ["OWL BIBREF15 is the de-facto standard for machine processable and interoperable ontologies on the SW. In its second version, OWL is equivalent to the description logic $\\mathcal {SROIQ}(D)$. Such expressiveness has a higher computational cost but allows the development of interesting applications such as automated reasoning BIBREF16. OWL 2 ontologies consist of the following three different syntactic categories:", "The lexicalization of RDF triples must be able to deal with resources, classes, properties and literals.", "Despite the plethora of recent works written on handling RDF data, only a few have exploited the generation of NL from OWL and SPARQL. For instance, Androutsopoulos2013 generates sentences in English and Greek from OWL ontologies. Also, SPARQL2NL BIBREF14 uses rules to verbalize atomic constructs and combine their verbalization into sentences. Therefore, our goal with LD2NL is to provide a complete framework to verbalize SW concepts rather than become the state of the art on the respective tasks.", "OWL 2 ontologies consist of Entities, Expressions and Axioms as introduced in subsec:owl. While both expressions and axioms can be mapped to RDF, i.e. into a set of RDF triples, using this mapping and applying the triple-based verbalization on it would lead to a non-human understandable text in many cases. For example, the intersection of two classes :A and :B can be represented in RDF by the six triples", "As we described in sec:owl, OWL axioms can roughly be categorized into terminological and assertional axioms. Therefore, we have different procedures for processing each category:"]}
{"question_id": "7de0b2df60d3161dd581ed7915837d460020bc11", "predicted_answer": "The ENE tag set includes 200 labels.", "predicted_evidence": ["In the next step, we hired annotators to label the collected articles using our desired tag set (ENEs). Initially, they were instructed to look through the tag set labels and learn them. At the annotation time, we asked them to pick at most 6 labels from the 200 suggested ENE labels and we recorded the annotations for all the collected articles. Although annotators were allowed to choose up to 6 annotations, the final set of annotations showed a maximum of 5 annotations per article.", "Table TABREF3 contains the total number of annotated articles in each of the languages as well as the total number of ENE classes with at lease one article annotated in that class, the average number of articles collected in each of the classes, and the average number of annotations assigned to each article by the human annotators.", "Once the annotations were all collected, we started collecting the content of the same article titles in the English, French, German, and Farsi Wikipedias, relying only on Wikipedia language links. Language links essentially link the articles about the exact same topic from one language to another. We were allowed to use the labels assigned to the Japanese version of the articles to all the articles in the other four languages (in case any existed), since ENEs were designed in a language agnostic manner and the pages were presenting the exact same topic.", "Although providing useful insights, none of the works above have considered the multi-lingual nature of many Wikipedia articles. Hence, we decided to hire annotators and educate them on the Extended Named Entities (ENE) tag set to annotate each article with up to 6 different ENE classes, and exploit the Wikipedia language links in the annotated articles to create our multi-lingual Wikipedia classification dataset. Section 2 details our dataset creation process.", "Considering the mentioned problem requirements, we believe Extended Named Entities Hierarchy BIBREF8, containing 200 fine-grained categories tailored for Wikipedia articles, is the best fitting tag set."]}
{"question_id": "0a3a7e412682ce951329c37b06343d2114acad9d", "predicted_answer": "No, the paper does not evaluate the dataset for smaller NE (Named Entity) tag tests. The paper mentions that classes with a frequency less than 20 in the dataset have been ignored in the train/test procedure, but it does not provide any evaluation results for smaller NE tag tests.", "predicted_evidence": ["In the next step, we hired annotators to label the collected articles using our desired tag set (ENEs). Initially, they were instructed to look through the tag set labels and learn them. At the annotation time, we asked them to pick at most 6 labels from the 200 suggested ENE labels and we recorded the annotations for all the collected articles. Although annotators were allowed to choose up to 6 annotations, the final set of annotations showed a maximum of 5 annotations per article.", "We have performed the evaluation in a 10 fold cross validation manner in each fold of which 80% of the data has been used for training, 10% for validation and model selection, and 10% for testing. In addition, classes with a frequency less than 20 in the dataset have been ignored in the train/test procedure.", "Table TABREF11 depicts the benchmarked micro-averaged precision of classification prediction of the articles in the Shinra Dataset. The results initially demonstrate that the dataset is not a super easy one as the Binary Logistic Regression model is not achieving very high accuracy scores. Besides, the lower scores for Japanese in comparison to the other languages is demonstrating the higher difficulty of classification of the larger number of classes for all the models.", "The evaluation measure would then be the micro-averaged precision BIBREF14 of the predicted labels. In addition, to prevent the domination of more frequent classes on the training procedure, we suggest weighted gradient back-propagation. The back-propagation weight of each article would be calculated using $w = \\frac{N}{\\sum _{n=1}^{N}{f(l_n)}}$ where $N$ is the number of labels assigned to the article (with a maximum of 6) and $f(l_n)$ counts the total train-set articles to which label $l_n$ has been assigned. The loss function used for training all the models has been Binary Cross Entropy Loss averaged over all the possible classes.", "On the other hand, much larger categorization type sets such as Cyc-Taxonomy BIBREF5, Yago-Taxonomy BIBREF6, or Wikipedia's own taxonomy of categories BIBREF7 are not suitable for our task since the tags are not verifiable for annotators. In addition, taxonomies are not designed in a tree format, so some categories might have multiple super-categories and this would make the verification process much harder for the cases that the article is about multiple different topics."]}
{"question_id": "74cc0300e22f60232812019011a09df92bbec803", "predicted_answer": "No, the results are not reported only on English data. The text does not specify the language of the data, and it mentions the Internet Argumentation Corpus (IAC), which is a general corpus that can contain data in multiple languages. However, it is likely that the data is in English, given the context of the research and the references to previous work on English language corpora.", "predicted_evidence": ["For our experiments, we use only the response texts and assign a binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class. We did not use the responses with scores between -1 and 1 because they had a very weak Fact/Feeling assessment, which could be attributed to responses either containing aspects of both factual and feeling expression, or neither. The resulting set contains 3,466 fact and 2,382 feeling posts. We randomly partitioned the fact/feel responses into three subsets: a training set with 70% of the data (2,426 fact and 1,667 feeling posts), a development (tuning) set with 20% of the data (693 fact and 476 feeling posts), and a test set with 10% of the data (347 fact and 239 feeling posts). For the bootstrapping method, we also used 11,560 responses from the unannotated data.", "Figure FIGREF15 shows the distribution of syntactic forms (templates) among all of the high-precision patterns identified for each class during bootstrapping. The x-axes show the syntactic templates and the y-axes show the percentage of all patterns that had a specific syntactic form. Figure FIGREF15 counts each lexico-syntactic pattern only once, regardless of how many times it occurred in the data set. Figure FIGREF15 counts the number of instances of each lexico-syntactic pattern. For example, Figure FIGREF15 shows that the Adj Noun syntactic form produced 1,400 different patterns, which comprise 22.6% of the distinct patterns learned. Figure FIGREF15 captures the fact that there are 7,170 instances of the Adj Noun patterns, which comprise 17.8% of all patterns instances in the data set.", "Other types of language data also typically contains a mixture of subjective and objective sentences, e.g. Wiebe et al. wiebeetal2001a,wiebeetalcl04 found that 44% of sentences in a news corpus were subjective. Our work is also related to research on distinguishing subjective and objective text BIBREF33 , BIBREF34 , BIBREF14 , including bootstrapped pattern learning for subjective/objective sentence classification BIBREF15 . However, prior work has primarily focused on news texts, not argumentation, and the notion of objective language is not exactly the same as factual. Our work also aims to recognize emotional language specifically, rather than all forms of subjective language.", "The learning process in AutoSlog-TS has two phases. In the first phase, the pattern templates are applied to the texts exhaustively, so that lexico-syntactic patterns are generated for (literally) every instantiation of the templates that appear in the corpus. In the second phase, AutoSlog-TS uses the labels associated with the texts to compute statistics for how often each pattern occurs in each class of texts. For each pattern INLINEFORM0 , we collect P(factual INLINEFORM1 INLINEFORM2 ) and P(feeling INLINEFORM3 INLINEFORM4 ), as well as the pattern's overall frequency in the corpus.", "The IAC includes 10,003 Quote-Response (Q-R) pairs with annotations for factual vs. feeling argument style, across a range of topics. Figure FIGREF4 shows the wording of the survey question used to collect the annotations. Fact vs. Feeling was measured as a scalar ranging from -5 to +5, because previous work suggested that taking the means of scalar annotations reduces noise in Mechanical Turk annotations BIBREF26 . Each of the pairs was annotated by 5-7 annotators. For our experiments, we use only the response texts and assign a binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class. We did not use the responses with scores between -1 and 1 because they had a very weak Fact/Feeling assessment, which could be attributed to responses either containing aspects of both factual and feeling expression, or neither."]}
{"question_id": "865811dcf63a1dd3f22c62ec39ffbca4b182de31", "predicted_answer": "The most discriminating patterns analyzed are:\n\n1. Patterns with prepositional phrases (especially NP Prep) and passive voice verb phrases, which are more common in factual arguments.\n2. Expressions with adjectives and active voice verb phrases, which are more common in feeling arguments.\n3. Patterns containing the preposition \"of\", which are more common in factual arguments.\n4. Patterns with the preposition \"for\", which are more common in feeling arguments.\n\nThese patterns are considered discriminating because they are characteristic of one class (factual or feeling) and can help distinguish between the two classes.", "predicted_evidence": ["Table TABREF13 provides examples of patterns learned for each class that are characteristic of that class. We observe that patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases. In contrast, the patterns associated with feeling based arguments are often based on the speaker's own beliefs or claims, perhaps assuming that they themselves are credible BIBREF20 , BIBREF24 , or they involve assessment or evaluations of the arguments of the other speaker BIBREF29 . They are typically also very creative and diverse, which may be why it is hard to get higher accuracies for feeling classification, as shown by Table TABREF11 .", "We evaluate the effectiveness of the learned patterns by applying them to the test set of 586 posts (347 fact and 239 feeling posts, maintaining the original ratio of fact to feel data in train). We classify each post as factual or feeling using the same procedure as during bootstrapping: a post is labeled as factual or feeling if it matches at least three high-precision patterns for that category. If a document contains three patterns for both categories, then we leave it unlabeled. We ran the bootstrapping algorithm for four iterations.", "Figure FIGREF15 shows the distribution of syntactic forms (templates) among all of the high-precision patterns identified for each class during bootstrapping. The x-axes show the syntactic templates and the y-axes show the percentage of all patterns that had a specific syntactic form. Figure FIGREF15 counts each lexico-syntactic pattern only once, regardless of how many times it occurred in the data set. Figure FIGREF15 counts the number of instances of each lexico-syntactic pattern. For example, Figure FIGREF15 shows that the Adj Noun syntactic form produced 1,400 different patterns, which comprise 22.6% of the distinct patterns learned. Figure FIGREF15 captures the fact that there are 7,170 instances of the Adj Noun patterns, which comprise 17.8% of all patterns instances in the data set.", "Next, we further examine the NP Prep patterns since they are so prevalent. Figure FIGREF19 shows the percentages of the most frequently occurring prepositions found in the NP Prep patterns learned for each class. Patterns containing the preposition \"of\" make up the vast majority of prepositional phrases for both the fact and feel classes, but is more common in the fact class. In contrast, we observe that patterns with the preposition \u201cfor\u201d are substantially more common in the feel class than the fact class.", "For factual arguments, we see that patterns with prepositional phrases (especially NP Prep) and passive voice verb phrases are more common. Instantiations of NP Prep are illustrated by FC1, FC5, FC8, FC10 in Table TABREF13 . Instantiations of PassVP are illustrated by FC2 and FC4 in Table TABREF13 . For feeling arguments, expressions with adjectives and active voice verb phrases are more common. Almost every high probability pattern for feeling includes an adjective, as illustrated by every pattern except FE8 in Table TABREF13 . Figure FIGREF15 shows that three syntactic forms account for a large proportion of the instances of high-precision patterns in the data: Adj Noun, NP Prep, and ActVP."]}
{"question_id": "9e378361b6462034aaf752adf04595ef56370b86", "predicted_answer": "The bootstrapping methodology used to find new patterns is as follows:\n\n1. Start with an initial set of annotated data (Iter 0).\n2. Use the high-precision patterns learned from the annotated data to identify more factual and feeling texts from the unannotated posts.\n3. For each round of bootstrapping, match the current set of factual and feeling patterns against the unannotated texts, and assign posts that match at least 3 patterns associated with a given class to that class.\n4. Use the Bootstrapped Data Balancer to randomly select a balanced subset of the newly classified posts to maintain the same proportion of factual vs. feeling documents throughout the bootstrapping process.\n5. Add the new documents to the set of labeled documents, and repeat the bootstrapping process.\n6. Use the same threshold values to select new high-precision patterns for all iterations.\n\nThis process is repeated for four iterations, with the goal of learning additional patterns and improving the accuracy of the classification model.", "predicted_evidence": ["Table TABREF14 shows the number of patterns learned from the annotated data (Iter 0) and the number of new patterns added after each bootstrapping iteration. The first iteration dramatically increases the set of patterns, and more patterns are steadily added throughout the rest of bootstrapping process.", "The upper section of Table TABREF11 shows the Precision and Recall results for the patterns learned during bootstrapping. The Iter 0 row shows the performance of the patterns learned only from the original, annotated training data. The remaining rows show the results for the patterns learned from the unannotated texts during bootstrapping, added cumulatively. We show the results after each iteration of bootstrapping.", "The high-precision patterns are then used in the bootstrapping framework to identify more factual and feeling texts from the 11,561 unannotated posts, also from 4forums.com. For each round of bootstrapping, the current set of factual and feeling patterns are matched against the unannotated texts, and posts that match at least 3 patterns associated with a given class are assigned to that class. As shown in Figure FIGREF10 , the Bootstrapped Data Balancer then randomly selects a balanced subset of the newly classified posts to maintain the same proportion of factual vs. feeling documents throughout the bootstrapping process. These new documents are added to the set of labeled documents, and the bootstrapping process repeats. We use the same threshold values to select new high-precision patterns for all iterations.", "We evaluate the effectiveness of the learned patterns by applying them to the test set of 586 posts (347 fact and 239 feeling posts, maintaining the original ratio of fact to feel data in train). We classify each post as factual or feeling using the same procedure as during bootstrapping: a post is labeled as factual or feeling if it matches at least three high-precision patterns for that category. If a document contains three patterns for both categories, then we leave it unlabeled. We ran the bootstrapping algorithm for four iterations.", "Since the IAC data set contains a large number of unannotated debate forum posts, we embedd AutoSlog-TS in a bootstrapping framework to learn additional patterns. The flow diagram for the bootstrapping system is shown in Figure FIGREF10 ."]}
{"question_id": "667dce60255d8ab959869eaf8671312df8c0004b", "predicted_answer": "The patterns extracted that were correlated with emotional arguments include:\n\n* Patterns based on the speaker's own beliefs or claims, perhaps assuming that they themselves are credible\n* Assessment or evaluations of the arguments of the other speaker\n* Creative and diverse language, which may be why it is hard to get higher accuracies for feeling classification.\n\nThese patterns are characteristic of feeling-based arguments and are often used to express personal opinions or emotions, rather than relying on factual evidence.", "predicted_evidence": ["Table TABREF13 provides examples of patterns learned for each class that are characteristic of that class. We observe that patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases. In contrast, the patterns associated with feeling based arguments are often based on the speaker's own beliefs or claims, perhaps assuming that they themselves are credible BIBREF20 , BIBREF24 , or they involve assessment or evaluations of the arguments of the other speaker BIBREF29 . They are typically also very creative and diverse, which may be why it is hard to get higher accuracies for feeling classification, as shown by Table TABREF11 .", "In future work we aim to use these insights about argument structure to produce higher performing classifiers for identifying factual vs. feeling argument styles. We also hope to understand in more detail the relationship between these argument styles and the heurstic routes to persuasion and associated strategies that have been identified in previous work on argumentation and persuasion BIBREF2 , BIBREF0 , BIBREF4 .", "From the learned patterns, we derive some characteristic syntactic forms associated with the fact and feel that we use to discriminate between the classes. We observe distinctions between the way that different arguments are expressed, with respect to the technical and more opinionated terminologies used, which we analyze on the basis of grammatical forms and more direct syntactic patterns, such as the use of different prepositional phrases. Overall, we demonstrate how the learned patterns can be used to more precisely gather similarly-styled argument responses from a pool of unannotated responses, carrying the characteristics of factual and emotional argumentation style.", "The feeling responses may seem to lack argumentative merit, but previous work on argumentation describes situations in which such arguments can be effective, such as the use of emotive arguments to draw attention away from the facts, or to frame a discussion in a particular way BIBREF18 , BIBREF19 . Furthermore, work on persuasion suggest that feeling based arguments can be more persuasive in particular circumstances, such as when the hearer shares a basis for social identity with the source (speaker) BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 . However none of this work has documented the linguistic patterns that characterize the differences in these argument types, which is a necessary first step to their automatic recognition or classification. Thus the goal of this paper is to use computational methods for pattern-learning on conversational arguments to catalog linguistic expressions and stylistic properties that distinguish Factual from Emotional arguments in these on-line debate forums.", "Our research examines factual versus feeling argument styles, drawing on annotations provided in the Internet Argument Corpus (IAC) BIBREF6 . This corpus includes quote-response pairs that were manually annotated with respect to whether the response is primarily a factual or feeling based argument, as Section SECREF2 describes in more detail. Figure FIGREF1 provides examples of responses in the IAC (paired with preceding quotes to provide context), along with the response's factual vs. feeling label."]}
{"question_id": "d5e716c1386b6485e63075e980f80d44564d0aa2", "predicted_answer": "The patterns extracted that were correlated with factual arguments include:\n\n* Topic-specific terminology\n* Explanatory language\n* Argument phrases\n\nThese patterns are characteristic of factual arguments and distinguish them from feeling-based arguments.", "predicted_evidence": ["factual responses may try to bolster their argument by providing statistics related to a position, giving historical or scientific background, or presenting specific examples or data. There is clearly a relationship between a proposition being factual versus objective or veridical, although each of these different labelling tasks may elicit differences from annotators BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 .", "Table TABREF13 provides examples of patterns learned for each class that are characteristic of that class. We observe that patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases. In contrast, the patterns associated with feeling based arguments are often based on the speaker's own beliefs or claims, perhaps assuming that they themselves are credible BIBREF20 , BIBREF24 , or they involve assessment or evaluations of the arguments of the other speaker BIBREF29 . They are typically also very creative and diverse, which may be why it is hard to get higher accuracies for feeling classification, as shown by Table TABREF11 .", "In future work we aim to use these insights about argument structure to produce higher performing classifiers for identifying factual vs. feeling argument styles. We also hope to understand in more detail the relationship between these argument styles and the heurstic routes to persuasion and associated strategies that have been identified in previous work on argumentation and persuasion BIBREF2 , BIBREF0 , BIBREF4 .", "Our research examines factual versus feeling argument styles, drawing on annotations provided in the Internet Argument Corpus (IAC) BIBREF6 . This corpus includes quote-response pairs that were manually annotated with respect to whether the response is primarily a factual or feeling based argument, as Section SECREF2 describes in more detail. Figure FIGREF1 provides examples of responses in the IAC (paired with preceding quotes to provide context), along with the response's factual vs. feeling label.", "From the learned patterns, we derive some characteristic syntactic forms associated with the fact and feel that we use to discriminate between the classes. We observe distinctions between the way that different arguments are expressed, with respect to the technical and more opinionated terminologies used, which we analyze on the basis of grammatical forms and more direct syntactic patterns, such as the use of different prepositional phrases. Overall, we demonstrate how the learned patterns can be used to more precisely gather similarly-styled argument responses from a pool of unannotated responses, carrying the characteristics of factual and emotional argumentation style."]}
{"question_id": "1fd31fdfff93d65f36e93f6919f6976f5f172197", "predicted_answer": "The factual and feeling forum posts were annotated using a survey question (Figure FIGREF4) that asked annotators to rate the response on a scalar from -5 to +5, indicating the degree to which the response was factual or feeling-based. Each response was annotated by 5-7 annotators, and the mean of these annotations was used to reduce noise. The responses were then assigned a binary label: texts with a score \u2265 1 were labeled as \"fact\" and texts with a score \u2264 -1 were labeled as \"feeling\". Responses with scores between -1 and 1 were not used, as they had a weak Fact/Feeling assessment.", "predicted_evidence": ["feeling argument style, across a range of topics. Figure FIGREF4 shows the wording of the survey question used to collect the annotations. Fact vs. Feeling was measured as a scalar ranging from -5 to +5, because previous work suggested that taking the means of scalar annotations reduces noise in Mechanical Turk annotations BIBREF26 . Each of the pairs was annotated by 5-7 annotators. For our experiments, we use only the response texts and assign a binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class. We did not use the responses with scores between -1 and 1 because they had a very weak Fact/Feeling assessment, which could be attributed to responses either containing aspects of both factual and feeling expression, or neither. The resulting set contains 3,466 fact and 2,382 feeling posts.", "Our research examines factual versus feeling argument styles, drawing on annotations provided in the Internet Argument Corpus (IAC) BIBREF6 . This corpus includes quote-response pairs that were manually annotated with respect to whether the response is primarily a factual or feeling based argument, as Section SECREF2 describes in more detail. Figure FIGREF1 provides examples of responses in the IAC (paired with preceding quotes to provide context), along with the response's factual vs. feeling label.", "In this paper, we use observed differences in argumentation styles in online debate forums to extract patterns that are highly correlated with factual and emotional argumentation. From an annotated set of forum post responses, we are able extract high-precision patterns that are associated with the argumentation style classes, and we are then able to use these patterns to get a larger set of indicative patterns using a bootstrapping methodology on a set of unannotated posts.", "The IAC includes 10,003 Quote-Response (Q-R) pairs with annotations for factual vs. feeling argument style, across a range of topics. Figure FIGREF4 shows the wording of the survey question used to collect the annotations. Fact vs. Feeling was measured as a scalar ranging from -5 to +5, because previous work suggested that taking the means of scalar annotations reduces noise in Mechanical Turk annotations BIBREF26 . Each of the pairs was annotated by 5-7 annotators. For our experiments, we use only the response texts and assign a binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class.", "Figure FIGREF4 shows the wording of the survey question used to collect the annotations. Fact vs. Feeling was measured as a scalar ranging from -5 to +5, because previous work suggested that taking the means of scalar annotations reduces noise in Mechanical Turk annotations BIBREF26 . Each of the pairs was annotated by 5-7 annotators. For our experiments, we use only the response texts and assign a binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class. We did not use the responses with scores between -1 and 1 because they had a very weak Fact/Feeling assessment, which could be attributed to responses either containing aspects of both factual and feeling expression, or neither. The resulting set contains 3,466 fact and 2,382 feeling posts. We randomly partitioned the fact/feel responses into three subsets: a training set with 70% of the data (2,426 fact and 1,667 feeling posts), a development (tuning) set with 20% of the data (693 fact and 476 feeling posts), and a test set with 10% of the data (347 fact and 239 feeling posts)."]}
{"question_id": "d2d9c7177728987d9e8b0c44549bbe03c8c00ef2", "predicted_answer": "The evaluation metrics used in this study are:\n\n1. BLEU scores: used to compare the quantitative performance of the models.\n2. Qualitative analysis: performed by analyzing translated sentences and attention maps.\n\nNote that the study also uses ablation studies to evaluate the effect of different hyperparameters, such as the number of BPE tokens, on the performance of the models.", "predicted_evidence": ["The Fairseq(-py) toolkit was used to model the ConvS2S model BIBREF17 . Fairseq's named architecture \u201cfconv\u201d was used, with the default hyperparameters recommended by Fairseq documentation as follows: The learning rate was set to 0.25, a dropout of 0.2, and the maximum tokens for each mini-batch was set to 4000. The dataset was preprocessed using Fairseq's preprocess script to build the vocabularies and to binarize the dataset. To decode the test data, beam search was used, with a beam width of 5. For each language, a model was trained using traditional white-space tokenisation, as well as byte-pair encoding tokenisation (BPE). To appropriately select the number of tokens for BPE, for each target language, we performed an ablation study (described in Section SECREF25 ).", "Section SECREF9 describes the quantitative performance of the models by comparing BLEU scores, while a qualitative analysis is performed in Section SECREF10 by analysing translated sentences as well as attention maps. Section SECREF25 provides the results for an ablation study done regarding the effects of BPE.", "As can be seen in Figure FIGREF26 , the models for languages with the smallest datasets (namely isiZulu and N. Sotho) achieve higher BLEU scores when the number of BPE tokens is smaller, and decrease as the number of BPE tokens increases. In contrast, the performance of the models for languages with larger datasets (namely Setswana, Xitsonga, and Afrikaans) improves as the number of BPE tokens increases. There is a decrease in performance at 20 000 BPE tokens for Setswana and Afrikaans, which the authors cannot yet explain and require further investigation. The optimal number of BPE tokens were used for each language, as indicated in Table TABREF7 .", "Additional translation results from ConvS2S and Transformer are given in Table TABREF27 along with their back-translations for Afrikaans, N. Sotho, Setswana, and Xitsonga. We include these additional sentences as we feel that the single sentence provided per language in Section SECREF10 , is not enough demonstrate the capabilities of the models. Given the scarcity of research in this field, researchers might find the additional sentences insightful into understanding the real-world capabilities and potential, even if BLEU scores are low.", "Google Translate BIBREF2 , as of February 2019, provides translations for English, Afrikaans, isiZulu, isiXhosa and Southern Sotho, six of the official South African languages. Google Translate was tested with the Afrikaans and isiZulu test sets used in this paper to determine its performance. However, due to the uncertainty regarding how Google Translate was trained, and which data it was trained on, there is a possibility that the system was trained on the test set used in this study as this test set was created from publicly available governmental data. For this reason, we determined this system is not comparable to this paper's models for isiZulu and Afrikaans."]}
{"question_id": "6657ece018b1455035421b822ea2d7961557c645", "predicted_answer": "The NMT techniques explored in this paper are:\n\n1. ConvS2S\n2. Transformer", "predicted_evidence": ["We trained translation models for two established NMT architectures for each language, namely, ConvS2S and Transformer. As the purpose of this work is to provide a baseline benchmark, we have not performed significant hyperparameter optimization, and have left that as future work.", "This paper aims to address some of the above problems as follows: We trained models to translate English to Afrikaans, isiZulu, N. Sotho, Setswana and Xitsonga, using modern NMT techniques. We have published the code, datasets and results for the above experiments on GitHub, and in doing so promote reproducibility, ensure discoverability and create a baseline leader board for the five languages, to begin to address the lack of benchmarks.", "African languages are numerous and low-resourced. Existing datasets and research for machine translation are difficult to discover, and the research hard to reproduce. Additionally, very little attention has been given to the African languages so no benchmarks or leader boards exist, and few attempts at using popular NMT techniques exist for translating African languages.", "This paper reviewed existing research in machine translation for South African languages and highlighted their problems of discoverability and reproducibility. In order to begin addressing these problems, we trained models to translate English to five South African languages, using modern NMT techniques, namely ConvS2S and Transformer. The results were promising for the languages that have more higher quality data (Xitsonga, Setswana, Afrikaans), while there is still extensive work to be done for isiZulu and N. Sotho which have exceptionally little data and the data is of worse quality. Additionally, an ablation study over the number of BPE tokens was performed for each language. Given that all data and code for the experiments are published on GitHub, these benchmarks provide a starting point for other researchers to find, compare and build upon.", "Overall, we notice that the performance of the NMT techniques on a specific target language is related to both the number of parallel sentences and the morphological typology of the language. In particular, isiZulu, N. Sotho, Setswana, and Xitsonga languages are all agglutinative languages, making them harder to translate, especially with very little data BIBREF22 . Afrikaans is not agglutinative, thus despite having less than half the number of parallel sentences as Xitsonga and Setswana, the Transformer model still achieves reasonable performance. Xitsonga and Setswana are both agglutinative, but have significantly more data, so their models achieve much higher performance than N. Sotho or isiZulu."]}
{"question_id": "175cddfd0bcd77b7327b62f99e57d8ea93f8d8ba", "predicted_answer": "The best performing model is not explicitly stated in the given context. However, it can be inferred that the Transformer model performed reasonably well for Afrikaans, and the models for Xitsonga and Setswana achieved much higher performance than N. Sotho or isiZulu, likely due to the larger amount of data available for these languages.", "predicted_evidence": ["Overall, we notice that the performance of the NMT techniques on a specific target language is related to both the number of parallel sentences and the morphological typology of the language. In particular, isiZulu, N. Sotho, Setswana, and Xitsonga languages are all agglutinative languages, making them harder to translate, especially with very little data BIBREF22 . Afrikaans is not agglutinative, thus despite having less than half the number of parallel sentences as Xitsonga and Setswana, the Transformer model still achieves reasonable performance. Xitsonga and Setswana are both agglutinative, but have significantly more data, so their models achieve much higher performance than N. Sotho or isiZulu.", "Lack of benchmarks: Due to the low discoverability and the lack of research in the field, there are no publicly available benchmarks or leader boards to new compare machine translation techniques to.", "Additional translation results from ConvS2S and Transformer are given in Table TABREF27 along with their back-translations for Afrikaans, N. Sotho, Setswana, and Xitsonga. We include these additional sentences as we feel that the single sentence provided per language in Section SECREF10 , is not enough demonstrate the capabilities of the models. Given the scarcity of research in this field, researchers might find the additional sentences insightful into understanding the real-world capabilities and potential, even if BLEU scores are low.", "The translation models for isiZulu achieved the worst performance when compared to the others, with the maximum BLEU score of 3.33. We attribute the bad performance to the morphological complexity of the language (as discussed in Section SECREF3 ), the very small size of the dataset as well as the poor quality of the data (as discussed in Section SECREF4 ).", "Future work involves improving the current datasets, specifically the isiZulu dataset, and thus improving the performance of the current machine translation models."]}
{"question_id": "f0afc116809b70528226d37190e8e79e1e9cd11e", "predicted_answer": "The datasets used are the Autshumato parallel corpora, which are aligned corpora of South African governmental data created for use in machine translation systems.", "predicted_evidence": ["The source code and the data used are available at https://github.com/LauraMartinus/ukuxhumana.", "The official Autshumato datasets contain many duplicates, therefore to avoid data leakage between training, development and test sets, all duplicate sentences were removed. These clean datasets were then split into 70% for training, 30% for validation, and 3000 parallel sentences set aside for testing. Summary statistics for each dataset are shown in Table TABREF2 , highlighting how small each dataset is.", "Reproducibility: The data and code of existing research are rarely shared, which means researchers cannot reproduce the results properly. Examples of papers that do not publicly provide their data and code are described in Section SECREF4 .", "The Fairseq(-py) toolkit was used to model the ConvS2S model BIBREF17 . Fairseq's named architecture \u201cfconv\u201d was used, with the default hyperparameters recommended by Fairseq documentation as follows: The learning rate was set to 0.25, a dropout of 0.2, and the maximum tokens for each mini-batch was set to 4000. The dataset was preprocessed using Fairseq's preprocess script to build the vocabularies and to binarize the dataset. To decode the test data, beam search was used, with a beam width of 5. For each language, a model was trained using traditional white-space tokenisation, as well as byte-pair encoding tokenisation (BPE). To appropriately select the number of tokens for BPE, for each target language, we performed an ablation study (described in Section SECREF25 ).", "The publicly-available Autshumato parallel corpora are aligned corpora of South African governmental data which were created for use in machine translation systems BIBREF15 . The datasets are available for download at the South African Centre for Digital Language Resources website. The datasets were created as part of the Autshumato project which aims to provide access to data to aid in the development of open-source translation systems in South Africa."]}
{"question_id": "3588988f2230f3329d7523fbb881b20bf177280d", "predicted_answer": "The three OWL ontologies used in the experiment are:\n\n1. The Wine Ontology\n2. The Consumer Electronics Ontology\n3. The Disease Ontology", "predicted_evidence": ["The new form of the ontology was produced automatically, using patterns that searched the definition strings for relation names (e.g., results_in), sentence breaks, and words introducing secondary clauses (e.g., \u201cthat\u201d, \u201cwhich\u201d). Some sentences of the original definition strings that did not include declared relation names (e.g., \u201cThe virus affects...and humans\u201d in the `Definition' string of Rift Valley Fever) were discarded, because they could not be automatically converted to appropriate owl statements.", "We experimented with three owl ontologies: (1) the Wine Ontology, which provides information about wines, wine producers etc.; (2) the Consumer Electronics Ontology, intended to help exchange information about consumer electronics products; and (3) the Disease Ontology, which describes diseases, including their symptoms, causes etc. The Wine Ontology is one of the most commonly used examples of owl ontologies and involves a wide variety of owl constructs; hence, it is a good test case for systems that produce texts from owl. The Consumer Electronics and Disease Ontologies were constructed by biomedical and e-commerce experts to address real-life information needs; hence, they constitute good real-world test cases from different domains.", "An ontology provides a conceptualization of a knowledge domain (e.g., consumer electronics, diseases) by defining the classes and subclasses of the individuals (entities) in the domain, the possible relations between them etc. The current standard to specify Semantic Web ontologies is owl BIBREF11 , BIBREF12 , a formal language based on description logics BIBREF13 , rdf, and rdf schema BIBREF14 . Given an owl ontology for a knowledge domain, one can publish on the Web machine-readable statements about the domain (e.g., available products, known diseases, their features or symptoms), with the statements having formally defined semantics based on the ontology. nlg can then produce texts describing classes or individuals of the ontology (e.g., product descriptions, information about diseases) from the same statements. This way the same information becomes more easily accessible to both computers (which read the machine-readable statements) and end-users (who read the texts), which is one of the main goals of the Semantic Web.", "An ontology provides a conceptualization of a knowledge domain (e.g., consumer electronics, diseases) by defining the classes and subclasses of the individuals (entities) in the domain, the possible relations between them etc. The current standard to specify Semantic Web ontologies is owl BIBREF11 , BIBREF12 , a formal language based on description logics BIBREF13 , rdf, and rdf schema BIBREF14 . Given an owl ontology for a knowledge domain, one can publish on the Web machine-readable statements about the domain (e.g., available products, known diseases, their features or symptoms), with the statements having formally defined semantics based on the ontology. nlg can then produce texts describing classes or individuals of the ontology (e.g., product descriptions, information about diseases) from the same statements.", "The experiments of the previous section tested the ability of ilpnlgextend to take into account the different lengths of nl names and the fact that some facts $\\left<S,R,O\\right>$ involve conjunctions (or disjunctions) in their $O$ . They did not, however, test the ability of ilpnlgextend to cope with multiple alternative nl names per individual or class. The Consumer Electronics and Disease Ontologies were inappropriate in this respect, because the names of electronic products tend to be unique and we did not have the expertise to create alternative names of diseases, as already noted. Instead, we returned to the Wine Ontology, which had been used in Section \"Experiments with the Wine Ontology\" with a single nl name per individual and class."]}
{"question_id": "78f8dad0f1acf024f69b3218b2d204b8019bb0d2", "predicted_answer": "According to the text, user satisfaction is estimated by collecting ratings from crowdworkers on a scale of 1-5 on the quality of each response given by the chatbot at the end of their own responses. This is done in a simulated deployment environment on a crowdsourcing platform, where workers are told to \"Talk as you would to a person you have just met.\"", "predicted_evidence": ["Table TABREF22 reports the maximum F1 scores achieved by each method on the Satisfaction test set. For the model uncertainty approach, we tested two variants: (a) predict a mistake when the confidence in the top rated response is below some threshold INLINEFORM0 , and (b) predict a mistake when the gap between the top two rated responses is below the threshold INLINEFORM1 . We used the best-performing standalone Dialogue model (one trained on the full 131k training examples) for assessing uncertainty and tuned the thresholds to achieve maximum F1 score. For the user satisfaction approach, we trained our dialogue agent on just the Satisfaction task. Finally, we also report the performance of a regular-expression-based method which we used during development, based on common ways of expressing dissatisfaction that we observed in our pilot studies, see Appendix SECREF12 for details.", "The objective of the Satisfaction auxiliary task is to predict whether or not a speaking partner is satisfied with the quality of the current conversation. Examples take the form of INLINEFORM0 pairs, where INLINEFORM1 is the same context as in the Dialogue task, and INLINEFORM2 , ranging from dissatisfied to satisfied. Crucially, it is hard to estimate from the bot's utterance itself whether the user will be satisfied, but much easier using the human's response to the utterance, as they may explicitly say something to that effect, e.g. \u201cWhat are you talking about?\u201d.", "Throughout this section, we use the ranking metric hits@X/Y, or the fraction of the time that the correct candidate response was ranked in the top X out of Y available candidates; accuracy is another name for hits@1/Y. Statistical significance for improvement over baselines is assessed with a two-sample one-tailed T-test.", "*(?:said|asked|told).*\" r\"((not|nt|n't).*mak.*sense)|(mak.*no .*sense)\" r\"u(m|h)+\\W\" r\"you.*what\\?\" r\"what.*you (?:mean|refer|talk).*\\?\" r\"what.*to do with.*\\?\" Hyperparameters Data Collection Interfaces Deployment Data Collection We simulated a deployment environment on a crowdsourcing platform with the following interface. Crowdworkers were told to \u201cTalk as you would to a person you have just met.\u201d Satisfaction Data Collection The environment for collecting Satisfaction ratings was very similar to the deployment environment, with the exception that workers were required to give a rating 1-5 on the quality of each response given by the chatbot at the end of their own responses. Sample Deployment Conversations A sample of the conversations that occurred between a Bot (B) and Human (H) during the collection of the deployment datasets (more specifically, feedback set B).", "Crowdworkers were told to \u201cTalk as you would to a person you have just met.\u201d Satisfaction Data Collection The environment for collecting Satisfaction ratings was very similar to the deployment environment, with the exception that workers were required to give a rating 1-5 on the quality of each response given by the chatbot at the end of their own responses. Sample Deployment Conversations A sample of the conversations that occurred between a Bot (B) and Human (H) during the collection of the deployment datasets (more specifically, feedback set B). The self-feeding chatbot used its satisfaction classifier trained on 1k examples to determine when to ask for feedback. The dialogue model at this point had only been trained on 20k HH Dialogue examples and 20k Feedback examples, so it still makes frequent mistakes. See Appendix SECREF30 for a snapshot of the actual interface. B: start a conversation with the chatbot."]}
{"question_id": "73a5783cad4ed468a8dbb31b5de2c618ce351ad1", "predicted_answer": "The performance improved by 0.4 points, which is a statistically significant improvement (p = 0.027) and reduces the gap to a model trained on fully supervised (HH) Dialogue examples by 17%.", "predicted_evidence": ["We also found that \u201cfresher\u201d feedback results in bigger gains. We compared two models trained on 20k HH Dialogue examples and 40k Feedback examples\u2014the first collected all 40k Feedback examples at once, whereas the second was retrained with its first 20k Feedback examples before collecting the remaining 20k. While the absolute improvement of the second model over the first was small (0.4 points), it was statistically significant ( INLINEFORM0 0.027) and reduced the gap to a model trained on fully supervised (HH) Dialogue examples by 17% while modifying only 33% of the training data. This improvement makes sense intuitively, since new Feedback examples are collected based on failure modes of the current model, making them potentially more efficient in a manner similar to new training examples selected via active learning. It also suggests that the gains we observe in Table TABREF16 might be further improved by (a) collecting Feedback examples specific to each model (rather than using the same 60k Feedback examples for all models), and (b) more frequently retraining the MTL model (e.g., every 5k examples instead of every 20k) or updating it in an online manner.", "While the absolute improvement of the second model over the first was small (0.4 points), it was statistically significant ( INLINEFORM0 0.027) and reduced the gap to a model trained on fully supervised (HH) Dialogue examples by 17% while modifying only 33% of the training data. This improvement makes sense intuitively, since new Feedback examples are collected based on failure modes of the current model, making them potentially more efficient in a manner similar to new training examples selected via active learning. It also suggests that the gains we observe in Table TABREF16 might be further improved by (a) collecting Feedback examples specific to each model (rather than using the same 60k Feedback examples for all models), and (b) more frequently retraining the MTL model (e.g., every 5k examples instead of every 20k) or updating it in an online manner. We leave further exploration of this observation for future work.", "Throughout this section, we use the ranking metric hits@X/Y, or the fraction of the time that the correct candidate response was ranked in the top X out of Y available candidates; accuracy is another name for hits@1/Y. Statistical significance for improvement over baselines is assessed with a two-sample one-tailed T-test.", "We also found that \u201cfresher\u201d feedback results in bigger gains. We compared two models trained on 20k HH Dialogue examples and 40k Feedback examples\u2014the first collected all 40k Feedback examples at once, whereas the second was retrained with its first 20k Feedback examples before collecting the remaining 20k. While the absolute improvement of the second model over the first was small (0.4 points), it was statistically significant ( INLINEFORM0 0.027) and reduced the gap to a model trained on fully supervised (HH) Dialogue examples by 17% while modifying only 33% of the training data. This improvement makes sense intuitively, since new Feedback examples are collected based on failure modes of the current model, making them potentially more efficient in a manner similar to new training examples selected via active learning.", "These were split into 1k train, 500 validation, and 1k test examples. The agent was retrained using the 20k HH Dialogue examples and 1k Satisfaction examples, then deployed to collect the first batch of deployment examples. We collected 40k Feedback examples (feedback set A) over the course of 17,250 conversations with 10 turns each (20 utterances, including the initial prompt). We then retrained the agent on all three datasets, using the same 20k HH Dialogue examples as before and only 20k of the available 40k Feedback examples. This model was deployed to collect another 20k Feedback examples (feedback set B), for a total of 60k Feedback examples (A + B). In Table TABREF16 we use these 60k Feedback examples interchangeably; in Appendix SECREF11 we compare them head-to-head. The 60k HB Dialogue examples were extracted from the logs of the deployment conversations."]}
{"question_id": "1128a600a813116cba9a2cf99d8568ae340f327a", "predicted_answer": "The datasets used in the experiment are:\n\n1. 14 product review datasets collected from Amazon, covering different domains such as Books, DVDs, Electronics, and Kitchen, etc. The goal is to classify a product review as either positive or negative.\n2. 2 sub-datasets about movie reviews.\n\nThese 16 datasets are used for the classification task.", "predicted_evidence": ["We first conduct our experiment on classification tasks.", "The remaining two datasets are two sub-datasets about movie reviews.", "Figure 5 shows the learning curves of various multi-task model on the 16 classification datasets.", "For classification task, we test our model on 16 classification datasets, the first 14 datasets are product reviews that collected based on the dataset, constructed by BIBREF27 , contains Amazon product reviews from different domains: Books, DVDs, Electronics and Kitchen and so on. The goal in each domain is to classify a product review as either positive or negative. The datasets in each domain are partitioned randomly into training data, development data and testing data with the proportion of 70%, 10% and 20% respectively. The detailed statistics are listed in Table 1 .", "The word embeddings for all of the models are initialized with the 200d GloVe vectors (6B token version, BIBREF32 ) and fine-tuned during training to improve the performance. The mini-batch size is set to 16. The final hyper-parameters are set as Table 2 ."]}
{"question_id": "d64fa192a7e9918c6a22d819abad581af0644c7d", "predicted_answer": "The answer is not explicitly stated in the given context, but it can be inferred that the new tasks used to show the transferring ability of the shared meta-knowledge are the unseen new tasks mentioned in the first sentence. The model is able to transfer knowledge across domains or tasks, and the meta network can be used for these new tasks, resulting in an average improvement of 3.1% over the average accuracy of single models.", "predicted_evidence": ["The meta network can be considered as off-the-shelf knowledge and then be used for unseen new tasks.", "Different from these models, our model captures the function-level sharing information, in which a meta-network captures the meta-knowledge across tasks and controls the parameters of task-specific networks.", "In this paper, we take a very different multi-task architecture from meta-learning perspective BIBREF25 . One goal of meta-learning is to find efficient mechanisms to transfer knowledge across domains or tasks BIBREF26 .", "The results are also shown in the last column of Table 3 . With the help of meta knowledge, we observe an average improvement of $3.1\\%$ over the average accuracy of single models, and even better than other competitor multi-task models. This observation indicates that we can save the meta knowledge into a meta network, which is quite useful for a new task.", "For multi-task learning, we can assign a basic network to each task, while sharing a meta network among tasks. The meta network captures the meta (shared) knowledge of different tasks. The meta network can learn at the \u201cmeta-level\u201d of predicting parameters for the basic task-specific network."]}
{"question_id": "788f70a39c87abf534f4a9ee519f6e5dbf2543c2", "predicted_answer": "The meta learning algorithm used is Meta-LSTMs.", "predicted_evidence": ["For multi-task learning, we can assign a basic network to each task, while sharing a meta network among tasks. The meta network captures the meta (shared) knowledge of different tasks. The meta network can learn at the \u201cmeta-level\u201d of predicting parameters for the basic task-specific network.", "The meta network can be considered as off-the-shelf knowledge and then be used for unseen new tasks.", "In this paper, we take a very different multi-task architecture from meta-learning perspective BIBREF25 . One goal of meta-learning is to find efficient mechanisms to transfer knowledge across domains or tasks BIBREF26 .", "With the meta network, our model can use quite a few parameters to achieve the state-of-the-art performances.", "For single-task learning, we compare our Meta-LSTMs with three models."]}
{"question_id": "3d1ad8a4aaa2653d0095bafba74738bd20795acf", "predicted_answer": "The datasets used are:\n\n1. A manually labeled text toxicity dataset of 300 comments from YouTube videos about the Ferguson unrest in 2014, consisting of 138 hate speech comments and 162 non-hate speech comments.\n2. A dataset of 3000 tweets, with 1430 labeled as hate speech and 1670 randomly sampled from the remaining tweets.\n3. The English dataset from the SemEval task \"Multilingual detection of hate speech against immigrants and women in Twitter (hatEval)\", consisting of 100 tweets labeled as hate speech or not hate speech.", "predicted_evidence": ["data set is a manually labeled text toxicity data, originally containing 1000 comments crawled from YouTube videos about the Ferguson unrest in 2014. Apart from the main label describing if the comment is hate speech, there are several other labels characterizing each comment, e.g., if it is a threat, provocative, racist, sexist, etc. (not used in our study). There are 138 comments labeled as a hate speech and 862 as non-hate speech. We produced a data set of 300 comments using all 138 hate speech comments and randomly sampled 162 non-hate speech comments.", "We use three data sets related to the hate speech.", "data set originates in a study regarding hate speech detection and the problem of offensive language BIBREF3. Our data set consists of 3000 tweets. We took 1430 tweets labeled as hate speech and randomly sampled 1670 tweets from the collection of remaining 23353 tweets.", "data set is taken from the SemEval task \"Multilingual detection of hate speech against immigrants and women in Twitter (hatEval)\". The competition was organized for two languages, Spanish and English; we only processed the English data set. The data set consists of 100 tweets labeled as 1 (hate speech) or 0 (not hate speech).", "Results for YouToxic and OffensiveTweets data sets are presented in Table TABREF32 (middle) and (right), respectively. Similarly to the HatEval data set, there is a difference between the logistic regression and the two LSTM models using the word2vec embeddings. For all data sets, the results with ELMo embeddings are similar across the four classifiers."]}
{"question_id": "ec54ae2f4811196fcaafa45e76130239e69995f9", "predicted_answer": "There is no mention of a \"baseline\" in the given context. The text describes a method for visualizing and analyzing the predictions of a neural network using Monte Carlo dropout, but it does not mention a baseline.", "predicted_evidence": ["In the next step, we overlay the obtained representation with other relevant information, obtained during sampling. Individual points (documents) are assigned the mean probabilities of samples, thus representing the reliability of individual predictions. We discretize the $[0,1]$ probability interval into four bins of equal size for readability purposes. Next, we shape individual points according to the correctness of predictions. We take into account four possible outcomes (TP - true positives, FP - false positives, TN - true negatives, FN - false negatives).", "The obtained estimations are plotted alongside individual predictions and represent densities of the neural network's focus, which can be inspected from the point of view of correctness and reliability.", "Common typos are corrected and typical contractions and hash-tags are expanded.", "where $\\hat{\\omega }_k$ $\\sim $ $q(\\omega )$. Thus, collecting information in $K$ dropout passes throughout the network during the training phase is used in the testing phase to generate (sample) $K$ predicted values for each of the test instance. The benefit of such results is not only to obtain more accurate prediction estimations but also the possibility to visualize the test instances within the generated outcome space.", "Kernel density estimation of the projected space \u2014 this step attempts to summarize the instance-level samples obtained by the MCD neural network."]}
{"question_id": "5102dc911913e9ca0311253e44fd31c73eed0a57", "predicted_answer": "The text embedding methods used in their approach are:\n\n1. Sparse TF-IDF\n2. Dense word2vec\n3. ELMo (pretrained model from TensorFlow Hub)\n4. Sentence embeddings (not specified which method, but mentioned as a separate category from word embeddings)", "predicted_evidence": ["We first describe experiments comparing different word representations, followed by sentence embeddings, and finally the visualization of predictive uncertainty.", "evaluation of different contextual embedding approaches in the area of hate speech,", "In the first set of experiments, we represented the text with word embeddings (sparse TF-IDF BIBREF31 or dense word2vec BIBREF32, and ELMo BIBREF33). We utilise the gensim library BIBREF34 for word2vec model, the scikit-learn for TFIDF, and the ELMo pretrained model from TensorFlow Hub. We compared different classification models using these word embeddings. The results are presented in Table TABREF32.", "In the second set of experiments, we compared different classifiers using sentence embeddings BIBREF35 as the representation. Table TABREF36 (left) displays results for HatEval. We can notice improvements in classification accuracy for all classifiers compared to the word embedding representation in Table TABREF32. The best model for this small data set is MCD LSTM. For larger YouToxic and OffensiveTweets data sets, all the models perform comparably. Apart from the prediction accuracy the four models were compared using precision, recall and F1 score BIBREF36.", "Prediction uncertainty estimation is rarely implemented for text classification and other NLP tasks, hence our future work will go in this direction. A recent emergence of cross-lingual embeddings possibly opens new opportunities to share data sets and models between languages. As evaluation in rare languages is difficult, the assessment of predictive reliability for such problems might be an auxiliary evaluation approach. In this context, we also plan to investigate convolutional neural networks with probabilistic interpretation."]}
{"question_id": "5752c8d333afc1e6c666b18d1477c8f669b7a602", "predicted_answer": "Yes, they compare their model's performance against state-of-the-art models.", "predicted_evidence": ["For each task INLINEFORM0 , we conduct TOS with INLINEFORM1 to improve its performance. After training our model on the generated sample collections, we evaluate the performance of task INLINEFORM2 by comparing INLINEFORM3 and INLINEFORM4 on the test set. We apply 10-fold cross-validation and different combinations of hyperparameters are investigated, of which the best one, as shown in Table TABREF41 , is reserved for comparisons with state-of-the-art models.", "As Table TABREF48 shows, our model obtains competitive or better performances on all tasks except for the QC dataset, as it contains poor correlations with other tasks. MT-RNN slightly outperforms our model on SST, as sentences from this dataset are much shorter than those from IMDB and MDSD, and another possible reason may be that our model are more complex and requires larger data for training. Our model proposes the designs of various interactions including coupling, local and global fusion, which can be further implemented by other state-of-the-art models and produce better performances.", "Multi-Domain Tasks involve contents of different domains, for example, product review datasets on books, DVDs, electronics and kitchen appliances.", "Multi-Cardinality Tasks are similar except for cardinality parameters, for example, movie review datasets with different average sequence lengths and class numbers.", "In order to measure the correlation strength between two task INLINEFORM0 and INLINEFORM1 , we learn them jointly with our model and define Pair-wise Performance Gain as INLINEFORM2 , where INLINEFORM3 are the performances of tasks INLINEFORM4 and INLINEFORM5 when learned individually and jointly."]}
{"question_id": "fcdafaea5b1c9edee305b81f6865efc8b8dc50d3", "predicted_answer": "The benchmark datasets mentioned are:\n\n1. IMDB\n2. RN (likely referring to Reuters News)\n3. QC (likely referring to Question Classification)\n\nThese datasets are used for text classification tasks, specifically for multi-objective classification and multi-cardinality tasks.", "predicted_evidence": ["As Table TABREF35 shows, we select five benchmark datasets for text classification and design three experiment scenarios to evaluate the performances of our model.", "Figure FIGREF45 shows the performances of datasets in Multi-Domain scenario with different INLINEFORM0 . Compared to INLINEFORM1 , our model can achieve considerable improvements when INLINEFORM2 as more samples combinations are available. However, there are no more salient gains as INLINEFORM3 gets larger and potential noises from other tasks may lead to performance degradations. For a trade-off between efficiency and effectiveness, we determine INLINEFORM4 as the optimal value for our experiments.", "For each task INLINEFORM0 , we conduct TOS with INLINEFORM1 to improve its performance. After training our model on the generated sample collections, we evaluate the performance of task INLINEFORM2 by comparing INLINEFORM3 and INLINEFORM4 on the test set. We apply 10-fold cross-validation and different combinations of hyperparameters are investigated, of which the best one, as shown in Table TABREF41 , is reserved for comparisons with state-of-the-art models.", "Multi-Objective Classification datasets with different objectives, including IMDB, RN BIBREF17 and QC BIBREF18 .", "Multi-Cardinality Tasks are similar except for cardinality parameters, for example, movie review datasets with different average sequence lengths and class numbers."]}
{"question_id": "91d4fd5796c13005fe306bcd895caaed7fa77030", "predicted_answer": "The models are trained on several tasks, including:\n\n1. QC (no specific description provided)\n2. SST (sentences are shorter than those in IMDB and MDSD)\n3. IMDB\n4. MDSD", "predicted_evidence": ["Input sequences of each task are transformed into vector representations INLINEFORM0 , which are later recurrently fed into the corresponding Single Layers. The hidden states at the last time step INLINEFORM1 of each Single Layer can be regarded as fix-length representations of the whole sequences, which are followed by a fully connected layer and a softmax non-linear layer to produce class distributions. DISPLAYFORM0", "As Table TABREF48 shows, our model obtains competitive or better performances on all tasks except for the QC dataset, as it contains poor correlations with other tasks. MT-RNN slightly outperforms our model on SST, as sentences from this dataset are much shorter than those from IMDB and MDSD, and another possible reason may be that our model are more complex and requires larger data for training. Our model proposes the designs of various interactions including coupling, local and global fusion, which can be further implemented by other state-of-the-art models and produce better performances.", "As a result, our architecture covers complicated interactions among different tasks. It is capable of mapping a collection of input sequences from different tasks into a combination of predicted class distributions in parallel, as shown in Eqs.( EQREF11 ).", "where INLINEFORM0 are sequences from each tasks and INLINEFORM1 are the corresponding predictions.", "Besides Single Layers, we design Coupling Layers to model direct pair-wise interactions between tasks. For each pair of tasks, hidden states and memory cells of the Single Layers can obtain extra information directly from each other, as shown in Figure FIGREF21 ."]}
{"question_id": "27d7a30e42921e77cfffafac5cb0d16ce5a7df99", "predicted_answer": "Recurrent Neural Networks (RNNs) are explored in the context of natural language processing tasks, particularly for their ability to represent variable-length text sequences as fixed-length vectors.", "predicted_evidence": ["A recurrent neural network maintains a internal hidden state vector INLINEFORM0 that is recurrently updated by a transition function INLINEFORM1 . At each time step INLINEFORM2 , the hidden state INLINEFORM3 is updated according to the current input vector INLINEFORM4 and the previous hidden state INLINEFORM5 . DISPLAYFORM0", "Recently neural network based models have obtained substantial interests in many natural language processing tasks for their capabilities to represent variable-length text sequences as fix-length vectors, for example, Neural Bag-of-Words (NBOW), Recurrent Neural Networks (RNN), Recursive Neural Networks (RecNN) and Convolutional Neural Network (CNN). Most of them first map sequences of words, n-grams or other semantic units into embedding representations with a pre-trained lookup table, then fuse these vectors with different architectures of neural networks, and finally utilize a softmax layer to predict categorical distribution for specific classification tasks. For recurrent neural network, input vectors are absorbed one by one in a recurrent way, which makes RNN particularly suitable for natural language processing tasks.", "In this way, recurrent neural networks can comprehend a sequence of arbitrary length into a fix-length vector and feed it to a softmax layer for text classification or other NLP tasks. However, gradient vector of INLINEFORM0 can grow or decay exponentially over long sequences during training, also known as the gradient exploding or vanishing problems, which makes it difficult to learn long-term dependencies and correlations for RNNs.", "Different from the above models, our model focuses on Type-III and utilize recurrent neural networks to comprehensively capture various interactions among tasks, both direct and indirect, local and global. Three or more tasks are learned simultaneously and samples from different tasks are trained in parallel benefitting from each other, thus obtaining better sentence representations.", "BIBREF9 , BIBREF10 , BIBREF11 all belong to Type-II where samples from different tasks are learned sequentially. BIBREF9 applies bag-of-word representation and information of word orders are lost. BIBREF10 introduces an external memory for information sharing with a reading/writing mechanism for communicating, and BIBREF11 proposes three different models for multi-task learning with recurrent neural networks. However, models of these two papers only involve pair-wise interactions, which can be regarded as specific implementations of Coupling Layer and Fusion Layer in our model."]}
{"question_id": "7561bd3b8ba7829b3a01ff07f9f3e93a7b8869cc", "predicted_answer": "The extractive models trained on this dataset are:\n\n1. LEAD-$k$\n2. TextRank\n3. LexRank\n4. SumBasic\n5. C_SKIP\n6. SemSenSum", "predicted_evidence": ["Table TABREF19 contains the results. LEAD-5 achieves less than 20 for ROUGE-L as well as ROUGE-1 and less than $3.5$ for ROUGE-2. Taking only 3 sentences leads to even worse results: below 13 and 3 respectively. Unlike in other datasets, these results are significantly outperformed by all other extractive models but surprisingly, abstractive models perform worse on average. This demonstrates the difficulty of the task in GameWikiSum compared to nallapati2016abstractive and graff2003.", "zopf2016next applied a similar strategy using Wikipedia, where they asked annotators to first tag and extract information nuggets from the lead section of Wikipedia articles. In a further step, the same annotators searched for source documents using web search engines. As the whole process depends on humans, they could only collect around one thousand samples. Other attempts such as BIBREF11 have been made using Twitter, but the resulting dataset size was even smaller.", "We use common abstractive sequence-to-sequence baselines such as Conv2Conv BIBREF9, Transformer BIBREF10 and its language model variant, TransformerLM BIBREF3. We use implementations from fairseq and tensor2tensor. As the corpus size is too large to train extractive and abstractive models in an end-to-end manner due to hardware constraints, we use Tf-Idf to coarsely select sentences before training similarly to wiki2018. We limit the input size to 2K tokens so that all models can be trained on a Titan Xp GPU (12GB GPU RAM). We run all models with their best reported parameters.", "For extractive models, we include LEAD-$k$ which is a strong baseline for single document summarization tasks and takes the first $k$ sentences in the document as summary BIBREF5. TextRank BIBREF6 and LexRank BIBREF7 are two graph-based methods, where nodes are text units and edges are defined by a similarity measure. SumBasic BIBREF8 is a frequency-based sentence selection method, which uses a component to re-weigh the word probabilities in order to minimize redundancy. The last extractive baselines are the near state-of-the-art models C_SKIP from rossiello2017centroid and SemSenSum from antognini2019. The former exploits the capability of word embeddings to leverage semantics, whereas the latter aggregates two types of sentence embeddings using a sentence semantic relation graph, followed by a graph convolution.", "To ensure consistent results across all comparative experiments, extractive models generate summaries of the same length as reference summaries. In realistic scenarios, summary lengths are not pre-defined and can be adjusted to produce different types of summaries (e.g., short, medium or long). We do not explicitly constrain the output length for abstractive models, as each summary is auto-regressively generated."]}
{"question_id": "a3ba21341f0cb79d068d24de33b23c36fa646752", "predicted_answer": "The abstractive models trained are:\n\n1. Conv2Conv\n2. Transformer\n3. TransformerLM (a language model variant of Transformer)", "predicted_evidence": ["To ensure consistent results across all comparative experiments, extractive models generate summaries of the same length as reference summaries. In realistic scenarios, summary lengths are not pre-defined and can be adjusted to produce different types of summaries (e.g., short, medium or long). We do not explicitly constrain the output length for abstractive models, as each summary is auto-regressively generated.", "We use common abstractive sequence-to-sequence baselines such as Conv2Conv BIBREF9, Transformer BIBREF10 and its language model variant, TransformerLM BIBREF3. We use implementations from fairseq and tensor2tensor. As the corpus size is too large to train extractive and abstractive models in an end-to-end manner due to hardware constraints, we use Tf-Idf to coarsely select sentences before training similarly to wiki2018. We limit the input size to 2K tokens so that all models can be trained on a Titan Xp GPU (12GB GPU RAM). We run all models with their best reported parameters.", "Table TABREF19 contains the results. LEAD-5 achieves less than 20 for ROUGE-L as well as ROUGE-1 and less than $3.5$ for ROUGE-2. Taking only 3 sentences leads to even worse results: below 13 and 3 respectively. Unlike in other datasets, these results are significantly outperformed by all other extractive models but surprisingly, abstractive models perform worse on average. This demonstrates the difficulty of the task in GameWikiSum compared to nallapati2016abstractive and graff2003.", "Overall, the abstractive performance of sequence-to-sequence and language models are significantly lower than C_SKIP and SemSentSum in terms of ROUGE-L and ROUGE-1. However, Conv2Conv obtains only $0.05$ less ROUGE-2 score compared to C_SKIP and $0.36$ to SemSentSum. We suspect ROUGE-2 to be easier for abstractive sequence-to-sequence models, as half of the samples only have a ROUGE-2 around $27.00$ without any limitation of the input size (see Table TABREF11). Consequently, copying sentences from a small subset of the whole input documents for extractive models leads to worse ROUGE-2 recall. A normal transformer underperforms compared to Conv2Conv, and its language model variant achieves significantly worse results than other models due to a lack of data.", "We highlight that GameWikiSum has two orders of magnitude fewer samples (see Table TABREF12) compared to wiki2018. Therefore, it is necessary to have either additional annotated data or pre-train TransformerLM on another corpus."]}
{"question_id": "96295e1fe8713417d2b4632438a95d23831fbbdc", "predicted_answer": "Yes, the reviews focus on a specific video game domain, as they are professional reviews of video games obtained from Metacritic and Wikipedia. The reviews assess various aspects of video games, including gameplay, dialogues, and soundtrack, and are written by journalists who specialize in video game reviews.", "predicted_evidence": ["Metacritic is a website aggregating music, game, TV series, and movie reviews. In our case, we only focus on the video game section and crawl different products with their associated links, pointing to professional reviews written by journalists. It is noteworthy that we consider reviews for the same game released on different platforms (e.g., Playstation, Xbox) separately. Indeed, the final product quality might differ due to hardware constraints and some websites are specialized toward a specific platform.", "Journalists are paid to write complete reviews for various types of entertainment products, describing different aspects thoroughly. Reviewed aspects in video games include the gameplay, richness, and diversity of dialogues, or the soundtrack. Compared to usual reviews written by users, these are assumed to be of higher-quality and longer.", "In contrast, we propose a novel domain-specific dataset containing $14\\,652$ samples, based on professional video game reviews obtained via Metacritic and gameplay sections from Wikipedia. By using Metacritic reviews in addition to Wikipedia articles, we benefit from a number of factors. First, the set of aspects used to assess a game is limited and consequently, reviews share redundancy. Second, because they are written by professional journalists, reviews tend to be in-depth and of high-quality. Additionally, when a video game is released, journalists have an incentive to write a complete review and publish it online as soon as possible to draw the attention of potential customers and increase the revenue of their website BIBREF0. Therefore, several reviews for the same product become quickly available and the first version of the corresponding Wikipedia page is usually made available shortly after. Lastly, reviews and Wikipedia pages are available in multiple languages, which opens up the possibility for multilingual multi-document summarization.", "We only keep games with at least one review and a matching Wikipedia page, containing a gameplay section.", "To validate our hypothesis that professional game reviews focus heavily on gameplay mechanics, we compute the proportion of unigrams and bigrams of the output given the input. We observe a significant overlap ($20\\%$ documents containing $67.7\\%$ of the words mentioned in the summary, and at least $27.4\\%$ bigrams in half of the documents), emphasizing the extractive nature of GameWikiSum. Several examples of summaries are shown in Section SECREF20"]}
{"question_id": "5bfbc9ca7fd41be9627f6ef587bb7e21c7983be0", "predicted_answer": "The size of the GameWikiSum dataset is 17,652 samples, divided into:\n\n* 11,744 training samples\n* 1,454 validation samples\n* 1,454 testing samples", "predicted_evidence": ["To the best of our knowledge, DUC and TAC are the first multi-document summarization datasets. They contain documents about the same event and human-written summaries. Unsurprisingly, this approach does not scale and they could only collect hundreds of samples as shown in Table TABREF12.", "zopf2016next applied a similar strategy using Wikipedia, where they asked annotators to first tag and extract information nuggets from the lead section of Wikipedia articles. In a further step, the same annotators searched for source documents using web search engines. As the whole process depends on humans, they could only collect around one thousand samples. Other attempts such as BIBREF11 have been made using Twitter, but the resulting dataset size was even smaller.", "We highlight that GameWikiSum has two orders of magnitude fewer samples (see Table TABREF12) compared to wiki2018. Therefore, it is necessary to have either additional annotated data or pre-train TransformerLM on another corpus.", "We divide GameWikiSum into train, validation and testing sets with a rough ratio of 80/10/10, resulting in $11\\,744$, $1\\,454$ and $1\\,454$ examples respectively. If a game has been released on several platforms (represented by different samples), we group them in the same subset to avoid review overlap between training, validation, and testing. The distribution of samples per platform is shown in Table TABREF13. We compute in addition the mean number of input documents, ROUGE-1, and ROUGE-2 scores of the output given the input. We observe that most platforms have a mean ROUGE-1 score above 80 and 30 for ROUGE-2.", "We build GameWikiSum corpus by considering English reviews and Wikipedia pages. Table TABREF11 describes its overall properties. Most samples contain several reviews, whose cumulative size is too large for extractive or abstractive models to be trained in an end-to-end manner. The total vocabulary is composed of $282\\,992$ words. Our dataset also comes from a diverse set of sources: over 480 video game websites appear as source documents in at least 6 video games; they are responsible for $99.95\\%$ of the reviews."]}
{"question_id": "5181527e6a61a9a192db5f8064e56ec263c42661", "predicted_answer": "The system answers questions in French.", "predicted_evidence": ["On the one hand, the system is able to answer complex out-of-context questions such as \u201cWhat are the capitals of the countries of the Iberian Peninsula?\", by correctly answering the list of capitals: \u201cAndorra la Vella, Gibraltar, Lisbon, Madrid\".", "Conversational question answering is an open research problem. It studies the integration of question answering (QA) systems in a dialogue system(DS). Not long ago, each of these research subjects were studied separately; only very recently has studying the intersection between them gained increasing interest BIBREF0, BIBREF1.", "We have presented a spoken conversational question answering system, in French. The DS orchestrates different QA systems and returns the response with the higher confidence score. The system contains modules specifically designed for dealing with common spoken conversation phenomena such as coreference and ellipsis.", "The high-level architecture of the proposed system consists of a speech-processing front-end, an understanding component, a context manager, a generation component, and a synthesis component. The context manager provides contextualised mediation between the dialogue components and several question answering back-ends, which rely on data provided by WikidataFOOTREF1. Interaction with a human user is achieved through a graphical user interface (GUI). Figure 1 depicts the components together with their interactions.", "We present a spoken conversational question answering system that is able to answer questions about general knowledge in French by calling two distinct QA systems. It solves coreference and ellipsis by modelling context. Furthermore, it is extensible, thus other components such as neural approaches for question-answering can be easily integrated. It is also possible to collect a dialogue corpus from its iterations."]}
{"question_id": "334aa5540c207768931a0fe78aa4981a895ba37c", "predicted_answer": "The metrics used for evaluation are:\n\n1. Macro precision\n2. Macro recall\n3. F-1 score\n4. Average precision\n5. Average recall\n6. Average F-1 score\n7. Labelled attached score (LAS)\n\nThese metrics are used to evaluate the performance of the system on various tasks, including out-of-context question answering, coreference resolution, and part-of-speech tagging.", "predicted_evidence": ["Performance on out-of-context questions was evaluated on Bench'It, a dataset containing 150 open ended questions about general knowledge in French (Figure FIGREF20). The system reached a macro precision, recall and F-1 of $64.14\\%$, $64.33\\%$ and $63.46\\%$ respectively.", "We also evaluated the coreference resolution model on the test-set of CALOR (Table TABREF11), obtaining an average precision, recall and F-1 of 65.59%, 48.86% and 55.77% respectively. The same model reached a average F-1 of 68.8% for English BIBREF6. Comparable measurements are not available for French. F-1 scores for French are believed to be lower because of the lower amount of annotated data.", "The understanding component relies on a linguistic module to parser the user's inputs. The linguistic module supports part-of-the-speech (POS) tagging, lemmatisation, dependency syntax and semantics provided by an adapted version of UDpipe BIBREF5, extended with a French full-form lexicon. UDpipe was trained on the French GSD treebank version 2.3. Since the syntax of questions in French differs from that of declaratives, we annotated manually about 500 questions to be merged into the UD treebank (which originally did not contain questions). Tests show that the labelled attached score (LAS) is thereby increased by 10% absolute, to 92%.", "The Search QA system uses an internal knowledge base, which finely indexes data using Elasticsearch. It is powered by Wikidata and enriched by Wikipedia, especially to calculate a Page-Rank BIBREF10 on each entity. This QA system first determines the potential named entities in the question (i.e. subjects, predicates, and types of subjects). Second, it constructs a correlation matrix by looking for the triplets in Wikidata that link these entities. This matrix is filtered according the coverage of the question and the relevance of each entity in order to find the best answer.", "Two complementary question answering components were integrated into the system: the Reasoning QA and Search QA. Each of these QA systems computes a confidence score for every answer by using icsiboost BIBREF9, an Adaboost-based classifier trained on a corpus of around 21 000 questions. The Context Manager takes into account these scores to pick the higher-confidence of the two answers."]}
{"question_id": "b8bbdc3987bb456739544426c6037c78ede01b77", "predicted_answer": "No, the proposed system is not compared to existing systems in the given context. The text explains the components of the proposed system, its architecture, and how it works, but it does not provide a comparison with other existing systems.", "predicted_evidence": ["In the remainder of this section, we explain the components of our system.", "The evaluation of the individual components of the proposed system was performed outside the scope of this work. We evaluated out-of-context questions, as well as the coreference resolution module.", "Conversational question answering is an open research problem. It studies the integration of question answering (QA) systems in a dialogue system(DS). Not long ago, each of these research subjects were studied separately; only very recently has studying the intersection between them gained increasing interest BIBREF0, BIBREF1.", "The high-level architecture of the proposed system consists of a speech-processing front-end, an understanding component, a context manager, a generation component, and a synthesis component. The context manager provides contextualised mediation between the dialogue components and several question answering back-ends, which rely on data provided by WikidataFOOTREF1. Interaction with a human user is achieved through a graphical user interface (GUI). Figure 1 depicts the components together with their interactions.", "Two complementary question answering components were integrated into the system: the Reasoning QA and Search QA. Each of these QA systems computes a confidence score for every answer by using icsiboost BIBREF9, an Adaboost-based classifier trained on a corpus of around 21 000 questions. The Context Manager takes into account these scores to pick the higher-confidence of the two answers."]}
{"question_id": "fea9b4d136156f23a88e5c7841874a467f2ba86d", "predicted_answer": "According to the text, they determine that a decoder handles an easier task than the encoder by comparing their convergence speed, where the decoder converges faster than the encoder, indicating that the task handled by the decoder is easier.", "predicted_evidence": ["We further investigate the difficulty of the task for the encoder and decoder by comparing their convergence speed. Encoder (decoder) that needs more training time indicates the task that encoder (decoder) handles is more difficult.", "We investigate the task difficulty by comparing the training effort of the encoder and decoder from two perspectives in the paper: 1) We vary the number of layers for the encoder and decoder respectively. All models are trained with the same configuration as described before except for the number of layers. 2) For training the encoder side, we initialize the decoder side with the parameters of a well-trained decoder, and initialize other components with random variable. For training decoder side, we follow the opposite operation.", "The consistent observations above suggest that the decoder is much more sensitive to the input noise than the encoder. Intuitively, the encoder aims at extracting abstract representations of the source sentence instead of depending on certain input tokens for prediction as the decoder does, demonstrating that the encoder is more robust than the decoder.", "The decoder is more sensitive to the input noise than the encoder. We randomly add different level of noise to the input of the encoder and decoder respectively during inference, and find that adding noise to the input of the decoder leads to better accuracy drop than that of the encoder.", "We believe our studies on the different characteristics of the encoder and decoder will inspire the following research on the encoder-decoder framework as well as improve the performance on NMT and other encoder-decoder based tasks."]}
{"question_id": "4e59808a7f73ac499b9838d3c0ce814196a02473", "predicted_answer": "According to the text, the conditional information strength is measured by dropping previous tokens from the decoder input and observing the performance changes. Specifically, they drop tokens at different distances from the predicted token and measure the decline in accuracy. The more the accuracy declines, the stronger the conditional information is. Additionally, they use a null baseline by randomly dropping tokens and fitting a curve between the translation accuracy and the number of dropping tokens, and then compare the delta BLEU between the accuracy when dropping a specific part-of-speech (POS) and the null baseline to determine the sensitivity of the model to that POS.", "predicted_evidence": ["We give an explanation on why decoder is easier and sensitive in Section 5 of the paper. We additionally train a non-autoregressive NMT model for comparison.", "The decoder in NMT model typically acts as a conditional language model, which generates tokens highly depending on the previous tokens, like the standard language model BIBREF25. We guess the conditional information (especially the tokens right before the predicted token) is too strong for the decoder. Therefore, we study the impact of the previous tokens as follows. For each predicted token $w_{t}$, where $t$ is the position in the target sentence, we drop its previous token $w_{t-n}$ from the decoder input and watch the performance changes, where $n\\in [1, t]$ is the distance between the dropping token and the current predicted token. Note that the experiments are conducted in the inference phase and evaluated with teacher forcing. As shown in Figure FIGREF14, when dropping the token close to the predicted token, the accuracy declines more heavily than dropping the token far away, which indicates the decoder depends more on the nearby tokens.", "For IWSLT14 De$\\leftrightarrow $En and IWSLT14 Ro$\\leftrightarrow $En, we use the same evaluation metrics as before. For WMT17 Zh$\\leftrightarrow $En translation, we calculate the detokenized BLEU score by sacreBLEU. As described in the paper, to eliminate the influence of error propagation, we apply teacher forcing BIBREF23 in the inference phase by feeding the previous ground-truth target tokens instead of the previously generated tokens. Therefore, we just add the noise to the ground-truth target tokens when adding noise to the decoder input. We use greedy inference for all settings and length penalty $\\alpha =1.1$.", "We further analyze why the decoder is more sensitive by masking the previous tokens, and comparing autoregressive NMT with the non-autoregressive counterpart. We find that the preceding tokens in the decoder provide strong conditional information, which partially explain the previous two observations on the decoder.", "We show the results of the popular POS types (the POS that covers more than 3000 tokens in the test set) in Figure FIGREF48. Since different POS cover different number of tokens in the test set, the direct comparison between different POS is unfair. We introduce a null baseline by randomly dropping different number of tokens and observe the accuracy changes. Based on this data, we fit the curve between the translation accuracy and the number of dropping tokens. We compare each POS with the null baseline by computing the delta BLEU between the accuracy when dropping this kind of POS and the accuracy when dropping tokens randomly. If the delta BLEU is less than 0 (dropping a certain POS achieves lower BLEU score than the null baseline), the model is supposed to be more sensitive to this kind of POS."]}
{"question_id": "7ef7a5867060f91eac8ad857c186e51b767c734b", "predicted_answer": "They generate input noise for the encoder and decoder by simulating three typical operations:\n\n1. Random dropping: randomly dropping input tokens with different drop rates.\n2. Random noising: randomly selecting tokens and replacing their embedding with random noise.\n3. Random swapping: randomly reversing the order of adjacent tokens.", "predicted_evidence": ["The consistent observations above suggest that the decoder is much more sensitive to the input noise than the encoder. Intuitively, the encoder aims at extracting abstract representations of the source sentence instead of depending on certain input tokens for prediction as the decoder does, demonstrating that the encoder is more robust than the decoder.", "The decoder is more sensitive to the input noise than the encoder. We randomly add different level of noise to the input of the encoder and decoder respectively during inference, and find that adding noise to the input of the decoder leads to better accuracy drop than that of the encoder.", "In this section, we compare the characteristics between the encoder and decoder by analyzing their robustness according to the input noise in the inference phase. We simulate the input noise with three typical operations BIBREF20, BIBREF21: 1) random dropping: we randomly drop the input tokens of encoder and decoder respectively with different drop rates; 2) random noising: we randomly select tokens and replace its embedding with random noise; 3) random swapping: we randomly reverse the order for the adjacent tokens. The decoder in NMT model typically generates the current token one-by-one conditioned on the previous generated tokens, which suffers from error propagation BIBREF22: if a token is incorrectly predicted by the decoder, it will affect the prediction of the following tokens. Adding input noise to the decoder will further enhance the effect of error propagation, and thus influence our analysis.", "Given the observations in Section SECREF3 and SECREF4, we find that the decoder is more sensitive to the input tokens compared with the encoder, and the task that decoder handles is easier. In this section, we give an explanation on this phenomenon. More details on experimental configurations are described in supplementary materials (Section 1.3). Besides, we also investigate which kind of input tokens the encoder and decoder are more sensitive to, from the perspective of Part-Of-Speech (POS), and show the results in the supplementary materials (Section 2) due to space limitation.", "We believe our studies on the different characteristics of the encoder and decoder will inspire the following research on the encoder-decoder framework as well as improve the performance on NMT and other encoder-decoder based tasks."]}
