{"question_id": "397a1e851aab41c455c2b284f5e4947500d797f0", "predicted_answer": "The size of the ANTISCAM dataset is not explicitly mentioned in the given context. However, it is mentioned that the dataset is newly collected and is used for evaluation and comparison with other models.", "predicted_evidence": ["We compare MISSA mainly with two baseline models:", "Fluency Fluency is used to explore different models' language generation quality.", "To enrich publicly available non-collaborative task datasets, we collect a new dataset AntiScam, where users defend themselves against attackers trying to collect personal information. As non-collaborative tasks are still relatively new to the study of dialog systems, there are insufficiently many meaningful datasets for evaluation and we hope this provides a valuable example. We evaluate MISSA on the newly collected AntiScam dataset and an existing PersuasionForGood dataset. Both automatic and human evaluations suggest that MISSA outperforms multiple competitive baselines.", "Table TABREF19 presents the main experiment results on AntiScam dataset, for both automatic evaluation metrics and human evaluation metrics. The experiment results on PersuasionForGood are shown in Table TABREF23. We observe that MISSA outperforms two baseline models (TransferTransfo and hybrid model) on almost all the metrics on both datasets. For further analysis, examples of real dialogs from the human evaluation are presented in Table TABREF21.", "In the intent annotation scheme shown in Table TABREF2, we list the designed intent annotation scheme for the newly collected AntiScam dataset and the PersuasionForGood dataset. We first define on-task intents for the datasets, which are key actions in the task. Since our AntiScam focuses on understanding and reacting towards elicitations, we define elicitation, providing_information and refusal as on-task intents. In the PersuasionForGood dataset, we define nine on-task intents in Table TABREF2 based on the original PersuasionForGood dialog act annotation scheme. All these intents are related to donation actions, which are salient on-task intents in the persuasion task. The off-task intents are the same for both tasks, including six general intents and six additional social intents."]}
{"question_id": "cc8b4ed3985f9bfbe1b5d7761b31d9bd6a965444", "predicted_answer": "Intent is annotated using a hierarchical intent annotation scheme, which involves the following steps:\n\n1. Defining on-task intents for the dataset, which are key actions in the task.\n2. Defining off-task intents, which are general intents and social intents that are not specific to the task.\n\nIn the provided context, the intent annotation scheme is shown in Table TABREF2, which lists the designed intent annotation scheme for the AntiScam dataset and the PersuasionForGood dataset.\n\nFor the AntiScam dataset, the on-task intents are:\n\n* Elicitation\n* Providing information\n* Refusal\n\nFor the PersuasionForGood dataset, the on-task intents are:\n\n* Nine intents related to donation actions, based on the original PersuasionForGood dialog act annotation scheme.\n\nThe off-task intents are the same for both tasks, including:\n\n* Six general intents:\n\t+ Open question\n\t+ Yes/no question\n\t+ Positive answer\n\t+ Negative answer\n\t+ Responsive statement\n\t+ Nonresponsive statement\n* Six social intents:\n\t+ Greeting\n\t+ Closing\n\t+ Apology\n\t+ Thanking\n\t+ Respond to thank\n\t+ Hold", "predicted_evidence": ["MISSA is able to classify multiple intents and multiple semantic slots in a single utterance with these classifiers. Figure FIGREF6 shows how it works on the AntiScam dataset. Specifically, we set a special token $<$sep$>$ at the end of each sentence in an utterance (an utterance can consist of multiple sentences). Next, we pass the token's position information to the transformer architecture and obtain the representation of the position (represented as colored position at last layer in Figure FIGREF6). After that, we concatenate the embeddings at these position with the hidden states of last sentence. We pass these concatenated representations to the intent classifier and the slot classifier to obtain an intent and a semantic slot for each sentence in the utterance. As shown in Figure FIGREF6, the loss function ${\\mathcal {L}}$ for the model combines all the task losses:", "To better understand user utterances and separate on-task and off-task content within a conversation, previous work has designed hierarchical annotation schemes for specific domains. BIBREF9 hardy2002multi followed the DAMSL schemeBIBREF10 and annotated a multilingual human-computer dialog corpus with a hierarchical dialog act annotation scheme. BIBREF11 gupta2018semantic used a hierarchical annotation scheme for semantic parsing. Inspired by these studies, our idea is to annotate the intent and semantic slot separately in non-collaborative tasks. We propose a hierarchical intent annotation scheme that can be adopted by all non-collaborative tasks. With this annotation scheme, MISSA is able to quickly build an end-to-end trainable dialog system for any non-collaborative task.", "In the intent annotation scheme shown in Table TABREF2, we list the designed intent annotation scheme for the newly collected AntiScam dataset and the PersuasionForGood dataset. We first define on-task intents for the datasets, which are key actions in the task. Since our AntiScam focuses on understanding and reacting towards elicitations, we define elicitation, providing_information and refusal as on-task intents. In the PersuasionForGood dataset, we define nine on-task intents in Table TABREF2 based on the original PersuasionForGood dialog act annotation scheme. All these intents are related to donation actions, which are salient on-task intents in the persuasion task. The off-task intents are the same for both tasks, including six general intents and six additional social intents.", "In the intent annotation scheme shown in Table TABREF2, we list the designed intent annotation scheme for the newly collected AntiScam dataset and the PersuasionForGood dataset. We first define on-task intents for the datasets, which are key actions in the task. Since our AntiScam focuses on understanding and reacting towards elicitations, we define elicitation, providing_information and refusal as on-task intents. In the PersuasionForGood dataset, we define nine on-task intents in Table TABREF2 based on the original PersuasionForGood dialog act annotation scheme. All these intents are related to donation actions, which are salient on-task intents in the persuasion task. The off-task intents are the same for both tasks, including six general intents and six additional social intents. General intents are more closely related to the syntactic meaning of the sentence (open_question, yes_no_question, positive_answer, negative_answer, responsive_statement, and nonresponsive_statement) while social intents are common social actions (greeting, closing, apology, thanking,respond_to_thank, and hold).", "For specific tasks, we also design a semantic slot annotation scheme for annotating sentences based on their semantic content. We identify 13 main semantic slots in the anti-scam task, for example, credit card numbers. We present a detailed semantic slot annotation in Table TABREF3. Following BIBREF1, we segment each conversation turn into single sentences and then annotate each sentence rather than turns."]}
{"question_id": "f7662b11e87c1e051e13799413f3db459ac3e19c", "predicted_answer": "The baselines outperformed by this work are:\n\n1. TransferTransfo\n2. Hybrid models", "predicted_evidence": ["We also apply our method to the PersuasionForGood dataset. As shown in Table TABREF23, MISSA and its variants outperform the TransferTransfo and the hybrid models on all evaluation metrics. Such good performance indicates MISSA can be easily applied to a different non-collaborative task and achieve good performance. Particularly, MISSA achieves the lowest perplexity, which confirms that using conditional response generation leads to high quality responses. Compared with the result on AntiScam dataset, MISSA-con performs the best in terms of RIP and ERIP. We suspect the underlying reason is that there are more possible responses with the same intent in PersuasionForGood than in AntiScam. This also suggests that we should adjust the model structure according to the nature of the dataset.", "Fluency Fluency is used to explore different models' language generation quality.", "In addition, we perform ablation studies on MISSA to show the effects of different components.", "Perplexity Since the canonical measure of a good language model is perplexity, which indicates the error rate of the expected word. We choose perplexity to evaluate the model performance.", "To explore the influence of the intent-based conditional response generation method and the designed response filter, we perform an ablation study. The results are shown in Table TABREF19. We find that MISSA has higher fluency score and coherence score than MISSA-con (4.18 vs 3.78 for fluency, and 3.75 vs 3.68 for coherence), which suggests that conditioning on the system intent to generate responses improves the quality of the generated sentences. Compared with MISSA-sel, MISSA achieves better performance on all the metrics. For example, the engagement score for MISSA is 3.69 while MISSA-sel only has 2.87. This is because the response filter removed all the incoherent responses, which makes the attacker more willing to keep chatting. The ablation study shows both the conditional language generation mechanism and the response filter are essential to MISSA's good performance."]}
{"question_id": "b584739622d0c53830e60430b13fd3ae6ff43669", "predicted_answer": "The evaluation metrics and criteria used to evaluate the model performance are:\n\n1. **Extended Response-Intent Prediction (ERIP)**: This metric evaluates the coherence of the dialog by verifying the predicted intents. It takes into account the complexity of the mapping between human-intent and system-intent, where there may be multiple acceptable system-intents for the same human-intent. The score is calculated as 1 if the predicted intent matches the ground truth, otherwise it is set as the transition probability $p(I_{predict}|I_i)$.\n2. **Extended Response-Slot Prediction (ERSP)**: This metric is not explicitly defined in the text, but it is mentioned alongside ERIP, suggesting that it may be a similar metric for evaluating the predicted semantic slots.\n\nThese metrics are used to evaluate the performance of the MISSA model in predicting the intents and semantic slots of human utterances and system responses, as well as the coherence of the dialog.", "predicted_evidence": ["We train MISSA in a multi-task fashion. In addition to the language model task and the next-utterance prediction task, we also use separate classifiers to predict the intents and semantic slots of both human utterances and system responses. The intent classifier and semantic slot classifier for human utterances capture the semantic and syntactic meaning of human utterances, providing information to select the appropriate response among response candidates while the classifiers for the system intents and semantic slots are designed to help select an appropriate next-sentence. We describe response filtering in the corresponding subsection. Classifiers are designed as the following equation:", "Traditional task-oriented dialog systems BIBREF12 are usually composed of multiple independent modules, for example, natural language understanding, dialog state tracking BIBREF13, BIBREF14, dialog policy manager BIBREF15, and natural language generation BIBREF16. Conversational intent is adopted to capture the meaning of task content in these dialog systems BIBREF2, BIBREF17. In comparison to this work, we use a hierarchical intent scheme that includes off-task and on-task intents to capture utterance meaning. We also train the model in a multi-task fashion to predict decoupled intents and semantic slots. The major defect of a separately trained pipeline is the laborious dialog state design and annotation. In order to mitigate this problem, recent work has explored replacing independent modules with end-to-end neural networks BIBREF18, BIBREF19, BIBREF20. Our model also follows this end-to-end fashion.", "Extended Response-Intent Prediction (ERIP) $\\&$ Extended Response-Slot Prediction (ERSP) With Response-Intent Prediction, we verify the predicted intents to evaluate the coherence of the dialog. However, the real mapping between human-intent and system-intent is much more complicated as there might be multiple acceptable system-intents for the same human-intent. Therefore, we also design a metric to evaluate if the predicted system-intent is in the set of acceptable intents. Specifically, we estimate the transition probability $p(I_i|I_j)$ by counting the frequency of all the bi-gram human-intent and system-intent pairs in the training data. During the test stage, if the predicted intent matches the ground truth, we set the score as 1, otherwise we set the score as $p(I_{predict}|I_i)$ where $I_i$ is the intent of the input human utterance. We then report the average value of those scores over turns as the final extended response-intent prediction result.", "Extended Response-Intent Prediction (ERIP) $\\&$ Extended Response-Slot Prediction (ERSP) With Response-Intent Prediction, we verify the predicted intents to evaluate the coherence of the dialog. However, the real mapping between human-intent and system-intent is much more complicated as there might be multiple acceptable system-intents for the same human-intent. Therefore, we also design a metric to evaluate if the predicted system-intent is in the set of acceptable intents. Specifically, we estimate the transition probability $p(I_i|I_j)$ by counting the frequency of all the bi-gram human-intent and system-intent pairs in the training data. During the test stage, if the predicted intent matches the ground truth, we set the score as 1, otherwise we set the score as $p(I_{predict}|I_i)$ where $I_i$ is the intent of the input human utterance.", "To tackle the issue of incoherent system responses to off-task content, previous studies have built hybrid systems to interleave off-task and on-task content. BIBREF4 used a rule-based dialog manager for on-task content and a neural model for off-task content, and trained a reinforcement learning model to select between these two models based on the dialog context. However, such a method is difficult to train and struggles to generalize beyond the movie promotion task they considered. To tackle these problems, we propose a hierarchical intent annotation scheme that separates on-task and off-task information in order to provide detailed supervision. For on-task information, we directly use task-related intents for representation. Off-task information, on the other hand, is too general to categorize into specific intents, so we choose dialog acts that convey syntax information. These acts, such as \u201copen question\" are general to all tasks."]}
{"question_id": "2849c2944c47cf1de62b539c5d3c396a3e8d283a", "predicted_answer": "The accuracy of this model is not explicitly stated in the given context. However, it is mentioned that the model is evaluated using the InKB micro and macro F1 scores on test sets, with GERBIL's weak annotation match method. To compare the accuracy of this model to the state-of-the-art (SOTA), you would need to know the SOTA F1 scores for the specific datasets used (ISTEX and RSS) and compare them to the F1 scores achieved by this model.", "predicted_evidence": ["This alleviates the need for an $\\alpha $ parameter while keeping the number of features small. We train a linear support vector classifier on these features and this defines the final score of each candidate entity. For each spot, our system picks the highest-scoring candidate entity that the classifier predicts as a match, if any.", "The probability $p(d[s])$ is estimated by a simple unigram language model that can be trained either on any large unannotated dataset.", "Once a notion of semantic similarity is chosen, we need to integrate it in the inference process. Most approaches build a graph of candidate entities, where edges indicate semantic relatedness: the difference between the heuristics lie in the way this graph is used for the matching decisions. BIBREF21 use an approximate algorithm to find the densest subgraph of the semantic graph. This determines choices of entities for each mention. In other approaches, the initial evidence given by the local compatibility score is propagated along the edges of the semantic graph BIBREF14 , BIBREF22 or aggregated at a global level with a Conditional Random Field BIBREF17 .", "We trained our classifier and its hyper-parameters by five-fold cross-validation on the training sets of the ISTEX and RSS datasets. We used GERBIL BIBREF23 to evaluate OpenTapioca against other approaches. We report the InKB micro and macro F1 scores on test sets, with GERBIL's weak annotation match method.", "Named Entity Linking is the task of detecting mentions of entities from a knowledge base in free text, as illustrated in Figure 1 ."]}
{"question_id": "1a6156189297b2fe17f174ef55cbd20341bb7dbf", "predicted_answer": "The authors compare their new features against two state-of-the-art early rumour detection baselines: Liu et al. (2015) and Yang et al. (2012).", "predicted_evidence": ["We frame the Real-time Rumour Detection task as a classification problem that assesses a document's likelihood of becoming a future rumour at the time of its publication. Consequently, prediction takes place in real-time with a single pass over the data.", "Training Pseudo Feedback Features", "The previous sections introduced two new categories of features for rumour detection. Now we test their performance and impact on detection effectiveness and efficiency. In a streaming setting, documents arrive on a continual basis one at a time. We require our features to compute a rumour-score instantaneously for each document in a single-pass over the data. Messages with high rumour scores are considered likely being rumours. The classification decision is based on an optimal thresholding strategy based on the trainings set.", "To evaluate our new features for rumour detection, we compare them with two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. We chose the algorithm by Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter. Both apply various message-, user-, topic- and propagation-based features and rely on an SVM classifier which they also found to perform best. The approaches advertise themselves as suitable for early or real-time detection and performed rumour detection with the smallest latency across all published methods.", "To evaluate our new features for rumour detection, we compare them with two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. We chose the algorithm by Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter. Both apply various message-, user-, topic- and propagation-based features and rely on an SVM classifier which they also found to perform best. The approaches advertise themselves as suitable for early or real-time detection and performed rumour detection with the smallest latency across all published methods. Yang performs early rumour detection and operates with a delay of 24 hours."]}
{"question_id": "3319d56556ae1597a86384057db0831e32774b90", "predicted_answer": "The evaluation metrics mentioned in the text are:\n\n1. Accuracy\n2. Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability\n3. Throughput per second, which measures the efficiency of computing the proposed features.\n\nNote that the DET curves are used to evaluate the detection tasks, and the accuracy is used as a single value metric to evaluate the effectiveness of the approach.", "predicted_evidence": ["Training Pseudo Feedback Features", "Pseudo Feedback for Detection/Classification", "We report accuracy to evaluate effectiveness, as is usual in the literature (Zhou et. al, 2015). Additionally we use the standard TDT evaluation procedure (Allan et. al, 2000; NIST, 2008) with the official TDT3 evaluation scripts (NIST, 2008) using standard settings. This procedure evaluates detection tasks using Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability. By visualizing the full range of thresholds, DET plots provide a more comprehensive illustration of effectiveness than single value metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages.", "More formally, we denote by $d_t$ the document that arrives from stream $S:\\lbrace d_0, d_1, . . . d_n\\rbrace $ at time $t$ . Upon arrival of document $d_t$ we compute its corresponding feature vector $f_{d,t}$ . Given $f_{d,t}$ and the previously obtained weigh vector $w$ we compute the rumour score $RS_{d,t} = w^T \\times f_{d,t}$ . The rumour prediction is based on a fixed thresholding strategy with respect to $\\theta $ . We predict that message $d_t$ is likely to become a rumour if its rumour score exceeds the detection threshold $S:\\lbrace d_0, d_1, . . . d_n\\rbrace $0 . The optimal parameter setting for weight vector $S:\\lbrace d_0, d_1, . .", "More formally, we denote by $d_t$ the document that arrives from stream $S:\\lbrace d_0, d_1, . . . d_n\\rbrace $ at time $t$ . Upon arrival of document $d_t$ we compute its corresponding feature vector $f_{d,t}$ . Given $f_{d,t}$ and the previously obtained weigh vector $w$ we compute the rumour score $RS_{d,t} = w^T \\times f_{d,t}$ . The rumour prediction is based on a fixed thresholding strategy with respect to $\\theta $ . We predict that message $d_t$ is likely to become a rumour if its rumour score exceeds the detection threshold $S:\\lbrace d_0, d_1, . . . d_n\\rbrace $0 . The optimal parameter setting for weight vector $S:\\lbrace d_0, d_1, ."]}
{"question_id": "8cbe3fa4ec0f66071e3d6b829b09b6395b631c44", "predicted_answer": "No, the methods are not fully supervised. While the training set is used to compute weights for the features, the PF features are computed using a semi-supervised approach, where the rumour scores are computed based on the model obtained from the initial training round, and then the PF feature values are computed using the minimum distance in term space between the current document vector and those previous documents whose rumour score exceeds a previously defined threshold. This approach is not fully supervised as it involves using the model to compute rumour scores, which are then used to compute the PF feature values.", "predicted_evidence": ["Training Pseudo Feedback Features", "We report accuracy to evaluate effectiveness, as is usual in the literature (Zhou et. al, 2015). Additionally we use the standard TDT evaluation procedure (Allan et. al, 2000; NIST, 2008) with the official TDT3 evaluation scripts (NIST, 2008) using standard settings. This procedure evaluates detection tasks using Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability. By visualizing the full range of thresholds, DET plots provide a more comprehensive illustration of effectiveness than single value metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages.", "The previous sections introduced two new categories of features for rumour detection. Now we test their performance and impact on detection effectiveness and efficiency. In a streaming setting, documents arrive on a continual basis one at a time. We require our features to compute a rumour-score instantaneously for each document in a single-pass over the data. Messages with high rumour scores are considered likely being rumours. The classification decision is based on an optimal thresholding strategy based on the trainings set.", "We ordered the rumours and non-rumours chronologically and divided them in half, forming a training and test set. We ensured that each of the sets consists of 50% rumours and non-rumours. This is important when effectiveness is measured by accuracy. All training and optimization use the trainings set. Performance is then reported based on a single run on the test set.", "In a first training round a SVM is used to compute weights for all features in the trainings set, except the PF features. This provides a model for all but the PF features. Then the trainings set is processed to computing rumour scores based on the model obtained from our initial trainings round. This time, we additionally compute the PF feature value by measuring the minimum distance in term space between the current document vector and those previous documents, whose rumour score exceeds a previously defined threshold. Since we operate on a stream, the number of documents previously considered as rumours grows without bound. To keep operation constant in time and space, we only compare against the k most recent documents considered to be rumours. Once we obtained the value for the PF feature, we compute its weight using the SVM. The combination of the weight for the PF feature with the weights for all other features, obtained in the initial trainings round, resembles the final model."]}
{"question_id": "85e417231a4bbb6691f7a89bd81710525f8fec4c", "predicted_answer": "No, the text does not mention building a dataset of rumors. It discusses a real-time rumor detection system that classifies incoming messages as rumors or not based on features computed from the messages and a trained SVM classifier. The system does not collect or store rumors, but rather makes predictions on individual messages as they arrive.", "predicted_evidence": ["On the arrival of a new message from a stream, all its features are computed and linearly combined using weights obtained from an SVM classifier, yielding the rumour score. We then judge rumours based on an optimal threshold strategy for the rumour score.", "We frame the Real-time Rumour Detection task as a classification problem that assesses a document's likelihood of becoming a future rumour at the time of its publication. Consequently, prediction takes place in real-time with a single pass over the data.", "We introduced a new category of features for instant rumour detection that harnesses trusted resources. Unconfirmed (novel) information with respect to trusted resources is considered as an indication of rumours.", "The previous sections introduced two new categories of features for rumour detection. Now we test their performance and impact on detection effectiveness and efficiency. In a streaming setting, documents arrive on a continual basis one at a time. We require our features to compute a rumour-score instantaneously for each document in a single-pass over the data. Messages with high rumour scores are considered likely being rumours. The classification decision is based on an optimal thresholding strategy based on the trainings set.", "Recent research mainly focuses on further improving the quality of rumour detection while neglecting the increasing delay between the publication and detection of a rumour. The motivation for rumour detection lies in debunking them to prevent them from spreading and causing harm. Unfortunately, state-of-the-art systems operate in a retrospective manner, meaning they detect rumours long after they have spread. The most accurate systems rely on features based on propagation graphs and clustering techniques. These features can only detect rumours after the rumours have spread and already caused harm."]}
{"question_id": "57ee20f494d8ce3fae46028c3f3551d180dba3e0", "predicted_answer": "The languages evaluated are not explicitly mentioned in the given context. However, it can be inferred that the languages are Chinese (for Sina Weibo) and English (for Twitter), as the baselines used are tailored to these specific social media platforms.", "predicted_evidence": ["Training Pseudo Feedback Features", "To evaluate our new features for rumour detection, we compare them with two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. We chose the algorithm by Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter. Both apply various message-, user-, topic- and propagation-based features and rely on an SVM classifier which they also found to perform best. The approaches advertise themselves as suitable for early or real-time detection and performed rumour detection with the smallest latency across all published methods.", "To evaluate our new features for rumour detection, we compare them with two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. We chose the algorithm by Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter. Both apply various message-, user-, topic- and propagation-based features and rely on an SVM classifier which they also found to perform best. The approaches advertise themselves as suitable for early or real-time detection and performed rumour detection with the smallest latency across all published methods. Yang performs early rumour detection and operates with a delay of 24 hours.", "The previous sections introduced two new categories of features for rumour detection. Now we test their performance and impact on detection effectiveness and efficiency. In a streaming setting, documents arrive on a continual basis one at a time. We require our features to compute a rumour-score instantaneously for each document in a single-pass over the data. Messages with high rumour scores are considered likely being rumours. The classification decision is based on an optimal thresholding strategy based on the trainings set.", "On the arrival of a new message from a stream, all its features are computed and linearly combined using weights obtained from an SVM classifier, yielding the rumour score. We then judge rumours based on an optimal threshold strategy for the rumour score."]}
{"question_id": "2974237446d04da33b78ce6d22a477cdf80877b7", "predicted_answer": "According to the given context, rumors are defined as \"unconfirmed (novel) information with respect to trusted resources\". In other words, if a piece of information cannot be verified or confirmed by trusted sources, it is considered a rumor.", "predicted_evidence": ["We introduced a new category of features for instant rumour detection that harnesses trusted resources. Unconfirmed (novel) information with respect to trusted resources is considered as an indication of rumours.", "On the arrival of a new message from a stream, all its features are computed and linearly combined using weights obtained from an SVM classifier, yielding the rumour score. We then judge rumours based on an optimal threshold strategy for the rumour score.", "We frame the Real-time Rumour Detection task as a classification problem that assesses a document's likelihood of becoming a future rumour at the time of its publication. Consequently, prediction takes place in real-time with a single pass over the data.", "The previous sections introduced two new categories of features for rumour detection. Now we test their performance and impact on detection effectiveness and efficiency. In a streaming setting, documents arrive on a continual basis one at a time. We require our features to compute a rumour-score instantaneously for each document in a single-pass over the data. Messages with high rumour scores are considered likely being rumours. The classification decision is based on an optimal thresholding strategy based on the trainings set.", ". . d_n\\rbrace $ at time $t$ . Upon arrival of document $d_t$ we compute its corresponding feature vector $f_{d,t}$ . Given $f_{d,t}$ and the previously obtained weigh vector $w$ we compute the rumour score $RS_{d,t} = w^T \\times f_{d,t}$ . The rumour prediction is based on a fixed thresholding strategy with respect to $\\theta $ . We predict that message $d_t$ is likely to become a rumour if its rumour score exceeds the detection threshold $S:\\lbrace d_0, d_1, . . . d_n\\rbrace $0 . The optimal parameter setting for weight vector $S:\\lbrace d_0, d_1, . . . d_n\\rbrace $1 and detection threshold $S:\\lbrace d_0, d_1, . . ."]}
{"question_id": "bc8526d4805e2554adb2e9c01736d3f3a3b19895", "predicted_answer": "According to the given context, the baselines they compared with are not explicitly stated. The text only mentions that they used two benchmark datasets for their experiments, but it does not specify what baselines they compared their results with.", "predicted_evidence": ["The following baselines were used in our experiments:", "We base our experiments on two benchmark datasets:", "Then the probability of observing document INLINEFORM0 can be written as: DISPLAYFORM0", "where INLINEFORM0 is the true distribution for corresponding variables.", "Sample the multinomial parameter INLINEFORM0 from INLINEFORM1 ;"]}
{"question_id": "a0fd0c0fe042ad045b8d5095c81643ef3a352b81", "predicted_answer": "The tasks explored in this paper are:\n\n1. Generative document evaluation\n2. Document classification\n3. Topic2sentence task", "predicted_evidence": ["We compare SLRTM with several state-of-the-art topic models on two tasks: generative document evaluation and document classification. The former task is to investigate the generation capability of the models, while the latter is to show the representation ability of the models.", "We measure the performances of different topic models according to the perplexity per word on the test set, defined as INLINEFORM0 , where INLINEFORM1 is the number of words in document INLINEFORM2 . The experimental results are summarized in Table TABREF33 . Based on the table, we have the following discussions:", "We base our experiments on two benchmark datasets:", "Then the probability of observing document INLINEFORM0 can be written as: DISPLAYFORM0", "We report our experimental results in this section. Our experiments include two parts: (1) quantitative experiments, including a generative document evaluation task and a document classification task, on two datasets; (2) qualitative inspection, including the examination of the sentences generated under each topic, in order to test whether SLRTM performs well in the topic2sentence task."]}
{"question_id": "6e040e80f2da69d50386a90a38ed6d2fa4f77bbd", "predicted_answer": "According to the given context, the NER datasets used are:\n\n1. Resume NER\n2. MSRA (Chinese NER dataset)\n\nAdditionally, it is mentioned that there are two English NER datasets and four Chinese NER datasets used in total, but the names of the other datasets are not specified.", "predicted_evidence": ["We evaluate our model in two English NER datasets and four Chinese NER datasets.", "(6) Resume NER was annotated by BIBREF33.", "(4) The corpus of the Chinese NER dataset MSRA came from news domain BIBREF37.", "NER is usually viewed as a sequence labeling task, the neural models usually contain three components: word embedding layer, context encoder layer, and decoder layer BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10. The difference between various NER models mainly lies in the variance in these components.", "In this paper, we utilize the Transformer encoder to model the long-range and complicated interactions of sentence for NER. The structure of proposed model is shown in Fig FIGREF12. We detail each parts in the following sections."]}
{"question_id": "aebd1f0d728d0de5f76238844da044a44109f76f", "predicted_answer": "They incorporate direction and relative distance in attention by using relative positional encoding instead of absolute position encoding. Specifically, they use a revised relative positional encoding that utilizes sine and cosine functions to encode the relative positions between tokens. The sine function helps to capture directional information, while the cosine function captures distance information. By using this encoding, the attention score can distinguish between different directions and distances, allowing the model to better understand the relationships between tokens in the input sequence.", "predicted_evidence": ["Therefore, to improve the Transformer with direction- and distance-aware characteristic, we calculate the attention scores using the equations below:", "because $\\sin (-x)=-\\sin (x), \\cos (x)=\\cos (-x)$. This means for an offset $t$, the forward and backward relative positional encoding are the same with respect to the $\\cos (c_it)$ terms, but is the opposite with respect to the $\\sin (c_it)$ terms. Therefore, by using $R_{t-j}$, the attention score can distinguish different directions and distances.", "The first is that the sinusoidal position embedding used in the vanilla Transformer is aware of distance but unaware of the directionality. In addition, this property will lose when used in the vanilla Transformer. However, both the direction and distance information are important in the NER task. For example in Fig FIGREF3, words after \u201cin\" are more likely to be a location or time than words before it, and words before \u201cInc.\" are mostly likely to be of the entity type \u201cORG\". Besides, an entity is a continuous span of words. Therefore, the awareness of distance might help the word better recognizes its neighbor. To endow the Transformer with the ability of direction- and distance-awareness, we adopt the relative positional encoding BIBREF17, BIBREF18, BIBREF19. instead of the absolute position encoding. We propose a revised relative positional encoding that uses fewer parameters and performs better.", "where $f(\\mathbf {y}_{t-1},\\mathbf {y}_t,\\mathbf {s})$ computes the transition score from $\\mathbf {y}_{t-1}$ to $\\mathbf {y}_t$ and the score for $\\mathbf {y}_t$. The optimization target is to maximize $P(\\mathbf {y}|\\mathbf {s})$. When decoding, the Viterbi Algorithm is used to find the path achieves the maximum probability.", "The output of the multi-head attention will be further processed by the position-wise feed-forward networks, which can be represented as follows,"]}
{"question_id": "cb4086ad022197da79f28dc609d0de90108c4543", "predicted_answer": "Yes, according to the text, the proposed TENER model outperforms current NER state-of-the-art models, achieving new state-of-the-art performance in two NER datasets.", "predicted_evidence": ["With the above improvements, we can greatly boost the performance of Transformer encoder for NER.", "In this paper, we propose TENER, a model adopting Transformer Encoder with specific customizations for the NER task. Transformer Encoder has a powerful ability to capture the long-range context. In order to make the Transformer more suitable to the NER task, we introduce the direction-aware, distance-aware and un-scaled attention. Experiments in two English NER tasks and four Chinese NER tasks show that the performance can be massively increased. Under the same pre-trained embeddings and external knowledge, our proposed modification outperforms previous models in the six datasets. Meanwhile, we also found the adapted Transformer is suitable for being used as the English character encoder, because it has the potentiality to extract intricate patterns from characters. Experiments in two English NER datasets show that the adapted Transformer character encoder performs better than BiLSTM and CNN character encoders.", "In summary, to improve the performance of the Transformer-based model in the NER task, we explicitly utilize the directional relative positional encoding, reduce the number of parameters and sharp the attention distribution. After the adaptation, the performance raises a lot, making our model even performs better than BiLSTM based models. Furthermore, in the six NER datasets, we achieve state-of-the-art performance among models without considering the pre-trained language models or designed features.", "In this paper, we analyze the properties of Transformer and propose two specific improvements for NER.", "The comparison between different NER models on English NER datasets is shown in Table TABREF32. The poor performance of the Transformer in the NER datasets was also reported by BIBREF16. Although performance of the Transformer is higher than BIBREF16, it still lags behind the BiLSTM-based models BIBREF5. Nonetheless, the performance is massively enhanced by incorporating the relative positional encoding and unscaled attention into the Transformer. The adaptation not only makes the Transformer achieve superior performance than BiLSTM based models, but also unveil the new state-of-the-art performance in two NER datasets when only the Glove 100d embedding and CNN character embedding are used. The same deterioration of performance was observed when using the scaled attention. Besides, if ELMo was used BIBREF28, the performance of TENER can be further boosted as depicted in Table TABREF33."]}
{"question_id": "756a8a9125e6984e0ca768b653c6c760efa3db66", "predicted_answer": "According to the given context, KALM-QA achieves an accuracy score of:\n\n* 95% for parsing the queries on the first dataset (manually constructed general questions based on the 50 logical frames)\n* 100% on the second dataset (MetaQA dataset)", "predicted_evidence": ["For KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy\u2014much higher than the state-of-the-art machine learning approach BIBREF14 . Details of the evaluations can be found in BIBREF5 and BIBREF6 .", "This thesis proposal provides an overview of KALM, a system for knowledge authoring. In addition, it introduces KALM-QA, the question answering part of KALM. Experimental results show that both KALM and KALM-QA achieve superior accuracy as compared to the state-of-the-art systems.", "Attempto Parsing Engine. The input sentences are CNL sentences based on ACE grammar. KALM starts with parsing the input sentence using ACE Parser and generates the DRS structure BIBREF17 which captures the syntactic information of the sentences.", "The first three arguments of an lvp-fact identify the lexical unit, its part of speech, and the frame. The fourth argument is a set of pattern-terms, each having three parts: the name of a role, a grammatical pattern, and the required/optional flag. The grammatical pattern determines the grammatical context in which the lexical unit, a role, and a role-filler word can appear in that frame. Each grammatical pattern is captured by a parsing rule (a Prolog rule) that can be used to extract appropriate role-filler words based on the APE parses.", "Frame Parser. KALM performs frame-based parsing based on the DRS and produces a set of frames that represent the semantic relations a sentence implies. A frame BIBREF18 represents a semantic relation of a set of entities where each plays a particular role in the frame relation. We have designed a frame ontology, called FrameOnt, which is based on the frames in FrameNet BIBREF7 and encoded as a Prolog fact. For instance, the Commerce_Buy frame is shown below:"]}
{"question_id": "fe52b093735bb456d7e699aa9a2b806d2b498ba0", "predicted_answer": "The state-of-the-art systems are not explicitly mentioned in the given context. The text only mentions that KALM and KALM-QA achieve superior accuracy compared to the state-of-the-art systems, but it does not specify what those systems are.", "predicted_evidence": ["", "", "", "This thesis proposal provides an overview of KALM, a system for knowledge authoring. In addition, it introduces KALM-QA, the question answering part of KALM. Experimental results show that both KALM and KALM-QA achieve superior accuracy as compared to the state-of-the-art systems.", "Knowledge representation and reasoning (KRR) is the process of representing the domain knowledge in formal languages (e.g., SPARQL, Prolog) such that it can be used by expert systems to execute querying and reasoning services. KRR have been applied in many fields including financial regulations, medical diagnosis, laws, and so on. One major obstacle in KRR is the creation of large-scale knowledge bases with high quality. For one thing, this requires the knowledge engineers (KEs) not only to have the background knowledge in a certain domain but have enough skills in knowledge representation as well. Unfortunately, qualified KEs are also in short supply. Therefore, it would be useful to build a tool that allows the domain experts without any background in logic to construct and query the knowledge base simply from text."]}
{"question_id": "7748c072e07d6c6db5a34be38b4a5e97ac6d7999", "predicted_answer": "The two datasets used to evaluate KALM-QA are:\n\n1. A manually constructed dataset of general questions based on 50 logical frames.\n2. The MetaQA dataset, which contains almost 29,000 test questions and over 260,000 training questions.", "predicted_evidence": ["Time Reasoning. Time-related information is a crucial part of human knowledge, but semantic parsing that takes the time into account is rather hard. However, we can develop a CNL that would incorporate enough time related idioms to be useful in a number of domains of discourse (e.g., tax law). Time can then be added to DRSs and incorporated into our frame based approach down to the very level of the logical facts into which sentences will be translated. This time information can be represented either via special time-aware relations among events (e.g., before, after, causality, triggering) or using a reserved argument to represent time in each fluent.", "lvp(buy,v,Commerce_Buy, [", "For KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy\u2014much higher than the state-of-the-art machine learning approach BIBREF14 . Details of the evaluations can be found in BIBREF5 and BIBREF6 .", "", ""]}
{"question_id": "c97306c1be5d59cf27b1054adfa8f1da47d292ce", "predicted_answer": "The contributions of this paper are:\n\n1. Analysis of the distribution of events per token part-of-speech (POS) and per event class in an Indo-European language, highlighting the importance of verbs, nouns, adjectives, and prepositional phrases in event annotation.\n2. Investigation of the impact of different embedding configurations on the performance of a Bi-LSTM-CRF network for event extraction, including the effects of embedding dimensionality, quantity and variety of data used to generate embeddings, and coverage of embeddings.\n3. Identification of the best embedding configuration (Fastext-It) and its approach to generating embeddings, which outperforms other configurations.\n4. Insights into the importance of considering out-of-vocabulary (OVV) tokens when using word2vec and GloVe embeddings, with BIBREF15's embeddings being the most affected.\n5. Application of the EVENTI exercise formulation for determining the extent and class of event mentions in a text, using the It-TimeML `<EVENT>` tag definition.", "predicted_evidence": ["input problem solution", "Tables 1 and 1 report, respectively, the distribution of the events per token part-of speech (POS) and per event class. Not surprisingly, verbs are the largest annotated category, followed by nouns, adjectives, and prepositional phrases. Such a distribution reflects both a kind of \u201cnatural\u201d distribution of the realization of events in an Indo-european language, and, at the same time, specific annotation choices. For instance, adjectives have been annotated only when in a predicative position and when introduced by a copula or a copular construction. As for the classes, OCCURRENCE and STATE represent the large majority of all events, followed by the intensional ones (I_STATE and I_ACTION), expressing some factual relationship between the target events and their arguments, and finally the others (REPORTING, ASPECTUAL, and PERCEPTION).", "The results of the Bi-LSTM-CRF network are varied in both evaluation configurations. The differences are mainly due to the embeddings used to initialize the network. The best embedding configuration is Fastext-It that differentiate from all the others for the approach used for generating the embeddings. Embedding's dimensionality impacts on the performances supporting the findings in BIBREF14 , but it seems that the quantity (and variety) of data used to generate the embeddings can have a mitigating effect, as shown by the results of the DH-FBK-100 configuration (especially in the classification subtask, and in the Recall scores for the event extent subtask). Coverage of the embeddings (and consequenlty, tokenization of the dataset and the embeddings) is a further aspect to keep into account, but it seems to have a minor impact with respect to dimensionality.", "The results of the Bi-LSTM-CRF network are varied in both evaluation configurations. The differences are mainly due to the embeddings used to initialize the network. The best embedding configuration is Fastext-It that differentiate from all the others for the approach used for generating the embeddings. Embedding's dimensionality impacts on the performances supporting the findings in BIBREF14 , but it seems that the quantity (and variety) of data used to generate the embeddings can have a mitigating effect, as shown by the results of the DH-FBK-100 configuration (especially in the classification subtask, and in the Recall scores for the event extent subtask). Coverage of the embeddings (and consequenlty, tokenization of the dataset and the embeddings) is a further aspect to keep into account, but it seems to have a minor impact with respect to dimensionality. It turns out that BIBREF15 's embeddings are those suffering the most from out of vocabulary (OVV) tokens (2.14% and 1.06% in training, 2.77% and 1.84% in test for the word2vec model and GloVe, respectively) with respect to the others.", "We follow the formulation of the task as specified in the EVENTI exercise: determine the extent and the class of event mentions in a text, according to the It-TimeML $<$ EVENT $>$ tag definition (Subtask B in EVENTI)."]}
{"question_id": "e42916924b69cab1df25d3b4e6072feaa0ba8084", "predicted_answer": "The baselines used in this paper are:\n\n1. FBK-HLT system, which is a cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features.\n2. A publicly available Bi-LSTM network with a CRF classifier as the last layer (BIBREF14).", "predicted_evidence": ["Results for the experiments are illustrated in Table 2 . We also report the results of the best system that participated at EVENTI Subtask B, FBK-HLT BIBREF23 . FBK-HLT is a cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features. Figure 1 plots charts comparing F1 scores of the network initialized with each of the five embeddings against the FBK-HLT system for the event detection and classification tasks, respectively.", "We adapted a publicly available Bi-LSTM network with a CRF classifier as last layer BIBREF14 . BIBREF14 demonstrated that word embeddings, among other hyper-parameters, have a major impact on the performance of the network, regardless of the specific task. On the basis of these experimental observations, we decided to investigate the impact of different Italian word embeddings for the Subtask B Main Task of the EVENTI exercise. We thus selected 5 word embeddings for Italian to initialize the network, differentiating one with respect to each other either for the representation model used (word2vec vs. GloVe; CBOW vs. skip-gram), dimensionality (300 vs. 100), or corpora used for their generation (Italian Wikipedia vs. crawled web document vs. large textual corpora or archives):", "pensa (B-STATE $|$ I-STATE $|$ ... $|$ O) B-ISTATE", "The results of the Bi-LSTM-CRF network are varied in both evaluation configurations. The differences are mainly due to the embeddings used to initialize the network. The best embedding configuration is Fastext-It that differentiate from all the others for the approach used for generating the embeddings. Embedding's dimensionality impacts on the performances supporting the findings in BIBREF14 , but it seems that the quantity (and variety) of data used to generate the embeddings can have a mitigating effect, as shown by the results of the DH-FBK-100 configuration (especially in the classification subtask, and in the Recall scores for the event extent subtask). Coverage of the embeddings (and consequenlty, tokenization of the dataset and the embeddings) is a further aspect to keep into account, but it seems to have a minor impact with respect to dimensionality. It turns out that BIBREF15 's embeddings are those suffering the most from out of vocabulary (OVV) tokens (2.14% and 1.06% in training, 2.77% and 1.84% in test for the word2vec model and GloVe, respectively) with respect to the others.", "The results of the Bi-LSTM-CRF network are varied in both evaluation configurations. The differences are mainly due to the embeddings used to initialize the network. The best embedding configuration is Fastext-It that differentiate from all the others for the approach used for generating the embeddings. Embedding's dimensionality impacts on the performances supporting the findings in BIBREF14 , but it seems that the quantity (and variety) of data used to generate the embeddings can have a mitigating effect, as shown by the results of the DH-FBK-100 configuration (especially in the classification subtask, and in the Recall scores for the event extent subtask). Coverage of the embeddings (and consequenlty, tokenization of the dataset and the embeddings) is a further aspect to keep into account, but it seems to have a minor impact with respect to dimensionality."]}
{"question_id": "079ca5810060e1cdc12b5935d8c248492f0478b9", "predicted_answer": "Yes, the model can be extended to other languages. The authors have already demonstrated the effectiveness of the Bi-LSTM-CRF network with Italian word embeddings, and the approach can be applied to other languages by using language-specific word embeddings and adapting the model to the specific linguistic characteristics of the target language.", "predicted_evidence": ["Future work should focus on the development of embeddings that move away from the basic word level, integrating extra layers of linguistic analysis (e.g. syntactic dependencies) BIBREF24 , that have proven to be very powerful for the same task in English.", "We adapted a publicly available Bi-LSTM network with a CRF classifier as last layer BIBREF14 . BIBREF14 demonstrated that word embeddings, among other hyper-parameters, have a major impact on the performance of the network, regardless of the specific task. On the basis of these experimental observations, we decided to investigate the impact of different Italian word embeddings for the Subtask B Main Task of the EVENTI exercise. We thus selected 5 word embeddings for Italian to initialize the network, differentiating one with respect to each other either for the representation model used (word2vec vs. GloVe; CBOW vs. skip-gram), dimensionality (300 vs. 100), or corpora used for their generation (Italian Wikipedia vs. crawled web document vs. large textual corpora or archives):", "The results of the Bi-LSTM-CRF network are varied in both evaluation configurations. The differences are mainly due to the embeddings used to initialize the network. The best embedding configuration is Fastext-It that differentiate from all the others for the approach used for generating the embeddings. Embedding's dimensionality impacts on the performances supporting the findings in BIBREF14 , but it seems that the quantity (and variety) of data used to generate the embeddings can have a mitigating effect, as shown by the results of the DH-FBK-100 configuration (especially in the classification subtask, and in the Recall scores for the event extent subtask). Coverage of the embeddings (and consequenlty, tokenization of the dataset and the embeddings) is a further aspect to keep into account, but it seems to have a minor impact with respect to dimensionality.", "The results of the Bi-LSTM-CRF network are varied in both evaluation configurations. The differences are mainly due to the embeddings used to initialize the network. The best embedding configuration is Fastext-It that differentiate from all the others for the approach used for generating the embeddings. Embedding's dimensionality impacts on the performances supporting the findings in BIBREF14 , but it seems that the quantity (and variety) of data used to generate the embeddings can have a mitigating effect, as shown by the results of the DH-FBK-100 configuration (especially in the classification subtask, and in the Recall scores for the event extent subtask). Coverage of the embeddings (and consequenlty, tokenization of the dataset and the embeddings) is a further aspect to keep into account, but it seems to have a minor impact with respect to dimensionality. It turns out that BIBREF15 's embeddings are those suffering the most from out of vocabulary (OVV) tokens (2.14% and 1.06% in training, 2.77% and 1.84% in test for the word2vec model and GloVe, respectively) with respect to the others.", "As for the other parameters, the network maintains the optimized configurations used for the event detection task for English BIBREF14 : two LSTM layers of 100 units each, Nadam optimizer, variational dropout (0.5, 0.5), with gradient normalization ( $\\tau $ = 1), and batch size of 8. Character-level embeddings, learned using a Convolutional Neural Network (CNN) BIBREF22 , are concatenated with the word embedding vector to feed into the LSTM network. Final layer of the network is a CRF classifier."]}
{"question_id": "a3e7d7389228a197c8c44e0c504a791b60f2c80d", "predicted_answer": "The semantic concept label of a particular cluster is decided by generating a set of possible cluster labels (L) by taking the union of all hypernyms of the synsets in S* (the set of words in the cluster). Then, the best label is selected based on how well it fits the words in the cluster and distinguishes them from words outside the cluster. This is done by presenting the label and a word to an annotator and asking if the word falls under the concept. The label with the highest in-cluster rate (i.e., the rate at which words in the cluster fall under the label) and the lowest out-of-cluster rate (i.e., the rate at which words outside the cluster fall under the label) is chosen as the best label.", "predicted_evidence": ["To test cluster labels, we present the annotator with a label and a word, and we ask them whether the word falls under the concept. The concept is a potential cluster label and the word is either a word from that cluster or drawn randomly from the domain vocabulary. For a good label, the rate at which in-cluster words fall under the label should be much higher than the rate at which out-of-cluster words fall under. In our experiments, we tested the top 4 predicted labels and the centroid of the cluster as a strong baseline label. The centroid achieved an in-cluster rate of .60 and out-of-cluster rate of .18 (difference of .42). Our best performing predicted label achieved an in-cluster rate of .65 and an out-of-cluster rate of .04 (difference of .61), thus outperforming the centroid on both rates and increasing the gap between rates by nearly 20 points. In the Appendix, we include more detailed results on both tasks.", "With word-level associations in hand, our next goals were to discover coherent clusters among the words and to automatically label those clusters.", "Candidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$.", "Methods to infer gender-associated words and labeled clusters in any domain.", "To automatically label the clusters, we combined the grounded knowledge of WordNet BIBREF34 and context-sensitive strengths of domain-specific word embeddings. Our algorithm is similar to BIBREF28's approach, but we extend their method by introducing domain-specific word embeddings for clustering as well as a new technique for sense disambiguation. Given a cluster, our algorithm proceeds with the following three steps:"]}
{"question_id": "8b4bd0a962241ea548752212ebac145e2ced7452", "predicted_answer": "To discover coherent word clusters, the authors use a combination of WordNet and domain-specific word embeddings. Here's a step-by-step overview of their approach:\n\n1. **Sense disambiguation**: For each cluster, they assign each word to one of its WordNet synsets (a set of synonyms). They choose the synsets that minimize the total distance between them, assuming that the words in the cluster are semantically close in the context of the domain.\n2. **Clustering**: They use domain-specific word embeddings to cluster the words. This step is not explicitly described in the provided text, but it's implied that they use the word embeddings to group similar words together.\n3. **Labeling**: To automatically label the clusters, they use the chosen synsets from step 1 and the clustering results from step 2. They likely use the synsets to identify a common theme or concept that describes the words in each cluster, and use this theme as the label for the cluster.\n\nBy combining the strengths of WordNet (grounded knowledge) and domain-specific word embeddings (context-sensitive), they are able to discover coherent clusters of words and automatically label them with meaningful concepts.", "predicted_evidence": ["With word-level associations in hand, our next goals were to discover coherent clusters among the words and to automatically label those clusters.", "Methods to infer gender-associated words and labeled clusters in any domain.", "To test cluster labels, we present the annotator with a label and a word, and we ask them whether the word falls under the concept. The concept is a potential cluster label and the word is either a word from that cluster or drawn randomly from the domain vocabulary. For a good label, the rate at which in-cluster words fall under the label should be much higher than the rate at which out-of-cluster words fall under. In our experiments, we tested the top 4 predicted labels and the centroid of the cluster as a strong baseline label. The centroid achieved an in-cluster rate of .60 and out-of-cluster rate of .18 (difference of .42). Our best performing predicted label achieved an in-cluster rate of .65 and an out-of-cluster rate of .04 (difference of .61), thus outperforming the centroid on both rates and increasing the gap between rates by nearly 20 points. In the Appendix, we include more detailed results on both tasks.", "To automatically label the clusters, we combined the grounded knowledge of WordNet BIBREF34 and context-sensitive strengths of domain-specific word embeddings. Our algorithm is similar to BIBREF28's approach, but we extend their method by introducing domain-specific word embeddings for clustering as well as a new technique for sense disambiguation. Given a cluster, our algorithm proceeds with the following three steps:", "Sense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets."]}
{"question_id": "d39059340a79bdc0ebab80ad3308e3037d7d5773", "predicted_answer": "Each of the two introduced datasets consists of over 300,000 sentences.", "predicted_evidence": ["Two datasets for studying language and gender, each consisting of over 300K sentences.", "To test cluster labels, we present the annotator with a label and a word, and we ask them whether the word falls under the concept. The concept is a potential cluster label and the word is either a word from that cluster or drawn randomly from the domain vocabulary. For a good label, the rate at which in-cluster words fall under the label should be much higher than the rate at which out-of-cluster words fall under. In our experiments, we tested the top 4 predicted labels and the centroid of the cluster as a strong baseline label. The centroid achieved an in-cluster rate of .60 and out-of-cluster rate of .18 (difference of .42). Our best performing predicted label achieved an in-cluster rate of .65 and an out-of-cluster rate of .04 (difference of .61), thus outperforming the centroid on both rates and increasing the gap between rates by nearly 20 points. In the Appendix, we include more detailed results on both tasks.", "Novel findings that demonstrate in both domains that people do talk about women and men in different ways.", "Our contributions include:", "To automatically label the clusters, we combined the grounded knowledge of WordNet BIBREF34 and context-sensitive strengths of domain-specific word embeddings. Our algorithm is similar to BIBREF28's approach, but we extend their method by introducing domain-specific word embeddings for clustering as well as a new technique for sense disambiguation. Given a cluster, our algorithm proceeds with the following three steps:"]}
{"question_id": "31d4b0204702907dc0cd0f394cf9c984649e1fbf", "predicted_answer": "The strong baseline authors used is the centroid of the cluster as a label.", "predicted_evidence": ["To test cluster labels, we present the annotator with a label and a word, and we ask them whether the word falls under the concept. The concept is a potential cluster label and the word is either a word from that cluster or drawn randomly from the domain vocabulary. For a good label, the rate at which in-cluster words fall under the label should be much higher than the rate at which out-of-cluster words fall under. In our experiments, we tested the top 4 predicted labels and the centroid of the cluster as a strong baseline label. The centroid achieved an in-cluster rate of .60 and out-of-cluster rate of .18 (difference of .42). Our best performing predicted label achieved an in-cluster rate of .65 and an out-of-cluster rate of .04 (difference of .61), thus outperforming the centroid on both rates and increasing the gap between rates by nearly 20 points. In the Appendix, we include more detailed results on both tasks.", "Methods to infer gender-associated words and labeled clusters in any domain.", "We have presented two substantial datasets and a novel integration of methods to automatically infer gender associations in language. We have demonstrated that in both datasets, there are clear differences in how people talk about women and men. Furthermore, we have shown that clustering and cluster labeling are effective at identifying higher-level patterns of gender associations, and that our methods outperform strong baselines in human evaluations. In future work, we hope to use our findings to improve performance on tasks such as abusive language detection. We also hope to delve into finer-grained analyses, exploring how language around gender interacts with other variables, such as sexual orientation or profession (e.g. actresses versus female athletes). Finally, we plan to continue widening the scope of our study \u2013 for example, expanding our methods to include non-binary gender identities, evaluating changes in gender norms over time, and spreading to more domains, such as the political sphere.", "With word-level associations in hand, our next goals were to discover coherent clusters among the words and to automatically label those clusters.", "We also build on methods to cluster words in word embedding space and automatically label clusters. Clustering word embeddings has proven useful for discovering salient patterns in text corpora BIBREF25, BIBREF26. Once clusters are derived, we would like them to be interpretable. Much research simply considers the top-n words from each cluster, but this method can be subjective and time-consuming to interpret. Thus, there are efforts to design methods of automatic cluster labeling BIBREF27. We take a similar approach to BIBREF28, who leverage word embeddings and WordNet during labeling, and we extend their method with additional techniques and evaluations."]}
{"question_id": "371433bd3fb5042bacec4dfad3cfff66147c14f0", "predicted_answer": "Data-driven models usually respond to abuse inappropriately, and may even reinforce harmful biases. For example, a Seq2Seq model trained on \"clean\" data where all utterances containing abusive words were removed was rated as inappropriate by users. This suggests that data-driven response generation needs more effective control mechanisms to mitigate abuse.", "predicted_evidence": ["However, we believe that the ultimate measure for abuse mitigation should come from users interacting with the system. chin2019should make a first step into this direction by investigating different response styles (Avoidance, Empathy, Counterattacking) to verbal abuse, and recording the user's emotional reaction \u2013 hoping that eliciting certain emotions, such as guilt, will eventually stop the abuse. While we agree that stopping the abuse should be the ultimate goal, BIBREF28's study is limited in that participants were not genuine (ab)users, but instructed to abuse the system in a certain way. BIBREF29 report that a pilot using a similar setup let to unnatural interactions, which limits the conclusions we can draw about the effectiveness of abuse mitigation strategies. Our next step therefore is to employ our system with real users to test different mitigation strategies \u201cin the wild\" with the ultimate goal to find the best strategy to stop the abuse. The results of this current paper suggest that the strategy should be adaptive to user type/ age, as well as to the severity of abuse.", "conducted by Ma:2019:handlingChall where study participants were asked to rate the system's emotional competence after watching videos of challenging user behaviour. However, we believe that the ultimate measure for abuse mitigation should come from users interacting with the system. chin2019should make a first step into this direction by investigating different response styles (Avoidance, Empathy, Counterattacking) to verbal abuse, and recording the user's emotional reaction \u2013 hoping that eliciting certain emotions, such as guilt, will eventually stop the abuse. While we agree that stopping the abuse should be the ultimate goal, BIBREF28's study is limited in that participants were not genuine (ab)users, but instructed to abuse the system in a certain way. BIBREF29 report that a pilot using a similar setup let to unnatural interactions, which limits the conclusions we can draw about the effectiveness of abuse mitigation strategies. Our next step therefore is to employ our system with real users to test different mitigation strategies \u201cin the wild\" with the ultimate goal to find the best strategy to stop the abuse.", "Ethical challenges related to dialogue systems and conversational agents raise novel research questions, such as learning from biased data sets BIBREF0, and how to handle verbal abuse from the user's side BIBREF1, BIBREF2, BIBREF3, BIBREF4. As highlighted by a recent UNESCO report BIBREF5, appropriate responses to abusive queries are vital to prevent harmful gender biases: the often submissive and flirty responses by the female-gendered systems reinforce ideas of women as subservient. In this paper, we investigate the appropriateness of possible strategies by gathering responses from current state-of-the-art systems and ask crowd-workers to rate them.", "Our results show that: (1) The user's age has an significant effect on the ratings. For example, older users find jokes as a response to harassment highly inappropriate. (2) Perceived appropriateness also depends on the type of previous abuse. For example, avoidance is most appropriate after sexual demands. (3) All system were rated significantly higher than our negative adult-only baselines - except two data-driven systems, one of which is a Seq2Seq model trained on \u201cclean\" data where all utterances containing abusive words were removed BIBREF1. This leads us to believe that data-driven response generation need more effective control mechanisms BIBREF30.", "BIBREF26, and for eliciting a moral stance from a population BIBREF27. Our crowdsourced setup is similar to an \u201coverhearer experiment\u201d as e.g. conducted by Ma:2019:handlingChall where study participants were asked to rate the system's emotional competence after watching videos of challenging user behaviour. However, we believe that the ultimate measure for abuse mitigation should come from users interacting with the system. chin2019should make a first step into this direction by investigating different response styles (Avoidance, Empathy, Counterattacking) to verbal abuse, and recording the user's emotional reaction \u2013 hoping that eliciting certain emotions, such as guilt, will eventually stop the abuse. While we agree that stopping the abuse should be the ultimate goal, BIBREF28's study is limited in that participants were not genuine (ab)users, but instructed to abuse the system in a certain way. BIBREF29 report that a pilot using a similar setup let to unnatural interactions, which limits the conclusions we can draw about the effectiveness of abuse mitigation strategies."]}
{"question_id": "f64449a21c452bc5395a0f0a49fb49825e6385f4", "predicted_answer": "They gathered 9960 HITs (Human Intelligence Tasks) from 472 crowd workers, which resulted in 18,826 scores after removing unsuitable ratings.", "predicted_evidence": ["4 Data-driven approaches:", "Crowdsourced user studies are widely used for related tasks, such as evaluating dialogue strategies, e.g. BIBREF26, and for eliciting a moral stance from a population BIBREF27. Our crowdsourced setup is similar to an \u201coverhearer experiment\u201d as e.g. conducted by Ma:2019:handlingChall where study participants were asked to rate the system's emotional competence after watching videos of challenging user behaviour. However, we believe that the ultimate measure for abuse mitigation should come from users interacting with the system. chin2019should make a first step into this direction by investigating different response styles (Avoidance, Empathy, Counterattacking) to verbal abuse, and recording the user's emotional reaction \u2013 hoping that eliciting certain emotions, such as guilt, will eventually stop the abuse. While we agree that stopping the abuse should be the ultimate goal, BIBREF28's study is limited in that participants were not genuine (ab)users, but instructed to abuse the system in a certain way.", "Crowdsourced user studies are widely used for related tasks, such as evaluating dialogue strategies, e.g. BIBREF26, and for eliciting a moral stance from a population BIBREF27. Our crowdsourced setup is similar to an \u201coverhearer experiment\u201d as e.g. conducted by Ma:2019:handlingChall where study participants were asked to rate the system's emotional competence after watching videos of challenging user behaviour. However, we believe that the ultimate measure for abuse mitigation should come from users interacting with the system. chin2019should make a first step into this direction by investigating different response styles (Avoidance, Empathy, Counterattacking) to verbal abuse, and recording the user's emotional reaction \u2013 hoping that eliciting certain emotions, such as guilt, will eventually stop the abuse.", "Ratings are then normalised on a scale from [0-1]. This methodology was shown to produce more reliable user ratings than commonly used Likert Scales. In addition, we collect demographic information, including gender and age group. In total we collected 9960 HITs from 472 crowd workers. In order to identify spammers and unsuitable ratings, we use the responses from the adult-only bots as test questions: We remove users who give high ratings to sexual bot responses the majority (more than 55%) of the time.18,826 scores remain - resulting in an average of 7.7 ratings per individual system reply and 1568.8 ratings per response type as listed in Table TABREF14.Due to missing demographic data - and after removing malicious crowdworkers - we only consider a subset of 190 raters for our demographic study. The group is composed of 130 men and 60 women.", "Ratings are then normalised on a scale from [0-1]. This methodology was shown to produce more reliable user ratings than commonly used Likert Scales. In addition, we collect demographic information, including gender and age group. In total we collected 9960 HITs from 472 crowd workers. In order to identify spammers and unsuitable ratings, we use the responses from the adult-only bots as test questions: We remove users who give high ratings to sexual bot responses the majority (more than 55%) of the time.18,826 scores remain - resulting in an average of 7.7 ratings per individual system reply and 1568.8 ratings per response type as listed in Table TABREF14.Due to missing demographic data - and after removing malicious crowdworkers - we only consider a subset of 190 raters for our demographic study. The group is composed of 130 men and 60 women. Most raters (62.6%) are under the age of 44, with similar proportions across age groups for men and women."]}
{"question_id": "3aeb25e334c8129b376f11c7077bcb2dd54f7e0e", "predicted_answer": "According to the text, 3 different response styles or strategies were evaluated:\n\n1. Avoidance\n2. Empathy\n3. Counterattacking", "predicted_evidence": ["We then use these prompts to elicit responses from the following systems, following methodology from Amanda:EthicsNLP2018.", "4 Data-driven approaches:", "Crowdsourced user studies are widely used for related tasks, such as evaluating dialogue strategies, e.g. BIBREF26, and for eliciting a moral stance from a population BIBREF27. Our crowdsourced setup is similar to an \u201coverhearer experiment\u201d as e.g. conducted by Ma:2019:handlingChall where study participants were asked to rate the system's emotional competence after watching videos of challenging user behaviour. However, we believe that the ultimate measure for abuse mitigation should come from users interacting with the system. chin2019should make a first step into this direction by investigating different response styles (Avoidance, Empathy, Counterattacking) to verbal abuse, and recording the user's emotional reaction \u2013 hoping that eliciting certain emotions, such as guilt, will eventually stop the abuse. While we agree that stopping the abuse should be the ultimate goal, BIBREF28's study is limited in that participants were not genuine (ab)users, but instructed to abuse the system in a certain way.", "Crowdsourced user studies are widely used for related tasks, such as evaluating dialogue strategies, e.g. BIBREF26, and for eliciting a moral stance from a population BIBREF27. Our crowdsourced setup is similar to an \u201coverhearer experiment\u201d as e.g. conducted by Ma:2019:handlingChall where study participants were asked to rate the system's emotional competence after watching videos of challenging user behaviour. However, we believe that the ultimate measure for abuse mitigation should come from users interacting with the system. chin2019should make a first step into this direction by investigating different response styles (Avoidance, Empathy, Counterattacking) to verbal abuse, and recording the user's emotional reaction \u2013 hoping that eliciting certain emotions, such as guilt, will eventually stop the abuse.", "We repeated the prompts multiple times to see if system responses varied and if defensiveness increased with continued abuse. If this was the case, we included all responses in the study. Following this methodology, we collected a total of 2441 system replies in July-August 2018 - 3.5 times more data than Amanda:EthicsNLP2018 - which 2 expert annotators manually annotated according to the categories in Table TABREF14 ($\\kappa =0.66$)."]}
{"question_id": "c19e9fd2f1c969e023fb99b74e78eb1f3db8e162", "predicted_answer": "Yes, the automatic annotation was evaluated.", "predicted_evidence": ["In the following, we describe the rationale behind the annotation of the dataset including the definition of the various semantic classes and the annotation guidelines.", "The remainder of this article is structured as follows. First, Section SECREF3 gives a brief overview of related work. Section SECREF4 describes, in detail, the rationale behind the annotation of the dataset including the different semantic classes annotated. Section SECREF5 describes several characteristics of the dataset, followed by a short evaluation (Section SECREF6) and conclusions as well as future work (Section SECREF7).", "This evaluation has revealed deficiencies in the annotation guidelines, especially regarding court decision and legal literature as well as non-entities. It would also be helpful for the identification and classification to list well-known sources of law, court decision, legal literature etc.", "The dataset was thoroughly evaluated, see leitner2019fine for more details. As state of the art models, Conditional Random Fields (CRFs) and bidirectional Long-Short Term Memory Networks (BiLSTMs) were tested with the two variants of annotation. For CRFs, these are: CRF-F (with features), CRF-FG (with features and gazetteers), CRF-FGL (with features, gazetteers and lookup). For BiLSTM, we used models with pre-trained word embeddings BIBREF22: BiLSTM-CRF BIBREF23, BiLSTM-CRF+ with character embeddings from BiLSTM BIBREF24, and BiLSTM-CNN-CRF with character embeddings from CNN BIBREF25. To evaluate the performance we used stratified 10-fold cross-validation. As expected, BiLSTMs perform best (see Table ).", "The dataset was thoroughly evaluated, see leitner2019fine for more details. As state of the art models, Conditional Random Fields (CRFs) and bidirectional Long-Short Term Memory Networks (BiLSTMs) were tested with the two variants of annotation. For CRFs, these are: CRF-F (with features), CRF-FG (with features and gazetteers), CRF-FGL (with features, gazetteers and lookup). For BiLSTM, we used models with pre-trained word embeddings BIBREF22: BiLSTM-CRF BIBREF23, BiLSTM-CRF+ with character embeddings from BiLSTM BIBREF24, and BiLSTM-CNN-CRF with character embeddings from CNN BIBREF25. To evaluate the performance we used stratified 10-fold cross-validation. As expected, BiLSTMs perform best (see Table ). The F$_1$ score for the fine-grained classification reaches 95.46 and 95.95 for the coarse-grained one."]}
{"question_id": "230ff86b7b90b87c33c53014bb1e9c582dfc107f", "predicted_answer": "The morphological typologies considered are not explicitly stated in the given context. However, it can be inferred that the authors are interested in analyzing the performance of character-level models on languages with varying morphological typologies, suggesting that they are considering different language families or types with distinct morphological characteristics.", "predicted_evidence": ["We use three types of units: (1) words (2) characters and character sequences and (3) outputs of morphological analysis. Words serve as a lower bound; while morphology is used as an upper bound for comparison. Table 1 shows sample outputs of various $\\rho $ functions.", "Morphological analysis already provides the aforementioned information about the words. However access to useful morphological features may be problematic due to software licensing issues, lack of robust morphological analyzers and high ambiguity among analyses. Character-level models (CLM), being a cheaper and accessible alternative to morphology, have been reported as performing competitively on various NLP tasks BIBREF0 , BIBREF1 , BIBREF2 . However the extent to which these tasks depend on morphology is small; and their relation to semantics is weak. Hence, little is known on their true ability to reveal the underlying morphological structure of a word and their semantic capabilities. Furthermore, their behaviour across languages from different families; and their limitations and strengths such as handling of long-range dependencies, reaction to model complexity or performance on out-of-domain data are unknown. Analyzing such issues is a key to fully understanding the character-level models.", "Our main results on test and development sets for models that use words, characters (char), character trigrams (char3) and morphological analyses (morph) are given in Table 3 . We calculate improvement over word (IOW) for each subword model and improvement over the best character model (IOC) for the morph. IOW and IOC values are calculated on the test set.", "Statistics for the training split for all languages are given in Table 2 . Here, #pred is number of predicates, and #role refers to number distinct semantic roles that occur more than 10 times. More detailed statistics about the datasets can be found in BIBREF27 , BIBREF19 , BIBREF20 .", "We use a simple method based on bidirectional LSTMs to train three types of base semantic role labelers that employ (1) words (2) characters and character sequences and (3) gold morphological analysis. The gold morphology serves as the upper bound for us to compare and analyze the performances of character-level models on languages of varying morphological typologies. We carry out an exhaustive error analysis for each language type and analyze the strengths and limitations of character-level models compared to morphology. In regard to the diversity hypothesis which states that diversity of systems in ensembles lead to further improvement, we combine character and morphology-level models and measure the performance of the ensemble to better understand how similar they are."]}
{"question_id": "dc23006d67f20f430f1483398de4a89c0be4efe2", "predicted_answer": "The answer is yes, the model considers both derivational and inflectional morphology. This is implied by the mention of \"morphological analysis\" and \"morphological analyses (morph)\" in the text, which suggests that the model takes into account the internal structure of words, including both derivational and inflectional morphology.", "predicted_evidence": ["We use three types of units: (1) words (2) characters and character sequences and (3) outputs of morphological analysis. Words serve as a lower bound; while morphology is used as an upper bound for comparison. Table 1 shows sample outputs of various $\\rho $ functions.", "Our main results on test and development sets for models that use words, characters (char), character trigrams (char3) and morphological analyses (morph) are given in Table 3 . We calculate improvement over word (IOW) for each subword model and improvement over the best character model (IOC) for the morph. IOW and IOC values are calculated on the test set.", "Long range dependency is considered as an important linguistic issue that is hard to solve. Therefore the ability to handle it is a strong performance indicator. To gain insights on this issue, we measure how models perform as the distance between the predicate and the argument increases. The unit of measure is number of tokens between the two; and argument is defined as the head of the argument phrase in accordance with dependency-based SRL task. For that purpose, we created bins of [0-4], [5-9], [10-14] and [15-19] distances. Then, we have calculate F1 scores for arguments in each bin. Due to low number of predicate-argument pairs in buckets, we could not analyze German and Turkish; and also the bin [15-19] is only used for Czech. Our results are shown in Fig. 3 . We observe that either char or char3 closely follows the oracle for all languages.", "Long range dependency is considered as an important linguistic issue that is hard to solve. Therefore the ability to handle it is a strong performance indicator. To gain insights on this issue, we measure how models perform as the distance between the predicate and the argument increases. The unit of measure is number of tokens between the two; and argument is defined as the head of the argument phrase in accordance with dependency-based SRL task. For that purpose, we created bins of [0-4], [5-9], [10-14] and [15-19] distances. Then, we have calculate F1 scores for arguments in each bin. Due to low number of predicate-argument pairs in buckets, we could not analyze German and Turkish; and also the bin [15-19] is only used for Czech. Our results are shown in Fig. 3 . We observe that either char or char3 closely follows the oracle for all languages. The gap between the two does not increase with the distance, suggesting that the performance gap is not related to long range dependencies.", "To expand our understanding and reveal the limitations and strengths of the models, we analyze their ability to handle long range dependencies, their relation with training data and model size; and measure their performances on out of domain data."]}
{"question_id": "887d7f3edf37ccc6bf2e755dae418b04d2309686", "predicted_answer": "The morphological features used are:\n\n* Lemma of the token\n* Language-specific morphological tags\n* Additional information for some languages, such as parts-of-speech tags for Turkish.", "predicted_evidence": ["We use three types of units: (1) words (2) characters and character sequences and (3) outputs of morphological analysis. Words serve as a lower bound; while morphology is used as an upper bound for comparison. Table 1 shows sample outputs of various $\\rho $ functions.", "Morphological analysis already provides the aforementioned information about the words. However access to useful morphological features may be problematic due to software licensing issues, lack of robust morphological analyzers and high ambiguity among analyses. Character-level models (CLM), being a cheaper and accessible alternative to morphology, have been reported as performing competitively on various NLP tasks BIBREF0 , BIBREF1 , BIBREF2 . However the extent to which these tasks depend on morphology is small; and their relation to semantics is weak. Hence, little is known on their true ability to reveal the underlying morphological structure of a word and their semantic capabilities. Furthermore, their behaviour across languages from different families; and their limitations and strengths such as handling of long-range dependencies, reaction to model complexity or performance on out-of-domain data are unknown. Analyzing such issues is a key to fully understanding the character-level models.", "Here, char function simply splits the token into its characters. Similar to n-gram language models, char3 slides a character window of width $n=3$ over the token. Finally, gold morphological features are used as outputs of morph-language. Throughout this paper, we use morph and oracle interchangably, i.e., morphology-level models (MLM) have access to gold tags unless otherwise is stated. For all languages, morph outputs the lemma of the token followed by language specific morphological tags. As an exception, it outputs additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units. Due to low scores obtained from our preliminary experiments and unsatisfactory results from previous studies BIBREF13 , we excluded these units.", "Our main results on test and development sets for models that use words, characters (char), character trigrams (char3) and morphological analyses (morph) are given in Table 3 . We calculate improvement over word (IOW) for each subword model and improvement over the best character model (IOC) for the morph. IOW and IOC values are calculated on the test set.", "We use a simple method based on bidirectional LSTMs to train three types of base semantic role labelers that employ (1) words (2) characters and character sequences and (3) gold morphological analysis. The gold morphology serves as the upper bound for us to compare and analyze the performances of character-level models on languages of varying morphological typologies. We carry out an exhaustive error analysis for each language type and analyze the strengths and limitations of character-level models compared to morphology. In regard to the diversity hypothesis which states that diversity of systems in ensembles lead to further improvement, we combine character and morphology-level models and measure the performance of the ensemble to better understand how similar they are."]}
{"question_id": "b8a3ab219be6c1e6893fe80e1fbf14f0c0c3c97c", "predicted_answer": "The paper doesn't explicitly mention the specific datasets used, but it implies that the datasets consist of scientific publications with figures and their corresponding captions.", "predicted_evidence": ["We have used the following datasets for training and evaluation:", "Scientific knowledge is heterogeneous and can present itself in many forms, including text, mathematical equations, figures and tables. Like many other manifestations of human thought, the scientific discourse usually adopts the form of a narrative, a scientific publication where related knowledge is presented in mutually supportive ways over different modalities. In the case of scientific figures, like charts, images and diagrams, these are usually accompanied by a text paragraph, a caption, that elaborates on the analysis otherwise visually represented.", "The main idea of our approach is to learn a correspondence task between scientific figures and their captions as they appear in a scientific publication. The information captured in the caption explains the corresponding figure in natural language, providing guidance to identify the key features of the figure and vice versa. By seeing a figure and reading the textual description in its caption we ultimately aim to learn representations that capture e.g. what it means that two plots are similar or what gravity looks like.", "In this paper, we make use of this observation and tap on the potential of learning from the enormous source of free supervision available in the scientific literature, with millions of figures and their captions. We build models that learn from the scientific discourse both visually and textually by simply looking at the figures and reading their explanatory captions, inspired in how humans learn by reading a scientific publication. To this purpose, we explore how multi-modal scientific knowledge can be learnt from the correspondence between figures and captions.", "We inspect the features learnt by our FCC task to gain a deeper understanding of the syntactic and semantic patterns captured for figure and caption representation. The findings reported herein are qualitatively consistent for all the FCC variations in table TABREF15."]}
{"question_id": "780c7993d446cd63907bb38992a60bbac9cb42b1", "predicted_answer": "The language of the captions is natural language.", "predicted_evidence": ["A corpus of scientific figures and captions extracted from SN SciGraph and AI2 Semantic Scholar.", "A qualitative and quantitative analysis of the learnt text and visual features through transfer learning tasks.", "In this paper, we make use of this observation and tap on the potential of learning from the enormous source of free supervision available in the scientific literature, with millions of figures and their captions. We build models that learn from the scientific discourse both visually and textually by simply looking at the figures and reading their explanatory captions, inspired in how humans learn by reading a scientific publication. To this purpose, we explore how multi-modal scientific knowledge can be learnt from the correspondence between figures and captions.", "The main idea of our approach is to learn a correspondence task between scientific figures and their captions as they appear in a scientific publication. The information captured in the caption explains the corresponding figure in natural language, providing guidance to identify the key features of the figure and vice versa. By seeing a figure and reading the textual description in its caption we ultimately aim to learn representations that capture e.g. what it means that two plots are similar or what gravity looks like.", "In this section, first we evaluate the actual FCC task against two supervised baselines. Then, we situate our work in the more general image-sentence matching problem, showing empirical evidence of the additional complexity associated to the scientific domain and the figure-caption case compared to natural images. Next, we test the visual and text features learnt in the FCC task in two different transfer learning settings: classification of scientific figures and captions and multi-modal machine comprehension for question answering given a context of text, figures and images."]}
{"question_id": "3da4606a884593f7702d098277b9a6ce207c080b", "predicted_answer": "The ad-hoc approaches explored are:\n\n1. Random\n2. BiDAF (focused on text machine comprehension)\n3. Text only ($TQA_1$, based on MemoryNet)\n4. Text+image ($TQA_2$, VQA)\n5. Text+diagrams ($TQA_3$, DSDP-NET)\n6. $TQA_4$ (replacing visual features in $TQA_2$ with those learnt by the FCC visual subnetwork in an unsupervised way)\n7. $TQA_5$ (replacing visual features in $TQA_2$ with those learnt by the FCC visual subnetwork with pre-trained semantic embeddings)\n\nThese approaches are compared to the proposed FCC method in the task of determining whether a figure and a caption correspond.", "predicted_evidence": ["We focus on multiple-choice questions, 73% of the dataset. Table TABREF24 shows the performance of our model against the results reported in BIBREF23 for five TQA baselines: random, BiDAF (focused on text machine comprehension), text only ($TQA_1$, based on MemoryNet), text+image ($TQA_2$, VQA), and text+diagrams ($TQA_3$, DSDP-NET). We successfully reproduced the $TQA_1$ and $TQA_2$ architectures and adapted the latter. Then, we replaced the visual features in $TQA_2$ with those learnt by the FCC visual subnetwork both in a completely unsupervised way ($FCC_6$ in table TABREF15) and with pre-trained semantic embeddings ($FCC_7$), resulting in $TQA_4$ and $TQA_5$, respectively.", "We evaluate our method in the task it was trained to solve: determining whether a figure and a caption correspond. We also compare the performance of the FCC task against two supervised baselines, training them on a classification task against the SciGraph taxonomy. For such baselines we first train the vision and language networks independently and then combine them. The feature extraction parts of both networks are the same as described in section SECREF6. On top of them, we attach a fully connected layer with 128 neurons and ReLU activation and a softmax layer, with as many neurons as target classes.", "Knowledge fusion approaches like BIBREF12 investigate the potential of complementing KG embeddings with text and natural images by integrating information across the three modalities in a single latent representation. They assume pre-trained entity representations exist in each individual modality, e.g. the visual features encoding the image of a ball, the word embeddings associated to the token \"ball\", and the KG embeddings related to the ball entity, which are then stitched together. In contrast, FCC co-trains text and visual features from figures and their captions and supports the enrichment of such features with lexical and semantic knowledge transferred from a KG during the training of the FCC task.", "Scientific knowledge is heterogeneous and can present itself in many forms, including text, mathematical equations, figures and tables. Like many other manifestations of human thought, the scientific discourse usually adopts the form of a narrative, a scientific publication where related knowledge is presented in mutually supportive ways over different modalities. In the case of scientific figures, like charts, images and diagrams, these are usually accompanied by a text paragraph, a caption, that elaborates on the analysis otherwise visually represented.", "We present the FCC task in section SECREF3, including the network architecture, training protocol, and how adding pre-trained word and semantic embeddings can enrich the resulting text and visual features. In section SECREF4, we first introduce our datasets and evaluate the performance of our method in the task it was trained to solve, the correspondence between scientific figures and captions. Then, we relate our work to the state of the art in image-sentence matching and evaluate our approach in two challenging transfer learning tasks: caption and figure classification and multi-modal machine comprehension. In section SECREF5 we perform a qualitative study that illustrates how the FCC task leads to detailed textual and visual discrimination. Finally, in section SECREF6 we conclude the paper and advance future work."]}
{"question_id": "91336f12ab94a844b66b607f8621eb8bbd209f32", "predicted_answer": "The two supervised baselines they compared with are:\n\n1. Direct combination baseline: This baseline computes the figure-caption correspondence through the scalar product between the softmax outputs of both networks.\n2. Supervised pre-training baseline: This baseline freezes the weights of the feature extraction trunks from the two trained networks, assembles them in the FCC architecture, and trains the FCC task on the fully connected layers.", "predicted_evidence": ["We evaluate our method in the task it was trained to solve: determining whether a figure and a caption correspond. We also compare the performance of the FCC task against two supervised baselines, training them on a classification task against the SciGraph taxonomy. For such baselines we first train the vision and language networks independently and then combine them. The feature extraction parts of both networks are the same as described in section SECREF6. On top of them, we attach a fully connected layer with 128 neurons and ReLU activation and a softmax layer, with as many neurons as target classes.", "The direct combination baseline computes the figure-caption correspondence through the scalar product between the softmax outputs of both networks. If it exceeds a threshold, which we heuristically fixed on 0.325, the result is positive. The supervised pre-training baseline freezes the weights of the feature extraction trunks from the two trained networks, assembles them in the FCC architecture as shown in section SECREF6, and trains the FCC task on the fully connected layers. While direct combination provides a notion of the agreement between the two branches, supervised pre-training is the most similar supervised approach to our method.", "In this section, first we evaluate the actual FCC task against two supervised baselines. Then, we situate our work in the more general image-sentence matching problem, showing empirical evidence of the additional complexity associated to the scientific domain and the figure-caption case compared to natural images. Next, we test the visual and text features learnt in the FCC task in two different transfer learning settings: classification of scientific figures and captions and multi-modal machine comprehension for question answering given a context of text, figures and images.", "We present the FCC task in section SECREF3, including the network architecture, training protocol, and how adding pre-trained word and semantic embeddings can enrich the resulting text and visual features. In section SECREF4, we first introduce our datasets and evaluate the performance of our method in the task it was trained to solve, the correspondence between scientific figures and captions. Then, we relate our work to the state of the art in image-sentence matching and evaluate our approach in two challenging transfer learning tasks: caption and figure classification and multi-modal machine comprehension. In section SECREF5 we perform a qualitative study that illustrates how the FCC task leads to detailed textual and visual discrimination. Finally, in section SECREF6 we conclude the paper and advance future work.", "We propose a 2-branch neural architecture (figure FIGREF7) that has three main parts: the vision and language subnetworks, respectively extracting visual and text features, and a fusion subnetwork that takes the resulting features from the visual and text blocks and uses them to evaluate figure-caption correspondence."]}
{"question_id": "c5221bb28e58a4f13cf2eccce0e1b1bec7dd3c13", "predicted_answer": "Yes, the data is specific to the domain of scientific publications, particularly figures and captions extracted from scientific papers.", "predicted_evidence": ["We have used the following datasets for training and evaluation:", "A study of the complexity of figure-caption correspondence compared to classical image-sentence matching.", "We also estimated a notion of semantic specificity based on the concepts of a KG. For each visual feature, we aggregated the captions of the figures that most activate it and used Cogito to disambiguate the Sensigrafo concepts that appear in them. Then, we estimated how important each concept is to each feature by calculating its tf-idf. Finally, we averaged the resulting values to obtain a consolidated semantic specificity score per feature.", "A corpus of scientific figures and captions extracted from SN SciGraph and AI2 Semantic Scholar.", "Scientific knowledge is heterogeneous and can present itself in many forms, including text, mathematical equations, figures and tables. Like many other manifestations of human thought, the scientific discourse usually adopts the form of a narrative, a scientific publication where related knowledge is presented in mutually supportive ways over different modalities. In the case of scientific figures, like charts, images and diagrams, these are usually accompanied by a text paragraph, a caption, that elaborates on the analysis otherwise visually represented."]}
{"question_id": "42a4ab4607a9eec42c427a817b7e898230d26444", "predicted_answer": "The figures and captions come from scientific publications.", "predicted_evidence": ["In this paper, we make use of this observation and tap on the potential of learning from the enormous source of free supervision available in the scientific literature, with millions of figures and their captions. We build models that learn from the scientific discourse both visually and textually by simply looking at the figures and reading their explanatory captions, inspired in how humans learn by reading a scientific publication. To this purpose, we explore how multi-modal scientific knowledge can be learnt from the correspondence between figures and captions.", "The main idea of our approach is to learn a correspondence task between scientific figures and their captions as they appear in a scientific publication. The information captured in the caption explains the corresponding figure in natural language, providing guidance to identify the key features of the figure and vice versa. By seeing a figure and reading the textual description in its caption we ultimately aim to learn representations that capture e.g. what it means that two plots are similar or what gravity looks like.", "We propose a 2-branch neural architecture (figure FIGREF7) that has three main parts: the vision and language subnetworks, respectively extracting visual and text features, and a fusion subnetwork that takes the resulting features from the visual and text blocks and uses them to evaluate figure-caption correspondence.", "We inspect the features learnt by our FCC task to gain a deeper understanding of the syntactic and semantic patterns captured for figure and caption representation. The findings reported herein are qualitatively consistent for all the FCC variations in table TABREF15.", "Scientific knowledge is heterogeneous and can present itself in many forms, including text, mathematical equations, figures and tables. Like many other manifestations of human thought, the scientific discourse usually adopts the form of a narrative, a scientific publication where related knowledge is presented in mutually supportive ways over different modalities. In the case of scientific figures, like charts, images and diagrams, these are usually accompanied by a text paragraph, a caption, that elaborates on the analysis otherwise visually represented."]}
{"question_id": "622efbecd9350a0f4487bdff2b8b362ef2541f3c", "predicted_answer": "No, the top teams did not experiment with lexicons. In fact, the paper explicitly states that they tackled the task \"without the usage of external information, such as lexicons\".", "predicted_evidence": ["We experimented with GloVe BIBREF7 as pre-trained word embedding vectors, for sizes 25, 50 and 100. These are vectors trained on a dataset of 2B tweets, with a total vocabulary of 1.2 M. To pre-process the data, we used Twokenizer BIBREF8 , which basically provides a set of curated rules to split the tweets into tokens. We also use Tweeboparser BIBREF9 to get the POS-tags for each tweet.", "To test our model, we experiment using the training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger. These were annotated using Best-Worst Scaling (BWS) to obtain very reliable scores BIBREF6 .", "On the fear dataset, again we observed that embeddings of size 50 provided the best results, offering average gains of 0.12 ( INLINEFORM0 ) and 0.11 ( INLINEFORM1 ) for sizes 25 and 100, respectively. When it comes to the size of the RNN hidden state, our experiments showed that using 100 hidden units offered the best results, with average absolute gains of 0.117 ( INLINEFORM2 ) and 0.108 ( INLINEFORM3 ) over sizes 50 and 200.", "For training, we used mini-batch stochastic gradient descent with a batch size of 16 and padded sequences to a maximum size of 50 tokens, given the nature of the data. We used exponential decay of ratio INLINEFORM0 and early stopping on the validation when there was no improvement after 1000 steps. Our code is available for download on GitHub .", "The task is specially challenging since tweets contain informal language, spelling errors and text referring to external content. Given the 140 character limit of tweets, it is also possible to find some phenomena such as the intensive usage of emoticons and of other special Twitter features, such as hashtags and usernames mentions \u2014used to call or notify other users. In this paper we describe our system designed for the WASSA-2017 Shared Task on Emotion Intensity, which we tackle based on the premise of representation learning without the usage of external information, such as lexicons. In particular, we use a Bi-LSTM model with intra-sentence attention on top of word embeddings to generate a tweet representation that is suitable for emotion intensity. Our results show that our proposed model offers interesting capabilities compared to approaches that do rely on external information sources."]}
{"question_id": "f54e19f7ecece1bb0ef3171403ae322ad572ff00", "predicted_answer": "No, they did not experiment with lexicons in this study, but they suggest that using additional features derived from emotion or sentiment lexicons could be a promising direction for future work.", "predicted_evidence": ["We experimented with GloVe BIBREF7 as pre-trained word embedding vectors, for sizes 25, 50 and 100. These are vectors trained on a dataset of 2B tweets, with a total vocabulary of 1.2 M. To pre-process the data, we used Twokenizer BIBREF8 , which basically provides a set of curated rules to split the tweets into tokens. We also use Tweeboparser BIBREF9 to get the POS-tags for each tweet.", "To avoid over-fitting, we used dropout regularization, experimenting with keep probabilities of INLINEFORM0 and INLINEFORM1 . We also added a weighed L2 regularization term to our loss function. We experimented with different values for weight INLINEFORM2 , with a minimum value of 0.01 and a maximum of 0.2.", "EmoAtt is based on a bidirectional RNN that receives an embedded input sequence INLINEFORM0 and returns a list of hidden vectors that capture the context each input token INLINEFORM1 . To improve the capabilities of the RNN to capture short-term temporal dependencies BIBREF4 , we define the following: DISPLAYFORM0", "While the structure of our introduced model allows us to easily include more linguistic features that could potentially improve our predictive power, such as lexicons, since our focus is to study sentence representation for emotion intensity, we do not experiment adding any additional sources of information as input.", "To validate the usefulness of our binary features, we performed an ablation experiment and trained our best models for each corpus without them. Table TABREF15 summarizes our results in terms of Pearson correlation on the development portion of the datasets. As seen, performance decreases in all cases, which shows that indeed these features are critical for performance, allowing the model to better capture the semantics of words missing in GloVe. In this sense, we think the usage of additional features, such as the ones derived from emotion or sentiment lexicons could indeed boost our model capabilities. This is proposed for future work."]}
{"question_id": "4137a82d7752be7a6c142ceb48ce784fd475fb06", "predicted_answer": "The baseline was the Weka baseline.", "predicted_evidence": ["We experimented with GloVe BIBREF7 as pre-trained word embedding vectors, for sizes 25, 50 and 100. These are vectors trained on a dataset of 2B tweets, with a total vocabulary of 1.2 M. To pre-process the data, we used Twokenizer BIBREF8 , which basically provides a set of curated rules to split the tweets into tokens. We also use Tweeboparser BIBREF9 to get the POS-tags for each tweet.", "In this paper we introduced an intra-sentence attention RNN for the of emotion intensity, which we developed for the WASSA-2017 Shared Task on Emotion Intensity. Our model does not make use of external information except for pre-trained embeddings and is able to outperform the Weka baseline for the development set, but not in the test set. In the shared task, it obtained the 13th place among 22 competitors.", "On the fear dataset, again we observed that embeddings of size 50 provided the best results, offering average gains of 0.12 ( INLINEFORM0 ) and 0.11 ( INLINEFORM1 ) for sizes 25 and 100, respectively. When it comes to the size of the RNN hidden state, our experiments showed that using 100 hidden units offered the best results, with average absolute gains of 0.117 ( INLINEFORM2 ) and 0.108 ( INLINEFORM3 ) over sizes 50 and 200.", "In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings.", "EmoAtt is based on a bidirectional RNN that receives an embedded input sequence INLINEFORM0 and returns a list of hidden vectors that capture the context each input token INLINEFORM1 . To improve the capabilities of the RNN to capture short-term temporal dependencies BIBREF4 , we define the following: DISPLAYFORM0"]}
{"question_id": "6c50871294562e4886ede804574e6acfa8d1a5f9", "predicted_answer": "The result of the experiment was that the intra-sentence attention RNN model outperformed the Weka baseline on the development dataset, but performed worse on the test dataset. Additionally, the model achieved the best results with 50-dimensional embeddings, and using 50 or 100 hidden units for the RNN offered statistically equivalent results.", "predicted_evidence": ["While analyzing the emotional content in text, mosts tasks are almost always framed as classification tasks, where the intention is to identify one emotion among many for a sentence or passage. However, it is often useful for applications to know the degree to which an emotion is expressed in text. To this end, the WASSA-2017 Shared Task on Emotion Intensity BIBREF0 represents the first task where systems have to automatically determine the intensity of emotions in tweets. Concretely, the objective is to given a tweet containing the emotion of joy, sadness, fear or anger, determine the intensity or degree of the emotion felt by the speaker as a real-valued score between zero and one.", "In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings.", "Finally, on the sadness datasets again we experimentally observed that using embeddings of 50 offered the best results, with a statistically significant average gain of 0.092 correlation points INLINEFORM0 over size 25. Results were statistically equivalent for size 100. We also observed that using 50 or 100 hidden units for the RNN offered statistically equivalent results, while both of these offered better performance than when using a hidden size of 200.", "While studying emotion in text it is commonly useful to characterize the emotional charge of a passage based on its words. Some words have affect as a core part of their meaning. For example, dejected and wistful denote some amount of sadness, and are thus associated with sadness. On the other hand, some words are associated with affect even though they do not denote affect. For example, failure and death describe concepts that are usually accompanied by sadness and thus they denote some amount of sadness.", "To evaluate our model, we wrapped the provided scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5."]}
{"question_id": "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02", "predicted_answer": "The dataset used is the WASSA-2017 dataset, specifically the Emotion Intensity task, which involves determining the intensity of emotions (joy, sadness, fear, or anger) in tweets.", "predicted_evidence": ["While analyzing the emotional content in text, mosts tasks are almost always framed as classification tasks, where the intention is to identify one emotion among many for a sentence or passage. However, it is often useful for applications to know the degree to which an emotion is expressed in text. To this end, the WASSA-2017 Shared Task on Emotion Intensity BIBREF0 represents the first task where systems have to automatically determine the intensity of emotions in tweets. Concretely, the objective is to given a tweet containing the emotion of joy, sadness, fear or anger, determine the intensity or degree of the emotion felt by the speaker as a real-valued score between zero and one.", "Our work is related to deep learning techniques for emotion recognition in images BIBREF1 and videos BIBREF2 , as well as and emotion classification BIBREF3 . Our work is also related to liuattention-based2016, who introduced an attention RNN for slot filling in Natural Language Understanding. Since in the task the input-output alignment is explicit, they investigated how the alignment can be best utilized in encoder-decoder models concluding that the attention mechanisms are helpful.", "On the other hand, our model also offers us very interesting insights on how the learning is performed, since we can inspect the attention weights that the neural network is assigning to each specific token when predicting the emotion intensity. By visualizing these weights we can have a clear notion about the parts of the sentence that the model considers are more important. As Figure FIGREF16 shows, we see the model seems to be have learned to attend the words that naturally bear emotion or sentiment. This is specially patent for the examples extracted from the Joy dataset, where positive words are generally identified. However, we also see some examples where the lack of semantic information about the input words, specially for hashtags or user mentions, makes the model unable to identify some of these the most salient words to predict emotion intensity. Several pre-processing techniques can be implemented to alleviate this problem, which we intend to explore in the future.", "Where INLINEFORM0 is the vector that compresses the input sentence INLINEFORM1 , focusing on the relevant parts to estimate emotion intensity. We input this compressed sentence representation into a feed-forward neural network, INLINEFORM2 , where INLINEFORM3 is the final predicted emotion intensity. As a loss function we use the mini-batch negative Pearson correlation with the gold-standard.", "To avoid over-fitting, we used dropout regularization, experimenting with keep probabilities of INLINEFORM0 and INLINEFORM1 . We also added a weighed L2 regularization term to our loss function. We experimented with different values for weight INLINEFORM2 , with a minimum value of 0.01 and a maximum of 0.2."]}
{"question_id": "ed44f7e698d6124cb86791841d02fc6f8b4d862a", "predicted_answer": "The definition of hate speech is not explicitly stated in the given context. However, based on the annotation scheme described, it can be inferred that hate speech is considered to be speech that is:\n\n* Direct or indirect\n* Offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal\n* Discriminatory towards an individual or group of people based on certain attributes (e.g. ethnicity, gender, sexual orientation)\n* Capable of generating negative sentiments in the annotators\n\nIt's worth noting that the authors acknowledge the subjectivity and complexity of hate speech data, and that different studies have used different label sets to categorize hate speech.", "predicted_evidence": ["We rely on the general public opinion and common linguistic knowledge to assess how people view and react to hate speech. Given the subjectivity and difficulty of the task, we reminded the annotators not to let their personal opinions about the topics being discussed in the tweets influence their annotation decisions.", "Treating hate speech classification as a binary task may not be enough to inspect the motivation and the behavior of the users promoting it and, how people would react to it. For instance, the hateful tweets presented in Figure FIGREF5 show toxicity directed towards different targets, with or without using slurs, and generating several types of reactions. We believe that, in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis. Hence, our annotations indicate (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments. To the best of our knowledge there are no other hate speech datasets that attempt to capture fear out of ignorance in hateful tweets or examine how people react to hate speech.", "Treating hate speech classification as a binary task may not be enough to inspect the motivation and the behavior of the users promoting it and, how people would react to it. For instance, the hateful tweets presented in Figure FIGREF5 show toxicity directed towards different targets, with or without using slurs, and generating several types of reactions. We believe that, in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis. Hence, our annotations indicate (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments.", "Annotators determine the explicitness of the tweet by labeling it as direct or indirect speech. This should be based on whether the target is explicitly named, or less easily discernible, especially if the tweet contains humor, metaphor, or figurative speech. Table TABREF20 shows that even when partly using equivalent keywords to search for candidate tweets, there are still significant differences in the resulting data.", "Given the subjectivity and the complexity of such data, annotation schemes have rarely been made fine-grained. Table TABREF10 compares different labelsets that exist in the literature. For instance, BIBREF12 use racist, sexist, and normal as labels; BIBREF13 label their data as hateful, offensive (but not hateful), and neither, while BIBREF16 present an English dataset that records the target category based on which hate speech discriminates against people, such as ethnicity, gender, or sexual orientation and ask human annotators to classify the tweets as hate and non hate. BIBREF15 label their data as offensive, abusive, hateful, aggressive, cyberbullying, spam, and normal. On the other hand, BIBREF20 have chosen to detect ideologies of hate speech counting 40 different hate ideologies among 13 extremist hate groups."]}
{"question_id": "d9e7633004ed1bc1ee45be58409bcc1fa6db59b2", "predicted_answer": "The text does not explicitly mention the languages contained in the dataset. However, it does mention \"single task single language models\" (STSL) and \"single task multilingual models\" (STML), which suggests that the dataset contains multiple languages.", "predicted_evidence": ["We present the labelset the annotators refer to, and statistics of our annotated data in the following.", "In this section, we present our data collection methodology and annotation process.", "Since macro-F1 is the average of all F1 scores of individual labels, all deep learning models have high macro-F1 scores in English which indicates that they are particularly good at classifying the direct class. STSL is also comparable or better than traditional BOW feature-based classifiers when performed on other tasks in terms of micro-F1 and for most of the macro-F1 scores. This shows the power of the deep learning approach.", "We report both the micro and macro-F1 scores of the different classification tasks in Tables TABREF27 and TABREF28. Majority refers to labeling based on the majority label, LR to logistic regression, STSL to single task single language models, STML to single task multilingual models, and MTML to multitask multilingual models.", "To fully exploit the collected annotations, we tested multitask learning on our dataset. Multitask learning BIBREF7 allows neural networks to share parameters with one another and, thus, learn from related tasks. It has been used in different NLP tasks such as parsing BIBREF9, dependency parsing BIBREF26, neural machine translation BIBREF27, sentiment analysis BIBREF28, and other tasks. Multitask learning architectures tackle challenges that include sharing the label space and the question of private and shared space for loosely related tasks BIBREF8, for which techniques may involve a massive space of potential parameter sharing architectures."]}
{"question_id": "c58ef13abe5fa91a761362ca962d7290312c74e4", "predicted_answer": "The five aspects considered in the annotation process are:\n\n1. The directness of the speech (explicitness of the tweet)\n2. The type of hostility it conveys\n3. The target attribute\n4. The group it dehumanizes\n5. The annotator's sentiment (how different people react to it)", "predicted_evidence": ["In this section, we present our data collection methodology and annotation process.", "We present the labelset the annotators refer to, and statistics of our annotated data in the following.", "Each of the annotated aspects represents a classification task of its own, that could either be evaluated independently, or, as intended in this paper, tested on how it impacts other tasks. The different labels are designed to facilitate the study of the correlations between the explicitness of the tweet, the type of hostility it conveys, its target attribute, the group it dehumanizes, how different people react to it, and the performance of multitask learning on the five tasks. We assigned each tweet to five annotators, then applied majority voting to each of the labeling tasks. Given the numbers of annotators and labels in each annotation sub-task, we allowed multilabel annotations in the most subjective classification tasks, namely the hostility type and the annotator's sentiment labels, in order to keep the right human-like approximations. If there are two annotators agreeing on two labels respectively, we add both labels to the annotation.", "We report both the micro and macro-F1 scores of the different classification tasks in Tables TABREF27 and TABREF28. Majority refers to labeling based on the majority label, LR to logistic regression, STSL to single task single language models, STML to single task multilingual models, and MTML to multitask multilingual models.", "We report and discuss the results of five classification tasks: (1) the directness of the speech, (2) the hostility type of the tweet, (3) the discriminating target attribute, (4) the target group, and (5) the annotator's sentiment."]}
{"question_id": "9ef0d2365bde0d18054511fbb53cec5fa2cda5ee", "predicted_answer": "The size of the dataset is not explicitly mentioned in the given context. However, it is mentioned that the authors \"fully exploit the collected annotations\" and report F1 scores for different classification tasks, which suggests that the dataset is of a sufficient size to support multitask learning and other experiments.", "predicted_evidence": ["In this section, we present our data collection methodology and annotation process.", "Since macro-F1 is the average of all F1 scores of individual labels, all deep learning models have high macro-F1 scores in English which indicates that they are particularly good at classifying the direct class. STSL is also comparable or better than traditional BOW feature-based classifiers when performed on other tasks in terms of micro-F1 and for most of the macro-F1 scores. This shows the power of the deep learning approach.", "We present the labelset the annotators refer to, and statistics of our annotated data in the following.", "We report both the micro and macro-F1 scores of the different classification tasks in Tables TABREF27 and TABREF28. Majority refers to labeling based on the majority label, LR to logistic regression, STSL to single task single language models, STML to single task multilingual models, and MTML to multitask multilingual models.", "To fully exploit the collected annotations, we tested multitask learning on our dataset. Multitask learning BIBREF7 allows neural networks to share parameters with one another and, thus, learn from related tasks. It has been used in different NLP tasks such as parsing BIBREF9, dependency parsing BIBREF26, neural machine translation BIBREF27, sentiment analysis BIBREF28, and other tasks. Multitask learning architectures tackle challenges that include sharing the label space and the question of private and shared space for loosely related tasks BIBREF8, for which techniques may involve a massive space of potential parameter sharing architectures."]}
{"question_id": "cbb3c1c1e6e1818b6480f929f1c299eaa5ffd07a", "predicted_answer": "The opportunities presented by the use of Semantic Web technologies (SWT) in Machine Translation (MT) are:\n\n1. **Improved translation quality**: Explicit semantic knowledge made available through SWT can empower MT systems to supply translations with significantly better quality while remaining scalable.\n2. **Disambiguation of ambiguous sentences or words**: The disambiguated knowledge about real-world entities, their properties, and their relationships made available on the Linked Data (LD) Web can be used to infer the right meaning of ambiguous sentences or words.\n3. **Translation of knowledge bases (KBs)**: SWT can be used to translate KBs, such as DBpedia, which can then be used to improve the translation of entities in MT systems.\n4. **Creation of multilingual linguistic ontologies**: SWT can be used to create multilingual linguistic ontologies that describe the syntax of rich morphologically languages, supporting MT approaches.\n5. **Creation of RDF multilingual dictionaries**: SWT can be used to create RDF multilingual dictionaries that can improve MT steps, such as alignment.\n6. **Addressing the ambiguity of named entities**: SWT can be used to address the ambiguity of named entities in MT systems.\n7. **Contributing to the OOV (Out-of-Vocabulary) words problem**: SWT can be used to contribute to the OOV words problem in MT systems.\n8. **Gathering knowledge from different SW resources**: SWT can be used to gather knowledge from different SW resources, enabling MT systems to address the ambiguity of named entities and contribute to the OOV words problem.\n\nOverall, the use of SWT in MT has the potential to significantly improve the quality and accuracy of machine translation systems.", "predicted_evidence": ["One possible solution to address the remaining issues of MT lies in the use of SWT, which have emerged over recent decades as a paradigm to make the semantics of content explicit so that it can be used by machines. It is believed that explicit semantic knowledge made available through these technologies can empower MT systems to supply translations with significantly better quality while remaining scalable. In particular, the disambiguated knowledge about real-world entities, their properties and their relationships made available on the LD Web can potentially be used to infer the right meaning of ambiguous sentences or words.", "Therefore, the semantic structure (i.e., triples) provided by DBpedia versions of these respective Wikipedia versions would be able to help translate from German to Portuguese. For example, the terms contained in triples would be translated to a given target language using a dictionary containing domain words. This dictionary may be acquired in two different ways. First, by performing localisation, as in the work by J. P. McCrae BIBREF17 which translates the terms contained in a monolingual ontology, thus generating a bilingual ontology. Second, by creating embeddings of both DBpedia versions in order to determine the similarity between entities through their vectors. This insight is supported by some recent works, such as Ristoski et al. BIBREF18 , which creates bilingual embeddings using RDF based on Word2vec algorithms. Therefore, we suggest investigating an MT approach mainly based on SWT using NN for translating KBs. Once the KBs are translated, we suggest including them in the language models for improving the translation of entities.", "In this extended abstract, we detailed the results of a systematic literature review of MT using SWT for improving the translation of natural language sentences. Our goal was to present the current open MT translation problems and how SWT can address these problems and enhance MT quality. Considering the decision power of SWT, they cannot be ignored by future MT systems. As a next step, we intend to continue elaborating a novel MT approach which is capable of simultaneously gathering knowledge from different SW resources and consequently being able to address the ambiguity of named entities and also contribute to the OOV words problem. This insight relies on our recent works, such as BIBREF15 , which have augmented NMT models with the usage of external knowledge for improving the translation of entities in texts. Additionally, future works that can be expected from fellow researchers, include the creation of multilingual linguistic ontologies describing the syntax of rich morphologically languages for supporting MT approaches.", "In this extended abstract, we detailed the results of a systematic literature review of MT using SWT for improving the translation of natural language sentences. Our goal was to present the current open MT translation problems and how SWT can address these problems and enhance MT quality. Considering the decision power of SWT, they cannot be ignored by future MT systems. As a next step, we intend to continue elaborating a novel MT approach which is capable of simultaneously gathering knowledge from different SW resources and consequently being able to address the ambiguity of named entities and also contribute to the OOV words problem. This insight relies on our recent works, such as BIBREF15 , which have augmented NMT models with the usage of external knowledge for improving the translation of entities in texts. Additionally, future works that can be expected from fellow researchers, include the creation of multilingual linguistic ontologies describing the syntax of rich morphologically languages for supporting MT approaches. Also, the creation of more RDF multilingual dictionaries which can improve some MT steps, such as alignment.", "According to our research, it is clear that SWT may be used for translating KBs in order to be applied in MT systems. For instance, some content provided by the German Wikipedia version are not contained in the Portuguese one. Therefore, the semantic structure (i.e., triples) provided by DBpedia versions of these respective Wikipedia versions would be able to help translate from German to Portuguese. For example, the terms contained in triples would be translated to a given target language using a dictionary containing domain words. This dictionary may be acquired in two different ways. First, by performing localisation, as in the work by J. P. McCrae BIBREF17 which translates the terms contained in a monolingual ontology, thus generating a bilingual ontology. Second, by creating embeddings of both DBpedia versions in order to determine the similarity between entities through their vectors. This insight is supported by some recent works, such as Ristoski et al. BIBREF18 , which creates bilingual embeddings using RDF based on Word2vec algorithms."]}
{"question_id": "9f74f3991b8681619d95ab93a7c8733a843ddffe", "predicted_answer": "The challenges associated with the use of Semantic Web technologies (SWT) in Machine Translation (MT) include:\n\n1. **Complexity of natural languages and their structure**: Natural languages are inherently complex, making it difficult to represent their semantics explicitly using SWT.\n\n2. **Scalability**: While SWT can provide explicit semantic knowledge, integrating this knowledge into MT systems in a scalable manner is a significant challenge.\n\n3. **Ambiguity of named entities and Out-of-Vocabulary (OOV) words**: SWT can help disambiguate named entities and provide knowledge about OOV words, but addressing these issues in a multilingual setting is a complex task.\n\n4. **Creation of multilingual linguistic ontologies**: Developing ontologies that describe the syntax of rich morphologically languages is essential for supporting MT approaches, but this is a challenging task.\n\n5. **Integration of knowledge from different SW resources**: Gathering knowledge from various SW resources and integrating it into MT systems in a coherent manner is a significant challenge.\n\n6. **Alignment of multilingual dictionaries**: Creating RDF multilingual dictionaries that can improve MT steps, such as alignment, is another challenge that needs to be addressed.\n\nThese challenges highlight the need for further research and development in the area of SWT-based MT to overcome the limitations of current MT systems.", "predicted_evidence": ["One possible solution to address the remaining issues of MT lies in the use of SWT, which have emerged over recent decades as a paradigm to make the semantics of content explicit so that it can be used by machines. It is believed that explicit semantic knowledge made available through these technologies can empower MT systems to supply translations with significantly better quality while remaining scalable. In particular, the disambiguated knowledge about real-world entities, their properties and their relationships made available on the LD Web can potentially be used to infer the right meaning of ambiguous sentences or words.", "However, translation is a difficult task due to the complexity of natural languages and their structure BIBREF0 . In addition, manual translation does not scale to the magnitude of the Web. One remedy for this problem is MT. The main goal of MT is to enable people to assess content in languages other than the languages in which they are fluent BIBREF1 . From a formal point of view, this means that the goal of MT is to transfer the semantics of text from an input language to an output language BIBREF2 .", "Alongside increasing globalization comes a greater need for readers to understand texts in languages foreign to them. For example, approximately 48% of the pages on the Web are not available in English. The technological progress of recent decades has made both the distribution and access to content in different languages ever simpler. Translation aims to support users who need to access content in a language in which they are not fluent BIBREF0 .", "In this extended abstract, we detailed the results of a systematic literature review of MT using SWT for improving the translation of natural language sentences. Our goal was to present the current open MT translation problems and how SWT can address these problems and enhance MT quality. Considering the decision power of SWT, they cannot be ignored by future MT systems. As a next step, we intend to continue elaborating a novel MT approach which is capable of simultaneously gathering knowledge from different SW resources and consequently being able to address the ambiguity of named entities and also contribute to the OOV words problem. This insight relies on our recent works, such as BIBREF15 , which have augmented NMT models with the usage of external knowledge for improving the translation of entities in texts. Additionally, future works that can be expected from fellow researchers, include the creation of multilingual linguistic ontologies describing the syntax of rich morphologically languages for supporting MT approaches.", "In this extended abstract, we detailed the results of a systematic literature review of MT using SWT for improving the translation of natural language sentences. Our goal was to present the current open MT translation problems and how SWT can address these problems and enhance MT quality. Considering the decision power of SWT, they cannot be ignored by future MT systems. As a next step, we intend to continue elaborating a novel MT approach which is capable of simultaneously gathering knowledge from different SW resources and consequently being able to address the ambiguity of named entities and also contribute to the OOV words problem. This insight relies on our recent works, such as BIBREF15 , which have augmented NMT models with the usage of external knowledge for improving the translation of entities in texts. Additionally, future works that can be expected from fellow researchers, include the creation of multilingual linguistic ontologies describing the syntax of rich morphologically languages for supporting MT approaches. Also, the creation of more RDF multilingual dictionaries which can improve some MT steps, such as alignment."]}
{"question_id": "7c2c15ea3f1b1375b8aaef1103a001069d9915bb", "predicted_answer": "Based on the provided context, some other obstacles to automatic translations that are not mentioned in the abstract are:\n\n1. **Homophones and homographs**: Words that are pronounced or spelled the same but have different meanings, which can lead to incorrect translations.\n2. **Idiomatic expressions and colloquialisms**: Phrases or expressions that have a specific meaning in one language or culture, but may not have a direct equivalent in another language.\n3. **Cultural and domain-specific knowledge**: Translations may require specific knowledge about a particular culture, domain, or industry, which can be challenging to incorporate into MT systems.\n4. **Emotional and nuanced language**: Translating emotional or nuanced language, such as sarcasm, irony, or figurative language, can be difficult for MT systems to capture accurately.\n5. **Out-of-vocabulary words**: Words that are not present in the training data or are newly coined, which can make it difficult for MT systems to translate them correctly.\n6. **Dialects and regional variations**: Translating dialects or regional variations of a language can be challenging, as they may have different grammar, vocabulary, or pronunciation.\n7. **Ambiguity in sentence structure**: Sentences with ambiguous structures, such as those with multiple possible parses, can be difficult for MT systems to translate accurately.\n8. **Linguistic and cultural biases**: MT systems may perpetuate linguistic and cultural biases present in the training data, which can lead to inaccurate or unfair translations.\n\nThese obstacles highlight the complexity of automatic translation and the need for continued research and development to improve the accuracy and fluency of MT systems.", "predicted_evidence": ["For instance, the English language contains irregular verbs like \u201cset\u201d or \u201cput\u201d. Depending on the structure of a sentence, it is not possible to recognize their verbal tense, e.g., present or past tense. Even statistical approaches trained on huge corpora may fail to find the exact meaning of some words due to the structure of the language. Although this challenge has successfully been dealt with since NMT has been used for European languages, implementations of NMT for some non-European languages have not been fully exploited (e.g., Brazilian Portuguese, Latin-America Spanish, Zulu, Hindi) due to the lack of large bilingual data sets on the Web to be trained on. Thus, we suggest gathering relationships among properties within an ontology by using the reasoning technique for handling this issue. For instance, the sentence \u201cAnna usually put her notebook on the table for studying\" may be annotated using a certain vocabulary and represented by triples. Thus, the verb \u201cput\", which is represented by a predicate that groups essential information about the verbal tense, may support the generation step of a given MT system.", "Alongside increasing globalization comes a greater need for readers to understand texts in languages foreign to them. For example, approximately 48% of the pages on the Web are not available in English. The technological progress of recent decades has made both the distribution and access to content in different languages ever simpler. Translation aims to support users who need to access content in a language in which they are not fluent BIBREF0 .", "Although MT systems are now popular on the Web, they still generate a large number of incorrect translations. Recently, Popovi\u0107 BIBREF3 has classified five types of errors that still remain in MT systems. According to research, the two main faults that are responsible for 40% and 30% of problems respectively, are reordering errors and lexical and syntactic ambiguity. Thus, addressing these barriers is a key challenge for modern translation systems. A large number of MT approaches have been developed over the years that could potentially serve as a remedy. For instance, translators began by using methodologies based on linguistics which led to the family of RBMT. However, RBMT systems have a critical drawback in their reliance on manually crafted rules, thus making the development of new translation modules for different languages even more difficult.", "Even statistical approaches trained on huge corpora may fail to find the exact meaning of some words due to the structure of the language. Although this challenge has successfully been dealt with since NMT has been used for European languages, implementations of NMT for some non-European languages have not been fully exploited (e.g., Brazilian Portuguese, Latin-America Spanish, Zulu, Hindi) due to the lack of large bilingual data sets on the Web to be trained on. Thus, we suggest gathering relationships among properties within an ontology by using the reasoning technique for handling this issue. For instance, the sentence \u201cAnna usually put her notebook on the table for studying\" may be annotated using a certain vocabulary and represented by triples. Thus, the verb \u201cput\", which is represented by a predicate that groups essential information about the verbal tense, may support the generation step of a given MT system. This sentence usually fails when translated to rich morphological languages, such as Brazilian-Portuguese and Arabic, for which the verb influences the translation of \u201cusually\" to the past tense.", "On the other hand, there is also a syntactic disambiguation problem which as yet lacks good solutions. For instance, the English language contains irregular verbs like \u201cset\u201d or \u201cput\u201d. Depending on the structure of a sentence, it is not possible to recognize their verbal tense, e.g., present or past tense. Even statistical approaches trained on huge corpora may fail to find the exact meaning of some words due to the structure of the language. Although this challenge has successfully been dealt with since NMT has been used for European languages, implementations of NMT for some non-European languages have not been fully exploited (e.g., Brazilian Portuguese, Latin-America Spanish, Zulu, Hindi) due to the lack of large bilingual data sets on the Web to be trained on. Thus, we suggest gathering relationships among properties within an ontology by using the reasoning technique for handling this issue."]}
{"question_id": "a77d38427639d54461ae308f3045434f81e497d0", "predicted_answer": "The EEG features used in this study were from sensors T7 and T8, as well as from all 31 sensors with dimension reduction.", "predicted_evidence": ["We used Brain Vision EEG recording hardware. Our EEG cap had 32 wet EEG electrodes including one electrode as ground as shown in Figure 1. We used EEGLab BIBREF17 to obtain the EEG sensor location mapping. It is based on standard 10-20 EEG sensor placement method for 32 electrodes.", "In this paper we demonstrated the feasibility of using EEG features, concatenation of EEG and acoustic features for performing noisy continuous speech recognition. To our best knowledge this is the first time a continuous noisy speech recognition is demonstrated using only EEG features.", "In BIBREF0 we have demonstrated that EEG sensors T7 and T8 features contributed most towards ASR performance. Table vi@ shows the CTC model test time results when we trained the model using EEG features from only T7 and T8 sensors on the most noisy data set B. We observed that as vocabulary size increase, error rates were slightly lower than the error rates from Table iv@ where we used EEG features from all 31 sensors with dimension reduction. Table iii@ shows the results for attention model when trained with EEG features from sensors T7 and T8 only on data set B. We observed that error rates were higher in this case compared to the error rates reported in table ii@.", "An end-to-end ASR model maps input feature vectors to an output sequence of vectors of posterior probabilities of tokens without using separate acoustic model, pronunciation model and language model. In this work we implemented two different types of state of art end to end ASR models used for the task of continuous speech recognition and the input feature vectors can be EEG features or concatenation of acoustic and EEG features. We used Google's tensorflow and keras deep learning libraries for building our ASR models.", "For future work, we plan to build a much larger speech EEG data base and also perform experiments with data collected from subjects with speaking disabilities."]}
{"question_id": "010fd15696580d9924ac0275a4ff269005e5808d", "predicted_answer": "The baselines mentioned in the text are:\n\n1. BIBREF6: Classification approach for identifying phonological categories in imagined and silent speech.\n2. BIBREF7: Neural network-based classification approach for predicting phonemes.\n\nThese baselines are mentioned as previous works that the current work is building upon and improving.", "predicted_evidence": ["This work is mainly motivated by the results explained in BIBREF0 , BIBREF6 , BIBREF7 , BIBREF3 . In BIBREF6 the authors used classification approach for identifying phonological categories in imagined and silent speech but in our work we used continuous speech recognition state of art models and our models were predicting words, characters at each time step. Similarly in BIBREF7 neural network based classification approach was used for predicting phonemes.", "The main ideas behind CTC based ASR were first introduced in the following papers BIBREF8 , BIBREF9 . In our work we used a single layer gated recurrent unit (GRU) BIBREF10 with 128 hidden units as encoder for the CTC network. The decoder consists of a combination of a dense layer ( fully connected layer) and a softmax activation. Output at every time step of the GRU layer is fed into the decoder network. The number of time steps of the GRU encoder is equal to product of the sampling frequency of the input features and the length of the input sequence. Since different speakers have different rate of speech, we used dynamic recurrent neural network (RNN) cell. There is no fixed value for time steps of the encoder.", "The way of computing value for INLINEFORM0 depends on the type of attention used. In this work, we used bahdanau's additive style attention BIBREF13 , which defines INLINEFORM1 as INLINEFORM2 ) where INLINEFORM3 and INLINEFORM4 are learnable parameters during training of the model.", "Electroencephalography (EEG) is a non invasive way of measuring electrical activity of human brain. In BIBREF0 we demonstrated deep learning based automatic speech recognition (ASR) using EEG signals for a limited English vocabulary of four words and five vowels. In this paper we extend our work for a much larger English vocabulary and we use state-of-art end-to-end continuous speech recognition models to perform recognition. In our prior work we predicted isolated words and vowels.", "Usually the number of time steps of the encoder (T) is greater than the length of output tokens for a continuous speech recognition problem. A RNN based CTC network tries to make length of output tokens equal to T by allowing the repetition of output prediction unit tokens and by introducing a special token called blank token BIBREF8 across all the frames. We used CTC loss function with adam optimizer BIBREF11 and during inference time we used CTC beam search decoder."]}
{"question_id": "d36a6447bfe58204e0d29f9213d84be04d875624", "predicted_answer": "The dataset used is a speech EEG database, which is divided into two parts: dataset A and dataset B.", "predicted_evidence": ["For data set A, we used data from first 8 subjects for training the model, remaining two subjects data for validation and test set respectively.", "The way of computing value for INLINEFORM0 depends on the type of attention used. In this work, we used bahdanau's additive style attention BIBREF13 , which defines INLINEFORM1 as INLINEFORM2 ) where INLINEFORM3 and INLINEFORM4 are learnable parameters during training of the model.", "For data set B, we used data from first 6 subjects for training the model, remaining two subjects data for validation and test set respectively.", "We further plan to publish our speech EEG data base used in this work to help advancement of research in this area.", "We will also investigate whether it is possible to improve the attention model results by tuning hyper parameters to improve the model's ability to condition on the input,improve CTC model results by training with more number of examples and by using external language model during inference time."]}
{"question_id": "5ed02ae6c534cd49d405489990f0e4ba0330ff1b", "predicted_answer": "The answer is not explicitly stated in the given context. However, it is implied that LadaBERT is designed to achieve comparable accuracy to its teacher model using significantly less time and resource consumption, which suggests that LadaBERT may not necessarily outperform its teacher model in terms of accuracy. The goal of LadaBERT is to provide a lightweight and efficient method for model compression, rather than to surpass the performance of the teacher model.", "predicted_evidence": ["Knowledge distillation is first proposed by BIBREF1, which trains a compact or smaller model to approximate the function learned by a large and complex model. A preliminary step of knowledge distillation is to train a deep network (the teacher model) that automatically generates soft labels for training instances. This \u201csynthetic\" label is then used to train a smaller network (the student model), which assimilates the function that is learned by the teacher model. Chen et al. BIBREF24 successfully applied knowledge distillation to object detection tasks by introducing several modifications, including a weighted cross-entropy loss, a teacher bounded loss, and adaptation layers to model intermediate teacher distributions. Li et al. BIBREF25 developed a framework to learn from noisy labels, where the knowledge learned from a clean dataset and semantic knowledge graph were leveraged to correct the wrong labels. Anil et al. BIBREF26 proposed online distillation, a variant of knowledge distillation which enabled extra parallelism for training large-scale data.", "Knowledge distillation is first proposed by BIBREF1, which trains a compact or smaller model to approximate the function learned by a large and complex model. A preliminary step of knowledge distillation is to train a deep network (the teacher model) that automatically generates soft labels for training instances. This \u201csynthetic\" label is then used to train a smaller network (the student model), which assimilates the function that is learned by the teacher model. Chen et al. BIBREF24 successfully applied knowledge distillation to object detection tasks by introducing several modifications, including a weighted cross-entropy loss, a teacher bounded loss, and adaptation layers to model intermediate teacher distributions. Li et al. BIBREF25 developed a framework to learn from noisy labels, where the knowledge learned from a clean dataset and semantic knowledge graph were leveraged to correct the wrong labels. Anil et al.", "where $f^t({x})$ and $f^s({x})$ are the predictive logits of teacher and student models respectively.", "A preliminary step of knowledge distillation is to train a deep network (the teacher model) that automatically generates soft labels for training instances. This \u201csynthetic\" label is then used to train a smaller network (the student model), which assimilates the function that is learned by the teacher model. Chen et al. BIBREF24 successfully applied knowledge distillation to object detection tasks by introducing several modifications, including a weighted cross-entropy loss, a teacher bounded loss, and adaptation layers to model intermediate teacher distributions. Li et al. BIBREF25 developed a framework to learn from noisy labels, where the knowledge learned from a clean dataset and semantic knowledge graph were leveraged to correct the wrong labels. Anil et al. BIBREF26 proposed online distillation, a variant of knowledge distillation which enabled extra parallelism for training large-scale data. In addition, knowledge distillation is also useful for aggregating model ensembles into a single model by treating the ensemble model as a teacher.", "Existing BERT-oriented model compression solutions largely depend on knowledge distillation BIBREF1, which is inefficient and resource-consuming because a large training corpus is required to learn the behaviors of a teacher. For example, DistilBERT BIBREF2 is re-trained on the same corpus as pre-training a vanilla BERT from scratch; and TinyBERT BIBREF3 utilizes expensive data augmentation to fit the distillation target. The costs of these model compression methods are as large as pre-training and unaffordable for low-resource settings. Therefore, it is straight-forward to ask, can we design a lightweight method to generate adaptive models with comparable accuracy using significantly less time and resource consumption? In this paper, we propose LadaBERT (Lightweight adaptation of BERT through hybrid model compression) to tackle the raised questions. Specifically, LadaBERT is based on an iterative hybrid model compression framework consisting of weighting pruning, matrix factorization and knowledge distillation. Initially, the architecture and weights of student model are inherited from the BERT teacher."]}
{"question_id": "f6346828c2f44529dc307abf04dd246bfeb4a9b2", "predicted_answer": "The answer is implied, but not explicitly stated. The text discusses various model compression techniques, including hybrid approaches, but it does not explicitly compare or evaluate which compression method yields the most gains. However, the fact that hybrid approaches are mentioned, and that iterative knowledge distillation iterations can improve accuracy and efficiency, suggests that the authors are aware of the importance of evaluating and optimizing compression methods.", "predicted_evidence": ["in the knowledge distillation iterations, the accuracy and efficiency of model compression can be greatly improved.", "Therefore, for a target model compression ratio $P_{svd}$, the desired rank $r$ can be calculated by:", "In LadaBERT, the hybrid compression produce is applied to each layer of the pre-trained BERT model. Given an overall model compression target $P$, the following constraint should be satisfied:", "Deep Neural Networks (DNNs) have achieved great success in many areas in recent years, but the memory consumption and computational cost expand greatly with the growing complexity of models. Therefore, model compression has become an indispensable technique for practice, especially in low-resource settings. In this section, we review the current progresses of model compression techniques briefly, which can be divided into four categories, namely weight pruning, matrix factorization, weight quantization and knowledge distillation. We also present hybrid approaches and the applications of model compression to pre-trained BERT models.", "Hybrid pruning is a combination of matrix factorization and weight pruning, which conducts iterative weight pruning on the basis of SVD-based matrix factorization. It is performed iteratively until the desired compression ratio is achieved."]}
{"question_id": "935873b97872820b7b6100d6a785fba286b94900", "predicted_answer": "LadaBERT achieves state-of-the-art results on five public datasets of natural language understanding, including MNLI-m.", "predicted_evidence": ["Different from traditional knowledge distillation methods, BERT-PKD enables an exploitation of rich information in the teacher's hidden layers by utilizing a layer-wise distillation constraint. DistillBERT BIBREF2 pre-trains a smaller general-purpose language model on the same corpus as vanilla BERT. Distilled BiLSTM BIBREF34 adopts a single-layer BiLSTM as the student model and achieves comparable results with ELMo BIBREF35 through much fewer parameters and less inference time. TinyBERT BIBREF3 reports the best-ever performance on BERT model compression, which exploits a novel attention-based distillation schema that encourages the linguistic knowledge in teacher to be well transferred into the student model. It adopts a two-stage learning framework, including general distillation (pre-training from scratch via distillation loss) and task-specific distillation with data augmentation. Both procedures require huge resources and long training times (from several days to weeks), which is cumbersome for industrial applications.", "In the natural language processing community, there is a growing interest recently to study BERT-oriented model compression for shipping its performance gain into latency-critical or low-resource scenarios. Most existing works focus on knowledge distillation. For instance, BERT-PKD BIBREF33 is a patient knowledge distillation approach that compresses the original BERT model into a lightweight shallow network. Different from traditional knowledge distillation methods, BERT-PKD enables an exploitation of rich information in the teacher's hidden layers by utilizing a layer-wise distillation constraint. DistillBERT BIBREF2 pre-trains a smaller general-purpose language model on the same corpus as vanilla BERT. Distilled BiLSTM BIBREF34 adopts a single-layer BiLSTM as the student model and achieves comparable results with ELMo BIBREF35 through much fewer parameters and less inference time. TinyBERT BIBREF3 reports the best-ever performance on BERT model compression, which exploits a novel attention-based distillation schema that encourages the linguistic knowledge in teacher to be well transferred into the student model.", "In the natural language processing community, there is a growing interest recently to study BERT-oriented model compression for shipping its performance gain into latency-critical or low-resource scenarios. Most existing works focus on knowledge distillation. For instance, BERT-PKD BIBREF33 is a patient knowledge distillation approach that compresses the original BERT model into a lightweight shallow network. Different from traditional knowledge distillation methods, BERT-PKD enables an exploitation of rich information in the teacher's hidden layers by utilizing a layer-wise distillation constraint. DistillBERT BIBREF2 pre-trains a smaller general-purpose language model on the same corpus as vanilla BERT. Distilled BiLSTM BIBREF34 adopts a single-layer BiLSTM as the student model and achieves comparable results with ELMo BIBREF35 through much fewer parameters and less inference time.", "Model compression is a common way to deal with latency-critical or memory-intensive scenarios. Existing model compression methods for BERT need to be re-trained on a large corpus to reserve its original performance, which is inapplicable in low-resource settings. In this paper, we propose LadaBERT to address this problem. LadaBERT is a lightweight model compression pipeline that generates adaptive BERT model efficiently based on a given task and specific constraint. It is based on a hybrid solution, which conducts matrix factorization, weight pruning and knowledge distillation in a reinforce manner. The experimental results verify that EAdaBERT is able to achieve comparable performance with other state-of-the-art solutions using much less training data and time budget. Therefore, LadaBERT can be easily plugged into various applications with competitive performances and little training overheads. In the future, we would like to apply LadaBERT to large-scale industrial applications, such as search relevance and query recommendation.", "We conduct extensive experiments on five public datasets of natural language understanding. As an example, the performance comparison of LadaBERT and state-of-the-art models on MNLI-m dataset is illustrated in Figure FIGREF1. We can see that LadaBERT outperforms other BERT-oriented model compression baselines at various model compression ratios. Especially, LadaBERT-1 outperforms BERT-PKD significantly under $2.5\\times $ compression ratio, and LadaBERT-3 outperforms TinyBERT under $7.5\\times $ compression ratio while the training speed is accelerated by an order of magnitude."]}
{"question_id": "f2bcfdbebb418e7da165c19b8c7167719432ee48", "predicted_answer": "The domain of text they are working with is news articles, specifically from the DailyMail news highlights corpus, as well as documents from the DUC 2002 single document summarization corpus.", "predicted_evidence": ["In this section we formally define the summarization tasks considered in this paper. Given a document $D$ consisting of a sequence of sentences $\\lbrace s_1, \\cdots , s_m\\rbrace $ and a word set $\\lbrace w_1, \\cdots , w_n\\rbrace $ , we are interested in obtaining summaries at two levels of granularity, namely sentences and words.", "Data-driven neural summarization models require a large training corpus of documents with labels indicating which sentences (or words) should be in the summary. Until now such corpora have been limited to hundreds of examples (e.g., the DUC 2002 single document summarization corpus) and thus used mostly for testing BIBREF7 . To overcome the paucity of annotated data for training, we adopt a methodology similar to hermann2015teaching and create two large-scale datasets, one for sentence extraction and another one for word extraction.", "The role of the reader is to derive the meaning representation of the document from its constituent sentences, each of which is treated as a sequence of words. We first obtain representation vectors at the sentence level using a single-layer convolutional neural network (CNN) with a max-over-time pooling operation BIBREF16 , BIBREF17 , BIBREF18 . Next, we build representations for documents using a standard recurrent neural network (RNN) that recursively composes sentences. The CNN operates at the word level, leading to the acquisition of sentence-level representations that are then used as inputs to the RNN that acquires document-level representations, in a hierarchical fashion. We describe these two sub-components of the text reader below.", "We evaluate our models both automatically (in terms of Rouge) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing hand-engineered features and sophisticated linguistic constraints.", "The need to access and digest large amounts of textual data has provided strong impetus to develop automatic summarization systems aiming to create shorter versions of one or more documents, whilst preserving their information content. Much effort in automatic summarization has been devoted to sentence extraction, where a summary is created by identifying and subsequently concatenating the most salient text units in a document."]}
{"question_id": "0fe49431db5ffaa24372919daf24d8f84117bfda", "predicted_answer": "The dataset used is a large-scale dataset constructed from hundreds of thousands of news articles and corresponding highlights from the DailyMail website.", "predicted_evidence": ["In the following section, we discuss the data elicitation methods which allow us to train neural networks based on the above defined objectives.", "In this section we present our experimental setup for assessing the performance of our summarization models. We discuss the datasets used for training and evaluation, give implementation details, briefly introduce comparison models, and explain how system output was evaluated.", "One stumbling block to applying neural network models to extractive summarization is the lack of training data, i.e., documents with sentences (and words) labeled as summary-worthy. Inspired by previous work on summarization BIBREF7 , BIBREF13 and reading comprehension BIBREF9 we retrieve hundreds of thousands of news articles and corresponding highlights from the DailyMail website. Highlights usually appear as bullet points giving a brief overview of the information contained in the article (see Figure 1 for an example). Using a number of transformation and scoring algorithms, we are able to match highlights to document content and construct two large scale training datasets, one for sentence extraction and the other for word extraction. Previous approaches have used small scale training data in the range of a few hundred examples.", "In practice, there is a discrepancy between training and testing such a model. During training we know the true label $p_{t-1}$ of the previous sentence, whereas at test time $p_{t-1}$ is unknown and has to be predicted by the model. The discrepancy can lead to quickly accumulating prediction errors, especially when mistakes are made early in the sequence labeling process. To mitigate this, we adopt a curriculum learning strategy BIBREF21 : at the beginning of training when $p_{t-1}$ cannot be predicted accurately, we set it to the true label of the previous sentence; as training goes on, we gradually shift its value to the predicted label $p(y_L(t-1)=1\n| d )$ .", "In the above equations, $\\mathbf {w}_i$ corresponds to the vector of the $i$ -th word in the input document, whereas $\\mathbf {z}$ , $\\mathbf {W}_e$ , $\\mathbf {W}_r$ , $\\mathbf {v}$ , $\\mathbf {W}_{e^{\\prime }}$ , and $\\mathbf {W}_{r^{\\prime }}$ are model weights. The model architecture is shown in Figure 3 ."]}
{"question_id": "0f9c1586f1b4b531fa4fd113e767d06af90b1ae8", "predicted_answer": "The text does not explicitly compare the word-based extraction method to abstractive summarization methods. It only mentions that the approach is similar in spirit to previous work on extractive summarization, but with some key differences. It does not discuss or compare the performance of the proposed method to abstractive summarization methods.", "predicted_evidence": ["Our work touches on several strands of research within summarization and neural sequence modeling. The idea of creating a summary by extracting words from the source document was pioneered in bankoetal00 who view summarization as a problem analogous to statistical machine translation and generate headlines using statistical models for selecting and ordering the summary words. Our word-based model is similar in spirit, however, it operates over continuous representations, produces multi-sentence output, and jointly selects summary words and organizes them into sentences. A few recent studies BIBREF14 , BIBREF15 perform sentence extraction based on pre-trained sentence embeddings following an unsupervised optimization paradigm. Our work also uses continuous representations to express the meaning of sentences and documents, but importantly employs neural networks more directly to perform the actual summarization task.", "In this section we present our experimental setup for assessing the performance of our summarization models. We discuss the datasets used for training and evaluation, give implementation details, briefly introduce comparison models, and explain how system output was evaluated.", "In the following section, we discuss the data elicitation methods which allow us to train neural networks based on the above defined objectives.", "In this section we formally define the summarization tasks considered in this paper. Given a document $D$ consisting of a sequence of sentences $\\lbrace s_1, \\cdots , s_m\\rbrace $ and a word set $\\lbrace w_1, \\cdots , w_n\\rbrace $ , we are interested in obtaining summaries at two levels of granularity, namely sentences and words.", "We evaluate our models both automatically (in terms of Rouge) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing hand-engineered features and sophisticated linguistic constraints."]}
{"question_id": "52faf319e37aa15fff1ab47f634a5a584dc42e75", "predicted_answer": "The types of commonsense knowledge being referred to are:\n\n* A room may have a door\n* When a person enters a room, it is generally through a door\n\nIn general, the authors are concerned with representing the type of background knowledge that an ordinary person would have, which is crucial for solving natural language problems in coreference resolution, question answering, and other reasoning tasks.", "predicted_evidence": ["More than just curating a static collection of facts, we would like commonsense knowledge to be represented in a way that lends itself to machine reasoning and inference of missing information. We concern ourselves in this paper with the problem of learning commonsense knowledge representations.", "A core problem in artificial intelligence is to capture, in machine-usable form, the collection of information that an ordinary person would have, known as commonsense knowledge. For example, a machine should know that a room may have a door, and that when a person enters a room, it is generally through a door. This background knowledge is crucial for solving many difficult, ambiguous natural language problems in coreference resolution and question answering, as well as the creation of other reasoning machines.", "We focus on the order-embedding model BIBREF0 which was proposed for general hierarchical prediction including multimodal problems such as image captioning. While the original work included results on ontology prediction on WordNet, we focus exclusively on the model's application to commonsense knowledge, with its unique characteristics including complex ordering structure, compositional, multi-word entities, and the wealth of commonsense knowledge to be found in large-scale unstructured text data.", "We aim to augment our ontology prediction embedding model with more general commonsense knowledge mined from raw text. A standard method for learning word representations is word2vec BIBREF14 , which predicts current word embeddings using a context of surrounding word embeddings. We incorporate a modification of the CBOW model in this work, which uses the average embedding from a window around the current word as a context vector $v_2$ to predict the current word vector $v_1$ : $\nv_2 = \\frac{1}{window}\\sum _{k \\in \\lbrace -window/2,...,window/2\\rbrace \\setminus \\lbrace t\\rbrace }v_{t+k}\n$", "In machine learning settings, knowledge is usually represented as a hypergraph of triplets such as Freebase BIBREF1 , WordNet BIBREF2 , and ConceptNet BIBREF3 . In these knowledge graphs, nodes represent entities or terms $t$ , and hyperedges are relations $R$ between these entities or terms, with each fact in the knowledge graph represented as a triplet $<t_1, R, t_2>$ . Researchers have developed many models for knowledge representation and learning in this setting BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , under the umbrella of knowledge graph completion. However, none of these naturally lend themselves to traditional methods of logical reasoning such as transitivity and negation."]}
{"question_id": "0c7cb3010ed92b8d46583a67e72946a6c0115f1f", "predicted_answer": "In the context of representation learning, the \"intrinsic geometry of spaces of learned representations\" refers to the geometric structure of the embedding space that is learned from the data. In other words, it's the way the representations of concepts or entities are arranged in the high-dimensional space, and the relationships between them.\n\nIn this specific context, the intrinsic geometry is related to the hierarchical structure of the concepts, where a subcategory (e.g., \"dog\") is represented as a point in the embedding space that is \"below\" or \"inside\" the point representing its supercategory (e.g., \"animal\"). This geometry is \"intrinsic\" because it's a property of the embedding space itself, rather than being imposed by an external structure or constraint.\n\nThe authors are using this intrinsic geometry to enforce consistency in the predictions made by the model, by designing the embedding space such that the relationships between concepts are reflected in their geometric arrangement. For example, the distance between the embeddings of a subcategory and its supercategory should reflect their hierarchical relationship.\n\nIn the example given, the authors define a surrogate energy function `d(x, y)` that measures the \"distance\" between two concepts in the embedding space, and use this function to enforce the hierarchical ordering of the concepts. The learning objective is designed to minimize this energy function, which encourages the model to learn an embedding space with a geometry that reflects the hierarchical relationships between the concepts.\n\nOverall, the intrinsic geometry of the embedding space is a key aspect of representation learning, as it allows the model to capture complex relationships between concepts and entities in a way that is both flexible and interpretable.", "predicted_evidence": ["Recently, a thread of research on representation learning has aimed to create embedding spaces that automatically enforce consistency in these predictions using the intrinsic geometry of the embedding space BIBREF9 , BIBREF0 , BIBREF10 . In these models, the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than the local relation predictions of previous models.", "In machine learning settings, knowledge is usually represented as a hypergraph of triplets such as Freebase BIBREF1 , WordNet BIBREF2 , and ConceptNet BIBREF3 . In these knowledge graphs, nodes represent entities or terms $t$ , and hyperedges are relations $R$ between these entities or terms, with each fact in the knowledge graph represented as a triplet $<t_1, R, t_2>$ . Researchers have developed many models for knowledge representation and learning in this setting BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , under the umbrella of knowledge graph completion. However, none of these naturally lend themselves to traditional methods of logical reasoning such as transitivity and negation.", "More than just curating a static collection of facts, we would like commonsense knowledge to be represented in a way that lends itself to machine reasoning and inference of missing information. We concern ourselves in this paper with the problem of learning commonsense knowledge representations.", "where $x$ is the subcategory and $y$ is the supercategory. This means the general concept embedding should be smaller than the specific concept embedding in every coordinate of the embeddings. An illustration of this geometry can be found in Figure 1. We can define a surrogate energy for this ordering function as $d(x, y) = \\left\\Vert  \\max (0,y-x) \\right\\Vert ^2$ . The learning objective for order embeddings becomes the following, where $m$ is a margin parameter, $x$ and $y$ are the hierarchically supervised pairs, and $x^{\\prime }$ and $y^{\\prime }$ are negatively sampled concepts: $\nL_{\\text{Order}} = \\sum _{x,y}\\max (0, m+d(x,y)-d(x^{\\prime }, y^{\\prime }))\n$", "A core problem in artificial intelligence is to capture, in machine-usable form, the collection of information that an ordinary person would have, known as commonsense knowledge. For example, a machine should know that a room may have a door, and that when a person enters a room, it is generally through a door. This background knowledge is crucial for solving many difficult, ambiguous natural language problems in coreference resolution and question answering, as well as the creation of other reasoning machines."]}
{"question_id": "9c2cacf77041e02d38f92a4c490df1e04552f96f", "predicted_answer": "Yes, they pre-trained on existing sentiment corpora. Specifically, they used a bilateral Long Short-Term Memory model trained on the Stanford Sentiment Treebank, and word embeddings (word2vec, GloVe, and Sent2Vec) trained on large news corpora.", "predicted_evidence": ["We now describe the models used for this work. Our models can be broken down into two groups: our first approach explores state-of-the-art models in targeted and untargeted sentiment analysis to evaluate their performance in the context of the SEC task. These models were pre-trained on larger corpora and evaluated directly on the task without any further adaptation. In a second approach we explore a data augmentation technique based on a proposed simplification of the task. In this approach, traditional machine learning classifiers were trained to identify which segments contain sentiment towards a SF regardless of sentiment polarity. For the classifiers, we explored the use of Support Vector Machines and Random Forests. Model performance was estimated through 10-fold cross validation on the train set. Hyper-parameters, such as of regularization, were selected based on the performance on grid-search using an 10-fold inner-cross validation loop. After choosing the parameters, models were re-trained on all the available data.", "Word embeddings pretrained on large corpora allow models to efficiently leverage word semantics as well as similarities between words. This can help with vocabulary generalization as models can adapt to words not previously seen in training data. In our feature set we include a 300-dimensional word2vec word representation trained on a large news corpus BIBREF36 . We obtain a representation for each segment by averaging the embedding of each word in the segment. We also experimented with the use of GloVe BIBREF37 , and Sent2Vec BIBREF38 , an extension of word2vec for sentences.", "Neural networks have been shown to capture specific task related subtleties which can complement the manually constructed sentiment lexica described in the previous subsection. For this work, we learn sentiment representations using a bilateral Long Short-Term Memory model BIBREF41 trained on the Stanford Sentiment Treebank BIBREF42 . This model was selected because it provided a good trade off between simplicity and performance on a fine-grained sentiment task, and has been shown to achieve competitive results to the state-of-the-art BIBREF43 .", "Given a dataset of text documents and manually annotated situation frames, the task is to automatically detect sentiment polarity relevant to existing frames and identify the source and target for each sentiment instance. The source is defined as a person or a group of people expressing the sentiment, and can be either a PER/ORG/GPE (person, organization or geo political entity) construct in the frame, the author of the text document, or an entity not explicitly expressed in the document. The target toward which the sentiment is expressed, is either the frame or an entity in the document.", "We use two sources of sentiment features: manually constructed lexica, and pre-trained sentiment embeddings. When available, manually constructed lexica are a useful resource for identifying expressions of sentiment BIBREF21 . We obtained word percentages across 192 lexical categories using Empath BIBREF39 , which extends popular tools such as the Linguistic Inquiry and Word Count (LIWC) BIBREF22 and General Inquirer (GI) BIBREF40 by adding a wider range of lexical categories. These categories include emotion classes such as surprise or disgust."]}
{"question_id": "35cdaa0fff007add4a795850b139df80af7d1ffc", "predicted_answer": "The question asks about the most salient features extracted by the models, but the provided context does not explicitly mention the features extracted by the models. The context primarily focuses on the approaches, models, and difficulties of the task, as well as the importance of identifying the source and target of expressed sentiment, justifying segments, and considering multiple information sources.\n\nHowever, it can be inferred that the models used linguistic features, as mentioned in the sentence: \"In this section we present the linguistic features and describe the models using for the evaluation.\" Unfortunately, the specific linguistic features used are not detailed in the provided context.", "predicted_evidence": ["We now describe the models used for this work. Our models can be broken down into two groups: our first approach explores state-of-the-art models in targeted and untargeted sentiment analysis to evaluate their performance in the context of the SEC task. These models were pre-trained on larger corpora and evaluated directly on the task without any further adaptation. In a second approach we explore a data augmentation technique based on a proposed simplification of the task. In this approach, traditional machine learning classifiers were trained to identify which segments contain sentiment towards a SF regardless of sentiment polarity. For the classifiers, we explored the use of Support Vector Machines and Random Forests. Model performance was estimated through 10-fold cross validation on the train set. Hyper-parameters, such as of regularization, were selected based on the performance on grid-search using an 10-fold inner-cross validation loop. After choosing the parameters, models were re-trained on all the available data.", "We approach the SEC task, particularly the polarity and emotion identification, as a classification problem. Our systems are based on English, and are extended to other languages via automatic machine translation (to English). In this section we present the linguistic features and describe the models using for the evaluation.", "There are multiple difficulties beyond the limited amount of data. For example, in order to provide decision-makers with actionable and usable information, it is not enough for the system to correctly classify sentiment or emotional state, it also ought to identify the source and target of the expressed sentiment. To provide a sense of trust and accountability on the system's decisions, it makes sense to identify a justifying segment. Moreover, these systems should consider a variety of information sources to create a broader and richer picture on how a situation unfolds. Thus, it is important that systems take into account the possible differences in the way sentiment is expressed in each one of these sources. In this work, we presented two approaches to the task of providing actionable and useful information. Our results show that state-of-the-art sentiment classifiers can be leveraged out-of-the-box for a reasonable performance on English data. By identifying possible differences coming from the information sources, as well as by exploiting the information communicated as the situation unfolds, we showed significant performance gains on both English and Spanish.", "Understanding the expressed sentiment from an affected population during the on-set of a crisis is a particularly difficult task, especially in low-resource scenarios. There are multiple difficulties beyond the limited amount of data. For example, in order to provide decision-makers with actionable and usable information, it is not enough for the system to correctly classify sentiment or emotional state, it also ought to identify the source and target of the expressed sentiment. To provide a sense of trust and accountability on the system's decisions, it makes sense to identify a justifying segment. Moreover, these systems should consider a variety of information sources to create a broader and richer picture on how a situation unfolds. Thus, it is important that systems take into account the possible differences in the way sentiment is expressed in each one of these sources. In this work, we presented two approaches to the task of providing actionable and useful information. Our results show that state-of-the-art sentiment classifiers can be leveraged out-of-the-box for a reasonable performance on English data.", "Understanding the expressed sentiment from an affected population during the on-set of a crisis is a particularly difficult task, especially in low-resource scenarios. There are multiple difficulties beyond the limited amount of data. For example, in order to provide decision-makers with actionable and usable information, it is not enough for the system to correctly classify sentiment or emotional state, it also ought to identify the source and target of the expressed sentiment. To provide a sense of trust and accountability on the system's decisions, it makes sense to identify a justifying segment. Moreover, these systems should consider a variety of information sources to create a broader and richer picture on how a situation unfolds. Thus, it is important that systems take into account the possible differences in the way sentiment is expressed in each one of these sources. In this work, we presented two approaches to the task of providing actionable and useful information."]}
{"question_id": "3de3a083b8ba3086792d38ae9667e095070f7f37", "predicted_answer": "The answer is not explicitly stated in the given context, but it can be inferred that there are more than one language in the dataset. The text mentions that the systems are based on English and are extended to other languages via automatic machine translation (to English). This implies that there are other languages in the dataset, but the exact number is not specified.", "predicted_evidence": ["We consider some of the most popular baseline models in the literature: (i) minority class baseline (due to the heavily imbalanced dataset), (ii) Support Vector Machines trained on TF-IDF bi-gram language model, (iii) and Support Vector Machines trained on word2vec representations. These models were trained using English documents only.", "We approach the SEC task, particularly the polarity and emotion identification, as a classification problem. Our systems are based on English, and are extended to other languages via automatic machine translation (to English). In this section we present the linguistic features and describe the models using for the evaluation.", "We extract word unigrams and bigrams. These features were then transformed using term frequencies (TF) and Inverse document-frequency (IDF).", "Word embeddings pretrained on large corpora allow models to efficiently leverage word semantics as well as similarities between words. This can help with vocabulary generalization as models can adapt to words not previously seen in training data. In our feature set we include a 300-dimensional word2vec word representation trained on a large news corpus BIBREF36 . We obtain a representation for each segment by averaging the embedding of each word in the segment. We also experimented with the use of GloVe BIBREF37 , and Sent2Vec BIBREF38 , an extension of word2vec for sentences.", "This section describes the SEC task in the LORELEI program along with the dataset, evaluation conditions and metrics."]}
{"question_id": "04914917d01c9cd8718cd551dc253eb3827915d8", "predicted_answer": "The answer is not explicitly stated in the given context. The LORELEI program is designed to work with low-resource languages, but the performance of the system on these languages is not evaluated or mentioned. The context primarily focuses on the framework, objectives, and approaches used in the program, rather than the system's performance on low-resource languages.", "predicted_evidence": ["The LORELEI program provides a framework for developing and testing systems for real-time humanitarian crises response in the context of low-resource languages. The working scenario is as follows: a sudden state of danger requiring immediate action has been identified in a region which communicates in a low resource language. Under strict time constraints, participants are expected to build systems that can: translate documents as necessary, identify relevant named entities and identify the underlying situation BIBREF14 . Situational information is encoded in the form of Situation Frames \u2014 data structures with fields identifying and characterizing the crisis type. The program's objective is the rapid deployment of systems that can process text or speech audio from a variety of sources, including newscasts, news articles, blogs and social media posts, all in the local language, and populate these Situation Frames.", "The LORELEI program provides a framework for developing and testing systems for real-time humanitarian crises response in the context of low-resource languages. The working scenario is as follows: a sudden state of danger requiring immediate action has been identified in a region which communicates in a low resource language. Under strict time constraints, participants are expected to build systems that can: translate documents as necessary, identify relevant named entities and identify the underlying situation BIBREF14 . Situational information is encoded in the form of Situation Frames \u2014 data structures with fields identifying and characterizing the crisis type. The program's objective is the rapid deployment of systems that can process text or speech audio from a variety of sources, including newscasts, news articles, blogs and social media posts, all in the local language, and populate these Situation Frames. While the task of identifying Situation Frames is similar to existing tasks in literature (e.g., slot filling), it is defined by the very limited availability of data BIBREF15 .", "Neural networks have been shown to capture specific task related subtleties which can complement the manually constructed sentiment lexica described in the previous subsection. For this work, we learn sentiment representations using a bilateral Long Short-Term Memory model BIBREF41 trained on the Stanford Sentiment Treebank BIBREF42 . This model was selected because it provided a good trade off between simplicity and performance on a fine-grained sentiment task, and has been shown to achieve competitive results to the state-of-the-art BIBREF43 .", "In this work, we develop systems that identify positive and negative sentiments expressed in social media posts, news articles and blogs in the context of a humanitarian emergency. Our systems work for both English and Spanish by using an automatic machine translation system. This makes our approach easily extendable to other languages, bypassing the scalability issues that arise from the need to manually construct lexica resources.", "Word embeddings pretrained on large corpora allow models to efficiently leverage word semantics as well as similarities between words. This can help with vocabulary generalization as models can adapt to words not previously seen in training data. In our feature set we include a 300-dimensional word2vec word representation trained on a large news corpus BIBREF36 . We obtain a representation for each segment by averaging the embedding of each word in the segment. We also experimented with the use of GloVe BIBREF37 , and Sent2Vec BIBREF38 , an extension of word2vec for sentences."]}
{"question_id": "20632fc4d2b693b5aabfbbc99ee5c1e9fc485dea", "predicted_answer": "The parts of the \"multimodal\" resources (AI2D and AI2D-RST) are:\n\n1. Graphs: used to represent the multimodal structure of diagrams.\n2. Nodes: components of the graphs that represent different elements of the diagrams.\n3. Edge types: connections between nodes that represent relationships between diagram elements.\n\nThese components work together to capture the multimodal structure of diagrams, enabling the analysis and understanding of multimodal discourse.", "predicted_evidence": ["GraphSAGE (SAGE) BIBREF28 with LSTM aggregation", "This section introduces the two multimodal resources compared in this study and discusses related work, beginning with the crowd-sourced annotations in AI2D and continuing with the alternative expert annotations in AI2D-RST, which are built on top of the crowd-sourced descriptions and cover a 1000-diagram subset of the original data. Figure FIGREF1 provides an overview of the two datasets, explains their relation to each other and provides an overview of the experiments reported in Section SECREF4", "Both AI2D and AI2D-RST use graphs to represent the multimodal structure of diagrams. This section explicates how the graphs and their node and edge types differ across the two multimodal resources.", "Understanding and making inferences about the structure of diagrams and other forms of multimodal discourse may be broadly conceptualised as multimodal discourse parsing. Recent examples of work in this area include alikhanietal2019 and ottoetal2019, who model discourse relations between natural language and photographic images, drawing on linguistic theories of coherence and text\u2013image relations, respectively. In most cases, however, predicting a single discourse relation covers only a part of the discourse structure. sachanetal2019 note that there is a need for comprehensive theories and models of multimodal communication, as they can be used to rethink tasks that have been previously considered only from the perspective of natural language processing.", "The 11 fine-grained classes in AI2D-RST shown in Figure FIGREF8 are complemented by an additional class (`mixed'), which includes diagrams that combine multiple diagram types, whose inclusion avoids performing multi-label classification (see the example in Figure FIGREF28). The coarse classes, which are derived by grouping fine-grained classes for tables, tabular and spatial organisations, networks and cycles, diagrammatic and pictorial representations, and so on, are also complemented by a `mixed' class."]}
{"question_id": "a57e266c936e438aeeab5e8d20d9edd1c15a32ee", "predicted_answer": "According to the text, the annotators of AI2D-RST are \"trained experts\", implying that they are familiar with the primary school science topics being annotated. In contrast, the annotators of AI2D are \"crowd-sourced workers\", which does not necessarily imply that they are familiar with the science topics.", "predicted_evidence": ["This section presents two experiments that compare AI2D and AI2D-RST annotations in classifying diagrams and their parts using various graph neural networks.", "This task compares the performance of graph-based representations in AI2D and AI2D-RST for classifying entire diagrams. Here the aim is to evaluate to what extent graph neural networks can learn about the generic structure of primary school science diagrams from the graph-based representations in AI2D and AI2D-RST. Correctly identifying what the diagram attempts to communicate and how carries implications for tasks such as visual question answering, as the type of a diagram constrains the interpretation of key diagrammatic elements, such as the meaning of lines and arrows BIBREF1, BIBREF17.", "In this article, I compared graph-based representations of diagrams representing primary school science topics from two datasets that contain the same diagrams, which have been annotated by either crowd-sourced workers or trained experts. The comparison involved two tasks, graph and node classification, using four different architectures for graph neural networks, which were compared to baselines from dummy, random forest and support vector machine classifiers.", "The purpose of the node classification task is to evaluate how well algorithms learn to classify the parts of a diagram using the graph-based representations in AI2D and AI2D-RST and node features representing the position, size and shape of the element, as described in Section SECREF11 Identifying the correct node type is a key step when populating a graph with candidate nodes from object detectors, particularly if the nodes will be processed further, for instance, to extract semantic representations from CNN features or word embeddings. Furthermore, the node representations learned during this task can be used as node features for graph classification, as will be shown shortly below in Section SECREF26", "Both AI2D and AI2D-RST represent the multimodal structure of diagrams using graphs. This enables learning their representations using graph neural networks, which are gaining currency as a graph is a natural choice for representing many types of data BIBREF15. This article reports on two experiments that evaluate the capability of AI2D and AI2D-RST to represent the multimodal structure of diagrams using graphs, focusing particularly on spatial layout, the hierarchical organisation of diagram elements and their connections expressed using arrows and lines."]}
{"question_id": "27356a99290fcc01e3e5660af3405d2a6c6f6e7c", "predicted_answer": "The expert and crowd-sourced annotations are compared through two tasks: graph and node classification, using four different architectures for graph neural networks, which are compared to baselines from dummy, random forest, and support vector machine classifiers.", "predicted_evidence": ["In this article, I compared graph-based representations of diagrams representing primary school science topics from two datasets that contain the same diagrams, which have been annotated by either crowd-sourced workers or trained experts. The comparison involved two tasks, graph and node classification, using four different architectures for graph neural networks, which were compared to baselines from dummy, random forest and support vector machine classifiers.", "This task compares the performance of graph-based representations in AI2D and AI2D-RST for classifying entire diagrams. Here the aim is to evaluate to what extent graph neural networks can learn about the generic structure of primary school science diagrams from the graph-based representations in AI2D and AI2D-RST. Correctly identifying what the diagram attempts to communicate and how carries implications for tasks such as visual question answering, as the type of a diagram constrains the interpretation of key diagrammatic elements, such as the meaning of lines and arrows BIBREF1, BIBREF17.", "The promising results AI2D-RST suggest is that domain experts in multimodal communication should be involved in planning crowd-sourced annotation tasks right from the beginning. Segmentation, in particular, warrants attention as this phase defines the units of analysis: cut-outs and cross-sections, for instance, use labels and lines to pick out sub-regions of graphical objects, whereas in illustrations the labels often refer to entire objects. Such distinctions should preferably be picked out at the very beginning to be incorporated fully into the annotation schema.", "This section presents two experiments that compare AI2D and AI2D-RST annotations in classifying diagrams and their parts using various graph neural networks.", "The grouping graph, which is initially populated by diagram elements from the AI2D layout segmentation, provides a foundation for describing connectivity and discourse structure by adding nodes to the grouping graph that stand for groups of diagram elements, as shown in the upper part of Figure FIGREF1. In addition, the grouping graph includes annotations for 11 different diagram types identified in the data (e.g. cycles, cross-sections and networks), which may be used as target labels during training, as explained in Section SECREF26 The coarse and fine-grained diagram types identified in the data are shown in Figure FIGREF8."]}
{"question_id": "6e37f43f4f54ffc77c785d60c6058fbad2147922", "predicted_answer": "The platform from which the crowd-sourced workers come is not explicitly mentioned in the given context. It is likely that the author is referring to a generic crowdsourcing platform, such as Amazon Mechanical Turk, Clickworker, or Figure Eight, but the specific platform is not specified.", "predicted_evidence": ["Tasks related to grouping and connectivity annotation could be crowd-sourced relatively easily, whereas annotating diagram types and discourse relations may require multi-step procedures and assistance in the form of prompts, as yungetal2019 have recently shown for RST. Involving both expert and crowd-sourced annotators could also alleviate problems related to circularity by forcing domain experts to frame the tasks in terms understandable to crowd-sourced workers BIBREF24.", "In this article, I compared graph-based representations of diagrams representing primary school science topics from two datasets that contain the same diagrams, which have been annotated by either crowd-sourced workers or trained experts. The comparison involved two tasks, graph and node classification, using four different architectures for graph neural networks, which were compared to baselines from dummy, random forest and support vector machine classifiers.", "I evaluated the following graph neural network architectures for both graph and node classification tasks:", "For the experiments reported in Section 4, self-loops are added to each node in the graph. A self-loop is an edge that originates in and terminates at the same node. Self-loops essentially add the graph's identity matrix to the adjacency matrix, which allow the graph neural networks to account for the node's own features during message passing, that is, when sending and receiving features from adjacent nodes.", "Grouping: A tree graph that groups together diagram elements that are likely to be visually perceived as belonging together, based loosely on Gestalt principles of visual perception BIBREF19. These groups are organised into a hierarchy, which represents the organisation of content in the 2D layout space BIBREF13, BIBREF14."]}
{"question_id": "fff1ed2435ba622d884ecde377ff2de127167638", "predicted_answer": "According to the text, \"trained experts\" are individuals who have annotated diagrams representing primary school science topics, as opposed to crowd-sourced workers.", "predicted_evidence": ["The purpose of the node classification task is to evaluate how well algorithms learn to classify the parts of a diagram using the graph-based representations in AI2D and AI2D-RST and node features representing the position, size and shape of the element, as described in Section SECREF11 Identifying the correct node type is a key step when populating a graph with candidate nodes from object detectors, particularly if the nodes will be processed further, for instance, to extract semantic representations from CNN features or word embeddings. Furthermore, the node representations learned during this task can be used as node features for graph classification, as will be shown shortly below in Section SECREF26", "This task compares the performance of graph-based representations in AI2D and AI2D-RST for classifying entire diagrams. Here the aim is to evaluate to what extent graph neural networks can learn about the generic structure of primary school science diagrams from the graph-based representations in AI2D and AI2D-RST. Correctly identifying what the diagram attempts to communicate and how carries implications for tasks such as visual question answering, as the type of a diagram constrains the interpretation of key diagrammatic elements, such as the meaning of lines and arrows BIBREF1, BIBREF17.", "This section presents two experiments that compare AI2D and AI2D-RST annotations in classifying diagrams and their parts using various graph neural networks.", "I evaluated the following graph neural network architectures for both graph and node classification tasks:", "In this article, I compared graph-based representations of diagrams representing primary school science topics from two datasets that contain the same diagrams, which have been annotated by either crowd-sourced workers or trained experts. The comparison involved two tasks, graph and node classification, using four different architectures for graph neural networks, which were compared to baselines from dummy, random forest and support vector machine classifiers."]}
{"question_id": "7ff7c286d3118a8be5688e2d18e9a56fe83679ad", "predicted_answer": "The model architectures mentioned in the text are:\n\n1. CNN (Convolutional Neural Network)\n2. HAN (Hierarchical Attention Network)\n3. LM (Language Model)\n4. SVM (Support Vector Machine)\n5. TILE-Clique model (a matching model)\n\nNote that the text does not provide a single \"opted for\" model architecture, as it discusses and compares the performance of different models on various tasks.", "predicted_evidence": ["To be useful as a flagging mechanism for moderation, a controversy detection algorithm should satisfy both Precision and Recall criteria. F1 scores will therefore be used to evaluate this balance. The AUC values are used to measure classification performance in the unbalanced controversy datasets. The test-train split depends on the task investigated and is listed in the results section for the respective task. To test for significant results, all models were evaluated using a bootstrap approach: by drawing 1000 samples with replacements INLINEFORM0 documents from the test set equal to the test-set size. The resulting confidence intervals based on percentiles provide a measure of significance.", "To compare the results of neural approaches to prior work we implemented the previous state-of-the-art controversy detection method: the language model from BIBREF7 . Together with an SVM baseline they act as controversy detection alternatives using only full text features, thus meeting the task-requirements of platform-independence. Note: the implementation of BIBREF7 additionally requires ranking methods to select a subset of the training data for each language model. A simplified version of this, excluding the ranking method but using the same dataset and lexicon to select documents as BIBREF7 , is implemented and included in the baselines comparison section (LM-DBPedia). We also included the same language model trained on the full text Wikipedia pages (LM-wiki). Similarly, for completeness sake, we also include both the state-of-the-art matching model, the TILE-Clique model from BIBREF1 and the sentiment analysis baseline (using the state-of-the-art Polyglot library for python) from BIBREF6 in the comparison with previous work.", "To compare the results of neural approaches to prior work we implemented the previous state-of-the-art controversy detection method: the language model from BIBREF7 . Together with an SVM baseline they act as controversy detection alternatives using only full text features, thus meeting the task-requirements of platform-independence. Note: the implementation of BIBREF7 additionally requires ranking methods to select a subset of the training data for each language model. A simplified version of this, excluding the ranking method but using the same dataset and lexicon to select documents as BIBREF7 , is implemented and included in the baselines comparison section (LM-DBPedia). We also included the same language model trained on the full text Wikipedia pages (LM-wiki).", "First, we have demonstrated that neural methods perform as state-of-the-art tools in controversy detection on the ClueWeb09 BIBREF0 based testset, even beating matching models. Second, we investigated temporal stability, and demonstrated neural -and especially CNN- robustness in terms of Recall, F1 and AUC performance and stability with train and test sets that are 9 years apart. Thirdly, we show that CNN and HAN models outperform the SVM and LM baselines on Precision, F1 and AUC when tested on held-out-topics. Fourthly, we show that neural methods are better able to generalize from Wikipedia pages to unseen general web pages in terms of Precision, F1 and AUC. Lastly, neural methods seem better in line with human annotators with regard to certainty and disagreement.", "Most work on controversy has looked into using existing knowledge bases as a source of controversy information BIBREF6 , BIBREF1 . In this paper, we focus on text-based classification methods that do not aim to explicitly link general web pages to their knowledge-base counterparts. Therefore, we are interested in the ability of neural models to generalize beyond their training context. In addition to testing across time and topics, we also investigate robustness to changes in domain. By training only on Wikipedia data, and evaluating only on general web-pages, we look at the ability of the four methods to deal with out-of-domain documents."]}
{"question_id": "1ecbbb60dc44a701e9c57c22167dd412711bb0be", "predicted_answer": "The dataset used is a Wikipedia crawl-based dataset that includes general web pages and is sufficiently large to train and test high capacity models such as neural networks.", "predicted_evidence": ["To be useful as a flagging mechanism for moderation, a controversy detection algorithm should satisfy both Precision and Recall criteria. F1 scores will therefore be used to evaluate this balance. The AUC values are used to measure classification performance in the unbalanced controversy datasets. The test-train split depends on the task investigated and is listed in the results section for the respective task. To test for significant results, all models were evaluated using a bootstrap approach: by drawing 1000 samples with replacements INLINEFORM0 documents from the test set equal to the test-set size. The resulting confidence intervals based on percentiles provide a measure of significance.", "A proven approach in modelling text with neural networks is to use Recurrent Neural Networks (RNNs) which enjoy weight sharing capabilities to model words irrespective of their sequence location. A specific type, the Hierarchical Attention Network (HAN) proposed by BIBREF10 makes use of attention to build document representations in a hierarchical manner. It uses bi-directional Gated Recurrent Units (GRUs) BIBREF12 to selectively update representations of both words and sentences. This allows the network to both capture the hierarchy from words to sentences to documents and to explicitly weigh all parts of the document relevant during inference.", "Controversy detection is a hard task, as it forms a latent concept sensitive to vocabulary gaps between topics and vocabulary shifts over time. We analysed the performance of language model, SVM, CNN and HAN models on different tasks.", "Currently, there is no open large-size controversy detection dataset that lends itself to test cross-temporal and cross-topic stability. Thus we generate a Wikipedia crawl-based dataset that includes general web pages and is sufficiently large to train and test high capacity models such as neural networks.", "To evaluate robustness towards unseen topics, 10-fold cross validation was used on the top ten largest topics present in the Wikipedia dataset in a leave-one-out fashion. The results are shown in table 4. In line with previous results, the language model scores best on Recall, beating all other models with a significant difference (p < 0.01). However in balancing Recall with Precision, the HAN model scores best, significantly outperforming both lexical models in F1 score (p < 0.05). Overall, when grouping together all neural and lexical results, the neural methods outperform the lexical models in Precision (p < 0.01), F1 (p < 0.05) and AUC (p < 0.01) with no significant difference found on the overall Recall scores. These results indicate that neural methods seem better able to generalize to unseen topics."]}
{"question_id": "592df9831692b8fde213257ed1894344da3e0594", "predicted_answer": "According to the text, the hardest setup is the cross-topic setup. This is implied by the statement \"Controversy detection is a hard task, as it forms a latent concept sensitive to vocabulary gaps between topics and vocabulary shifts over time.\" This suggests that the differences in vocabulary and language use between topics make it challenging to detect controversies.", "predicted_evidence": ["To be useful as a flagging mechanism for moderation, a controversy detection algorithm should satisfy both Precision and Recall criteria. F1 scores will therefore be used to evaluate this balance. The AUC values are used to measure classification performance in the unbalanced controversy datasets. The test-train split depends on the task investigated and is listed in the results section for the respective task. To test for significant results, all models were evaluated using a bootstrap approach: by drawing 1000 samples with replacements INLINEFORM0 documents from the test set equal to the test-set size. The resulting confidence intervals based on percentiles provide a measure of significance.", "Controversy detection is a hard task, as it forms a latent concept sensitive to vocabulary gaps between topics and vocabulary shifts over time. We analysed the performance of language model, SVM, CNN and HAN models on different tasks.", "Controversy detection is an increasingly important task. Controversial content can signal the need for moderation on social platforms, either to prevent conflict between users or limit the spread of misinformation. More generally, controversies provide insight into societies BIBREF0 . Often, the controversial content is outside the direct control of a platform on which it is shared, mentioned or discussed. This raises the requirement of generally applicable methods to gauge controversial content on the web for moderation purposes. Unfortunately, what is controversial changes, and may lie more in the way topics are discussed rather than what is discussed, making it difficult to detect controversies in a robust fashion. We take the task of controversy detection and evaluate robustness of different methodologies with respect to the varying nature of controversies.", "Certainty is the distance of human annotations to the midpoint of the four-point controversy scale, i.e. a score between 0 and 2.5 that expresses how sure annotators are of document (non)controversy. Here, the HAN shows errors most strongly negatively correlated to the certainty of annotators. Finally, annotators disagree on the controversy of some documents, expressed as the standard deviation of their controversy annotations. Again, the HAN model seems preferable, as it's errors are most strongly correlated to annotator disagreement. Overall, the neural methods have less biased performance in relation to (non)controversial documents, correlate more strongly with the certainty of human annotators and are susceptible to errors in similar conditions as when annotators disagree.", "Interestingly, the SVM and HAN model show some unexpected improvement with regard to Precision when applied to unseen timeframes. For both models, this increase in Precision is offset by a greater loss in Recall, which seems to indicate both models `memorize` the controversial topics in a given timeframe instead of the controversial language. Overall, the neural approaches seem to compare favorably in terms of cross-temporal stability."]}
{"question_id": "6822ca5f7a19866ffc3c985b790a4aadcecf2d1c", "predicted_answer": "The text does not explicitly mention \"weak signal data\". However, it discusses the challenges of controversy detection, including the fact that controversies are latent and often not directly mentioned in text, which implies that the data may contain weak signals or indirect indicators of controversy.", "predicted_evidence": ["Interestingly, the SVM and HAN model show some unexpected improvement with regard to Precision when applied to unseen timeframes. For both models, this increase in Precision is offset by a greater loss in Recall, which seems to indicate both models `memorize` the controversial topics in a given timeframe instead of the controversial language. Overall, the neural approaches seem to compare favorably in terms of cross-temporal stability.", "Within year, the hierarchical attention model (HAN) outperforms all other models on Recall, F1 and AUC, losing Precision to the CNN and SVM models. However, our main interest is the robustness when a model is trained on a different year (2009) than the test set (2018). These between year experiments show a superior score for the HAN model compared to the non-neural models on Recall, and show significant improvements on F1 (p < 0.05) and AUC (p < 0.05), losing only to the SVM model on Precision (non significantly). In terms of robustness, we can also take the percentage change between the within year and between year experiment into account (were smaller absolute changes are preferable), shown by the delta values. With regard to temporal sensitivity, the CNN shows the least change across all four metrics. In Figure 1, we show the pooled results for the lexical and neural models to illustrate the overall increase in robustness by neural approaches.", "Within year, the hierarchical attention model (HAN) outperforms all other models on Recall, F1 and AUC, losing Precision to the CNN and SVM models. However, our main interest is the robustness when a model is trained on a different year (2009) than the test set (2018). These between year experiments show a superior score for the HAN model compared to the non-neural models on Recall, and show significant improvements on F1 (p < 0.05) and AUC (p < 0.05), losing only to the SVM model on Precision (non significantly). In terms of robustness, we can also take the percentage change between the within year and between year experiment into account (were smaller absolute changes are preferable), shown by the delta values. With regard to temporal sensitivity, the CNN shows the least change across all four metrics.", "Controversy detection is a difficult task because 1) controversies are latent, like ideology, meaning they are often not directly mentioned as controversial in text. 2) Controversies occur across a vast range of topics with varying topic-specific vocabularies. 3) Controversies change over time, with some topics and actors becoming controversial whereas others stop to be so. Previous approaches lack the power to deal with such changes. Matching and explicit approaches are problematic when the source corpus (e.g. Wikipedia) lags after real-world changes BIBREF8 . Furthermore, lexical methods trained on common (e.g. fulltext) features are likely to memorize the controversial topics in the training set rather than the `language of controversy'. Alleviating dependence on platform specific features and reducing sensitivity to an exact lexical representation is paramount to robust controversy detection. To this end, we focus only on fulltext features and suggest to leverage the semantic representations of word embeddings to reduce the vocabulary-gap for unseen topics and exact lexical representations.", "Controversy detection is a difficult task because 1) controversies are latent, like ideology, meaning they are often not directly mentioned as controversial in text. 2) Controversies occur across a vast range of topics with varying topic-specific vocabularies. 3) Controversies change over time, with some topics and actors becoming controversial whereas others stop to be so. Previous approaches lack the power to deal with such changes. Matching and explicit approaches are problematic when the source corpus (e.g. Wikipedia) lags after real-world changes BIBREF8 . Furthermore, lexical methods trained on common (e.g. fulltext) features are likely to memorize the controversial topics in the training set rather than the `language of controversy'. Alleviating dependence on platform specific features and reducing sensitivity to an exact lexical representation is paramount to robust controversy detection."]}
{"question_id": "60e6296ca2a697892bd67558a21a83ef01a38177", "predicted_answer": "No, they do not compare their semantic feature approach to lexical approaches. They compare their neural network approaches (RNN, CNN, HAN) to prior work, including a language model and an SVM baseline, which use full text features, but they do not specifically compare their approach to lexical approaches.", "predicted_evidence": ["The majority of NLP-task related neural architectures rely on word embeddings, popularized by Mikolov et al BIBREF9 to represent texts. In essence these embeddings are latent-vector representations that aim to capture the underlying meaning of words. Distances between such latent-vectors are taken to express semantic relatedness, despite having different surface forms. By using embeddings, neural architectures are also able to leverage features learned on other texts (e.g. pretrained word embeddings) and create higher level representations of input (e.g. convolutional feature maps or hidden-states). These properties suggest that neural approaches are better able to generalize to unseen examples that poorly match the training set. We use two often applied network architectures adopting word embeddings, to classify controversy: Recurrent Neural Networks BIBREF10 and Convolutional Neural Networks BIBREF11 to answer the following research question. RQ: Can we increase robustness of controversy detection using neural methods?", "A proven approach in modelling text with neural networks is to use Recurrent Neural Networks (RNNs) which enjoy weight sharing capabilities to model words irrespective of their sequence location. A specific type, the Hierarchical Attention Network (HAN) proposed by BIBREF10 makes use of attention to build document representations in a hierarchical manner. It uses bi-directional Gated Recurrent Units (GRUs) BIBREF12 to selectively update representations of both words and sentences. This allows the network to both capture the hierarchy from words to sentences to documents and to explicitly weigh all parts of the document relevant during inference.", "Controversy detection is a hard task, as it forms a latent concept sensitive to vocabulary gaps between topics and vocabulary shifts over time. We analysed the performance of language model, SVM, CNN and HAN models on different tasks.", "To compare the results of neural approaches to prior work we implemented the previous state-of-the-art controversy detection method: the language model from BIBREF7 . Together with an SVM baseline they act as controversy detection alternatives using only full text features, thus meeting the task-requirements of platform-independence. Note: the implementation of BIBREF7 additionally requires ranking methods to select a subset of the training data for each language model. A simplified version of this, excluding the ranking method but using the same dataset and lexicon to select documents as BIBREF7 , is implemented and included in the baselines comparison section (LM-DBPedia). We also included the same language model trained on the full text Wikipedia pages (LM-wiki). Similarly, for completeness sake, we also include both the state-of-the-art matching model, the TILE-Clique model from BIBREF1 and the sentiment analysis baseline (using the state-of-the-art Polyglot library for python) from BIBREF6 in the comparison with previous work.", "To compare the results of neural approaches to prior work we implemented the previous state-of-the-art controversy detection method: the language model from BIBREF7 . Together with an SVM baseline they act as controversy detection alternatives using only full text features, thus meeting the task-requirements of platform-independence. Note: the implementation of BIBREF7 additionally requires ranking methods to select a subset of the training data for each language model. A simplified version of this, excluding the ranking method but using the same dataset and lexicon to select documents as BIBREF7 , is implemented and included in the baselines comparison section (LM-DBPedia). We also included the same language model trained on the full text Wikipedia pages (LM-wiki)."]}
{"question_id": "9b868c7d17852f46a8fe725f24cb9548fdbd2b05", "predicted_answer": "The dataset used for training is a set of YouTube videos, but the specific dataset is not mentioned.", "predicted_evidence": ["where INLINEFORM0 are learned matrices.", "where each word is assigned a probability given the previous word history.", "Our work is distinguishable from previous work with respect to three dimensions:", "For evaluation we used two datasets, YouCook2 and sth-sth, allowing us to evaluate our models in cases where the visual context is relevant to the modelled language. Note that no data from these datasets are present in the YouTube videos used for training. The perplexity of our models is shown in Table .", "A language model assigns to a sentence INLINEFORM0 the probability: INLINEFORM1"]}
{"question_id": "243cf21c4e34c4b91fcc4905aa4dc15a72087f0c", "predicted_answer": "The size of the training data is approximately 64M segments from YouTube videos, comprising a total of INLINEFORM0 B tokens.", "predicted_evidence": ["where INLINEFORM0 are learned matrices.", "A language model assigns to a sentence INLINEFORM0 the probability: INLINEFORM1", "where each word is assigned a probability given the previous word history.", "In this case, the RNNLM is given as input a vector INLINEFORM0 that is a weighted sum of the two embeddings: INLINEFORM1", "Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens BIBREF14 . We tokenize the training data using a vocabulary of 66K wordpieces BIBREF15 . Thus, the input to the model is a sequence of wordpieces. Using wordpieces allows us to address out-of-vocabulary (OOV) word issues that would arise from having a fixed word vocabulary. In practice, a wordpiece RNNLM gives similar performance as a word-level model BIBREF16 . For about INLINEFORM1 of the segments, we were able to obtain visual features at the frame level. The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 , BIBREF18 . For a INLINEFORM2 -second video and INLINEFORM3 wordpieces, each feature is uniformly allocated to INLINEFORM4 wordpieces."]}
{"question_id": "488e3c4fd1103c46e12815d1bf414a0356fb0d0e", "predicted_answer": "The features derived from the videos are the visual representations, denoted by INLINEFORM1.", "predicted_evidence": ["For a given video segment, we assume that there is a sequence of INLINEFORM0 video frames represented by features INLINEFORM1 , and the corresponding transcription INLINEFORM2 . In practice, we assume INLINEFORM3 since we can always assign a video frame to each word by replicating the video frames the requisite number of times. Thus, our visually-grounded language model models the probability of the next word given the history of previous words as well as video frames: INLINEFORM4", "There are several options for combining the text and video modalities. We opt for the simplest strategy, which concatenates the representations. For a word embedding INLINEFORM0 and corresponding visual representation INLINEFORM1 , the input to our RNNLM will be the concatenated vector INLINEFORM2 . For the examples where we were unable to compute visual features (see Section \u00a7 SECREF3 ), we set INLINEFORM3 to be a zero-vector.", "In this case, the RNNLM is given as input a vector INLINEFORM0 that is a weighted sum of the two embeddings: INLINEFORM1", "For evaluation we used two datasets, YouCook2 and sth-sth, allowing us to evaluate our models in cases where the visual context is relevant to the modelled language. Note that no data from these datasets are present in the YouTube videos used for training. The perplexity of our models is shown in Table .", "We present a simple strategy to augment a standard recurrent neural network language model with temporal visual features. Through an exploration of candidate architectures, we show that the Middle Fusion of visual and textual features leads to a 20-28% reduction in perplexity relative to a text only baseline. These experiments were performed using datasets of unprecedented scale, with more than 1.2 billion tokens \u2013 two orders of magnitude more than any previously published work. Our work is a first step towards creating and deploying large-scale multimodal systems that properly situate themselves into a given context, by taking full advantage of every available signal."]}
{"question_id": "84765903b8c7234ca2919d0a40e3c6a5bcedf45d", "predicted_answer": "The text does not explicitly mention attention, which is a common mechanism used in neural machine translation (NMT) models to focus on specific parts of the input data. However, it is likely that the NMT models used in the experiments do employ attention, as it is a standard component of many NMT architectures.", "predicted_evidence": ["Then experiments on more challenging but also much larger data sets for polynomial normalization were performed. Depending on the difficulty of the data, accuracy on the test sets achieved in our experiments varied between $70\\%$ and $99\\%$. The results in terms of accuracy are shown in Table TABREF13.", "NMT is not typically applied to symbolic problems, but surprisingly, it performed very well for both described tasks. The first one was easier in terms of complexity of the rewriting (only one application of a rewrite rule was performed) but the number of examples was quite limited. The second task involved more difficult rewriting \u2013 multiple different rewrite steps were performed to construct the examples. Nevertheless, provided many examples, NMT could learn normalizing polynomials.", "Firstly, more interesting and difficult rewriting problems need to be provided for better delineation of the strength of the neural models. The described data are relatively simple and with no direct relevance to the real unsolved symbolic problems. But the results on these simple problems are encouraging enough to try with more challenging ones, related to real difficulties \u2013 e.g. these from TPDB data base.", "Many of the inferences in the proofs are paramodulations from an equation and have the form s = t", "Each data set consists of different 300 000 examples, see Table TABREF7 for examples. These data sets were split into training, validation and test sets for our experiments ($60 \\%, 10 \\%, 30 \\%$, respectively)."]}
{"question_id": "38363a7ed250bc729508c4c1dc975696a65c53cb", "predicted_answer": "The translation models explored in this context are:\n\n1. Neural Machine Translation (NMT) models\n2. TreeNN models, which are tailored for comprehending symbolic expressions and are built recursively from modules that correspond to parts of symbolic expressions.", "predicted_evidence": ["Neural networks (NNs) turned out to be very useful in several domains. In particular, one of the most spectacular advances achieved with use of NNs has been natural language processing. One of the tasks in this domain is translation between natural languages \u2013 neural machine translation (NMT) systems established here the state-of-the-art performance. Recently, NMT produced first encouraging results in the autoformalization task BIBREF0, BIBREF1, BIBREF2, BIBREF3 where given an informal mathematical text in the goal is to translate it to its formal (computer understandable) counterpart. In particular, the NMT performance on a large synthetic -to-Mizar dataset produced by a relatively sophisticated toolchain developed for several decades BIBREF4 is surprisingly good BIBREF3, indicating that neural networks can learn quite complicated algorithms for symbolic data. This inspired us to pose a question: Can NMT models be used in the formal-to-formal setting?", "Neural networks (NNs) turned out to be very useful in several domains. In particular, one of the most spectacular advances achieved with use of NNs has been natural language processing. One of the tasks in this domain is translation between natural languages \u2013 neural machine translation (NMT) systems established here the state-of-the-art performance. Recently, NMT produced first encouraging results in the autoformalization task BIBREF0, BIBREF1, BIBREF2, BIBREF3 where given an informal mathematical text in the goal is to translate it to its formal (computer understandable) counterpart. In particular, the NMT performance on a large synthetic -to-Mizar dataset produced by a relatively sophisticated toolchain developed for several decades BIBREF4 is surprisingly good BIBREF3, indicating that neural networks can learn quite complicated algorithms for symbolic data.", "In particular, one of the most spectacular advances achieved with use of NNs has been natural language processing. One of the tasks in this domain is translation between natural languages \u2013 neural machine translation (NMT) systems established here the state-of-the-art performance. Recently, NMT produced first encouraging results in the autoformalization task BIBREF0, BIBREF1, BIBREF2, BIBREF3 where given an informal mathematical text in the goal is to translate it to its formal (computer understandable) counterpart. In particular, the NMT performance on a large synthetic -to-Mizar dataset produced by a relatively sophisticated toolchain developed for several decades BIBREF4 is surprisingly good BIBREF3, indicating that neural networks can learn quite complicated algorithms for symbolic data. This inspired us to pose a question: Can NMT models be used in the formal-to-formal setting? In particular: Can NMT models learn symbolic rewriting?", "Secondly, we are going to develop and test new kinds of neural models tailored for the problem of comprehending symbolic expressions. Specifically, we are going to implement an approach based on the idea of TreeNN, which may be another effective approach for this kind of tasks BIBREF7, BIBREF12, BIBREF13. TreeNNs are built recursively from modules, where the modules corresponds to parts of symbolic expression (symbols) and the shape of the network reflects the parse tree of the processed expression. This way model is explicitly informed on the exact structure of the expression, which in case of formal logic is always unambiguous and easy to extract. Perhaps this way the model could learn more efficiently from examples (and achieve higher results even on the small AIM data sets). The authors have a positive experience of applying TreeNNs to learn remainders of arithmetical expressions modulo small natural numbers \u2013 TreeNNs outperformed here neural models based on LSTM cells, giving almost perfect accuracy.", "Secondly, we are going to develop and test new kinds of neural models tailored for the problem of comprehending symbolic expressions. Specifically, we are going to implement an approach based on the idea of TreeNN, which may be another effective approach for this kind of tasks BIBREF7, BIBREF12, BIBREF13. TreeNNs are built recursively from modules, where the modules corresponds to parts of symbolic expression (symbols) and the shape of the network reflects the parse tree of the processed expression. This way model is explicitly informed on the exact structure of the expression, which in case of formal logic is always unambiguous and easy to extract. Perhaps this way the model could learn more efficiently from examples (and achieve higher results even on the small AIM data sets)."]}
{"question_id": "e862ebfdb1b3425af65fec81c8984edca6f89a76", "predicted_answer": "Symbolic rewriting refers to the process of transforming a symbolic expression into an equivalent form, often in a normalized or simplified form. In the context of the provided text, symbolic rewriting involves rewriting polynomial terms from an arbitrary form to a normalized form.", "predicted_evidence": ["Firstly, more interesting and difficult rewriting problems need to be provided for better delineation of the strength of the neural models. The described data are relatively simple and with no direct relevance to the real unsolved symbolic problems. But the results on these simple problems are encouraging enough to try with more challenging ones, related to real difficulties \u2013 e.g. these from TPDB data base.", "Secondly, we are going to develop and test new kinds of neural models tailored for the problem of comprehending symbolic expressions. Specifically, we are going to implement an approach based on the idea of TreeNN, which may be another effective approach for this kind of tasks BIBREF7, BIBREF12, BIBREF13. TreeNNs are built recursively from modules, where the modules corresponds to parts of symbolic expression (symbols) and the shape of the network reflects the parse tree of the processed expression. This way model is explicitly informed on the exact structure of the expression, which in case of formal logic is always unambiguous and easy to extract. Perhaps this way the model could learn more efficiently from examples (and achieve higher results even on the small AIM data sets). The authors have a positive experience of applying TreeNNs to learn remainders of arithmetical expressions modulo small natural numbers \u2013 TreeNNs outperformed here neural models based on LSTM cells, giving almost perfect accuracy.", "Secondly, we are going to develop and test new kinds of neural models tailored for the problem of comprehending symbolic expressions. Specifically, we are going to implement an approach based on the idea of TreeNN, which may be another effective approach for this kind of tasks BIBREF7, BIBREF12, BIBREF13. TreeNNs are built recursively from modules, where the modules corresponds to parts of symbolic expression (symbols) and the shape of the network reflects the parse tree of the processed expression. This way model is explicitly informed on the exact structure of the expression, which in case of formal logic is always unambiguous and easy to extract. Perhaps this way the model could learn more efficiently from examples (and achieve higher results even on the small AIM data sets).", "An examination of the examples wrongly rewritten by the model was done. It turns out that the wrong outputs almost always parse (in $97 - 99 \\%$ of cases they are legal polynomial terms). Notably, depending on the difficulty of the data set, as much as $18 - 64 \\%$ of incorrect outputs are wrong only with respect to the constants in the terms. (Typically, NMT model proposes too low constants compared to the correct ones.) Below $1 \\%$ of wrong outputs are correct modulo variable renaming.", "This is a synthetically created data set where the examples are pairs of equivalent polynomial terms. The first element of each pair is a polynomial in an arbitrary form and the second element is the same polynomial in a normalized form. The arbitrary polynomials are created randomly in a recursive manner from a set of available (non-nullary) function symbols, variables and constants. First, one of the symbols is randomly chosen. If it is a constant or a variable it is returned and the process terminates. If a function symbol is chosen, its subterm(s) are constructed recursively in a similar way."]}
{"question_id": "ec8f39d32084996ab825debd7113c71daac38b06", "predicted_answer": "They incorporate expert knowledge into their topic model by allowing a clinician to manually specify \"anchor terms\" that are relevant to a particular topic, such as \"diabetes\" or \"insulin\". These anchor terms are used to guide the discovery of latent topics and to impose semantics onto the learned latent factors. The user can then add or remove additional anchor terms to improve the quality of the learned topics.", "predicted_evidence": ["We have introduced a simple information theoretic approach to topic modeling that can leverage domain knowledge specified informally as anchors. Our framework uses a novel combination of CorEx and the information bottleneck. Preliminary results suggest it can extract more precise, interpretable topics through a lightweight interactive process. We next plan to perform further empirical evaluations and to extend the algorithm to handle complex latent structures present in health care data.", "In this workshop paper, we introduce a simple approach to anchored information theoretic topic modeling using a novel combination of Correlation Explanation (CorEx) BIBREF3 and the information bottleneck BIBREF4 . This flexible framework enables the user to leverage domain knowledge to guide exploration of a collection of documents and to impose semantics onto latent factors learned by CorEx. We present preliminary experimental results on two text corpora (including a corpus of clinical notes), showing that anchors can be used to discover topics that are more specific and relevant. What is more, we demonstrate the potential for this framework to perform weakly supervised learning in settings where labeling documents is prohibitively expensive BIBREF5 , BIBREF6 .", "A clinician can look at a patient's electronic health record (EHR) and not only decide whether the patient has diabetes but also produce a succinct summary of the clinical evidence. Replicating this feat with computational tools has been the focus of much research in clinical informatics. There are major initiatives underway to codify clinical knowledge into formal representations, most often as deterministic rules that can be applied in a semi-automated fashion BIBREF0 . However, representing the intuitive judgments of human experts can be challenging, particularly when the formal system does not match the expert's knowledge. For example, many deterministic disease classifiers used in clinical informatics rely heavily upon administrative codes not available at time of diagnosis. Further, developing and testing such systems is time- and labor-intensive.", "We propose instead a lightweight information theoretic framework for codifying informal human knowledge and then use it to extract interpretable latent topics from text corpora. For example, to discover patients with diabetes in a set of clinical notes, a doctor can begin by specifying disease-specific anchor terms BIBREF1 , BIBREF2 , such as \u201cdiabetes\u201d or \u201cinsulin.\u201d Our framework then uses these to help discover both latent topics associated with diabetes and records in which diabetes-related topics occur. The user can then add (or remove) additional anchor terms (e.g., \u201cmetformin\u201d) to improve the quality of the learned (diabetes) topics.", "BIBREF1 first proposed anchors in the context of topic modeling: words that are high precision indicators of underlying topics. In contrast to our approach, anchors are typically selected automatically, constrained to appear in only one topic, and used primarily to aid optimization BIBREF17 . In our information theoretic framework, anchors are specified manually and more loosely defined as words having high mutual information with one or more latent factors. The effects of anchors on the interpretability of traditional topic models are often mixed BIBREF18 , but our experiments suggest that our approach yields more coherent topics."]}
{"question_id": "a67a2d9acad1787b636ca2681330f4c29a0b0254", "predicted_answer": "They evaluate on two corpora: 20 Newsgroups and the i2b2 2008 Obesity Challenge data set.", "predicted_evidence": ["To demonstrate the utility of Anchored CorEx, we run experiments on two document collections: 20 Newsgroups and the i2b2 2008 Obesity Challenge BIBREF22 data set. Both corpora provide ground truth labels for latent classes that may be thought of as topics.", "In a series of follow-up experiments, we investigate the suitability of using anchored CorEx to perform weakly supervised classification. We interpret each anchored latent factor as a classifier for an associated class label and then compute test set F1 (using a threshold of 0.5) and area under the curve (AUC) scores (Obesity Challenge only).", "In this workshop paper, we introduce a simple approach to anchored information theoretic topic modeling using a novel combination of Correlation Explanation (CorEx) BIBREF3 and the information bottleneck BIBREF4 . This flexible framework enables the user to leverage domain knowledge to guide exploration of a collection of documents and to impose semantics onto latent factors learned by CorEx. We present preliminary experimental results on two text corpora (including a corpus of clinical notes), showing that anchors can be used to discover topics that are more specific and relevant. What is more, we demonstrate the potential for this framework to perform weakly supervised learning in settings where labeling documents is prohibitively expensive BIBREF5 , BIBREF6 .", "We propose instead a lightweight information theoretic framework for codifying informal human knowledge and then use it to extract interpretable latent topics from text corpora. For example, to discover patients with diabetes in a set of clinical notes, a doctor can begin by specifying disease-specific anchor terms BIBREF1 , BIBREF2 , such as \u201cdiabetes\u201d or \u201cinsulin.\u201d Our framework then uses these to help discover both latent topics associated with diabetes and records in which diabetes-related topics occur. The user can then add (or remove) additional anchor terms (e.g., \u201cmetformin\u201d) to improve the quality of the learned (diabetes) topics.", "A clinician can look at a patient's electronic health record (EHR) and not only decide whether the patient has diabetes but also produce a succinct summary of the clinical evidence. Replicating this feat with computational tools has been the focus of much research in clinical informatics. There are major initiatives underway to codify clinical knowledge into formal representations, most often as deterministic rules that can be applied in a semi-automated fashion BIBREF0 . However, representing the intuitive judgments of human experts can be challenging, particularly when the formal system does not match the expert's knowledge. For example, many deterministic disease classifiers used in clinical informatics rely heavily upon administrative codes not available at time of diagnosis. Further, developing and testing such systems is time- and labor-intensive."]}
{"question_id": "1efaf3bcd66d1b6bdfb124f0cec0cfeee27e6124", "predicted_answer": "No, the paper does not explicitly compare their approach against popular topic models such as Latent Dirichlet Allocation (LDA). However, it does mention SeededLDA, which is a variant of LDA that allows for the specification of word-topic relationships, and notes that it assumes a more complex latent structure than Anchored CorEx.", "predicted_evidence": ["In this workshop paper, we introduce a simple approach to anchored information theoretic topic modeling using a novel combination of Correlation Explanation (CorEx) BIBREF3 and the information bottleneck BIBREF4 . This flexible framework enables the user to leverage domain knowledge to guide exploration of a collection of documents and to impose semantics onto latent factors learned by CorEx. We present preliminary experimental results on two text corpora (including a corpus of clinical notes), showing that anchors can be used to discover topics that are more specific and relevant. What is more, we demonstrate the potential for this framework to perform weakly supervised learning in settings where labeling documents is prohibitively expensive BIBREF5 , BIBREF6 .", "We propose instead a lightweight information theoretic framework for codifying informal human knowledge and then use it to extract interpretable latent topics from text corpora. For example, to discover patients with diabetes in a set of clinical notes, a doctor can begin by specifying disease-specific anchor terms BIBREF1 , BIBREF2 , such as \u201cdiabetes\u201d or \u201cinsulin.\u201d Our framework then uses these to help discover both latent topics associated with diabetes and records in which diabetes-related topics occur. The user can then add (or remove) additional anchor terms (e.g., \u201cmetformin\u201d) to improve the quality of the learned (diabetes) topics.", "There is a large body of work on integrating domain knowledge into topic models and other unsupervised latent variable models, often in the form of constraints BIBREF13 , prior distributions BIBREF14 , and token labels BIBREF15 . Like Anchored CorEx, seeded latent dirichlet allocation (SeededLDA) allows the specification of word-topic relationships BIBREF16 . However, SeededLDA assumes a more complex latent structure, in which each topic is a mixture of two distributions, one unseeded and one seeded.", "A clinician can look at a patient's electronic health record (EHR) and not only decide whether the patient has diabetes but also produce a succinct summary of the clinical evidence. Replicating this feat with computational tools has been the focus of much research in clinical informatics. There are major initiatives underway to codify clinical knowledge into formal representations, most often as deterministic rules that can be applied in a semi-automated fashion BIBREF0 . However, representing the intuitive judgments of human experts can be challenging, particularly when the formal system does not match the expert's knowledge. For example, many deterministic disease classifiers used in clinical informatics rely heavily upon administrative codes not available at time of diagnosis. Further, developing and testing such systems is time- and labor-intensive.", "BIBREF1 first proposed anchors in the context of topic modeling: words that are high precision indicators of underlying topics. In contrast to our approach, anchors are typically selected automatically, constrained to appear in only one topic, and used primarily to aid optimization BIBREF17 . In our information theoretic framework, anchors are specified manually and more loosely defined as words having high mutual information with one or more latent factors. The effects of anchors on the interpretability of traditional topic models are often mixed BIBREF18 , but our experiments suggest that our approach yields more coherent topics."]}
{"question_id": "fcdbaa08cccda9968f3fd433c99338cc60f596a7", "predicted_answer": "The F-score is obtained by comparing the corrected label sequence and the predicted label sequence.", "predicted_evidence": ["where INLINEFORM0 is a factor to adjust the weight of label accuracy and F-Score.", "The label sequence with the highest score can be obtained by carrying out viterbi algorithm. The regularized objective function is as follows: DISPLAYFORM0 INLINEFORM0", "We construct a semi-supervised model which is based on B-LSTM neural network and combine transition probability to form structured output. We propose a method to train directly on F-Score in our model. In addition, we propose an integrated method to train on both F-Score and label accuracy.", "Because F-Score depends on the whole label sequence, we use beam search to find INLINEFORM0 label sequences with top sentece-level score INLINEFORM1 and then use trigger function to rerank the INLINEFORM2 label sequences and select the best.", "where INLINEFORM0 is the F-Score between corrected label sequence and predicted label sequence."]}
{"question_id": "2e4688205c8e344cded7a053b6014cce04ef1bd5", "predicted_answer": "The state-of-the-art is the use of beam search to find the top-scoring label sequences and then reranking them using a trigger function to select the best, specifically in the context of training a B-LSTM based MMNN (Multi-Modal Neural Network) for sequence labeling tasks.", "predicted_evidence": ["The label sequence with the highest score can be obtained by carrying out viterbi algorithm. The regularized objective function is as follows: DISPLAYFORM0 INLINEFORM0", "where INLINEFORM0 is the length of setence INLINEFORM1 , INLINEFORM2 is a discount parameter, INLINEFORM3 a given correct label sequence and INLINEFORM4 a predicted label sequence. For a given training instance INLINEFORM5 , our predicted label sequence is the label sequence with highest score: INLINEFORM6", "where INLINEFORM0 is the F-Score between corrected label sequence and predicted label sequence.", "Because F-Score depends on the whole label sequence, we use beam search to find INLINEFORM0 label sequences with top sentece-level score INLINEFORM1 and then use trigger function to rerank the INLINEFORM2 label sequences and select the best.", "In the subgradient, we can know that structured margin loss INLINEFORM0 contributes nothing to the subgradient of the regularized objective function INLINEFORM1 . The margin loss INLINEFORM2 serves as a trigger function to conduct the training process of B-LSTM based MMNN. We can introduce a new trigger function to guide the training process of neural network."]}
{"question_id": "fc436a4f3674e42fb280378314bfe77ba0c99f2e", "predicted_answer": "The Chinese social media platform is Sina Weibo.", "predicted_evidence": ["Word segmentation takes an important part in Chinese text processing. Both Peng and Dredze peng-dredze:2015:EMNLP and Peng and Dredze peng-dredze:2016:P16-2 show the value of word segmentation to Chinese NER in social media. We present two methods to use word segmentation information in neural network model.", "To better understand the impact of the factor INLINEFORM0 , we show the results of our integrated model with different values of INLINEFORM1 in Figure UID13 . From Figure UID13 , we can know that INLINEFORM2 is an important factor for us to balance F-score and accuracy. Our integrated model may help alleviate the influence of noise in NER in Chinese social media.", "We use a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media. Details of the data are listed in Table TABREF19 . We also use the same unlabelled text as Peng and Dredze peng-dredze:2016:P16-2 from Sina Weibo service in China and the text is word segmented by a Chinese word segmentation system Jieba as Peng and Dredze peng-dredze:2016:P16-2 so that our results are more comparable to theirs.", "We evaluate two methods to incorporate word segmentation information. The results of two methods are shown as Table TABREF22 . We can see that positional character embeddings perform better in neural network. This is probably because positional character embeddings method can learn word segmentation information from unlabeled text while word segmentation can only use training corpus.", "To address the problem, one approach is to use the lexical embeddings learnt from massive unlabeled text. To take better advantage of unlabeled text, Peng and Dredze peng-dredze:2015:EMNLP evaluates three types of embeddings for Chinese text, and shows the effectiveness of positional character embeddings with experiments. Considering the value of word segmentation in Chinese NER, another approach is to construct an integrated model to jointly train learned representations for both predicting word segmentations and NER BIBREF14 ."]}
{"question_id": "a71fb012631e6a8854d5945b6d0ab2ab8e7b7ee6", "predicted_answer": "The dataset used is not explicitly mentioned in the given context.", "predicted_evidence": ["where INLINEFORM0 is a factor to adjust the weight of label accuracy and F-Score.", "We evaluate two methods to incorporate word segmentation information. The results of two methods are shown as Table TABREF22 . We can see that positional character embeddings perform better in neural network. This is probably because positional character embeddings method can learn word segmentation information from unlabeled text while word segmentation can only use training corpus.", "Max Margin training method use structured margin loss INLINEFORM0 to describe the difference between the corrected label sequence INLINEFORM1 and predicted label sequence INLINEFORM2 . In fact, the structured margin loss INLINEFORM3 reflect the loss in label accuracy. Considering the gap between label accuracy and F-Score in NER, we introduce a new training method to train directly on F-Score. To introduce F-Score driven training method, we need to take a look at the subgradient of equation ( EQREF9 ): INLINEFORM4", "where INLINEFORM0 is the F-Score between corrected label sequence and predicted label sequence.", "The label sequence with the highest score can be obtained by carrying out viterbi algorithm. The regularized objective function is as follows: DISPLAYFORM0 INLINEFORM0"]}
{"question_id": "b70e4c49300dc3eab18e907ab903afd2a0c6075a", "predicted_answer": "The five downstream tasks are:\n\n1. NLI (Natural Language Inference) - a classification task\n2. Document classification - a classification task\n3. Intent classification - a classification task\n4. POS tagging - a sequence tagging task\n5. NER (Named Entity Recognition) - a sequence tagging task", "predicted_evidence": ["We use representations from a Massively Multilingual Translation Encoder (MMTE) that can handle 103 languages to achieve cross-lingual transfer on 5 classification and sequence tagging tasks spanning more than 50 languages.", "As stated earlier, we use MMTE to perform downstream cross-lingual transfer on 5 NLP tasks. These include 3 classification tasks: NLI (XNLI dataset), document classification (MLDoc dataset) and intent classification, and 2 sequence tagging tasks: POS tagging and NER. We detail all of the experiments in this section.", "We train a massively multilingual NMT system using parallel data from 103 languages and exploit representations extracted from the encoder for cross-lingual transfer on various classification and sequence tagging tasks spanning over 50 languages. We find that the positive language transfer visible in improved translation quality for low resource languages is also reflected in the cross-lingual transferability of the extracted representations. The gains observed on various tasks over mBERT suggest that the translation objective is competitive with specialized approaches to learn cross-lingual embeddings.", "We use universal dependencies POS tagging data from the Universal Dependency v2.3 BIBREF6, BIBREF20. Gold segmentation is used for training, tuning and testing. The POS tagging task has 17 labels for all languages. We consider 48 different languages. These languages are chosen based on intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model. The task-specific network consists of a one layer feed-forward neural network with 784 units. Since MMTE operates on the subword-level, we only consider the representation of the first subword token of each word. The optimizer used is Adafactor with learning rate schedule (0.1,40k). The evaluation metric used is F1-score, which is same as accuracy in our case since we use gold-segmented data. Results of both in-language and zero-shot setting are reported in Table TABREF27.", "Fine-tuning involves taking the encoder of our mNMT model, named Massively Multilingual Translation Encoder (MMTE), and adapting it to the downstream task. For tasks which involve single input, the text is directly fed into the encoder. For tasks such as entailment which involve input pairs, we concatenate the two inputs using a separator token and pass this through the encoder. For each downstream task, the inputs and outputs are passed through the encoder and we fine-tune all the parameters end-to-end. The encoder encodes the input through the stack of Transformer layers and produces representations for each token at the output. For sequence tagging tasks, these token level representations are individually fed into a task-specific output layer. For classification or entailment tasks, we apply max-pooling on the token level representations and feed this into the task-specific output layer."]}
{"question_id": "088d42ecb1e15515f6a97a0da2fed81b61d61a23", "predicted_answer": "This strategy is more effective for low-resource languages. The authors state that the naive strategy would \"starve low-resource language pairs\", implying that the proposed strategy helps to mitigate this issue. By setting $T=5$, the authors achieve a balanced sampling strategy that oversamples low-resource language pairs, which helps to improve their representation in the model. This is particularly important for low-resource languages, which may not have as much data available for training.", "predicted_evidence": ["However, this strategy would starve low resource language pairs. To control for the ratio of samples from different language pairs, we sample a fixed number of sentences from the training data, with the probability of a sentence belonging to language pair $l$ being proportional to $p_l^{\\frac{1}{T}}$, where $T$ is the sampling temperature. As a result, $T=1$ would correspond to a true data distribution, and, $T=100$ yields an (almost) equal number of samples for each language pair (close to a uniform distribution with over-sampling for low-resource language-pairs). We set $T=5$ for a balanced sampling strategy. To control the contribution of each language pair when constructing the vocabulary, we use the same temperature based sampling strategy with $T=5$. Our SPM vocabulary has a character coverage of $0.999995$.", "Then if we adopt a naive strategy and sample from the union of the datasets, the probability of the sample being from language pair $l$ will be $p_l=\\frac{D_l}{\\Sigma _lD_l}$. However, this strategy would starve low resource language pairs. To control for the ratio of samples from different language pairs, we sample a fixed number of sentences from the training data, with the probability of a sentence belonging to language pair $l$ being proportional to $p_l^{\\frac{1}{T}}$, where $T$ is the sampling temperature. As a result, $T=1$ would correspond to a true data distribution, and, $T=100$ yields an (almost) equal number of samples for each language pair (close to a uniform distribution with over-sampling for low-resource language-pairs). We set $T=5$ for a balanced sampling strategy. To control the contribution of each language pair when constructing the vocabulary, we use the same temperature based sampling strategy with $T=5$.", "Then if we adopt a naive strategy and sample from the union of the datasets, the probability of the sample being from language pair $l$ will be $p_l=\\frac{D_l}{\\Sigma _lD_l}$. However, this strategy would starve low resource language pairs. To control for the ratio of samples from different language pairs, we sample a fixed number of sentences from the training data, with the probability of a sentence belonging to language pair $l$ being proportional to $p_l^{\\frac{1}{T}}$, where $T$ is the sampling temperature. As a result, $T=1$ would correspond to a true data distribution, and, $T=100$ yields an (almost) equal number of samples for each language pair (close to a uniform distribution with over-sampling for low-resource language-pairs). We set $T=5$ for a balanced sampling strategy.", "We find that there is a trade off between the number of languages in the multilingual model and efficiency of the learned representations due to the limited capacity. Scaling up the model to include more languages without diminishing transfer learning capability is a direction for future work. Finally, one could also consider integrating mBERT's objective with the translation objective to pre-train the mNMT system.", "Recent progress in NMT has enabled one to train multilingual systems that support translation from multiple source languages into multiple target languages within a single model BIBREF2, BIBREF3, BIBREF0. Such multilingual NMT (mNMT) systems often demonstrate large improvements in translation quality on low resource languages. This positive transfer originates from the model's ability to learn representations which are transferable across languages. Previous work has shown that these representations can then be used for cross-lingual transfer in other downstream NLP tasks - albeit on only a pair of language pairs BIBREF4, or by limiting the decoder to use a pooled vector representation of the entire sentence from the encoder BIBREF5."]}
{"question_id": "8599d6d14ac157169920c73b98a79737c7a68cf5", "predicted_answer": "No, mBERT is not fine-tuned for each language. According to the context, mBERT is compared to MMTE in different cross-lingual transfer scenarios, including fine-tuning, which implies that mBERT is not fine-tuned for each language by default. Instead, it is used as a pre-trained model that can be fine-tuned for specific languages or tasks.", "predicted_evidence": ["In this section, we consider some additional settings for comparing mBERT and MMTE. We also investigate the impact of the number of languages and the target language token on MMTE performance.", "We train a massively multilingual NMT system which is capable of translating between a large number of language pairs at the same time by optimizing the translation objective between language pairs. To train such a multilingual system within a single model, we use the strategy proposed in BIBREF3 which suggests prepending a target language token to every source sequence to be translated. This simple and effective strategy enables us to share the encoder, decoder, and attention mechanisms across all language pairs.", "We compare MMTE to mBERT in different cross-lingual transfer scenarios including zero-shot, few-shot, fine-tuning, and feature extraction scenarios.", "In this section, we describe our massively multilingual NMT system. Similar to BERT, our transfer learning setup has two distinct steps: pre-training and fine-tuning. During pre-training, the NMT model is trained on large amounts of parallel data to perform translation. During fine-tuning, we initialize our downstream model with the pre-trained parameters from the encoder of the NMT system, and then all of the parameters are fine-tuned using labeled data from the downstream tasks.", "We point out some of the major difference between mBERT and MMTE are:"]}
{"question_id": "f1d61b44105e651925d02a51e6d7ea10ea28ebd8", "predicted_answer": "The selection of the 50 languages for testing is not explicitly stated in the provided text. However, it can be inferred that the languages were likely chosen from the 103 languages used to train the massively multilingual NMT system, which were in turn selected from the 102 languages available in the in-house corpus of parallel documents crawled from the web. The specific criteria for selecting the 50 languages are not provided, but it is possible that they were chosen based on factors such as language family, script, or resource availability.", "predicted_evidence": ["We use representations from a Massively Multilingual Translation Encoder (MMTE) that can handle 103 languages to achieve cross-lingual transfer on 5 classification and sequence tagging tasks spanning more than 50 languages.", "We train a massively multilingual NMT system using parallel data from 103 languages and exploit representations extracted from the encoder for cross-lingual transfer on various classification and sequence tagging tasks spanning over 50 languages. We find that the positive language transfer visible in improved translation quality for low resource languages is also reflected in the cross-lingual transferability of the extracted representations. The gains observed on various tasks over mBERT suggest that the translation objective is competitive with specialized approaches to learn cross-lingual embeddings.", "In this section, we describe our massively multilingual NMT system. Similar to BERT, our transfer learning setup has two distinct steps: pre-training and fine-tuning. During pre-training, the NMT model is trained on large amounts of parallel data to perform translation. During fine-tuning, we initialize our downstream model with the pre-trained parameters from the encoder of the NMT system, and then all of the parameters are fine-tuned using labeled data from the downstream tasks.", "In this section we provide the list of languages codes used throughout this paper and the statistics of the datasets used for the downstream tasks.", "We train our multilingual NMT system on a massive scale, using an in-house corpus generated by crawling and extracting parallel sentences from the web BIBREF14. This corpus contains parallel documents for 102 languages, to and from English, comprising a total of 25 billion sentence pairs. The number of parallel sentences per language in our corpus ranges from around 35 thousand to almost 2 billion. Figure FIGREF10 illustrates the data distribution for all 204 language pairs used to train the NMT model. Language ids for all the languages are also provided in supplementary material."]}
{"question_id": "108f99fcaf620fab53077812e8901870896acf36", "predicted_answer": "According to the text, the evaluations used to evaluate dialogue are:\n\n1. Automatic evaluation\n2. Human evaluation\n\nBoth of these evaluations use human-annotated dialogues.", "predicted_evidence": ["We show the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses.", "Evaluating open-domain chit-chat models is challenging, especially in multiple languages and at the dialogue-level. Hence, we evaluate our models using both automatic and human evaluation. In both cases, human-annotated dialogues are used, which show the importance of the provided dataset.", "Figure FIGREF9 shows the conceptual differences between the encoder-decoder and casual decoder. Note that in both multilingual models, the dialogue history encoding process is language-agnostic, while decoding language is controlled by the language embedding. Such design allows the model to understand mixed-language dialogue contexts and to responds in the desired language (details in Section SECREF44).", "Let us define a dialogue $\\mathcal {D}=\\lbrace U_1,S_1,U_2,S_2, \\dots , U_n, S_n\\rbrace $ as an alternating set of utterances from two speakers, where $U$ and $S$ represent the user and the system, respectively. Each speaker has its corresponding persona description that consists of a set of sentences $\\mathcal {P}=\\lbrace P_1,\\dots ,P_m\\rbrace $. Given the system persona sentences $\\mathcal {P}_s$ and dialogue history $\\mathcal {D}_t=\\lbrace U_1,S_1,U_2, \\dots ,S_{t-1}, U_t\\rbrace $, we are interested in predicting the system utterances $S_t$.", "As an alternative to encoder-decoders, the causal-decoders BIBREF62, BIBREF63, BIBREF64 have been used to model conversational responses BIBREF26, BIBREF27 by giving as a prefix the dialogue history. In our model, we concatenate the persona $\\mathcal {P}_s$ and the dialogue history $\\mathcal {D}_t$ as the language model prefix, and autoregressively decode the system response $S_t$ based on language embedding (i.e. $l_{id}$):"]}
{"question_id": "6c8dc31a199b155e73c84173816c1e252137a0af", "predicted_answer": "The question is not explicitly answered in the given context. However, it is mentioned that the current state-of-the-art cross-lingual generation approach XNLG shows inferior performance on multi-turn dialogue tasks and generates repetitive responses, implying that it lags behind other models in terms of performance. However, the exact amount by which it lags behind is not specified.", "predicted_evidence": ["We consider two training strategies to learn a multilingual conversational model: multilingual training and cross-lingual training.", "For each language, we evaluate responses generated by the models using perplexity (ppl.) and BLEU BIBREF67 with reference to the human-annotated responses. Although these automatic measures are not perfect BIBREF68, they help to roughly estimate the performance of different models under the same test set. More recently, BIBREF69 has shown the correlation between perplexity and human judgment in open-domain chit-chat models.", "We provide both cross-lingual and multilingual baselines and discuss their limitations to inspire future research.", "Figure FIGREF55 and FIGREF56 illustrates the details of the multilingual causal decoder and the multilingual encoder-decoder models.", "The current state-of-the-art cross-lingual generation approach XNLG BIBREF4 shows inferior performance on multi-turn dialogue tasks, and generates repetitive responses. Although cross-lingual dialogue generation is challenging, it reduces the human effort for data annotation in different languages. Therefore, the cross-language transfer is an important direction to investigate."]}
{"question_id": "7125db8334a7efaf9f7753f2c2f0048a56e74c49", "predicted_answer": "They compare their baselines against translation pipeline models, specifically those using automatic translation systems such as Google Translate.", "predicted_evidence": ["Furthermore, we propose competitive baselines in two training settings, namely, cross-lingual and multilingual, and compare them with translation pipeline models. Our baselines leverage pre-trained cross-lingual BIBREF4 and multilingual BIBREF5 models.", "We consider two training strategies to learn a multilingual conversational model: multilingual training and cross-lingual training.", "We show the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses.", "In this section, we show the instructions for French annotation:", "There are two existing columns of conversations: the first column (en) is the original conversations in English, the second column (fr) is the conversations translated by an automatic system (e.g., Google Translate)."]}
{"question_id": "43729be0effb5defc62bae930ceacf7219934f1e", "predicted_answer": "The newly created dataset contains 7 languages: English (source language) and 6 other target languages, which are not specified by name in the given context.", "predicted_evidence": ["transfers knowledge from the source language data to the target languages. In this setting, the model is trained on English (source language) conversational samples, and evaluated on the other 6 languages. Following the methodology proposed by BIBREF4, we align the embedded representations of different languages into the same embedding space by applying cross-lingual pre-training to the encoder-decoder model. The pre-training procedure consists of two stages:", "We explore both encoder-decoder and causal decoder architectures, and we leverage existing pre-trained contextualized multilingual language models as weights initialization. Hence, we firstly define the multilingual embedding layer and then the two multilingual models used in our experiments.", "We report our iterative revised training set statistics in Table TABREF53.", "We consider two training strategies to learn a multilingual conversational model: multilingual training and cross-lingual training.", "We report more the mixed-language samples generated by M-CausalBert in Table TABREF61 and TABREF62."]}
{"question_id": "ae2142ee9e093ce485025168f4bcb3da4602739d", "predicted_answer": "Yes, they did collect their own contrastive test set, as mentioned in the sentence \"To evaluate pronoun translation, we perform contrastive evaluation and report the accuracy of models on our contrastive test set.\" and also in \"Section SECREF3 describes our test suite.\"", "predicted_evidence": ["Section SECREF14 describes how we extract our data set. Section SECREF26 explains how, given a set of contrastive examples, contrastive evaluation works.", "If the model score of the reference is indeed higher, we refer to this outcome as a \u201ccorrect decision\u201d by the model. The model's decision is only correct if the reference translation has a higher score than any contrastive translation. In our evaluation, we aggregate model decisions on the whole test set and report the overall percentage of correct decisions as accuracy.", "To evaluate pronoun translation, we perform contrastive evaluation and report the accuracy of models on our contrastive test set.", "Contrastive evaluation requires a large set of suitable examples that involve the translation of pronouns. As additional goals, our test set is designed to 1) focus on hard cases, so that it can be used as a benchmark to track progress in context-aware translation and 2) allow for fine-grained analysis.", "Section SECREF2 explains how our paper relates to existing work on context-aware models and the evaluation of pronoun translation. Section SECREF3 describes our test suite. The context-aware models we use in our experiments are detailed in Section SECREF4 . We discuss our experiments in Section SECREF5 and the results in Section SECREF6 ."]}
{"question_id": "ebe1084a06abdabefffc66f029eeb0b69f114fd9", "predicted_answer": "According to the text, the baselines are:\n\n1. A standard context-agnostic Transformer (baseline A)\n2. A concatenative baseline, similar to BIBREF5", "predicted_evidence": ["We consider the following recurrent baselines:", "Our experiments confirm the importance of careful architecture design, with multi-encoder architectures outperforming a model that simply concatenates context sentences. We also demonstrate the effectiveness of parameter sharing between encoders of a context-aware model.", "Our experiments are based on models from BIBREF9 , who have released their source code. We extend their models with parameter sharing, which was shown to be beneficial by BIBREF8 . Additionally, we consider a concatenative baseline, similar to BIBREF5 , and Transformer-based models BIBREF8 .", "baseline A standard context-agnostic Transformer. All model parameters are identical to a Transformer-base in BIBREF2 .", "For our Transformer-based experiments, we use a custom implementation and follow the hyperparameters from BIBREF2 , BIBREF8 . Systems are trained on lowercased text that was encoded using BPE (32k merge operations). Models consist of 6 encoder and decoder layers with 8 attention heads. The hidden state size is 512, the size of feedforward layers is 2048."]}
{"question_id": "cfdd583d01abaca923f5c466bb20e1d4b8c749ff", "predicted_answer": "The context-aware models experimented with are categorized into two types:\n\n1. Models based on RNNs (Recurrent Neural Networks)\n2. Models based on the Transformer architecture\n\nThese models consider additional context on:\n\n* Source side\n* Target side\n* Both source and target sides", "predicted_evidence": ["Section SECREF14 describes how we extract our data set. Section SECREF26 explains how, given a set of contrastive examples, contrastive evaluation works.", "We hope the test set will prove useful for empirically validating novel architectures for context-aware NMT. So far, we have only evaluated models that consider one sentence of context, but the nominal antecedent is more distant for a sizable proportion of the test set, and the evaluation of variable-size context models BIBREF7 , BIBREF10 is interesting future work.", "This section describes several context-aware NMT models that we use in our experiments. They fall into two major categories: models based on RNNs and models based on the Transformer architecture BIBREF2 . We experiment with additional context on the source side and target side.", "Apart from differences in the architectures, related work varies in whether it considers source context, target context, or both (see Table TABREF9 for an overview of language arcs and context types). Some work considers only source context, but for pronoun translation, target-side context is intuitively important for disambiguation, especially if the antecedent itself is ambiguous. In our evaluation, we therefore emphasize models that take into account both source and target context.", "If the translation of a pronoun requires context beyond the current sentence (see the example in Figure FIGREF1 ), a natural extension of sentence-level NMT models is to condition the model prediction on this necessary context. In the following, we describe a number of existing approaches to making models \u201caware\u201d of additional context."]}
{"question_id": "554d798e4ce58fd30820200c474d7e796dc8ba89", "predicted_answer": "The languages used in the experiment are not explicitly mentioned in the given context. However, based on the mention of \"source language\" and \"target language\", it can be inferred that the experiment involves multiple languages, but the specific languages are not specified.", "predicted_evidence": ["We exploit the fact that NMT systems are in fact language models of the target language, conditioned on source text. Like language models, NMT systems can be used to compute a model score (the negative log probability) for an existing translation. Contrastive evaluation, then, means to compare the model score of two pairs of inputs: INLINEFORM0 and INLINEFORM1 . If the model score of the actual reference translation is higher, we assume that this model can detect wrong pronoun translations.", "For each variant, we also introduce and test weight tying: we share the parameters of embedding matrices between encoders that read the same kind of text (source or target side).", "For our Transformer-based experiments, we use a custom implementation and follow the hyperparameters from BIBREF2 , BIBREF8 . Systems are trained on lowercased text that was encoded using BPE (32k merge operations). Models consist of 6 encoder and decoder layers with 8 attention heads. The hidden state size is 512, the size of feedforward layers is 2048.", "concat21 Trained on data where the preceding sentence is concatenated to the current one only on the source side. This model is also taken from BIBREF5 .", "Section SECREF2 explains how our paper relates to existing work on context-aware models and the evaluation of pronoun translation. Section SECREF3 describes our test suite. The context-aware models we use in our experiments are detailed in Section SECREF4 . We discuss our experiments in Section SECREF5 and the results in Section SECREF6 ."]}
{"question_id": "91e361e85c6d3884694f3c747d61bfcef171bab0", "predicted_answer": "According to the text, they obtain the entity linking results by using \"our entity linking algorithm\", which returns a score indicating its confidence on the linking result. This score is denoted as a one-dimensional vector $\\mathbf{g}$.", "predicted_evidence": ["We propose a deep neural model to improve fine-grained entity typing with entity linking. The problem of overfitting the weakly labeled training data is addressed by using a variant of the hinge loss and introducing noise during training. We conduct experiments on two commonly used dataset. The experimental results demonstrates the effectiveness of our approach.", "Apart from the three representations, we also obtain the score returned by our entity linking algorithm, which indicates its confidence on the linking result. We denote it as a one dimensional vector $\\mathbf {g}$. Then, we get $\\mathbf {f}=\\mathbf {f}_c\\oplus \\mathbf {f}_s\\oplus \\mathbf {f}_e\\oplus \\mathbf {g}$, where $\\oplus $ means concatenation. $\\mathbf {f}$ is then fed into an MLP that contains three dense layers to obtain $\\mathbf {u}_m$, out final representation for the current mention sample $m$. Let $t_1,t_2,...,t_k$ be all the types in $T$, where $k=|T|$. We embed them into the same space as $\\mathbf {u}_m$ by assigning each of them a dense vector BIBREF15.", "Apart from the three representations, we also obtain the score returned by our entity linking algorithm, which indicates its confidence on the linking result. We denote it as a one dimensional vector $\\mathbf {g}$. Then, we get $\\mathbf {f}=\\mathbf {f}_c\\oplus \\mathbf {f}_s\\oplus \\mathbf {f}_e\\oplus \\mathbf {g}$, where $\\oplus $ means concatenation. $\\mathbf {f}$ is then fed into an MLP that contains three dense layers to obtain $\\mathbf {u}_m$, out final representation for the current mention sample $m$. Let $t_1,t_2,...,t_k$ be all the types in $T$, where $k=|T|$. We embed them into the same space as $\\mathbf {u}_m$ by assigning each of them a dense vector BIBREF15. These vectors are denoted as $\\mathbf {t}_1,...,\\mathbf {t}_k$.", "Our contributions are summarized as follows:", "We propose a deep neural fine-grained entity typing model that utilizes type information from KB obtained through entity linking."]}
{"question_id": "6295951fda0cfa2eb4259d544b00bc7dade7c01e", "predicted_answer": "The model architecture used is a deep neural fine-grained entity typing model.", "predicted_evidence": ["This paper was supported by the Early Career Scheme (ECS, No. 26206717) from Research Grants Council in Hong Kong and WeChat-HKUST WHAT Lab on Artificial Intelligence Technology.", "We propose a deep neural fine-grained entity typing model that utilizes type information from KB obtained through entity linking.", "To make it more flexible, we also propose to use a variant of the hinge loss used by BIBREF16 to train our model:", "Our code is available at https://github.com/HKUST-KnowComp/IFETEL.", "We use strict accuracy, Macro F1, and Micro F1 to evaluate fine-grained typing performance BIBREF0."]}
{"question_id": "3f717e6eceab0a066af65ddf782c1ebc502c28c0", "predicted_answer": "The authors conduct experiments on two commonly used datasets, but the specific names of the datasets are not mentioned in the given context.", "predicted_evidence": ["The experimental results are listed in Table TABREF16. As we can see, our approach performs much better than existing approaches on both datasets.", "Our contributions are summarized as follows:", "We propose a deep neural model to improve fine-grained entity typing with entity linking. The problem of overfitting the weakly labeled training data is addressed by using a variant of the hinge loss and introducing noise during training. We conduct experiments on two commonly used dataset. The experimental results demonstrates the effectiveness of our approach.", "Let $T$ be a predefined tag set, which includes all the types we want to assign to mentions. Given a mention $m$ and its context, the task is to predict a set of types $\\mathbf {\\tau }\\subset T$ suitable for this mention. Thus, this is a multi-class, multi-label classification problem BIBREF0. Next, we will introduce our approach for this problem in detail, including the neural model, the training of the model, and the entity linking algorithm we use.", "Apart from the three representations, we also obtain the score returned by our entity linking algorithm, which indicates its confidence on the linking result. We denote it as a one dimensional vector $\\mathbf {g}$. Then, we get $\\mathbf {f}=\\mathbf {f}_c\\oplus \\mathbf {f}_s\\oplus \\mathbf {f}_e\\oplus \\mathbf {g}$, where $\\oplus $ means concatenation. $\\mathbf {f}$ is then fed into an MLP that contains three dense layers to obtain $\\mathbf {u}_m$, out final representation for the current mention sample $m$. Let $t_1,t_2,...,t_k$ be all the types in $T$, where $k=|T|$. We embed them into the same space as $\\mathbf {u}_m$ by assigning each of them a dense vector BIBREF15. These vectors are denoted as $\\mathbf {t}_1,...,\\mathbf {t}_k$."]}
{"question_id": "f5603271a04452cbdbb07697859bef2a2030d75c", "predicted_answer": "The answer is not explicitly stated in the given context. However, it can be inferred that at least one domain expert was involved in the creation of the dataset, as the text mentions \"domain experts\" in the context of extracting elementary common-knowledge concepts using the SE handbook.", "predicted_evidence": ["Various locations of the handbook and the glossary provide definitions of several SE concepts. We collect these and compile a comprehensive definitions document which is also used for the concept recognition task. An example definition and its description is shown below:", "SE commonsense comes from years of experience and learning which involves background knowledge that goes beyond any handbook. Although constructing an assistant like SEVA system is the overarching objective, a key problem to first address is to extract elementary common-knowledge concepts using the SE handbook and domain experts. We use the term `common-knowledge' as the `commonsense' knowledge of a specific domain. This knowledge can be seen as a pivot that can be used later to collect `commonsense' knowledge for the SE domain. We propose a preliminary research study that can pave a path towards a comprehensive commonsense knowledge acquisition for an effective Artificial Intelligence (AI) application for the SE domain. Overall structure of this work is summarized in Figure 1. Implementation with demo and dataset is available at: https://github.com/jitinkrishnan/NASA-SE .", "At NASA, although there exists knowledge engines and ontologies for the SE domain such as MBSE BIBREF2, IMCE BIBREF3, and OpenCaesar BIBREF4, generic commonsense acquisition is rarely discussed; we aim to address this challenge. SE commonsense comes from years of experience and learning which involves background knowledge that goes beyond any handbook. Although constructing an assistant like SEVA system is the overarching objective, a key problem to first address is to extract elementary common-knowledge concepts using the SE handbook and domain experts. We use the term `common-knowledge' as the `commonsense' knowledge of a specific domain. This knowledge can be seen as a pivot that can be used later to collect `commonsense' knowledge for the SE domain. We propose a preliminary research study that can pave a path towards a comprehensive commonsense knowledge acquisition for an effective Artificial Intelligence (AI) application for the SE domain.", "At NASA, although there exists knowledge engines and ontologies for the SE domain such as MBSE BIBREF2, IMCE BIBREF3, and OpenCaesar BIBREF4, generic commonsense acquisition is rarely discussed; we aim to address this challenge. SE commonsense comes from years of experience and learning which involves background knowledge that goes beyond any handbook. Although constructing an assistant like SEVA system is the overarching objective, a key problem to first address is to extract elementary common-knowledge concepts using the SE handbook and domain experts. We use the term `common-knowledge' as the `commonsense' knowledge of a specific domain. This knowledge can be seen as a pivot that can be used later to collect `commonsense' knowledge for the SE domain. We propose a preliminary research study that can pave a path towards a comprehensive commonsense knowledge acquisition for an effective Artificial Intelligence (AI) application for the SE domain. Overall structure of this work is summarized in Figure 1.", "grp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager."]}
{"question_id": "6575ffec1844e6fde5a668bce2afb16b67b65c1f", "predicted_answer": "The metrics used for evaluation are not explicitly mentioned in the given context. The text primarily focuses on the concept recognition task, dataset construction, and future directions for the Systems Engineer's Virtual Assistant (SEVA) system. It does not provide information on the evaluation metrics used to assess the performance of the concept extractor or other tasks.", "predicted_evidence": ["Definition: Acceptable Risk", "Various locations of the handbook and the glossary provide definitions of several SE concepts. We collect these and compile a comprehensive definitions document which is also used for the concept recognition task. An example definition and its description is shown below:", "An iterative regex matching procedure using this pattern over the preceding words will produce the full phrase of the abbreviation. `A process to determine a system\u2019s technological maturity based on Technology Readiness Levels (TRLs)' produces the abbreviation TRL which stands for Technology Readiness Levels. `Define one or more initial Concept of Operations (ConOps) scenarios' produces the abbreviation ConOps which stands for Concept of Operations. We pre-label these abbreviations as concept entities. Many of these abbreviations are also provided in the Appendix section of the handbook which is also extracted and used as concepts.", "mea: represents measures, features, or behaviors such as cost, risk, or feasibility.", "We presented a common-knowledge concept extractor for the Systems Engineer's Virtual Assistant (SEVA) system and showed how it can be beneficial for downstream tasks such as relation extraction and knowledge graph construction. We construct a word-level annotated dataset with the help of a domain expert by carefully defining a labelling scheme to train a sequence labelling task to recognize SE concepts. Further, we also construct some essential datasets from the SE domain which can be used for future research. Future directions include constructing a comprehensive common-knowledge relation extractor from SE handbook and incorporating such human knowledge into a more comprehensive machine-processable commonsense knowledge base for the SE domain."]}
{"question_id": "77c3416578b52994227bae7f2529600f02183e12", "predicted_answer": "The performance of the fine-tuned model on this dataset is not explicitly mentioned in the given context. The text primarily focuses on introducing the concept of common-knowledge concept extraction, describing the dataset creation process, and outlining the objectives and challenges of the project. It does not provide specific metrics or results regarding the performance of a fine-tuned model on the dataset.", "predicted_evidence": ["Definition: Acceptable Risk", "We presented a common-knowledge concept extractor for the Systems Engineer's Virtual Assistant (SEVA) system and showed how it can be beneficial for downstream tasks such as relation extraction and knowledge graph construction. We construct a word-level annotated dataset with the help of a domain expert by carefully defining a labelling scheme to train a sequence labelling task to recognize SE concepts. Further, we also construct some essential datasets from the SE domain which can be used for future research. Future directions include constructing a comprehensive common-knowledge relation extractor from SE handbook and incorporating such human knowledge into a more comprehensive machine-processable commonsense knowledge base for the SE domain.", "Various locations of the handbook and the glossary provide definitions of several SE concepts. We collect these and compile a comprehensive definitions document which is also used for the concept recognition task. An example definition and its description is shown below:", "In this work, we address a system element by constructing a common-knowledge concept recognition system for improving the performance of SEVA, using the static knowledge collected from the Systems Engineering Handbook BIBREF1 that is widely used in projects across the organization as domain-specific commonsense knowledge. At NASA, although there exists knowledge engines and ontologies for the SE domain such as MBSE BIBREF2, IMCE BIBREF3, and OpenCaesar BIBREF4, generic commonsense acquisition is rarely discussed; we aim to address this challenge. SE commonsense comes from years of experience and learning which involves background knowledge that goes beyond any handbook. Although constructing an assistant like SEVA system is the overarching objective, a key problem to first address is to extract elementary common-knowledge concepts using the SE handbook and domain experts. We use the term `common-knowledge' as the `commonsense' knowledge of a specific domain.", "In this work, we address a system element by constructing a common-knowledge concept recognition system for improving the performance of SEVA, using the static knowledge collected from the Systems Engineering Handbook BIBREF1 that is widely used in projects across the organization as domain-specific commonsense knowledge. At NASA, although there exists knowledge engines and ontologies for the SE domain such as MBSE BIBREF2, IMCE BIBREF3, and OpenCaesar BIBREF4, generic commonsense acquisition is rarely discussed; we aim to address this challenge. SE commonsense comes from years of experience and learning which involves background knowledge that goes beyond any handbook. Although constructing an assistant like SEVA system is the overarching objective, a key problem to first address is to extract elementary common-knowledge concepts using the SE handbook and domain experts. We use the term `common-knowledge' as the `commonsense' knowledge of a specific domain. This knowledge can be seen as a pivot that can be used later to collect `commonsense' knowledge for the SE domain."]}
{"question_id": "2abcff4fdedf9b17f76875cc338ba4ab8d1eccd3", "predicted_answer": "According to the text, the answer is: Yes, the constructed datasets are open-sourced. The implementation, demo, and dataset are available at https://github.com/jitinkrishnan/NASA-SE.", "predicted_evidence": ["We presented a common-knowledge concept extractor for the Systems Engineer's Virtual Assistant (SEVA) system and showed how it can be beneficial for downstream tasks such as relation extraction and knowledge graph construction. We construct a word-level annotated dataset with the help of a domain expert by carefully defining a labelling scheme to train a sequence labelling task to recognize SE concepts. Further, we also construct some essential datasets from the SE domain which can be used for future research. Future directions include constructing a comprehensive common-knowledge relation extractor from SE handbook and incorporating such human knowledge into a more comprehensive machine-processable commonsense knowledge base for the SE domain.", "Various locations of the handbook and the glossary provide definitions of several SE concepts. We collect these and compile a comprehensive definitions document which is also used for the concept recognition task. An example definition and its description is shown below:", "SE commonsense comes from years of experience and learning which involves background knowledge that goes beyond any handbook. Although constructing an assistant like SEVA system is the overarching objective, a key problem to first address is to extract elementary common-knowledge concepts using the SE handbook and domain experts. We use the term `common-knowledge' as the `commonsense' knowledge of a specific domain. This knowledge can be seen as a pivot that can be used later to collect `commonsense' knowledge for the SE domain. We propose a preliminary research study that can pave a path towards a comprehensive commonsense knowledge acquisition for an effective Artificial Intelligence (AI) application for the SE domain. Overall structure of this work is summarized in Figure 1. Implementation with demo and dataset is available at: https://github.com/jitinkrishnan/NASA-SE .", "Definition: Acceptable Risk", "Relation extraction tasks extract semantic relationships from text. These extractors aim to connect named entities such as `Shakespeare' and `England' using relations such as `born-in'. Relations can be as simple as using hand-built patterns or as challenging as using unsupervised methods like Open IE BIBREF10; with bootstrapping, supervised, and semi-supervised methods in between. BIBREF11 and BIBREF12 are some of the high performing models that extract relations from New York Times Corpus BIBREF13 and TACRED challenges BIBREF14 respectively. Hyponyms represent hierarchical connection between entities of a domain and represent important relationships. For instance, a well-known work by BIBREF15 uses syntactic patterns such as [Y such as A, B, C], [Y including X], or [Y, including X] to extract hyponyms."]}
{"question_id": "6df57a21ca875e63fb39adece6a9ace5bb2b2cfa", "predicted_answer": "Based on the context, the labeling scheme appears to involve assigning a label to each word in the dataset, where the label indicates whether the word is a concept entity or not. Specifically, the abbreviations (e.g. TRL, ConOps) are pre-labeled as concept entities. This suggests that the labeling scheme is a binary classification scheme, where each word is labeled as either a concept entity (1) or not a concept entity (0).", "predicted_evidence": ["In the hand-labelled dataset, each word gets a label. The idea is to perform multi-class classification using BERT's pre-trained cased language model. We use pytorch transformers and hugging face as per the tutorial by BIBREF17 which uses $BertForTokenClassification$. The text is embedded as tokens and masks with a maximum token length. This embedded tokens are provided as the input to the pre-trained BERT model for a full fine-tuning. The model gives an F1-score of $0.89$ for the concept recognition task. An 80-20 data split is used for training and evaluation. Detailed performance of the CR is shown in Table 2 and 3. Additionally, we also implemented CR using spaCy BIBREF18 which also produced similar results.", "abb: represents abbreviations such as TRL representing Technology Readiness Level.", "Various locations of the handbook and the glossary provide definitions of several SE concepts. We collect these and compile a comprehensive definitions document which is also used for the concept recognition task. An example definition and its description is shown below:", "An iterative regex matching procedure using this pattern over the preceding words will produce the full phrase of the abbreviation. `A process to determine a system\u2019s technological maturity based on Technology Readiness Levels (TRLs)' produces the abbreviation TRL which stands for Technology Readiness Levels. `Define one or more initial Concept of Operations (ConOps) scenarios' produces the abbreviation ConOps which stands for Concept of Operations. We pre-label these abbreviations as concept entities. Many of these abbreviations are also provided in the Appendix section of the handbook which is also extracted and used as concepts.", "with at least one verb, can extract relation-like phrases from the phrase that links two concepts. An example is shown in Figure FIGREF27. Further investigation of relation extraction from SE handbook is left as future work."]}
{"question_id": "b39b278aa1cf2f87ad4159725dff77b387f2df84", "predicted_answer": "The pretrained language model used is BERT's pre-trained cased language model.", "predicted_evidence": ["with at least one verb, can extract relation-like phrases from the phrase that links two concepts. An example is shown in Figure FIGREF27. Further investigation of relation extraction from SE handbook is left as future work.", "In the hand-labelled dataset, each word gets a label. The idea is to perform multi-class classification using BERT's pre-trained cased language model. We use pytorch transformers and hugging face as per the tutorial by BIBREF17 which uses $BertForTokenClassification$. The text is embedded as tokens and masks with a maximum token length. This embedded tokens are provided as the input to the pre-trained BERT model for a full fine-tuning. The model gives an F1-score of $0.89$ for the concept recognition task. An 80-20 data split is used for training and evaluation. Detailed performance of the CR is shown in Table 2 and 3. Additionally, we also implemented CR using spaCy BIBREF18 which also produced similar results.", "Any language model can be used for the purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings.", "abb: represents abbreviations such as TRL representing Technology Readiness Level.", "An iterative regex matching procedure using this pattern over the preceding words will produce the full phrase of the abbreviation. `A process to determine a system\u2019s technological maturity based on Technology Readiness Levels (TRLs)' produces the abbreviation TRL which stands for Technology Readiness Levels. `Define one or more initial Concept of Operations (ConOps) scenarios' produces the abbreviation ConOps which stands for Concept of Operations. We pre-label these abbreviations as concept entities. Many of these abbreviations are also provided in the Appendix section of the handbook which is also extracted and used as concepts."]}
{"question_id": "814e945668e2b6f31b088918758b120fb00ada7d", "predicted_answer": "The size of the constructed dataset is not explicitly mentioned in the given context. However, it is mentioned that the dataset is available at https://github.com/jitinkrishnan/NASA-SE, so you can check the repository for more information about the dataset.", "predicted_evidence": ["Definition: Acceptable Risk", "SE commonsense comes from years of experience and learning which involves background knowledge that goes beyond any handbook. Although constructing an assistant like SEVA system is the overarching objective, a key problem to first address is to extract elementary common-knowledge concepts using the SE handbook and domain experts. We use the term `common-knowledge' as the `commonsense' knowledge of a specific domain. This knowledge can be seen as a pivot that can be used later to collect `commonsense' knowledge for the SE domain. We propose a preliminary research study that can pave a path towards a comprehensive commonsense knowledge acquisition for an effective Artificial Intelligence (AI) application for the SE domain. Overall structure of this work is summarized in Figure 1. Implementation with demo and dataset is available at: https://github.com/jitinkrishnan/NASA-SE .", "We presented a common-knowledge concept extractor for the Systems Engineer's Virtual Assistant (SEVA) system and showed how it can be beneficial for downstream tasks such as relation extraction and knowledge graph construction. We construct a word-level annotated dataset with the help of a domain expert by carefully defining a labelling scheme to train a sequence labelling task to recognize SE concepts. Further, we also construct some essential datasets from the SE domain which can be used for future research. Future directions include constructing a comprehensive common-knowledge relation extractor from SE handbook and incorporating such human knowledge into a more comprehensive machine-processable commonsense knowledge base for the SE domain.", "Various locations of the handbook and the glossary provide definitions of several SE concepts. We collect these and compile a comprehensive definitions document which is also used for the concept recognition task. An example definition and its description is shown below:", "abb: represents abbreviations such as TRL representing Technology Readiness Level."]}
{"question_id": "d4456e9029fcdcb6e0149dd8f57b77d16ead1bc4", "predicted_answer": "The metric considered is the classification performance, which is evaluated using various methods and techniques, including SMOTE and 10-cross validation.", "predicted_evidence": ["Table. TABREF19 and TABREF20 show the classification performance of different methods.", "Using softmax technique, the prediction can be formalized as:", "The results were examined from the following aspects:", "To deal with the problem of classification on unbalanced data, synthetic Minority Over-sampling TEchnique (SMOTE) BIBREF28 were performed on the original dataset. 10-cross validation scheme was adopted and the results were averaged from 10 iterations.", "When the feature dimension is set to 100 and the training corpus is ACL, the results generated by different models were compared (AVGWVEC,"]}
{"question_id": "d0b967bfca2039c7fb05b931c8b9955f99a468dc", "predicted_answer": "The hand-crafted features used are:\n\n1. Position of sentence\n2. Sentence length\n3. Tense\n4. Content features\n5. Qualifying adjectives\n6. Meta-discourse features", "predicted_evidence": ["The results were examined from the following aspects:", "Using softmax technique, the prediction can be formalized as:", "Table. TABREF19 and TABREF20 show the classification performance of different methods.", "Those problems have been addressed from many angles, one of which is feature extraction and representation. At the early stage of document summarization, features are usually engineered manually. Although the hand-crafted features have shown the ability for document summarization and sentiment analysis BIBREF13 , BIBREF9 , there are not enough efficient features to capture the semantic relations between words, phrases and sentences. Moreover, building a sufficient pool of features manually is difficult, because it requires expert knowledge and it is time-consuming. Teufel et. al. BIBREF2 have built feature pool of sixteen types of features to classify sentences, such as the position of sentence, sentence length and tense. Widyantoro et. al. used content features, qualifying adjectives and meta-discourse features BIBREF14 to explore AZ task. It took efforts to engineer these features and it is also time consuming to optimize the combination of the entire features. With the advent of neural networks BIBREF15 , it is possible for computers to learn feature representations automatically.", "Although the hand-crafted features have shown the ability for document summarization and sentiment analysis BIBREF13 , BIBREF9 , there are not enough efficient features to capture the semantic relations between words, phrases and sentences. Moreover, building a sufficient pool of features manually is difficult, because it requires expert knowledge and it is time-consuming. Teufel et. al. BIBREF2 have built feature pool of sixteen types of features to classify sentences, such as the position of sentence, sentence length and tense. Widyantoro et. al. used content features, qualifying adjectives and meta-discourse features BIBREF14 to explore AZ task. It took efforts to engineer these features and it is also time consuming to optimize the combination of the entire features. With the advent of neural networks BIBREF15 , it is possible for computers to learn feature representations automatically. Recently, word embedding technique BIBREF16 has been widely used in the NLP community."]}
{"question_id": "31e6062ba45d8956791e1b86bad7efcb6d1b191a", "predicted_answer": "The word embeddings used are based on the classical word2vec algorithm, specifically the Skip-gram model and continuous bag of words (CBOW) model.", "predicted_evidence": ["The characteristics of word embeddings based on different model and dataset are listed in Table. TABREF12 .", "where INLINEFORM0 is the word embedding for word INLINEFORM1 , which is learned by the classical word2vec algorithm BIBREF16 .", "With word2vec tool, word embeddings can be learnt from big amount of text corpus and the semantic relationships between words can be measured by the cosine distances between the vectors. The idea behind word embeddings is to use distributed representation BIBREF19 to map each word into k-dimension vector. How these vectors are generated using word2vec tool? The common method to derive the vectors is using neural probabilistic language model BIBREF20 . The underlying word representations for each word are obtained while training the language model. Similar to the mechanism in language model, Mikolov et al. BIBREF16 introduced two architectures: Skip-gram model and continuous bag of words (CBOW) model. Each of the model has two different training strategies, such as hierarchical softmax and negative sampling. Both these two models have three layers: input, projection and output layer. The word vectors are obtained once the models are optimized. Usually, this optimizing process is done using stochastic gradient descent method.", "The idea behind word embeddings is to use distributed representation BIBREF19 to map each word into k-dimension vector. How these vectors are generated using word2vec tool? The common method to derive the vectors is using neural probabilistic language model BIBREF20 . The underlying word representations for each word are obtained while training the language model. Similar to the mechanism in language model, Mikolov et al. BIBREF16 introduced two architectures: Skip-gram model and continuous bag of words (CBOW) model. Each of the model has two different training strategies, such as hierarchical softmax and negative sampling. Both these two models have three layers: input, projection and output layer. The word vectors are obtained once the models are optimized. Usually, this optimizing process is done using stochastic gradient descent method. It doesn't need labels when training the models, which makes word2vec algorithm more valuable compared with traditional supervised machine learning methods that require a big amount of annotated data.", "The first model, averaging word vectors ( INLINEFORM0 ), is to average the vectors in word sequence INLINEFORM1 . The main process in this model is to learn the word embedding matrix INLINEFORM2 :"]}
{"question_id": "38b29b0dcb87868680f9934af71ef245ebb122e4", "predicted_answer": "No, they do not annotate their own dataset. They use an existing ACL dataset (introduced in the dataset section) and perform techniques such as SMOTE to deal with unbalanced data, but they do not annotate their own dataset.", "predicted_evidence": ["The results were examined from the following aspects:", "The third model is constructed for the purpose of improving classification results for a certain category. In this study specifically, the optimization task was focused on identifying the category INLINEFORM0 . In this study, INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 ) inspired by Tang et al. BIBREF21 's model: Sentiment-Specific Word Embedding (unified model: INLINEFORM3 ). After obtaining the word vectors via INLINEFORM4 , the same scheme was used to average the vectors in one sentence as in the model INLINEFORM5 .", "To compare the three models effectiveness on the AZ task, the three models on a same ACL dataset (introduced int he dataset section) were trained. The word2vec were also trained using different parameters, such as different dimension of features. To evaluate the impact from different domains, the first model was trained on different corpus.", "The characteristics of word embeddings based on different model and dataset are listed in Table. TABREF12 .", "To deal with the problem of classification on unbalanced data, synthetic Minority Over-sampling TEchnique (SMOTE) BIBREF28 were performed on the original dataset. 10-cross validation scheme was adopted and the results were averaged from 10 iterations."]}
{"question_id": "6e134d51a795c385d72f38f36bca4259522bcf51", "predicted_answer": "The sentence embeddings are not explicitly generated in the given context. The context only discusses how word embeddings are generated using the word2vec tool, specifically through the Skip-gram model and continuous bag of words (CBOW) model, which are trained using a neural probabilistic language model and optimized using stochastic gradient descent. The resulting word vectors are obtained once the models are optimized.\n\nHowever, it is possible to generate sentence embeddings by combining the word embeddings of the words in a sentence. One common method is to take the average or weighted average of the word vectors in a sentence to obtain a sentence vector. Another approach is to use a more complex composition function, such as a recurrent neural network (RNN) or a convolutional neural network (CNN), to combine the word vectors into a sentence vector.\n\nIt's worth noting that the paper mentioned in the context compares different word embedding models on the task of argumentative zoning, which involves classifying sentences from scientific papers. Therefore, it's likely that the authors used sentence embeddings, which were generated by combining the word embeddings, as input features for the classification task. However, the exact method for generating sentence embeddings is not specified in the given context.", "predicted_evidence": ["With word2vec tool, word embeddings can be learnt from big amount of text corpus and the semantic relationships between words can be measured by the cosine distances between the vectors. The idea behind word embeddings is to use distributed representation BIBREF19 to map each word into k-dimension vector. How these vectors are generated using word2vec tool? The common method to derive the vectors is using neural probabilistic language model BIBREF20 . The underlying word representations for each word are obtained while training the language model. Similar to the mechanism in language model, Mikolov et al. BIBREF16 introduced two architectures: Skip-gram model and continuous bag of words (CBOW) model. Each of the model has two different training strategies, such as hierarchical softmax and negative sampling. Both these two models have three layers: input, projection and output layer. The word vectors are obtained once the models are optimized. Usually, this optimizing process is done using stochastic gradient descent method.", "The idea behind word embeddings is to use distributed representation BIBREF19 to map each word into k-dimension vector. How these vectors are generated using word2vec tool? The common method to derive the vectors is using neural probabilistic language model BIBREF20 . The underlying word representations for each word are obtained while training the language model. Similar to the mechanism in language model, Mikolov et al. BIBREF16 introduced two architectures: Skip-gram model and continuous bag of words (CBOW) model. Each of the model has two different training strategies, such as hierarchical softmax and negative sampling. Both these two models have three layers: input, projection and output layer. The word vectors are obtained once the models are optimized. Usually, this optimizing process is done using stochastic gradient descent method. It doesn't need labels when training the models, which makes word2vec algorithm more valuable compared with traditional supervised machine learning methods that require a big amount of annotated data.", "BIBREF16 has gained a lot attention recently. With word2vec tool, word embeddings can be learnt from big amount of text corpus and the semantic relationships between words can be measured by the cosine distances between the vectors. The idea behind word embeddings is to use distributed representation BIBREF19 to map each word into k-dimension vector. How these vectors are generated using word2vec tool? The common method to derive the vectors is using neural probabilistic language model BIBREF20 . The underlying word representations for each word are obtained while training the language model. Similar to the mechanism in language model, Mikolov et al. BIBREF16 introduced two architectures: Skip-gram model and continuous bag of words (CBOW) model. Each of the model has two different training strategies, such as hierarchical softmax and negative sampling. Both these two models have three layers: input, projection and output layer. The word vectors are obtained once the models are optimized.", "In this paper, different word embedding models on the task of argumentative zoning were compared . The results showed that word embeddings are effective on sentence classification from scientific papers. Word embeddings trained on a relevant corpus can capture the semantic features of statements and they are easier to be obtained than hand engineered features.", "The characteristics of word embeddings based on different model and dataset are listed in Table. TABREF12 ."]}
{"question_id": "0778cbbd093f8b779f7cf26302b2a8e081ccfb40", "predicted_answer": "Argumentative zoning (AZ) is a task that involves classifying sentences into seven categories.", "predicted_evidence": ["where INLINEFORM0 is the word embedding for word INLINEFORM1 , which is learned by the classical word2vec algorithm BIBREF16 .", "In this paper, different word embedding models on the task of argumentative zoning were compared . The results showed that word embeddings are effective on sentence classification from scientific papers. Word embeddings trained on a relevant corpus can capture the semantic features of statements and they are easier to be obtained than hand engineered features.", "The first model, averaging word vectors ( INLINEFORM0 ), is to average the vectors in word sequence INLINEFORM1 . The main process in this model is to learn the word embedding matrix INLINEFORM2 :", "In this study, sentence embeddings were learned from large text corpus as features to classify sentences into seven categories in the task of AZ. Three models were explored to obtain the sentence vectors: averaging the vectors of the words in one sentence, paragraph vectors and specific word vectors.", "p INLINEFORM0 = INLINEFORM1 (2)"]}
{"question_id": "578add9d3dadf86cd0876d42b03bf0114f83d0e7", "predicted_answer": "They obtained the tweets by crawling two blackmarket sites (YouLikeHits and Like4Like) between February and April 2019, creating dummy accounts on these sites, and recording the Tweet IDs of the tweets posted for gaining retweets. They then used Twitter's REST API to collect the tweet objects of these tweets. Additionally, they collected the timelines of the authors of these tweets to find genuine tweets that were not posted to these blackmarket sites.", "predicted_evidence": ["We use the following features based on the tweet content:", "Table TABREF1 shows a sample tweet that was posted on a blackmarket service and another sample tweet that was not. In this paper, we make the first attempt to detect tweets that are posted on blackmarket services. Our aim is to build a system that can flag tweets soon after they are posted, which is why we do not consider temporal features such as the number of retweets or likes that a tweet keeps gaining over time. Instead, we only rely on the features and representations extracted from the content of the tweets.", "INLINEFORM0 : Is the tweet a reply to another tweet?", "We collected data from Credit-based Freemium services because their service model is easy to understand. We crawled two blackmarket sites \u2013 YouLikeHits and Like4Like, between the period of February and April 2019. We created dummy accounts (after careful IRB approval) on these sites to participate in the platform and recorded Tweet IDs of the tweets that were posted for gaining retweets. We used Twitter's REST API to collect the tweet objects of these tweets. The timelines of the authors of these tweets were also collected, allowing us to find genuine tweets by the same users that have not been posted to these blackmarket sites.", "Since there is no prior work on blackmarket tweet detection, we chose state-of-the-art Twitter spam detection methods as baselines, along with training some state-of-the-art classifiers on the features we generated for our dataset."]}
{"question_id": "4d5b74499804ea5bc5520beb88d0f9816f67205a", "predicted_answer": "According to the given context, they compare their approach to the baseline approach proposed by Rajdev et al. (BIBREF11), which uses a Random Forest classifier with a standard set of features to classify spam, fake, and legitimate tweets.", "predicted_evidence": ["Spam Detection 2: For baseline 2, we consider the approach proposed by Rajdev et. al. BIBREF11 . They proposed flat and hierarchical classifications approaches with few of the standard set of features which can classify spam, fake and legitimate tweets. We use their experimental setup with Random Forest classifier on our dataset.", "We consider the problem as a binary classification problem, where the tweets are classified into two classes - blackmarket and genuine. The performance of each competing method is measured using the following metrics: Precision, Recall, and F1-score. The primary output of the multitask learning model gives us the classification result, which is what we use to evaluate our model. All hyperparameters of the models are appropriately tuned. The average results are reported after 5-fold cross-validation.", "As shown in Figure FIGREF21 , the inputs are fed into separate fully connected (FC) layers with cross-stitch units stacked between successive layers. The cross-stitch units find the best shared representations using linear combinations, and learn the optimal linear combinations for a given set of tasks. The cross-stitch units essentially allow us to unify two separate networks for two separate tasks into a single network wherein each layer of the network shares the parameters from the other network using linear combinations. The network also employs batch-normalization and dropout to avoid overfitting.", "Multitask Learning: Multitask learning is used whenever we have two or more similar tasks to optimise together. Most of the related studies on multitask learning are based on how the tasks can be better learned together. Zhang et al. BIBREF14 classified multitask learning models into five types and reported the characteristics of each approach. Cross-Stitch units were introduced by Misra et al. BIBREF6 , which can learn an optimal combination of shared and task-specific representations. Gupta et al. BIBREF15 proposed GIRNet, a unified position-sensitive multitask recurrent neural network architecture.", "Several studies have focused on detecting malicious activities such as spam, fake content and blackmarket services. Here, we mention some of these studies which we deem as pertinent to our work. We also mention the prior usage of multitask learning in a similar context."]}
{"question_id": "baec99756b80eec7c0234a08bc2855e6770bcaeb", "predicted_answer": "The language explored in this paper is English. This can be inferred from the features mentioned, such as \"Number of adjective words\" and \"Number of noun words\", which are parts of speech specific to the English language.", "predicted_evidence": ["INLINEFORM0 : Is the tweet a reply to another tweet?", "INLINEFORM0 : Number of URLs in the tweet", "The model takes a different input feature vector for each of the tasks.", "INLINEFORM0 : Number of adjective words in the tweet", "INLINEFORM0 : Number of noun words in the tweet"]}
{"question_id": "46d051b8924ad0ef8cfba9c7b5b84707ee72f26a", "predicted_answer": "According to the text, the blackmarket services being referred to are:\n\n1. Social-share services (request customers to spread the content on social media)\n2. Credit-based services (customers earn credits by providing appraisals, and can then use the credits earned to gain appraisals for their content)\n3. Auto-time retweet services (customers need to provide their Twitter access tokens, upon which their content is retweeted 10-20 times for each 15-minute window)", "predicted_evidence": ["Since there is no prior work on blackmarket tweet detection, we chose state-of-the-art Twitter spam detection methods as baselines, along with training some state-of-the-art classifiers on the features we generated for our dataset.", "blackAs studied in BIBREF0 , there are two prevalent models of blackmarket services, namely premium and freemium. Premium services are only available upon payment from customers, whereas freemium services offer both paid and unpaid options. The unpaid services are available to the users when they contribute to the blackmarket by providing appraisals for other users' content. Here, we mainly concentrate on freemium services. The freemium services can be further divided into three categories: (i) social-share services (request customers to spread the content on social media), (ii) credit-based services (customers earn credits by providing appraisals, and can then use the credits earned to gain appraisals for their content), and (iii) auto-time retweet services (customers need to provide their Twitter access tokens, upon which their content is retweeted 10-20 times for each 15-minute window).", "Several studies have focused on detecting malicious activities such as spam, fake content and blackmarket services. Here, we mention some of these studies which we deem as pertinent to our work. We also mention the prior usage of multitask learning in a similar context.", "We consider the problem as a binary classification problem, where the tweets are classified into two classes - blackmarket and genuine. The performance of each competing method is measured using the following metrics: Precision, Recall, and F1-score. The primary output of the multitask learning model gives us the classification result, which is what we use to evaluate our model. All hyperparameters of the models are appropriately tuned. The average results are reported after 5-fold cross-validation.", "In this paper, we presented a novel multitask learning approach to solve the problem of identification of tweets that are submitted to blackmarket services, without the use of any temporal features. To sum up, our contributions are three-fold: (i) Characterization: We proposed 12 tweet content based features that are useful in the task of identifying blackmarket tweets, (ii) Classification: We developed a novel Multitask Learning based model to classify tweets as blackmarket tweets or genuine tweets, (iii) Dataset: We collected a dataset consisting of tweets that have been submitted to blackmarket services in order to gain inorganic appraisals."]}
{"question_id": "dae2f135e50d77867c3f57fc3cb0427b2443e126", "predicted_answer": "The languages used during pretraining are not explicitly stated in the given context. However, based on the information provided, it can be inferred that the pretraining involves multiple languages, including English, Chinese, and French, as these languages are mentioned in the context of the evaluation datasets.", "predicted_evidence": ["Xnlg is a pre-trained sequence-to-sequence model, which is based on Transformer BIBREF22. Both the encoder and the decoder are supposed to support multiple languages. Following BIBREF5, we use language tag embeddings to distinguish the source and target languages. Given a sentence and its corresponding language tag, Xnlg encodes the input into vector representations. By conditioning on the encoding vectors and a specific language tag, the decoder generates the output sequence in the target language. Figure FIGREF6 illustrates the pre-training objectives and the pre-training protocol designed for Xnlg.", "Pipeline (Xlm) The pipeline of translating input Chinese sentences into English first, then performing En-En-QG with the XLM model, and finally translating back to the Chinese. We use the Transformer as the translator, which is also trained on the MultiUN dataset.", "In this paper, we propose a cross-lingual pre-trained model (named as Xnlg) in order to transfer monolingual NLG supervision to other pre-trained languages by fine-tuning. Specifically, Xnlg shares the same sequence-to-sequence model across languages, and is pre-trained with both monolingual and cross-lingual objectives. The model not only learns to understand multilingual input, but also is able to generate specific languages by conditioning on the encoded semantics. Figure FIGREF2 demonstrates how to use Xnlg to perform cross-lingual transfer for downstream tasks. The proposed model enables us to fine-tune the pre-trained model on monolingual NLG training data, and then evaluate it beyond a single language, including zero-shot cross-lingual generation. Besides, we explore several fine-tuning strategies to make a compromise between cross-lingual ability and task ability.", "In this paper, we propose a cross-lingual pre-trained model (named as Xnlg) in order to transfer monolingual NLG supervision to other pre-trained languages by fine-tuning. Specifically, Xnlg shares the same sequence-to-sequence model across languages, and is pre-trained with both monolingual and cross-lingual objectives. The model not only learns to understand multilingual input, but also is able to generate specific languages by conditioning on the encoded semantics. Figure FIGREF2 demonstrates how to use Xnlg to perform cross-lingual transfer for downstream tasks. The proposed model enables us to fine-tune the pre-trained model on monolingual NLG training data, and then evaluate it beyond a single language, including zero-shot cross-lingual generation. Besides, we explore several fine-tuning strategies to make a compromise between cross-lingual ability and task ability. In addition, we introduce two cross-lingual NLG datasets (i.e., question generation, and abstractive summarization) for evaluation, which includes three languages, namely English, Chinese, and French.", "Various training objectives are designed to pretrain text encoders used for general-purpose language representations, such as language modeling BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15, auto-encoding BIBREF16, and machine translation BIBREF17. Apart from pre-training encoders, several pre-trained models BIBREF18, BIBREF19 are proposed for generation tasks. In comparison, our goal is to investigate a pre-training method for cross-lingual NLG tasks."]}
{"question_id": "38055717edf833566d912f14137b92a1d9c4f65a", "predicted_answer": "The architecture of the decoder is based on the Transformer model, specifically a sequence-to-sequence model. It takes the encoding vectors from the encoder and a specific language tag as input, and generates the output sequence in the target language.", "predicted_evidence": ["Although the pre-trained encoder in the first stage enables the model to encode multilingual sentences. However, it cannot directly be used in cross-lingual NLG because: 1) encoder-decoder attention is not pre-trained; 2) the decoding algorithm is different between masked language modeling and autoregressive decoding, resulting in the mismatch between pre-training and fine-tuning. Therefore, we conduct decoding pre-training in the second stage by using DAE and XAE as the tasks. Besides, we only update decoder parameters and keep the encoder fixed. The objective of the second stage is to minimize: 2 = (x,y) pXAE(x,y) + x mDAE(x)", "Cross-lingual pre-training aims at building universal cross-lingual encoders that can encode multilingual sentences to a shared embedding space. BIBREF20 artetxe2018massively use the sequence encoder of the multilingual translation model BIBREF3 to produce cross-lingual sentence embeddings. However, as shown in the experiments (Section SECREF4), it is difficult to control the target language by directly fine-tuning the pre-trained translation model on downstream NLG tasks. BIBREF4 xnli propose an alignment loss function to encourage parallel sentences to have similar representations. By pre-training BERT BIBREF13 on corpora of multiple languages, it shows a surprising ability to produce cross-lingual representations BIBREF21. More recently, BIBREF5 xlm extend mask language modeling pre-training to cross-lingual settings, which shows significant improvements on cross-lingual text classification and unsupervised machine translation.", "Cross-lingual pre-training aims at building universal cross-lingual encoders that can encode multilingual sentences to a shared embedding space. BIBREF20 artetxe2018massively use the sequence encoder of the multilingual translation model BIBREF3 to produce cross-lingual sentence embeddings. However, as shown in the experiments (Section SECREF4), it is difficult to control the target language by directly fine-tuning the pre-trained translation model on downstream NLG tasks. BIBREF4 xnli propose an alignment loss function to encourage parallel sentences to have similar representations. By pre-training BERT BIBREF13 on corpora of multiple languages, it shows a surprising ability to produce cross-lingual representations BIBREF21. More recently, BIBREF5 xlm extend mask language modeling pre-training to cross-lingual settings, which shows significant improvements on cross-lingual text classification and unsupervised machine translation. By comparison, we pretrain both encoder and decoder for cross-lingual generation tasks, rather than only focusing on encoder.", "Xnlg is a pre-trained sequence-to-sequence model, which is based on Transformer BIBREF22. Both the encoder and the decoder are supposed to support multiple languages. Following BIBREF5, we use language tag embeddings to distinguish the source and target languages. Given a sentence and its corresponding language tag, Xnlg encodes the input into vector representations. By conditioning on the encoding vectors and a specific language tag, the decoder generates the output sequence in the target language. Figure FIGREF6 illustrates the pre-training objectives and the pre-training protocol designed for Xnlg.", "We conduct experiments over two cross-lingual NLG downstream tasks, i.e., cross-lingual question generation, and cross-lingual abstractive summarization. We compare Xnlg with state-of-the-art cross-lingual pre-trained models, and machine-translation-based pipelines."]}
{"question_id": "b6aa5665c981e3b582db4760759217e2979d5626", "predicted_answer": "The architecture of the encoder is based on the Transformer model, specifically the Xnlg model, which is a pre-trained sequence-to-sequence model that supports multiple languages. The encoder takes a sentence and its corresponding language tag as input and encodes it into vector representations.", "predicted_evidence": ["Although the pre-trained encoder in the first stage enables the model to encode multilingual sentences. However, it cannot directly be used in cross-lingual NLG because: 1) encoder-decoder attention is not pre-trained; 2) the decoding algorithm is different between masked language modeling and autoregressive decoding, resulting in the mismatch between pre-training and fine-tuning. Therefore, we conduct decoding pre-training in the second stage by using DAE and XAE as the tasks. Besides, we only update decoder parameters and keep the encoder fixed. The objective of the second stage is to minimize: 2 = (x,y) pXAE(x,y) + x mDAE(x)", "Cross-lingual pre-training aims at building universal cross-lingual encoders that can encode multilingual sentences to a shared embedding space. BIBREF20 artetxe2018massively use the sequence encoder of the multilingual translation model BIBREF3 to produce cross-lingual sentence embeddings. However, as shown in the experiments (Section SECREF4), it is difficult to control the target language by directly fine-tuning the pre-trained translation model on downstream NLG tasks. BIBREF4 xnli propose an alignment loss function to encourage parallel sentences to have similar representations. By pre-training BERT BIBREF13 on corpora of multiple languages, it shows a surprising ability to produce cross-lingual representations BIBREF21. More recently, BIBREF5 xlm extend mask language modeling pre-training to cross-lingual settings, which shows significant improvements on cross-lingual text classification and unsupervised machine translation.", "Cross-lingual pre-training aims at building universal cross-lingual encoders that can encode multilingual sentences to a shared embedding space. BIBREF20 artetxe2018massively use the sequence encoder of the multilingual translation model BIBREF3 to produce cross-lingual sentence embeddings. However, as shown in the experiments (Section SECREF4), it is difficult to control the target language by directly fine-tuning the pre-trained translation model on downstream NLG tasks. BIBREF4 xnli propose an alignment loss function to encourage parallel sentences to have similar representations. By pre-training BERT BIBREF13 on corpora of multiple languages, it shows a surprising ability to produce cross-lingual representations BIBREF21. More recently, BIBREF5 xlm extend mask language modeling pre-training to cross-lingual settings, which shows significant improvements on cross-lingual text classification and unsupervised machine translation. By comparison, we pretrain both encoder and decoder for cross-lingual generation tasks, rather than only focusing on encoder.", "CorefNqg BIBREF33 A sequence-to-sequence model with attention mechanism and a feature-rich encoder.", "Xnlg is a pre-trained sequence-to-sequence model, which is based on Transformer BIBREF22. Both the encoder and the decoder are supposed to support multiple languages. Following BIBREF5, we use language tag embeddings to distinguish the source and target languages. Given a sentence and its corresponding language tag, Xnlg encodes the input into vector representations. By conditioning on the encoding vectors and a specific language tag, the decoder generates the output sequence in the target language. Figure FIGREF6 illustrates the pre-training objectives and the pre-training protocol designed for Xnlg."]}
{"question_id": "c0355afc7871bf2e12260592873ffdb5c0c4c919", "predicted_answer": "The baseline models are not explicitly stated in the given context. However, based on the text, it can be inferred that the baseline models are likely to be other sequence-to-sequence models, possibly without pre-training or with different pre-training methods, that are compared to the proposed Xnlg model. The text mentions that \"our model outperforms the baselines\", but it does not provide specific details about the baseline models.", "predicted_evidence": ["Xlm BIBREF5 The current state-of-the-art cross-lingual pre-training model. We initialize the Transformer-based sequence-to-sequence model with pre-trained XLM.", "We evaluate models with BLEU-4 (BL-4), ROUGE (RG) and METEOR (MTR) metrics. As shown in Table TABREF16, our model outperforms the baselines, which demonstrates that our pre-trained model provides a good initialization for NLG.", "CorefNqg BIBREF33 A sequence-to-sequence model with attention mechanism and a feature-rich encoder.", "We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder. For every Transformer layer, we use 1024 hidden units, 8 attention heads, and GELU activations BIBREF26. In the first pre-training stage, we directly use the 15-language pre-trained XLM BIBREF5 to initialize the parameters of our encoder and decoder. In the second stage, we use Wikipedia as the monolingual data for the DAE objective, and MultiUN BIBREF27 as the parallel data for the XAE objective. The DAE loss is trained with a weight of $0.5$. We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively. Following BIBREF5, we use the tokenizer provided by BIBREF28 for Chinese, and Moses for other languages, respectively.", "We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder. For every Transformer layer, we use 1024 hidden units, 8 attention heads, and GELU activations BIBREF26. In the first pre-training stage, we directly use the 15-language pre-trained XLM BIBREF5 to initialize the parameters of our encoder and decoder. In the second stage, we use Wikipedia as the monolingual data for the DAE objective, and MultiUN BIBREF27 as the parallel data for the XAE objective. The DAE loss is trained with a weight of $0.5$. We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively. Following BIBREF5, we use the tokenizer provided by BIBREF28 for Chinese, and Moses for other languages, respectively. Then the words in all languages are split with a shared subword vocabulary learned by BPE BIBREF29."]}
{"question_id": "afeceee343360d3fe715f405dac7760d9a6754a7", "predicted_answer": "According to the text, the human evaluation metrics used are:\n\n* Human judges' preference for stories generated by the hierarchical model compared to a non-hierarchical baseline (i.e., how often they prefer one over the other)\n* Correct pairing of stories with their corresponding prompts in a triple pairing task, where human evaluators are asked to select the correct pairing for all three prompts.", "predicted_evidence": ["We analyze the effect of our modeling improvements on the WritingPrompts dataset.", "Experiments show that our fusion and self-attention mechanisms improve over existing techniques on both automated and human evaluation measures. Our new dataset and neural architectures allow for models which can creatively generate longer, more consistent and more fluent passages of text. Human judges prefer our hierarchical model's stories twice as often as those of a non-hierarchical baseline.", "We propose a number of evaluation metrics to quantify the performance of our models. Many commonly used metrics, such as BLEU for machine translation or ROUGE for summarization, compute an n-gram overlap between the generated text and the human text\u2014however, in our open-ended generation setting, these are not useful. We do not aim to generate a specific story; we want to generate viable and novel stories. We focus on measuring both the fluency of our models and their ability to adhere to the prompt.", "For human evaluation, we use Amazon Mechanical Turk to conduct a triple pairing task. We use each model to generate stories based on held-out prompts from the test set. Then, groups of three stories are presented to the human judges. The stories and their corresponding prompts are shuffled, and human evaluators are asked to select the correct pairing for all three prompts. 105 stories per model are grouped into questions, and each question is evaluated by 15 judges.", "The output of a single attention head is given by DISPLAYFORM0"]}
{"question_id": "cc3dd701f3a674618de95a4196e9c7f4c8fbf1e5", "predicted_answer": "The automated evaluation metrics used are not explicitly mentioned in the given context. However, it is mentioned that human evaluation is conducted using a triple pairing task on Amazon Mechanical Turk, where human judges are asked to select the correct pairing of stories and prompts. This suggests that the automated evaluation metrics are not the primary focus of this particular experiment, and the authors are relying on human evaluation to assess the performance of their models.", "predicted_evidence": ["The cold fusion mechanism of BIBREF3 pretrains a language model and subsequently trains a seq2seq model with a gating mechanism that learns to leverage the final hidden layer of the language model during seq2seq training. We modify this approach by combining two seq2seq models as follows (see Figure FIGREF13 ): DISPLAYFORM0", "We implement models with the fairseq-py library in PyTorch. Similar to BIBREF6 , we train using the Nesterov accelerated gradient method BIBREF26 using gradient clipping BIBREF27 . We perform hyperparameter optimization on each of our models by cross-validating with random search on a validation set. We provide model architectures in the appendix.", "Figure FIGREF27 shows the values of the fusion gates for an example story, averaged at each timestep. The pretrained seq2seq model acts similarly to a language model producing common words and punctuation. The second seq2seq model learns to focus on rare words, such as horned and robe.", "For human evaluation, we use Amazon Mechanical Turk to conduct a triple pairing task. We use each model to generate stories based on held-out prompts from the test set. Then, groups of three stories are presented to the human judges. The stories and their corresponding prompts are shuffled, and human evaluators are asked to select the correct pairing for all three prompts. 105 stories per model are grouped into questions, and each question is evaluated by 15 judges.", "where the hidden state of the pretrained seq2seq model and training seq2seq model (represented by INLINEFORM0 ) are concatenated to learn gates INLINEFORM1 . The gates are computed using a linear projection with the weight matrix INLINEFORM2 . The gated hidden layers are combined by concatenation and followed by more fully connected layers with GLU activations (see Appendix). We use layer normalization BIBREF10 after each fully connected layer."]}
{"question_id": "d66550f65484696c1284903708b87809ea705786", "predicted_answer": "The baselines they compare against are two different Conv seq2seq models that were trained and ensembled together by averaging with equal weights.", "predicted_evidence": ["We evaluate a number of baselines:", "Two different Conv seq2seq models were trained and ensembled together by averaging with equal weights.", "We implement models with the fairseq-py library in PyTorch. Similar to BIBREF6 , we train using the Nesterov accelerated gradient method BIBREF26 using gradient clipping BIBREF27 . We perform hyperparameter optimization on each of our models by cross-validating with random search on a validation set. We provide model architectures in the appendix.", "We generate stories from our models using a top-k random sampling scheme. At each timestep, the model generates the probability of each word in the vocabulary being the likely next word. We randomly sample from the INLINEFORM0 most likely candidates from this distribution. Then, subsequent timesteps generate words based on the previously selected words. We find this sampling strategy substantially more effective than beam search, which tends to produce common phrases and repetitive text from the training set BIBREF28 , BIBREF29 . Sentences produced by beam search tend to be short and generic. Completely random sampling can introduce very unlikely words, which can damage generation as the model has not seen such mistakes at training time. The restriction of sampling from the 10 most likely candidates reduces the risk of these low-probability samples.", "where the hidden state of the pretrained seq2seq model and training seq2seq model (represented by INLINEFORM0 ) are concatenated to learn gates INLINEFORM1 . The gates are computed using a linear projection with the weight matrix INLINEFORM2 . The gated hidden layers are combined by concatenation and followed by more fully connected layers with GLU activations (see Appendix). We use layer normalization BIBREF10 after each fully connected layer."]}
{"question_id": "29ba93bcd99c2323d04d4692d3672967cca4915e", "predicted_answer": "No, the text does not mention using pre-trained embeddings like BERT. It discusses the integration of language models with seq2seq models, but it does not specifically mention using pre-trained language models like BERT. Instead, it talks about training language models and seq2seq models together, or using a pre-trained language model to initialize the encoder and decoder of a seq2seq model.", "predicted_evidence": ["Previous work has investigated the integration of language models with seq2seq models. The two models can be leveraged together without architectural modifications: BIBREF21 use language models to initialize the encoder and decoder side of the seq2seq model independently, and BIBREF22 combine the predictions of the language model and seq2seq model solely at inference time. Recent work has also proposed deeper integration. BIBREF23 combined a trained language model with a trained seq2seq model to learn a gating function that joins them. BIBREF3 propose training the seq2seq model given the fixed language model then learning a gate to filter the information from the language model.", "where the hidden state of the pretrained seq2seq model and training seq2seq model (represented by INLINEFORM0 ) are concatenated to learn gates INLINEFORM1 . The gates are computed using a linear projection with the weight matrix INLINEFORM2 . The gated hidden layers are combined by concatenation and followed by more fully connected layers with GLU activations (see Appendix). We use layer normalization BIBREF10 after each fully connected layer.", "We implement models with the fairseq-py library in PyTorch. Similar to BIBREF6 , we train using the Nesterov accelerated gradient method BIBREF26 using gradient clipping BIBREF27 . We perform hyperparameter optimization on each of our models by cross-validating with random search on a validation set. We provide model architectures in the appendix.", "The cold fusion mechanism of BIBREF3 pretrains a language model and subsequently trains a seq2seq model with a gating mechanism that learns to leverage the final hidden layer of the language model during seq2seq training. We modify this approach by combining two seq2seq models as follows (see Figure FIGREF13 ): DISPLAYFORM0", "We evaluate a number of baselines:"]}
{"question_id": "804bf5adc6dc5dd52f8079cf041ed3a710e03f8a", "predicted_answer": "The model used to generate the premise is a convolutional language model (as mentioned in the first sentence of the given context).", "predicted_evidence": ["High-level structure is integral to good stories, but language models generate on a strictly-word-by-word basis and so cannot explicitly make high-level plans. We introduce the ability to plan by decomposing the generation process into two levels. First, we generate the premise or prompt of the story using the convolutional language model from BIBREF4 . The prompt gives a sketch of the structure of the story. Second, we use a seq2seq model to generate a story that follows the premise. Conditioning on the prompt makes it easier for the story to remain consistent and also have structure at a level beyond single phrases.", "We tackle the challenges of story-telling with a hierarchical model, which first generates a sentence called the prompt describing the topic for the story, and then conditions on this prompt when generating the story. Conditioning on the prompt or premise makes it easier to generate consistent stories because they provide grounding for the overall plot. It also reduces the tendency of standard sequence models to drift off topic.", "To improve the relevance of the generated story to its prompt, we introduce a fusion mechanism BIBREF3 where our model is trained on top of an pre-trained seq2seq model. To improve over the pre-trained model, the second model must focus on the link between the prompt and the story. For the first time, we show that fusion mechanisms can help seq2seq models build dependencies between their input and output.", "We analyze the effect of our modeling improvements on the WritingPrompts dataset.", "We implement models with the fairseq-py library in PyTorch. Similar to BIBREF6 , we train using the Nesterov accelerated gradient method BIBREF26 using gradient clipping BIBREF27 . We perform hyperparameter optimization on each of our models by cross-validating with random search on a validation set. We provide model architectures in the appendix."]}
{"question_id": "f2dba5bf75967407cce5d0a9c2618269225081f5", "predicted_answer": "Yes, the stories in the dataset are fictional stories. They are written by users in response to prompts on the WritingPrompts forum on Reddit, and are intended to be creative and imaginative stories rather than factual or non-fictional accounts.", "predicted_evidence": ["We collect a hierarchical story generation dataset from Reddit's WritingPrompts forum. WritingPrompts is a community where online users inspire each other to write by submitting story premises, or prompts, and other users freely respond. Each prompt can have multiple story responses. The prompts have a large diversity of topic, length, and detail. The stories must be at least 30 words, avoid general profanity and inappropriate content, and should be inspired by the prompt (but do not necessarily have to fulfill every requirement). Figure FIGREF1 shows an example.", "Previous work on story generation has explored seq2seq RNN architectures BIBREF14 , but has focused largely on using various content to inspire the stories. For instance, BIBREF15 uses photos to inspire short paragraphs trained on romance novels, and BIBREF16 chain a series of independent descriptions together into a short story. BIBREF17 decompose story generation into two steps, first converting text into event representations, then modeling stories as sequences of events before translating back to natural language. Similarly, BIBREF18 generate summaries of movies as sequences of events using an RNN, then sample event representations using MCMC. They find this technique can generate text of the desired genre, but the movie plots are not interpretable (as the model outputs events, not raw text). However, we are not aware of previous work that has used hierarchical generation from a textual premise to improve the coherence and structure of stories.", "Story-telling is on the frontier of current text generation technology: stories must remain thematically consistent across the complete document, requiring modeling very long range dependencies; stories require creativity; and stories need a high level plot, necessitating planning ahead rather than word-by-word generation BIBREF0 .", "To train our models, we gathered a large dataset of 303,358 human generated stories paired with writing prompts from an online forum. Evaluating free form text is challenging, so we also introduce new evaluation metrics which isolate different aspects of story generation.", "For automatic evaluation, we measure model perplexity on the test set and prompt ranking accuracy. Perplexity is commonly used to evaluate the quality of language models, and it reflects how fluently the model can produce the correct next word given the preceding words. We use prompt ranking to assess how strongly a model's output depends on its input. Stories are decoded under 10 different prompts\u20149 randomly sampled prompts and 1 true corresponding prompt\u2014and the likelihood of the story given the various prompts is recorded. We measure the percentage of cases where the true prompt is the most likely to generate the story. In our evaluation, we examined 1000 stories from the test set for each model."]}
{"question_id": "b783ec5cb9ad595da7db2c0ddf871152ae382c5f", "predicted_answer": "The stories are collected from Reddit's WritingPrompts forum.", "predicted_evidence": ["For human evaluation, we use Amazon Mechanical Turk to conduct a triple pairing task. We use each model to generate stories based on held-out prompts from the test set. Then, groups of three stories are presented to the human judges. The stories and their corresponding prompts are shuffled, and human evaluators are asked to select the correct pairing for all three prompts. 105 stories per model are grouped into questions, and each question is evaluated by 15 judges.", "Previous work on story generation has explored seq2seq RNN architectures BIBREF14 , but has focused largely on using various content to inspire the stories. For instance, BIBREF15 uses photos to inspire short paragraphs trained on romance novels, and BIBREF16 chain a series of independent descriptions together into a short story. BIBREF17 decompose story generation into two steps, first converting text into event representations, then modeling stories as sequences of events before translating back to natural language. Similarly, BIBREF18 generate summaries of movies as sequences of events using an RNN, then sample event representations using MCMC. They find this technique can generate text of the desired genre, but the movie plots are not interpretable (as the model outputs events, not raw text). However, we are not aware of previous work that has used hierarchical generation from a textual premise to improve the coherence and structure of stories.", "We collect a hierarchical story generation dataset from Reddit's WritingPrompts forum. WritingPrompts is a community where online users inspire each other to write by submitting story premises, or prompts, and other users freely respond. Each prompt can have multiple story responses. The prompts have a large diversity of topic, length, and detail. The stories must be at least 30 words, avoid general profanity and inappropriate content, and should be inspired by the prompt (but do not necessarily have to fulfill every requirement). Figure FIGREF1 shows an example.", "Lastly, we conduct human evaluation to evaluate the importance of hierarchical generation for story writing. We use Amazon Mechanical Turk to compare the stories from hierarchical generation from a prompt with generation without a prompt. 400 pairs of stories were evaluated by 5 judges each in a blind test.", "For automatic evaluation, we measure model perplexity on the test set and prompt ranking accuracy. Perplexity is commonly used to evaluate the quality of language models, and it reflects how fluently the model can produce the correct next word given the preceding words. We use prompt ranking to assess how strongly a model's output depends on its input. Stories are decoded under 10 different prompts\u20149 randomly sampled prompts and 1 true corresponding prompt\u2014and the likelihood of the story given the various prompts is recorded. We measure the percentage of cases where the true prompt is the most likely to generate the story. In our evaluation, we examined 1000 stories from the test set for each model."]}
{"question_id": "3eb107f35f4f5f5f527a93ffb487aa2e3fe51efd", "predicted_answer": "The five pre-trained word embeddings experimented with are not explicitly mentioned in the given context. However, it is mentioned that the word embedding vector is concatenated from five pre-trained word embeddings, but the specific names of these embeddings are not provided.", "predicted_evidence": ["This section describes two experiments: i) compare our model against recent systems; ii) evaluate the efficiency of using multiple pre-trained word embeddings.", "Future work could apply our multiple word embeddings approach for transfer learning tasks. This strategy allows making use of pre-trained word embeddings as well as available resources.", "We study five pre-trained word embeddings for our model:", "Convolutional filters: we used 1600 filters. It is also the dimension of the word embedding concatenated from the five pre-trained word embeddings.", "where INLINEFORM0 is concatenation operator, INLINEFORM1 is the word embedding vector of INLINEFORM2 in the INLINEFORM3 th pre-trained embedding."]}
{"question_id": "47d54a6dd50cab8dab64bfa1f9a1947a8190080c", "predicted_answer": "The datasets used are not explicitly mentioned in the given context. However, it is mentioned that the evaluation is done on three tasks, and the datasets for these tasks will be described in Section 4.", "predicted_evidence": ["We evaluate our model on three tasks:", "This section describes two experiments: i) compare our model against recent systems; ii) evaluate the efficiency of using multiple pre-trained word embeddings.", "In this section, we describe the process for evaluating the similarity/relation between two sentences. We compare two sentences via three levels: word-word, sentence-sentence and word-sentence.", "Measuring the semantic similarity/relation of two pieces of short text plays a fundamental role in a variety of language processing tasks (i.e., plagiarism detection, question answering, and machine translation). Semantic textual similarity (STS) task is challenging because of the diversity of linguistic expression. For example, two sentences with different lexicons could have a similar meaning. Moreover, the task requires to measure similarity at several levels (e.g., word level, phrase level, sentence level). These challenges give difficulties to conventional approaches using hand-crafted features.", "The remainder of this paper is organized as follows: Section 2 reviews the previous research, Section 3 introduces the architecture of our model, Section 4 describes the three tasks and datasets, Section 5 describes the experiment setting, Section 6 reports and discusses the results of the experiments, and Section 7 concludes our work."]}
{"question_id": "67cb001f8ca122ea859724804b41529fea5faeef", "predicted_answer": "The state-of-the-art methods they compare with are not explicitly mentioned in the given context. The text only describes the experiments and the model architecture, but it does not mention specific state-of-the-art methods that are being compared with.", "predicted_evidence": ["This section describes two experiments: i) compare our model against recent systems; ii) evaluate the efficiency of using multiple pre-trained word embeddings.", "for INLINEFORM0 , and INLINEFORM1 is the similarity score.", "where INLINEFORM0 is the number of training pairs and INLINEFORM1 denotes the model parameters. The gradient descent optimization Adadelta is used to learn the model parameters. We also use mini-batch size as 30 and keep the pre-trained word embeddings fixed during the training phase. We evaluate our models through Pearson correlation INLINEFORM2 .", "To learn a multi-aspect word embedding INLINEFORM0 from the representation INLINEFORM1 , we design INLINEFORM2 convolutional filters. Each filter INLINEFORM3 is denoted as a weight vector with the same dimension as INLINEFORM4 and a bias value INLINEFORM5 . The INLINEFORM6 is obtained by applying these filters on the INLINEFORM7 as follows: DISPLAYFORM0", "where INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 are model parameters; INLINEFORM4 is a predicted target score/label."]}
{"question_id": "42eb7c5311fc1ac0344f0b38d3184ccd4faad3be", "predicted_answer": "The agreement measure used is cosine similarity. It is used to compute the timeline similarity between the author and target timelines, which is intended to capture common interests and shared topics of conversation between the two users.", "predicted_evidence": ["To construct realistic and competitive baseline models, we consider a set of standard text-based features that have been used widely throughout the literature. Specifically, we use the NLTK library BIBREF36 to construct unigrams, bigrams, and trigrams for each labeled message. This parallels the work of BIBREF8, BIBREF7, and BIBREF26. Following BIBREF30, we incorporate counts from the Linguistic Inquiry and Word Count (LIWC) dictionary to measure the linguistic and psychological processes that are represented in the text BIBREF37. We also use a modified version of the Flesch-Kincaid Grade Level and Flesch Reading Ease scores as computed in BIBREF35. Lastly, we encode the sentiment scores for each message using the Valence Aware Dictionary and sEntiment Reasoner (VADER) of BIBREF38.", "Here, we consider linguistic features, drawn from both the author and target timelines. These are intended to capture the social relationship between each user, their common interests, and the surprise of a given message relative to the author's timeline history.", "Timeline similarity is used to indicate common interests and shared topics of conversation between the author and target timelines. High similarity scores might reflect users' familiarity with one another, or suggest that they occupy similar social positions. This can be used to distinguish cyberbullying from harmless banter between friends and associates. To compute this metric, we represent the author and target timelines as TF-IDF vectors $\\vec{A}$ and $\\vec{T}$. We then take the cosine similarity between the vectors as", "Contextually-informed ground truth. We provide annotators with the social context surrounding each message, including the contents of the reply thread and the account information of each user involved.", "Downward overlap measures the number of two-hop paths from the author to the target along following relationships; upward overlap measures two-hop paths in the opposite direction. Inward overlap measures the similarity between the two users' follower sets, and outward overlap measures the similarity between their sets of friends. Bidirectional overlap then is a more generalized measure of social network similarity. We provide a graphical depiction for each of these features on the right side of Figure FIGREF18."]}
{"question_id": "8d14dd9c67d71494b4468000ff9683afdd11af7e", "predicted_answer": "No, the text does not mention reporting the annotation agreement.", "predicted_evidence": ["Contextually-informed ground truth. We provide annotators with the social context surrounding each message, including the contents of the reply thread and the account information of each user involved.", "To address the urgent need for reliable data, we provide an original annotation framework and an annotated Twitter dataset. The key advantages to our labeling approach are:", "To construct realistic and competitive baseline models, we consider a set of standard text-based features that have been used widely throughout the literature. Specifically, we use the NLTK library BIBREF36 to construct unigrams, bigrams, and trigrams for each labeled message. This parallels the work of BIBREF8, BIBREF7, and BIBREF26. Following BIBREF30, we incorporate counts from the Linguistic Inquiry and Word Count (LIWC) dictionary to measure the linguistic and psychological processes that are represented in the text BIBREF37. We also use a modified version of the Flesch-Kincaid Grade Level and Flesch Reading Ease scores as computed in BIBREF35. Lastly, we encode the sentiment scores for each message using the Valence Aware Dictionary and sEntiment Reasoner (VADER) of BIBREF38.", "Here, we consider linguistic features, drawn from both the author and target timelines. These are intended to capture the social relationship between each user, their common interests, and the surprise of a given message relative to the author's timeline history.", "Clear labeling criteria. We ask annotators to provide labels for five clear cyberbullying criteria. These criteria can be combined and adapted for revised definitions of cyberbullying."]}
{"question_id": "b857f3e3f1dad5df55f69d062978967fe023ac6f", "predicted_answer": "The question asks how many annotators participated, but the given context does not provide an answer to this question. The context appears to be discussing a research study on analyzing online messages, but it does not mention the number of annotators involved in the study.", "predicted_evidence": ["Multiset mention overlap: Let $\\hat{M}_a$ be the multiset of all accounts mentioned by author $a$ (with repeats for each mention), and let $\\hat{M}_t$ be the multiset of all accounts mentioned by target $t$. We measure $\\frac{|\\hat{M}_a \\cap ^{*} \\hat{M}_t|}{|\\hat{M}_a \\cup \\hat{M}_t|}$ where $\\cap ^{*}$ takes the multiplicity of each element to be the sum of the multiplicity from $\\hat{M}_a $ and the multiplicity from $\\hat{M}_b$", "To construct realistic and competitive baseline models, we consider a set of standard text-based features that have been used widely throughout the literature. Specifically, we use the NLTK library BIBREF36 to construct unigrams, bigrams, and trigrams for each labeled message. This parallels the work of BIBREF8, BIBREF7, and BIBREF26. Following BIBREF30, we incorporate counts from the Linguistic Inquiry and Word Count (LIWC) dictionary to measure the linguistic and psychological processes that are represented in the text BIBREF37. We also use a modified version of the Flesch-Kincaid Grade Level and Flesch Reading Ease scores as computed in BIBREF35. Lastly, we encode the sentiment scores for each message using the Valence Aware Dictionary and sEntiment Reasoner (VADER) of BIBREF38.", "Message count: Count the messages posted in the thread", "Mention overlap: Let $M_a$ be the set of all accounts mentioned by author $a$, and let $M_t$ be the set of all accounts mentioned by target $t$. We compute the ratio $\\frac{|M_a \\cap M_t|}{|M_a \\cup M_t|}$.", "Similar results were obtained by replacing our logistic regression model with any of a random forest model, support vector machine (SVM), AdaBoost, or Multilayer Perceptron (MLP). We report all precision, recall, and $F_1$ scores in Appendix 2, Tables TABREF69-TABREF77. We chose to highlight logistic regression because it can be more easily interpreted. As a result, we can identify the relative importance of our proposed features. The feature weights are also given in Appendix 2, Tables TABREF78-TABREF78. There we observe a trend. The aggressive language and repetition criteria are dominated by lexical features; the harmful intent is split between lexical and historical communication features; and the visibility among peers and target power criteria are dominated by our proposed social features."]}
{"question_id": "5a473f86052cf7781dfe40943ddf99bc9fe8a4e4", "predicted_answer": "The social-network features used are:\n\n1. Visibility among peers (peer): At least one other user besides the target has liked, retweeted, or responded to at least one of the author's messages.", "predicted_evidence": ["Here, we consider linguistic features, drawn from both the author and target timelines. These are intended to capture the social relationship between each user, their common interests, and the surprise of a given message relative to the author's timeline history.", "To more clearly represent the social relationship between the author and target users, we consider the messages sent between them as follows:", "Visibility among peers: (peer) At least one other user besides the target has liked, retweeted, or responded to at least one of the author's messages.", "Using our new dataset, we experiment with existing NLP features and compare results with a newly-proposed set of features. We designed these features to encode the dynamic relationship between a potential bully and victim, using comparative measures from their relative linguistic and social network profiles. Additionally, our features have low computational complexity, so they can scale to internet-scale datasets, unlike expensive network centrality and clustering measurements.", "Timeline similarity is used to indicate common interests and shared topics of conversation between the author and target timelines. High similarity scores might reflect users' familiarity with one another, or suggest that they occupy similar social positions. This can be used to distinguish cyberbullying from harmless banter between friends and associates. To compute this metric, we represent the author and target timelines as TF-IDF vectors $\\vec{A}$ and $\\vec{T}$. We then take the cosine similarity between the vectors as"]}
{"question_id": "235c7c7ca719068136928b18e19f9661e0f72806", "predicted_answer": "The five factors considered are the five cyberbullying criteria, which are not explicitly stated in the given context. However, it is mentioned that the criteria can be combined and adapted for revised definitions of cyberbullying, and that annotators are asked to provide labels for these five clear cyberbullying criteria.", "predicted_evidence": ["For the sake of comparison, we provide precision, recall, and $F_1$ scores for five different machine learning models: $k$-nearest neighbors (KNN), random forest, support vector machine (SVM), AdaBoost, and Multilayer Perceptron (MLP). Then we provide feature weights for our logistic regression model trained on each of the five cyberbullying criteria.", "Let $N^{+}(u)$ be the set of all accounts followed by user $u$ and let $N^{-}(u)$ be the set of all accounts that follow user $u$. Then $N(u) = N^{+}(u) \\cup N^{-}(u)$ is the neighborhood set of $u$. We consider five related measurements of neighborhood overlap for a given author $a$ and target $t$, listed here.", "Here, we provide an original annotation framework and a new dataset for cyberbullying research, built to unify existing methods of ground truth annotation. In this dataset, we decompose the complex issue of cyberbullying into five key criteria, which were drawn from the social science and machine learning communities. These criteria can be combined and adapted for revised definitions of cyberbullying.", "Clear labeling criteria. We ask annotators to provide labels for five clear cyberbullying criteria. These criteria can be combined and adapted for revised definitions of cyberbullying.", "where $m$ is composed of bigrams $b_1, b_2, \\dots , b_N$, and $P(b_i)$ is the probability of the $i$th bigram from the language model."]}
{"question_id": "c87966e7f497975b76a60f6be50c33d296a4a4e7", "predicted_answer": "According to the text, cyberbullying is not explicitly defined, and the definition is actually a topic of debate among researchers. However, some researchers view cyberbullying as an extension of traditional bullying behaviors, which are defined by Dan Olweus as involving repetition, harmful intent, and an imbalance of power. Others argue that cyberbullying may not necessarily meet these criteria, as a single message can be forwarded and publicly viewed without repeated actions, and power imbalances may take different forms online, such as anonymity and the permanence of online messages.", "predicted_evidence": ["Cyberbullying poses a serious threat to the safety of online communities. The Centers for Disease Control and Prevention (CDC) identify cyberbullying as a \u201cgrowing public health problem in need of additional research and prevention efforts\u201d BIBREF0. Cyberbullying has been linked to negative mental health outcomes, including depression, anxiety, and other forms of self-harm, suicidal ideation, suicide attempts, and difficulties with social and emotional processing BIBREF1, BIBREF2, BIBREF3. Where traditional bullying was once limited to a specific time and place, cyberbullying can occur at any hour and from any location on earth BIBREF4. Once the first message has been sent, the attack can escalate rapidly as harmful content is spread across shared media, compounding these negative effects BIBREF5, BIBREF6.", "Cyberbullying detection is an increasingly important and yet challenging problem to tackle. A lack of detailed and appropriate real-world datasets stymies progress towards more reliable detection methods. With cyberbullying being a systemic issue across social media platforms, we urge the development of a methodology for data sharing with researchers that provides adequate access to rich data to improve on the early detection of cyberbullying while also addressing the sensitive privacy issues that accompany such instances.", "The main contribution of our paper is not that we solved the problem of cyberbullying detection. Instead, we have exposed the challenge of defining and measuring cyberbullying activity, which has been historically overlooked in the research community.", "Some researchers view cyberbullying as an extension of more \u201ctraditional\u201d bullying behaviors BIBREF16, BIBREF17, BIBREF18. In one widely-cited book, the psychologist Dan Olweus defines schoolyard bullying in terms of three criteria: repetition, harmful intent, and an imbalance of power BIBREF19. He then identifies bullies by their intention to \u201cinflict injury or discomfort\u201d upon a weaker victim through repeated acts of aggression.", "Social scientists have extensively studied this form of bullying as it occurs among adolescents in school BIBREF20, BIBREF21. However, experts disagree whether cyberbullying should be studied as a form of traditional bullying or a fundamentally different phenomenon BIBREF20, BIBREF17. Some argue that, although cyberbullying might involve repeated acts of aggression, this condition might not necessarily hold in all cases, since a single message can be otherwise forwarded and publicly viewed without repeated actions from the author BIBREF22, BIBREF5. Similarly, the role of power imbalance is uncertain in online scenarios. Power imbalances of physical strength or numbers may be less relevant, whereas bully anonymity and the permanence of online messages may be sufficient to render the victim defenseless BIBREF23."]}
{"question_id": "c9eae337edea0edb12030a7d4b01c3a3c73c16d3", "predicted_answer": "The evaluation performed on the output includes:\n\n1. Objective evaluation of similarity with training data\n2. Subjective evaluation of humor content\n3. Evaluation of syntactic correctness of generated sentences", "predicted_evidence": ["Input sequence length: 13", "We briefly describe the interpretation scheme of the parser output. If the word in a sentence is out of vocabulary of the parser, it is followed by [?], followed by one of .n, .v, .a, or .e, depending on whether the word is being interpreted as a noun, verb, adjective, or adverb. If a word is enclosed in square brackets, this indicates that the parser was forced to delete this word in order to find a grammatical interpretation of the sentence. The parsing occurs in multiple phases. In the first phase, attempt is made to find \"complete\" linkage for a sentence without leaving out any word, in which all the words are linked together. If the parser cannot interpret the sentence, it begins to relax this constraint. The value INLINEFORM0 indicates that the parser is allowing INLINEFORM1 words to be ignored that is, it is allowing the sentence to be partitioned into INLINEFORM2 disconnected components. After finding a valid linkage, the linkage must satisfy a post-processing phase where it is evaluated against an exhaustive set of rules to be finally labeled as grammatically correct.", "We briefly describe the interpretation scheme of the parser output. If the word in a sentence is out of vocabulary of the parser, it is followed by [?], followed by one of .n, .v, .a, or .e, depending on whether the word is being interpreted as a noun, verb, adjective, or adverb. If a word is enclosed in square brackets, this indicates that the parser was forced to delete this word in order to find a grammatical interpretation of the sentence. The parsing occurs in multiple phases. In the first phase, attempt is made to find \"complete\" linkage for a sentence without leaving out any word, in which all the words are linked together. If the parser cannot interpret the sentence, it begins to relax this constraint. The value INLINEFORM0 indicates that the parser is allowing INLINEFORM1 words to be ignored that is, it is allowing the sentence to be partitioned into INLINEFORM2 disconnected components.", "We evaluated the similarity of the generated texts with training data objectively and the humor content subjectively. We also checked the syntactic correctness of the generated sentences.", "Generating texts containing human sentiments is one of the most challenging tasks in natural language processing. Among the human emotions, research on the characteristics of humor, the linguistic properties of humor and its interpretation by human cognition is an active field of study."]}
{"question_id": "9f1d81b2a6fe6835042a5229690e1951b97ff671", "predicted_answer": "The joke data came from a dataset consisting of 7699 jokes written by a single author, which gives the corpus a homogeneity of style.", "predicted_evidence": ["Examples of generated jokes:", "The first experiment was training the model with just jokes.", "The task of humor generation has been approached using deep neural networks with attention by BIBREF1 and unsupervised data mining to generate fixed-structure jokes by BIBREF0 . In the former work, an LSTM model with attention is used to generate jokes from a dataset consisting of 7699 jokes written by a single author giving the corpus a homogeneity of style. The jokes data is mixed with news data and a deep recurrent neural network is trained with weighted-pick strategy above the output layer to bring in randomness and a certain chance of producing funny sentences. This is the only work to the best of our knowledge which tried to mix jokes and non-jokes during training in order to bring in more information but it didn't train the network with a category tag so we have no control over what the model generates which makes judging it more subjective.", "The task of humor generation has been approached using deep neural networks with attention by BIBREF1 and unsupervised data mining to generate fixed-structure jokes by BIBREF0 . In the former work, an LSTM model with attention is used to generate jokes from a dataset consisting of 7699 jokes written by a single author giving the corpus a homogeneity of style. The jokes data is mixed with news data and a deep recurrent neural network is trained with weighted-pick strategy above the output layer to bring in randomness and a certain chance of producing funny sentences. This is the only work to the best of our knowledge which tried to mix jokes and non-jokes during training in order to bring in more information but it didn't train the network with a category tag so we have no control over what the model generates which makes judging it more subjective. It also means the network is trained in a kind of unsupervised manner when we could have trained it in a more supervised manner by telling it whether its a joke or not and later ask it to generate a joke specifically.", "Detailed studies have been conducted to identify the structure and principles of humor using mathematical models and generate fixed formulaic jokes using unsupervised learning from big data BIBREF0 . Recurrent Neural Networks are popular for text generation tasks and they were used for humor generation BIBREF1 . The technique described in the later paper was to give the topic words (proper nouns tagged by part-of-speech tagger) as input and generate jokes on them. We believe training models with jokes and non-jokes in a supervised manner will give it more contextual data to inference from and generate creative content."]}
{"question_id": "fae930129c2638ba6f9c9b3383e85aa130a73876", "predicted_answer": "The system is trying to generate inspirational quotes.", "predicted_evidence": ["Finally, we trained our model with three categories of data namely: jokes, quotes, and tweets. We show that the network is able to generate texts belonging to the specific category when we pass the category as input along with the seed text.", "We have proposed a novel method of training a recurrent neural network (RNN) with categorical data to generate texts with target sentiment. We showed how our proposed controlled LSTM architecture is able to learn the semantics of different kinds of text together in a supervised manner and generate text with given sentiment on demand. The mixture of information is able to generate more creative content. We gathered a large corpus of jokes, quotes, and tweets from multiple sources for training our model which can be used for further research. The produced texts were subjectively and objectively evaluated, they are found to be semantically and syntactically coherent while expressing the required sentiment majority of the time. The neural network introduces incongruity in sentences to make them funny when asked to generate a joke, it generates inspirational quotes with meaning when asked to generate a quote, and generates casual sentences when asked to generate a tweet thus showing the network is able to learn the nature of different texts.", "To evaluate the syntactic correctness of the generated sentences we have used the Link Grammar Parser for English language developed by BIBREF11 which uses the idea of linkages. A linkage is the relationship between different words in a sentence from a syntactic point of view. We use the Link Grammar Parser to find number of valid linkages of the sentence after post processing. We break the generated jokes, quotes, tweets into individual sentences. Since the parsing is dependent on capitalization and our training data has been normalized to all small letters, we add necessary capitalization before feeding sentences to the parser. Here are examples of linkage diagram produced by syntactic parsing of a generated sentence. The lines represents the links between the words and each link is labeled with the type of link. Details about the link labels can be found in the documentation of the parser.", "Giving the different category tags as input for the same prefix, we were able to generate texts of different categories.", "A controlled LSTM can be used to train a network in a supervised way on multiple categorical data like jokes, quotes, and tweets by augmenting the category tag to the input word at every time-step. This way the neural net can learn the difference in the semantics of a joke and quote and generate more creative content using the mix of knowledge gained by training in a supervised manner on multiple categorical data. We show how our model is able to generate a joke vs quote depending on the category input for the same prefix of words. We also found that a network trained on the combined dataset generated fewer offensive jokes compared to the one trained on just the jokes (as the jokes scraped from the internet were offensive with high probability). This is the first time anyone has used controlled LSTM architecture to generate texts with different sentiments. We show how the network learns to introduce incongruities in the generated text (making it funny) when asked to generate a joke as opposed to a quote (which is inspirational).With the current resurgence of deep-neural networks and its astounding success in natural language generation, our paper tries to achieve the above goal."]}
{"question_id": "1acfbdc34669cf19a778aceca941543f11b9a861", "predicted_answer": "The size of the filters used in the convolution layer is not explicitly stated in the given context. However, it is mentioned that the filters have the same shape, and they are repeatedly operated over every row of the matrix to produce feature maps.", "predicted_evidence": ["where the set of filters INLINEFORM0 is shared parameters in the convolution layer; INLINEFORM1 denotes a convolution operator; and INLINEFORM2 denotes a capsule network operator. We use the Adam optimizer BIBREF19 to train CapsE by minimizing the loss function BIBREF14 , BIBREF15 as follows: DISPLAYFORM0", "Different from the traditional modeling design of CapsNet where capsules are constructed by splitting feature maps, we use capsules to model the entries at the same dimension in the entity and relation embeddings. In our CapsE, INLINEFORM0 , INLINEFORM1 and INLINEFORM2 are unique INLINEFORM3 -dimensional embeddings of INLINEFORM4 , INLINEFORM5 and INLINEFORM6 , respectively. The embedding triple [ INLINEFORM7 , INLINEFORM8 , INLINEFORM9 ] of (s, r, o) is fed to the convolution layer where multiple filters of the same INLINEFORM10 shape are repeatedly operated over every row of the matrix to produce INLINEFORM11 -dimensional feature maps. Entries at the same dimension from all feature maps are then encapsulated into a capsule. Thus, each capsule can encode many characteristics in the embedding triple to represent the entries at the corresponding dimension. These capsules are then routed to another capsule which outputs a continuous vector whose length is used as a score for the triple.", "The first capsule layer consists of INLINEFORM0 capsules, for which each capsule INLINEFORM1 has a vector output INLINEFORM2 . Vector outputs INLINEFORM3 are multiplied by weight matrices INLINEFORM4 to produce vectors INLINEFORM5 which are summed to produce a vector input INLINEFORM6 to the capsule in the second layer. The capsule then performs the non-linear squashing function to produce a vector output INLINEFORM7 : DISPLAYFORM0", "We denote INLINEFORM0 , INLINEFORM1 and INLINEFORM2 as the INLINEFORM3 -dimensional embeddings of INLINEFORM4 , INLINEFORM5 and INLINEFORM6 , respectively. In our proposed CapsE, we follow BIBREF15 to view each embedding triple [ INLINEFORM7 , INLINEFORM8 , INLINEFORM9 ] as a matrix INLINEFORM10 , and denote INLINEFORM11 as the INLINEFORM12 -th row of INLINEFORM13 . We use a filter INLINEFORM14 operated on the convolution layer. This filter INLINEFORM15 is repeatedly operated over every row of INLINEFORM16 to generate a feature map INLINEFORM17 , in which INLINEFORM18 where INLINEFORM19 denotes a dot product, INLINEFORM20 is a bias term and INLINEFORM21 is a non-linear activation function such as ReLU. Our model uses multiple filters INLINEFORM22 to generate feature maps. We denote INLINEFORM23 as the set of filters and INLINEFORM24 as the number of filters, thus we have INLINEFORM25 INLINEFORM26 -dimensional feature maps, for which each feature map can capture one single characteristic among entries at the same dimension.", "We denote INLINEFORM0 , INLINEFORM1 and INLINEFORM2 as the INLINEFORM3 -dimensional embeddings of INLINEFORM4 , INLINEFORM5 and INLINEFORM6 , respectively. In our proposed CapsE, we follow BIBREF15 to view each embedding triple [ INLINEFORM7 , INLINEFORM8 , INLINEFORM9 ] as a matrix INLINEFORM10 , and denote INLINEFORM11 as the INLINEFORM12 -th row of INLINEFORM13 . We use a filter INLINEFORM14 operated on the convolution layer. This filter INLINEFORM15 is repeatedly operated over every row of INLINEFORM16 to generate a feature map INLINEFORM17 , in which INLINEFORM18 where INLINEFORM19 denotes a dot product, INLINEFORM20 is a bias term and INLINEFORM21 is a non-linear activation function such as ReLU. Our model uses multiple filters INLINEFORM22 to generate feature maps."]}
{"question_id": "864295caceb1e15144c1746ab5671d085d7ff7a1", "predicted_answer": "According to the text, CapsE outperforms TransE with relative improvements of 14.5% and 22% in MRR and Hits@1 scores, respectively.", "predicted_evidence": ["INLINEFORM0 We restate the prospective strategy of expanding the triple embedding models to improve the ranking quality of the search personalization systems. We adapt our model to search personalization and evaluate on SEARCH17 BIBREF12 \u2013 a dataset of the web search query logs. Experimental results show that our CapsE achieves the new state-of-the-art results with significant improvements over strong baselines.", "To illustrate our training progress, we plot performances of CapsE on the validation set over epochs in Figure FIGREF18 . We observe that the performance is improved with the increase in the number of filters since capsules can encode more useful properties for a large embedding size.", "We propose CapsE\u2014a novel embedding model using the capsule network to model relationship triples for knowledge graph completion and search personalization. Experimental results show that our CapsE outperforms other state-of-the-art models on two benchmark datasets WN18RR and FB15k-237 for the knowledge graph completion. We then show the effectiveness of our CapsE for the search personalization, in which CapsE outperforms the competitive baselines on the dataset SEARCH17 of the web search query logs. In addition, our CapsE is capable to effectively model many-to-many relationships. Our code is available at: https://github.com/daiquocnguyen/CapsE.", "Evaluation protocol: Our CapsE is used to re-rank the original list of documents returned by a search engine as follows: (i) We train our model and employ the trained model to calculate the score for each INLINEFORM0 triple. (ii) We then sort the scores in the descending order to obtain a new ranked list. To evaluate the performance of our proposed model, we use two standard evaluation metrics: mean reciprocal rank (MRR) and Hits@1. For each metric, the higher value indicates better ranking performance.", "Table TABREF17 presents the experimental results of the baselines and our model. Embedding models TranE, ConvKB and CapsE produce better ranking performances than traditional learning-to-rank search personalization models CI and SP. This indicates a prospective strategy of expanding the triple embedding models to improve the ranking quality of the search personalization systems. In particular, our MRR and Hits@1 scores are higher than those of TransE (with relative improvements of 14.5% and 22% over TransE, respectively). Specifically, our CapsE achieves the highest performances in both MRR and Hits@1 (our improvements over all five baselines are statistically significant with INLINEFORM0 using the paired t-test)."]}
{"question_id": "79e61134a6e29141cd19252571ffc92a0b4bc97f", "predicted_answer": "The answer is no, they did not test with other pre-trained models besides BERT in their experiments. They mentioned other pre-trained models such as CoVe, CVT, ELMo, ULMFiT, and Open-GPT, but they only used BERT in their experiments.", "predicted_evidence": ["We perform three different experiments to test our hypotheses. First, we perform a named entity recognition tasks, by adding a bi-LSTM on top of the BERT model. In this experiment, we hope to test whether, without any modification to the commonly used network structure, our proposed training strategy will improve the overall accuracy. Second, we perform a text classification experiments, in this experiments, we trained three models, and perform a model ensemble. We hope to show that even the added network has not contributed to significantly in improving the accuracy, it does provide opportunities for model ensembles. Finally, we perform the textual similarity tests, in which we show that if one can tailor make a network that specifically fit the characteristics of the pre-trained languages, more significant improvement can be expected.", "We choose this strategy for the following reasons. Pre-training models have been used to obtain more effective word representations through the study of a large number of corpora. In the paradigm proposed in the original work by Devlin et al. BIBREF1 , the author directly trained BERT along with with a light-weighted task-specific head. In our case though, we top BERT with a more complex network structure, using Kaiming initialization BIBREF28 . If one would fine-tune directly the top models along with the weights in BERT, one is faced with the following dilemma: on the one hand, if the learning rate is too large, it is likely to disturb the structure innate to the pre-trained language models; on the other hand, if the learning rate is too small, since we top BERT with relatively complex models, the convergence of the top models might be impeded. Therefore, in the first phase we fix the weights in the pre-training language models, and only train the model on top of it.", "In principles, there are three ways to train the networks with stacked neural networks on top of pre-trained language models, as shown in Table TABREF1 . In Peters et al . BIBREF0 , the authors compare the possibility of option stack-only and finetune-only, and conclude that option finetune-only is better than option stack-only. More specifically, Peter et al. BIBREF0 argue that it is better to add a task-specific head on top of BERT than to freeze the weights of BERT and add more complex network structures. However, Peters et al. BIBREF0 did not compare option stack-and-finetune and finetune-only. On the other hand, before pre-trained deep language models became popular, researchers often use a strategy analog to option stack-and-finetune. That is, modelers first train the model until convergence, and then fine-tune the word embeddings with a few epochs.", "In principles, there are three ways to train the networks with stacked neural networks on top of pre-trained language models, as shown in Table TABREF1 . In Peters et al . BIBREF0 , the authors compare the possibility of option stack-only and finetune-only, and conclude that option finetune-only is better than option stack-only. More specifically, Peter et al. BIBREF0 argue that it is better to add a task-specific head on top of BERT than to freeze the weights of BERT and add more complex network structures. However, Peters et al. BIBREF0 did not compare option stack-and-finetune and finetune-only. On the other hand, before pre-trained deep language models became popular, researchers often use a strategy analog to option stack-and-finetune. That is, modelers first train the model until convergence, and then fine-tune the word embeddings with a few epochs. If pre-trained language models can be understood as at least partially resemblance of word embeddings, then it will be imprudent not to consider the possibility of option stack-and-finetune.", "Some early attempts include pre-trained models includes, CoVe BIBREF11 , CVT BIBREF12 , BIBREF13 , ELMo BIBREF14 and ULMFiT BIBREF15 . However, the most successful ones are BERT BIBREF1 and Open-GPT BIBREF2 . Unlike standard NLP deep learning model, BERT and Open-GPT are built on top of transformer BIBREF16 structures, instead of LSTM BIBREF17 or GRU BIBREF18 . The difference between BERT and Open-GPT is that BERT uses bi-directional self-attentions while Open-GPT uses only unidirectional ones, as shown in Figure FIGREF2 . The transformer structures differ from the LSTM's in the two important aspects. First, it allows for stacking of multiple layers with residual connections and batch normalizations, which allows for free gradient flow."]}
{"question_id": "18fbfb1f88c5487f739aceffd23210a7d4057145", "predicted_answer": "The models compared in this study are:\n\n1. Strategy stack-and-finetune: This involves a two-phase training process, where the pre-trained language model's parameters are fixed in the first phase, and only the upper-level models are learned. In the second phase, the upper-level models are fine-tuned together with the pre-trained language models.\n\n2. Strategy finetune-only: This involves directly fine-tuning the pre-trained language models with the task-specific head.\n\nThe study aims to compare the performance of these two strategies on three NLP tasks: sequence labeling, text classification, and question similarity.", "predicted_evidence": ["We perform three different experiments to test our hypotheses. First, we perform a named entity recognition tasks, by adding a bi-LSTM on top of the BERT model. In this experiment, we hope to test whether, without any modification to the commonly used network structure, our proposed training strategy will improve the overall accuracy. Second, we perform a text classification experiments, in this experiments, we trained three models, and perform a model ensemble. We hope to show that even the added network has not contributed to significantly in improving the accuracy, it does provide opportunities for model ensembles. Finally, we perform the textual similarity tests, in which we show that if one can tailor make a network that specifically fit the characteristics of the pre-trained languages, more significant improvement can be expected.", "Another aspect that is worth commenting in the first phase is that it is most beneficial that one does not train the top model until it reaches the highest accuracy on the training or validation data sets, but rather only up to a point where the prediction accuracy of the training and validation data sets do not differ much. This is intuitively reasonable for the following reasons. Unlike word embeddings, the pre-trained language models possess a large number of parameters compared to the task-specific models we build on top them. Therefore, if one were to train the top models until they reach the highest prediction accuracy in the training or validation data sets, it would likely cause the models to over-fit. Therefore, in our experiment, we found that this leads to the highest performance increase in the fine-tuning stage.", "In this study, we aim to compare the strategy stack-and-finetune and strategy finetune-only. More specifically, we perform three NLP tasks, sequence labeling, text classification, and question similarity. In the first tasks, we demonstrate that even without modifying the network structures, building networks on top of pre-trained language models might improve accuracy. In the second tasks, we show that by ensembling different neural networks, one can even improve the accuracy of fine-tuning only methods even further. Finally, in the last task, we demonstrate that if one can tailor-made a neural network that specifically fit the characteristics of the pre-trained language models, one can improve the accuracy even further. All the results indicate the strategy stack-and-finetune is superior to strategy finetune-only. This leads us to conclude that, at least, by overlooking the possibility strategy stack-and-finetune is imprudent.", "Under our strategy stack-and-finetune, the model training process is divided into two phases, which are described in detail below. In the first phase, the parameters of the pre-training model are fixed, and only the upper-level models added for a specific task is learned. In the second phase, we fine-tune the upper-level models together with the pre-trained language models. We choose this strategy for the following reasons. Pre-training models have been used to obtain more effective word representations through the study of a large number of corpora. In the paradigm proposed in the original work by Devlin et al. BIBREF1 , the author directly trained BERT along with with a light-weighted task-specific head. In our case though, we top BERT with a more complex network structure, using Kaiming initialization BIBREF28 . If one would fine-tune directly the top models along with the weights in BERT, one is faced with the following dilemma: on the one hand, if the learning rate is too large, it is likely to disturb the structure innate to the pre-trained language models; on the other hand, if the learning rate is too small, since we top BERT with relatively complex models, the convergence of the top models might be impeded.", "Under our strategy stack-and-finetune, the model training process is divided into two phases, which are described in detail below. In the first phase, the parameters of the pre-training model are fixed, and only the upper-level models added for a specific task is learned. In the second phase, we fine-tune the upper-level models together with the pre-trained language models. We choose this strategy for the following reasons. Pre-training models have been used to obtain more effective word representations through the study of a large number of corpora. In the paradigm proposed in the original work by Devlin et al. BIBREF1 , the author directly trained BERT along with with a light-weighted task-specific head. In our case though, we top BERT with a more complex network structure, using Kaiming initialization BIBREF28 ."]}
{"question_id": "5d3e87937ecebf0695bece08eccefb2f88ad4a0f", "predicted_answer": "The datasets used for testing are not explicitly mentioned in the given context.", "predicted_evidence": ["This training strategy does not work when we replace pre-trained language models with word-embeddings. In our experiment, we first fix the pre-trained language models, and then we train the top neural networks only for a few epochs, until it reaches a reasonable accuracy, while closely monitoring the discrepancy between training accuracy and testing accuracy. After that, we fine-tune the pre-trained language model as well as our models on top together. This allows us to achieve better results on the experimentation. However, it is not yet clear to us when to stop the training of top neural networks. This poses an even more essential question for Auto ML researchers in the following sense. In the classical computer vision based Auto ML approaches, since one seldom build networks on already trained models, there is no particular need to auxiliary measure for over-fittings. While if Auto ML is to be performed on NLP tasks successfully, it might be essential that the gap between training accuracy and test accuracy to be incorporated when one evaluates the model.", "The contribution of this paper is two-fold. First, we propose a new strategy to improve the fine-tune-only strategy proposed by Peter et al. BIBREF0 , this allows us to achieve better results, at least on the selected tasks. More importantly, the results of this study demonstrate the importance of neural networks design, even in the presence of all-powerful pre-trained language models. Second, during the experiment, we have found that although simply using the proposed training strategy can result in higher accuracies compared to that of Peter et al. BIBREF0 , it is still a challenging task to find the appropriate methods to design and to utilize pre-trained networks. In this regard, we find that pre-trained models differ significantly from word embeddings in terms of their training strategies.", "The contribution of this paper is two-fold. First, we propose a new strategy to improve the fine-tune-only strategy proposed by Peter et al. BIBREF0 , this allows us to achieve better results, at least on the selected tasks. More importantly, the results of this study demonstrate the importance of neural networks design, even in the presence of all-powerful pre-trained language models. Second, during the experiment, we have found that although simply using the proposed training strategy can result in higher accuracies compared to that of Peter et al. BIBREF0 , it is still a challenging task to find the appropriate methods to design and to utilize pre-trained networks. In this regard, we find that pre-trained models differ significantly from word embeddings in terms of their training strategies. Especially, since word embeddings can be viewed as shallow transfer learning, while pre-trained model should be viewed as deep transfer learning, one must try to combat over-fitting problems with more care due to the enormous number of parameters presented in the pre-trained models.", "Another aspect that is worth commenting in the first phase is that it is most beneficial that one does not train the top model until it reaches the highest accuracy on the training or validation data sets, but rather only up to a point where the prediction accuracy of the training and validation data sets do not differ much. This is intuitively reasonable for the following reasons. Unlike word embeddings, the pre-trained language models possess a large number of parameters compared to the task-specific models we build on top them. Therefore, if one were to train the top models until they reach the highest prediction accuracy in the training or validation data sets, it would likely cause the models to over-fit. Therefore, in our experiment, we found that this leads to the highest performance increase in the fine-tuning stage.", "BIBREF0 , this allows us to achieve better results, at least on the selected tasks. More importantly, the results of this study demonstrate the importance of neural networks design, even in the presence of all-powerful pre-trained language models. Second, during the experiment, we have found that although simply using the proposed training strategy can result in higher accuracies compared to that of Peter et al. BIBREF0 , it is still a challenging task to find the appropriate methods to design and to utilize pre-trained networks. In this regard, we find that pre-trained models differ significantly from word embeddings in terms of their training strategies. Especially, since word embeddings can be viewed as shallow transfer learning, while pre-trained model should be viewed as deep transfer learning, one must try to combat over-fitting problems with more care due to the enormous number of parameters presented in the pre-trained models. Besides, we also find that in order to achieve the maximal performance in the post-pre-trained language model era, one must design, either manually or via Auto ML, networks that best fit the structure, especially the depth of the pre-trained language models."]}
{"question_id": "7d539258b948cd5b5ad1230a15e4b739f29ed947", "predicted_answer": "The text does not provide a specific inter-annotator agreement score or metric (e.g. Cohen's kappa, Fleiss' kappa, etc.) for the annotation task. It only mentions that the annotation was performed cooperatively and independently by the three annotators, followed by cooperative adjudication, and that two inter-annotator agreement studies were conducted to demonstrate the reproducibility and reliability of the adapted scheme for Chinese.", "predicted_evidence": ["Tokens that are considered generic prepositions can co-occur with the main predicate of the clause and introduce an NP argument to the clause BIBREF36 as in zho:shangtopic. These tokens are referred to as coverbs. In some cases, coverbs can also occur as the main predicate. For example, the coverb 4 heads the predicate phrase in zho:pred.", "The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.", "BeneficiaryExperiencer", "Our first challenge is to determine which tokens qualify as adpositions in Mandarin Chinese and merit supersense annotations. The English SNACS guidelines (we use version 2.3) broadly define the set of SNACS annotation targets to include canonical prepositions (taking an noun phrase (NP) complement) and their subordinating (clausal complement) uses. Possessives, intransitive particles, and certain uses of the infinitive marker to are also included BIBREF35.", "Prior to supersense annotation, we conducted the following preprocessing steps in order to identify the adposition targets that merit supersense annotation."]}
{"question_id": "9c1f70affc87024b4280f0876839309b8dddd579", "predicted_answer": "The corpus was annotated by three native Mandarin Chinese speakers who have received advanced training in theoretical and computational linguistics. The annotation process involved two phases:\n\n1. Cooperative annotation: Three annotators jointly annotated 25% of the adposition targets (235 out of 933).\n2. Independent annotation and adjudication: The remaining 75% of the adposition targets were annotated independently by the three annotators, followed by cooperative adjudication to resolve any disagreements.", "predicted_evidence": ["Prior to supersense annotation, we conducted the following preprocessing steps in order to identify the adposition targets that merit supersense annotation.", "In this paper, we presented the first corpus annotated with adposition supersenses in Mandarin Chinese. The corpus is a valuable resource for examining similarities and differences between adpositions in different languages with parallel corpora and can further support automatic disambiguation of adpositions in Chinese. We intend to annotate additional genres\u2014including native (non-translated) Chinese and learner corpora\u2014in order to more fully capture the semantic behavior of adpositions in Chinese as compared to other languages.", "This paper describes the first corpus with broad-coverage annotation of adpositions in Chinese. For this corpus we have adapted schneider-etal-2018-comprehensive Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see sec:snacs) to Chinese. Though other languages were taken into consideration in designing SNACS, no serious annotation effort has been undertaken to confirm empirically that it generalizes to other languages. After developing new guidelines for syntactic phenomena in Chinese (subsec:adpositioncriteria), we apply the SNACS supersenses to a translation of The Little Prince (3 2 3), finding the supersenses to be robust and achieving high inter-annotator agreement (sec:corpus-annotation). We analyze the distribution of adpositions and supersenses in the corpus, and compare to adposition behavior in a separate English corpus (see sec:corpus-analysis).", "This paper describes the first corpus with broad-coverage annotation of adpositions in Chinese. For this corpus we have adapted schneider-etal-2018-comprehensive Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see sec:snacs) to Chinese. Though other languages were taken into consideration in designing SNACS, no serious annotation effort has been undertaken to confirm empirically that it generalizes to other languages. After developing new guidelines for syntactic phenomena in Chinese (subsec:adpositioncriteria), we apply the SNACS supersenses to a translation of The Little Prince (3 2 3), finding the supersenses to be robust and achieving high inter-annotator agreement (sec:corpus-annotation). We analyze the distribution of adpositions and supersenses in the corpus, and compare to adposition behavior in a separate English corpus (see sec:corpus-analysis). We also examine the predictions of a part-of-speech tagger in relation to our criteria for annotation targets (sec:adpositionidentification).", "The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese."]}
{"question_id": "2694a679a703ccd6139897e4d9ff8e053dabd0f2", "predicted_answer": "Based on the given context, I'm going to take a stab at it.\n\nThe answer is: Portion.\n\nThe corpus refers to a body or collection of texts, and in this context, the size of the corpus would refer to the portion or amount of text within it.", "predicted_evidence": ["CircumstanceTime", "PartPortionLocus", "", "", ""]}
{"question_id": "65c9aee2051ff7c47112b2aee0d928d9b6a8c2fe", "predicted_answer": "The datasets used are not explicitly mentioned in the given context. The text appears to be a general discussion of natural language processing and machine learning techniques for text classification, specifically for fake news detection. It does not specify a particular dataset being used.", "predicted_evidence": ["Mean Test Scores", "https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/", "We had devised this problem into 3 different phases: pre-processing, text-to-numeric representation conversion using pre-trained algorithms, and then evaluate the models using state-of-the-art machine learning algorithms. We had analysed the data set and in particular the text part of the data explaining how it is distributed and then we converted each text into numeric representation using pre-training models such as TFIDF, CV and W2V for vector representation. Finally, we evaluated our numeric conversion data using significant machine learning algorithms such as neural networks, classification algorithms etc to perform the classification.", "For this report, we are classifying news articles as \u201creal\u201d or \u201cfake\u201d, which will be a binary classification problem - classifying the samples as a positive (with fake news) or negative (not fake news) sample. Many studies have used machine learning algorithms and build classifiers based on features like content, the author\u2019s name and job-title, using lots of models like the convolutional neural network (CNN), recurrent neural network (RNN), feed-forward neural network (FFNN), long-short term memory (LSTM) and logistic regression to find the most optimal model and return its results. In [1], the author built a classifier using natural language processing and used models like CNN, RNN, FFNN, and Logistic Regression and concluded that the CNN classifiers could not be as competitive as the RNN classifiers. The authors in [2] think that their study can be improved by having more features like knowing the history of lies spoken by the news reporter or the speaker.", "Although TF-IDF is an old algorithm, it is simple and effective to be used in the phase of pre-training$^{11}$. The computation of TfidfVectorizer involves computing the product of term frequency and inverse document frequency. As the term implies, TF-IDF calculates values for each word in a document through an inverse proportion of the frequency of the word in a particular document to the percentage of documents the word appears in$^{12}$."]}
{"question_id": "f8264609a44f059b74168995ffee150182a0c14f", "predicted_answer": "According to the paper, the following models are explored:\n\n1. Convolutional Neural Network (CNN)\n2. Recurrent Neural Network (RNN)\n3. Feed-Forward Neural Network (FFNN)\n4. Long-Short Term Memory (LSTM)\n5. Logistic Regression\n\nAdditionally, the paper mentions the use of pre-trained models for vector representation, including:\n\n1. TFIDF\n2. CV\n3. W2V", "predicted_evidence": ["There also have been studies that have a different approach. A paper surveys the current state-of-the-art technologies that are imperative when adopting and developing fake news detection and provides a classification of several accurate assessment methods that analyze the text and detect anomalies$^{3}$.", "For this report, we are exploring the field of natural language processing, which is the broad study of how computers and machines can understand human to human communication and how texts are analyzed based on contextual information by machines.", "We had devised this problem into 3 different phases: pre-processing, text-to-numeric representation conversion using pre-trained algorithms, and then evaluate the models using state-of-the-art machine learning algorithms. We had analysed the data set and in particular the text part of the data explaining how it is distributed and then we converted each text into numeric representation using pre-training models such as TFIDF, CV and W2V for vector representation. Finally, we evaluated our numeric conversion data using significant machine learning algorithms such as neural networks, classification algorithms etc to perform the classification.", "Once the representations of text are pre-trained from previous unsupervised learning, the representations are then fed into 5 different models to perform supervised learning on the downstream task. In this case, the downstream task is a binary classification of the fake news as either real or fake. A k-fold prediction error is obtained from each of the 5 models, and since we have 3 different pre-training models, we have a total of 15 models to compare.", "For this report, we are classifying news articles as \u201creal\u201d or \u201cfake\u201d, which will be a binary classification problem - classifying the samples as a positive (with fake news) or negative (not fake news) sample. Many studies have used machine learning algorithms and build classifiers based on features like content, the author\u2019s name and job-title, using lots of models like the convolutional neural network (CNN), recurrent neural network (RNN), feed-forward neural network (FFNN), long-short term memory (LSTM) and logistic regression to find the most optimal model and return its results. In [1], the author built a classifier using natural language processing and used models like CNN, RNN, FFNN, and Logistic Regression and concluded that the CNN classifiers could not be as competitive as the RNN classifiers. The authors in [2] think that their study can be improved by having more features like knowing the history of lies spoken by the news reporter or the speaker."]}
{"question_id": "c728fe6137f114c02e921f9be4a02a5bd83ae787", "predicted_answer": "The features of the essays that are extracted can be partitioned into six groups, which include:\n\n1. Readability features\n2. Syntactic complexity features\n3. Content features\n4. Topic models (e.g. LDA)\n5. Readability and complexity scores\n6. Other features (not specified)\n\nThese features are used to describe an essay's readability, syntactic complexity, and content, and are used as predictors in the machine learning model to automatically predict the level of English of non-native speakers from their written text.", "predicted_evidence": ["In this section I present the extracted features partitioned in six groups and detail each of them separately.", "The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data. Section 3 presents the feature extaction and engineering techniques used. Section 4 describes the machine learning algorithms for prediction as well as the achieved results. Finally, Section 5 concludes with discussion and avenues for future research.", "While in terms of accuracy the system performed excellent achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used. In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.", "Automatically predicting the level of English of non-native speakers from their written text is an interesting text mining task. Systems that perform well in the task can be useful components for online, second-language learning platforms as well as for organisations that tutor students for this purpose. In this paper I present the system balikasg that achieved the state-of-the-art performance in the CAp 2018 data science challenge among 14 systems. In order to achieve the best performance in the challenge, I decided to use a variety of features that describe an essay's readability and syntactic complexity as well as its content. For the prediction step, I found Gradient Boosted Trees, whose efficiency is proven in several data science challenges, to be the most efficient across a variety of classifiers.", "In this work I presented the feature extraction, feature engineering and model evaluation steps I followed while developing balikasg for CAp 2018 that was ranked first among 14 other systems. I evaluated the efficiency of the different feature groups and found that readbility and complexity scores as well as topic models to be effective predictors. Further, I evaluated the the effectiveness of different classification algorithms and found that Gradient Boosted Trees outperform the rest of the models in this problem."]}
{"question_id": "50bda708293532f07a3193aaea0519d433fcc040", "predicted_answer": "The evaluation metric used is a cost measure that takes into account the confusion matrix of the prediction and prior knowledge, which is designed to capture the explicit ordering of the language levels.", "predicted_evidence": ["While in terms of accuracy the system performed excellent achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used. In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.", "In order to approach the language-level prediction task as a supervised classification problem, I frame it as an ordinal classification problem. In particular, given a written essay INLINEFORM0 from a candidate, the goal is to associate the essay with the level INLINEFORM1 of English according to the Common European Framework of Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most advanced level. Notice that the levels of INLINEFORM6 are ordered, thus defining an ordered classification problem. In this sense, care must be taken both during the phase of model selection and during the phase of evaluation. In the latter, predicting a class far from the true should incur a higher penalty. In other words, given a INLINEFORM7 essay, predicting INLINEFORM8 is worse than predicting INLINEFORM9 , and this difference must be captured by the evaluation metrics.", "In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0", "The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data. Section 3 presents the feature extaction and engineering techniques used. Section 4 describes the machine learning algorithms for prediction as well as the achieved results. Finally, Section 5 concludes with discussion and avenues for future research.", "In this section I present the extracted features partitioned in six groups and detail each of them separately."]}
{"question_id": "46e660becd727c994a2a35c6587e15ea8bf8272d", "predicted_answer": "The model used is Gradient Boosted Trees.", "predicted_evidence": ["In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0", "While in terms of accuracy the system performed excellent achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used. In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.", "In order to approach the language-level prediction task as a supervised classification problem, I frame it as an ordinal classification problem. In particular, given a written essay INLINEFORM0 from a candidate, the goal is to associate the essay with the level INLINEFORM1 of English according to the Common European Framework of Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most advanced level. Notice that the levels of INLINEFORM6 are ordered, thus defining an ordered classification problem. In this sense, care must be taken both during the phase of model selection and during the phase of evaluation. In the latter, predicting a class far from the true should incur a higher penalty. In other words, given a INLINEFORM7 essay, predicting INLINEFORM8 is worse than predicting INLINEFORM9 , and this difference must be captured by the evaluation metrics.", "In this section I present the extracted features partitioned in six groups and detail each of them separately.", "In this work I presented the feature extraction, feature engineering and model evaluation steps I followed while developing balikasg for CAp 2018 that was ranked first among 14 other systems. I evaluated the efficiency of the different feature groups and found that readbility and complexity scores as well as topic models to be effective predictors. Further, I evaluated the the effectiveness of different classification algorithms and found that Gradient Boosted Trees outperform the rest of the models in this problem."]}
{"question_id": "d1a4529ea32aaab5ca3b9d9ae5c16f146c23af6b", "predicted_answer": "The future work described is \"avenues for future research\" which is mentioned in Section 5 of the paper.", "predicted_evidence": ["The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data. Section 3 presents the feature extaction and engineering techniques used. Section 4 describes the machine learning algorithms for prediction as well as the achieved results. Finally, Section 5 concludes with discussion and avenues for future research.", "In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0", "In order to approach the language-level prediction task as a supervised classification problem, I frame it as an ordinal classification problem. In particular, given a written essay INLINEFORM0 from a candidate, the goal is to associate the essay with the level INLINEFORM1 of English according to the Common European Framework of Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most advanced level. Notice that the levels of INLINEFORM6 are ordered, thus defining an ordered classification problem. In this sense, care must be taken both during the phase of model selection and during the phase of evaluation. In the latter, predicting a class far from the true should incur a higher penalty. In other words, given a INLINEFORM7 essay, predicting INLINEFORM8 is worse than predicting INLINEFORM9 , and this difference must be captured by the evaluation metrics.", "In this section I present the extracted features partitioned in six groups and detail each of them separately.", "Automatically predicting the level of English of non-native speakers from their written text is an interesting text mining task. Systems that perform well in the task can be useful components for online, second-language learning platforms as well as for organisations that tutor students for this purpose. In this paper I present the system balikasg that achieved the state-of-the-art performance in the CAp 2018 data science challenge among 14 systems. In order to achieve the best performance in the challenge, I decided to use a variety of features that describe an essay's readability and syntactic complexity as well as its content. For the prediction step, I found Gradient Boosted Trees, whose efficiency is proven in several data science challenges, to be the most efficient across a variety of classifiers."]}
{"question_id": "7fba61426737394304e307cdc7537225f6253150", "predicted_answer": "The baseline is not explicitly mentioned in the given context. However, it can be inferred that the baseline might be a random or naive classifier, as the author is discussing the performance of a system that achieved 98.2% accuracy on the test data, and is now investigating whether there are any biases in the process. The author is also proposing a cost measure that takes into account the ordinal nature of the classification problem, which suggests that the baseline might be a simple accuracy metric that does not account for the ordering of the classes.", "predicted_evidence": ["In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0", "In order to approach the language-level prediction task as a supervised classification problem, I frame it as an ordinal classification problem. In particular, given a written essay INLINEFORM0 from a candidate, the goal is to associate the essay with the level INLINEFORM1 of English according to the Common European Framework of Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most advanced level. Notice that the levels of INLINEFORM6 are ordered, thus defining an ordered classification problem. In this sense, care must be taken both during the phase of model selection and during the phase of evaluation. In the latter, predicting a class far from the true should incur a higher penalty. In other words, given a INLINEFORM7 essay, predicting INLINEFORM8 is worse than predicting INLINEFORM9 , and this difference must be captured by the evaluation metrics.", "While in terms of accuracy the system performed excellent achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used. In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.", "The rest of the paper is organized as follows: in Section 2 I frame the problem of language level as an ordinal classification problem and describe the available data. Section 3 presents the feature extaction and engineering techniques used. Section 4 describes the machine learning algorithms for prediction as well as the achieved results. Finally, Section 5 concludes with discussion and avenues for future research.", "where INLINEFORM0 is a cost matrix that uses prior knowledge to calculate the misclassification errors and INLINEFORM1 is the number of observations of class INLINEFORM2 classified with category INLINEFORM3 . The cost matrix INLINEFORM4 is given in Table TABREF3 . Notice that, as expected, moving away from the diagonal (correct classification) the misclassification costs are higher. The biggest error (44) occurs when a INLINEFORM5 essay is classified as INLINEFORM6 . On the contrary, the classification error is lower (6) when the opposite happens and an INLINEFORM7 essay is classified as INLINEFORM8 . Since INLINEFORM9 is not symmetric and the costs of the lower diagonal are higher, the penalties for misclassification are worse when essays of upper languages levels (e.g., INLINEFORM10 ) are classified as essays of lower levels."]}
{"question_id": "46aa61557c8d20b1223a30366a0704d7af68bbbe", "predicted_answer": "The sentence alignment quality is evaluated according to a 5-point scale used in KocabiyikogluETAL:18, and also according to a 3-point scale (in the context of audio-text alignment).", "predicted_evidence": ["Data filtering based on hunalign alignment scores", "The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:", "In the following, we present selected examples for text-text alignments for each bin. A closer inspection reveals properties and shortcomings of hunalign scores which are based on a combination of dictionary-based alignments and sentence-length information.", "Correct alignment and fully compositional translation", "The evaluation of the text alignment quality was conducted according to the 5-point scale used in KocabiyikogluETAL:18:"]}
{"question_id": "b3b9d7c8722e8ec41cbbae40e68458485a5ba25c", "predicted_answer": "The speech alignment quality is evaluated according to a 3-point scale, with the top rating being \"Correct alignment and fully compositional translation\".", "predicted_evidence": ["text2speech, a lookup table for speech alignments", "Speech-to-text alignments", "The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:", "Correct alignment and fully compositional translation", "To align sentences to onsets and endings of corresponding audio segments we made use of aeneas \u2013 a tool for an automatic synchronization of text and audio. In contrast to most forced aligners, aeneas does not use automatic speech recognition (ASR) to compare an obtained transcript with the original text. Instead, it works in the opposite direction by using dynamic time warping to align the mel-frequency cepstral coefficients extracted from the real audio to the audio representation synthesized from the text, thus aligning the text file to a time interval in the real audio."]}
{"question_id": "b569827ecd04ae8757dc3c9523ab97e3f47a6e00", "predicted_answer": "Yes, the gating mechanism is specially designed to handle one sentence bags. According to the text, the authors propose a selective gate mechanism to mitigate the problems of distantly supervised relation extraction, particularly when dealing with one-sentence bags, which are common in popular relation extraction benchmarks (almost 80% of bags). The gate mechanism is designed to dynamically align low gating values to wrongly labeled instances, preventing noise representation from being propagated. This is in contrast to selective attention mechanisms, which can be low-effective or even completely useless when dealing with one-sentence bags, especially when the only sentence is wrongly labeled.", "predicted_evidence": ["Unlike previous works under multi-instance framework that frequently use a selective attention module to aggregate sentence-level representations into bag-level one, we propose a innovative selective gate mechanism to perform this aggregation. The selective gate can mitigate problems existing in distantly supervised relation extraction and achieve a satisfactory empirical effectiveness. Specifically, when handling the noisy instance problem, selective attention tries to produce a distribution over all sentence in a bag; but if there is only one sentence in the bag, even the only sentence is wrongly labeled, the selective attention mechanism will be low-effective or even completely useless. Note that almost $80\\%$ of bags from popular relation extraction benchmark consist of only one sentence, and many of them suffer from the wrong label problem. In contrast, our proposed gate mechanism is competent to tackle such case by directly and dynamically aligning low gating value to the wrongly labeled instances and thus preventing noise representation being propagated.", "Each sentence in a bag can be regarded as independent individual and do not have any relationship with other sentences in the bag, which possibly leads to information loss among the multiple sentences in the bag when considering classification over bag level.", "Particularly, a two-layer feed forward network is applied to each $\\mathbf {u}_j$ to sentence-wisely produce gating value, which is formally denoted as", "Recently, many works BIBREF21, BIBREF4 employed selective attention BIBREF5 to alleviate wrongly labeled problem existing in distantly supervised RE. For example, BIBREF6 han2018hierarchical propose a hierarchical relation structure attention based on the insight of selective attention. And, BIBREF7 ye2019distant extend the sentence-level selective attention to bag-level, where the bags have same relation label. Differing from these works suffering from one-sentence bag problem due to the defect of selective attention, our proposed approach employ a gate mechanism as an aggregator to handle this problem.", "In this paper, we propose a brand-new framework for distantly supervised relation extraction, i.e., selective gate (SeG) framework, as a new alternative to previous ones. It incorporates an entity-aware embedding module and a self-attention enhanced selective gate mechanism to integrate task-specific entity information into word embedding and then generates a complementary context-enriched representation for PCNN. The proposed framework has certain merits over previously prevalent selective attention when handling wrongly labeled data, especially for a usual case that there are only one sentence in the most of bags. The experiments conduct on popular NYT dataset show that our model SeG can consistently deliver a new benchmark in state-of-the-art performance in terms of all P@N and precision-recall AUC. And further ablation study and case study also demonstrate the significance of the proposed modules to handle wrongly labeled data and thus set a new state-of-the-art performance for the benchmark dataset. In the future, we plan to incorporate an external knowledge base into our framework, which may further boost the prediction quality by overcoming the problems with a lack of background information as discussed in our error analysis."]}
{"question_id": "0d42bd759c84cbf3a293ab58283a3d0d5e27d290", "predicted_answer": "Yes, they do show examples where only one sentence appears in a bag and their method works, as opposed to using selective attention. They extract only the one-sentence bags from the NYT dataset, which occupy 80% of the original dataset, and evaluate the performance of their method on these bags. The results show that their method (SeG) outperforms PCNN+ATT on one-sentence bags, with an AUC improvement of +0.13, which verifies the effectiveness of SeG on one-sentence bags.", "predicted_evidence": ["Unlike previous works under multi-instance framework that frequently use a selective attention module to aggregate sentence-level representations into bag-level one, we propose a innovative selective gate mechanism to perform this aggregation. The selective gate can mitigate problems existing in distantly supervised relation extraction and achieve a satisfactory empirical effectiveness. Specifically, when handling the noisy instance problem, selective attention tries to produce a distribution over all sentence in a bag; but if there is only one sentence in the bag, even the only sentence is wrongly labeled, the selective attention mechanism will be low-effective or even completely useless. Note that almost $80\\%$ of bags from popular relation extraction benchmark consist of only one sentence, and many of them suffer from the wrong label problem. In contrast, our proposed gate mechanism is competent to tackle such case by directly and dynamically aligning low gating value to the wrongly labeled instances and thus preventing noise representation being propagated.", "Each sentence in a bag can be regarded as independent individual and do not have any relationship with other sentences in the bag, which possibly leads to information loss among the multiple sentences in the bag when considering classification over bag level.", "To further empirically evaluate the performance of our method in solving one-sentence bag problem, we extract only the one-sentence bags from NYT's training and test sets, which occupy 80% of the original dataset. The evaluation and comparison results in Table TABREF33 show that compared to PCNN+ATT, the AUC improvement (+0.13) between our model and PCNN+ATT on one-sentence bags is higher than the improvement of full NYT dataset, which verifies SeG's effectiveness on one-sentence bags. In addition, PCNN+ATT shows a light decrease compared with PCNN, which can also support the claim that selective attention is vulnerable to one-sentence bags.", "Compared to the baseline framework (i.e., selective attention for multi-instance learning), SeG is able to produce entity-aware embeddings and rich-contextual representations to facilitate downstream aggregation modules that stably learn from noisy training data. Moreover, SeG uses gate mechanism with pooling to overcome problem occurring in selective attention, which is caused by one-sentence bags. In addition, it still keeps a light-weight structure to ensure the scalability of this model.", "However, such selective attention framework is vulnerable to situations where a bag is merely comprised of one single sentence labeled; and what is worse, the only one sentence possibly expresses inconsistent relation information with the bag-level label. This scenario is not uncommon. For a popular distantly supervised relation extraction benchmark, e.g., NYT dataset BIBREF2, up to $80\\%$ of its training examples (i.e., bags) are one-sentence bags. From our data inspection, we randomly sample 100 one-sentence bags and find $35\\%$ of them is incorrectly labeled. Two examples of one-sentence bag are shown in Table TABREF1. These results indicate that, in training phrase the selective attention module is enforced to output a single-valued scalar for $80\\%$ examples, leading to an ill-trained attention module and thus hurting the performance."]}
{"question_id": "9f1e60ee86a5c46abe75b67ef369bf92a5090568", "predicted_answer": "According to the text, the proposed framework outperforms previous state-of-the-art approaches by 10.3% and 5.3% in terms of top-n precision.", "predicted_evidence": ["The experiments and extensive ablation studies on New York Time dataset BIBREF2 show that our proposed framework achieves a new state-of-the-art performance regarding both AUC and top-n precision metrics for distantly supervised relation extraction task, and also verify the significance of each proposed module. Particularly, the proposed framework can achieve AUC of 0.51, which outperforms selective attention baseline by 0.14 and improves previous state-of-the-art approach by 0.09.", "We first compare our proposed SeG with aforementioned approaches in Table TABREF19 for top-N precision (i.e., P@N). As shown in the top panel of the table, our proposed model SeG can consistently and significantly outperform baseline (i.e., PCNN+ATT) and all recently-promoted works in terms of all P@N metric. Compared to PCNN with selective attention (i.e., PCNN+ATT), our proposed SeG can significantly improve the performance by 23.6% in terms of P@N mean for all sentences; even if a soft label technique is applied (i.e., PCNN+ATT+SL) to alleviate wrongly labeled problem, our performance improvement is also very significant, i.e., 7.8%.", "We compare our proposed approach with extensive previous ones, including feature-engineering, competitive and state-of-the-art approaches, which are briefly summarized in the following.", "Compared to previous state-of-the-art approaches (i.e., PCNN+HATT and PCNN+BAG-ATT), the proposed model can also outperform them by a large margin, i.e., 10.3% and 5.3% , even if they propose sophisticated techniques to handle the noisy training data. These verify the effectiveness of our approach over previous works when solving the wrongly labeled problem that frequently appears in distantly supervised relation extraction.", "Moreover, for proposed approach and comparative ones, we also show AUC curves and available numerical values in Figure FIGREF31 and Table TABREF32 respectively. The empirical results for AUC are coherent with those of P@N, which shows that, our proposed approach can significantly improve previous ones and reach a new state-of-the-art performance by handling wrongly labeled problem using context-aware selective gate mechanism. Specifically, our approach substantially improves both PCNN+HATT and PCNN+BAG-ATT by 21.4% in aspect of AUC for precision-recall."]}
{"question_id": "4dc4180127761e987c1043d5f8b94512bbe74d4f", "predicted_answer": "The text does not provide a specific answer to the question of how much the proposed methods outperform existing methods. It discusses various techniques and approaches, including traditional methods and deep learning-based methods, but does not provide a direct comparison or quantitative results showing the performance improvement of the proposed methods over existing ones.", "predicted_evidence": ["In addition to the above measures, we also found the following feature plane can improve the performance: DISPLAYFORM0", "On top of convolutional neural network layers, we build another Highway Multilayer Perceptron (HMLP) layer to further enhance character-level word embeddings. Conventional MLP applies an affine transformation followed by a nonlinearity to obtain a new set of features: DISPLAYFORM0", "Recurrent neural networks (RNNs) are capable of modeling sequences of varying lengths via the recursive application of a transition function on a hidden state. For example, at each time step INLINEFORM0 , an RNN takes the input vector INLINEFORM1 and the hidden state vector INLINEFORM2 , then applies affine transformation followed by an element-wise nonlinearity such as hyperbolic tangent function to produce the next hidden state vector INLINEFORM3 : DISPLAYFORM0", "Traditional approaches BIBREF0 , BIBREF1 , BIBREF2 for sentence relation modeling tasks such as paraphrase identification, question answering, recognized textual entailment and semantic textual similarity prediction usually build the supervised model using a variety of hand crafted features. Hundreds of features generated at different linguistic levels are exploited to boost classification. With the success of deep learning, there has been much interest in applying deep neural network based techniques to further improve the prediction performances BIBREF3 , BIBREF4 , BIBREF5 .", "where INLINEFORM0 . The objective function then can be defined as the regularized KL-divergence between INLINEFORM1 and INLINEFORM2 : DISPLAYFORM0"]}
{"question_id": "420862798054f736128a6f0c4393c7f9cc648b40", "predicted_answer": "They evaluate on the SICK (Sentences Involving Compositional Knowledge) dataset, which consists of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment.", "predicted_evidence": ["The task of semantic relatedness prediction tries to measure the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 (very related). More formally, given a sentence pair, we wish to predict a real-valued similarity score in a range of INLINEFORM0 , where INLINEFORM1 is an integer. The sequence INLINEFORM2 is the ordinal scale of similarity, where higher scores indicate greater degrees of similarity. We can predict the similarity score INLINEFORM3 by predicting the probability that the learned hidden representation INLINEFORM4 belongs to the ordinal scale. This is done by projecting an input representation onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input will located in corresponding scale.", "We selected two related sentence relation modeling tasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 ( very related); and textual entailment task, which determines whether the truth of a text entails the truth of another text called hypothesis. We use standard SICK (Sentences Involving Compositional Knowledge) dataset for evaluation. It consists of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment.", "In order to introduce the task objective function, we define a sparse target distribution INLINEFORM0 that satisfies INLINEFORM1 : DISPLAYFORM0", "The matching feature planes can be viewed as channels of images in image processing. In our scenario, these feature planes hold the matching information. We will use temporal convolutional neural network to learn hidden matching features. The mechanism of temporal CNN here is the same as character-level temporal CNN. However, the kernels are totally different.", "In this experiment, we will compare tree LSTM with sequential LSTM. A limitation of the sequence LSTM architectures is that they only allow for strictly sequential information propagation. However, tree LSTMs allow richer network topologies where each LSTM unit is able to incorporate information from multiple child units. As in standard LSTM units, each Tree-LSTM unit (indexed by INLINEFORM0 ) contains input and output gates INLINEFORM1 and INLINEFORM2 , a memory cell INLINEFORM3 and hidden state INLINEFORM4 . The difference between the standard LSTM unit and tree LSTM units is that gating vectors and memory cell updates are dependent on the states of possibly many child units. Additionally, instead of a single forget gate, the tree LSTM unit contains one forget gate INLINEFORM5 for each child INLINEFORM6 . This allows the tree LSTM unit to selectively incorporate information from each child."]}
{"question_id": "ad8411edf11d3429c9bdd08b3e07ee671464d73c", "predicted_answer": "No, they do not separately evaluate the performance of their learned representations (before forwarding them to the CNN layer). The learned representations are used as inputs to the bidirectional LSTM and then to the CNN layer, but their performance is not evaluated separately. The evaluation is done on the final sentence representation and the matching features generated by the model.", "predicted_evidence": ["In this paper, we propose a new deep neural network architecture that jointly leverage pre-trained word embedding and character embedding to learn sentence meanings. Our new approach first generates two kinds of word sequence representations as inputs into bidirectional LSTM to learn sentence representation. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Our model shows that combining pre-trained word embeddings with auxiliary character-level embedding can improve the sentence representation. The enhanced sentence representation generated by multi-layer bidirectional LSTM will encapsulate the character and word levels informations. Furthermore, it may enhance matching features that generated by computing similarity measures on sentence pairs. Experimental results on benchmark datasets demonstrate that our new framework achieved the state-of-the-art performance compared with other deep neural networks based approaches.", "We explore treating each sentence as a kind of raw signal at character level, and applying temporal (one-dimensional) Convolution Neural Network (CNN) BIBREF6 , Highway Multilayer Perceptron (HMLP) and multi-layer bidirectional LSTM (Long Short Term Memory) BIBREF13 to learn sentence representations. We propose a new deep neural network architecture that jointly leverage pre-trained word embedding and character embedding to represent the meaning sentences. More specifically, our new approach first generates two kinds of word sequence representations. One kind of sequence representations are the composition of pre-trained word vectors. The other kind of sequence representation comprise word vectors that generating from character-level convolutional network. We then inject the two sequence representations into bidirectional LSTM, which means forward directional LSTM accept pre-trained word embedding output and backward directional LSTM accept auxiliary character CNN embedding output. The final sentence representation is the concatenation of the two direction. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations.", "In this work, we focus on deep neural network based sentence relation modeling tasks. We explore treating each sentence as a kind of raw signal at character level, and applying temporal (one-dimensional) Convolution Neural Network (CNN) BIBREF6 , Highway Multilayer Perceptron (HMLP) and multi-layer bidirectional LSTM (Long Short Term Memory) BIBREF13 to learn sentence representations. We propose a new deep neural network architecture that jointly leverage pre-trained word embedding and character embedding to represent the meaning sentences. More specifically, our new approach first generates two kinds of word sequence representations. One kind of sequence representations are the composition of pre-trained word vectors. The other kind of sequence representation comprise word vectors that generating from character-level convolutional network. We then inject the two sequence representations into bidirectional LSTM, which means forward directional LSTM accept pre-trained word embedding output and backward directional LSTM accept auxiliary character CNN embedding output. The final sentence representation is the concatenation of the two direction.", "In this work, we focus on deep neural network based sentence relation modeling tasks. We explore treating each sentence as a kind of raw signal at character level, and applying temporal (one-dimensional) Convolution Neural Network (CNN) BIBREF6 , Highway Multilayer Perceptron (HMLP) and multi-layer bidirectional LSTM (Long Short Term Memory) BIBREF13 to learn sentence representations. We propose a new deep neural network architecture that jointly leverage pre-trained word embedding and character embedding to represent the meaning sentences. More specifically, our new approach first generates two kinds of word sequence representations. One kind of sequence representations are the composition of pre-trained word vectors. The other kind of sequence representation comprise word vectors that generating from character-level convolutional network. We then inject the two sequence representations into bidirectional LSTM, which means forward directional LSTM accept pre-trained word embedding output and backward directional LSTM accept auxiliary character CNN embedding output.", "A key component of deep neural network is word embedding which serve as an lookup table to get word representations. From low level NLP tasks such as language modeling, POS tagging, name entity recognition, and semantic role labeling BIBREF6 , BIBREF7 , to high level tasks such as machine translation, information retrieval and semantic analysis BIBREF8 , BIBREF9 , BIBREF10 . Deep word representation learning has demonstrated its importance for these tasks. All the tasks get performance improvement via further learning either word level representations or sentence level representations. On the other hand, some researchers have found character-level convolutional networks BIBREF11 , BIBREF12 are useful in extracting information from raw signals for the task such as language modeling or text classification."]}
{"question_id": "11360385dff0a9d7b8f4b106ba2b7fe15ca90d7c", "predicted_answer": "The baseline is not explicitly mentioned in the given context. The text describes the primary system and its various subsystems, but it does not provide a baseline system or result for comparison.", "predicted_evidence": ["Our primary system is the linear fusion of all the above six subsystems by BOSARIS Toolkit on SRE19 dev and eval BIBREF9. Before the fusion, each score is calibrated by PAV method (pav_calibrate_scores) on our development database. It is evaluated by the primary metric provided by NIST SRE 2019.", "23-dimensional MFCC (20-3700Hz) is extracted as feature for etdnn/ams, ftdnn/as, eftdnn/ams, multitask and c-vector subsystems. 23-dimensional Fbank is used as feature for ResNet 16kHz subsystems. A simple energy-based VAD is used based on the C0 component of the MFCC feature BIBREF8.", "Extended ftdnn (eftdnn) is a combination of etdnn and ftdnn. Its architecture is listed in table TABREF10. The x-vector is extracted from layer 22 prior to the ReLU non-linearity.", "Only the first TDNN layer of the x-vector network is shared with the ASR network. The phonetic classification is done at the frame level, while the speaker labels are classified at the segment level.", "Multitask architecture is proposed in BIBREF6. It is a hybrid multi-task learning based on x-vector network and ASR network. It aims to introduce phonetic information by another neural acoustic model in ASR to help speaker recognition task. The architecture is shown in Fig. FIGREF13."]}
{"question_id": "875fbf4e5f93c3da63e28a233ce1d8405c7dfe63", "predicted_answer": "The datasets used in this challenge are:\n\n1. Phonetic dataset\n2. SRE (Speaker Recognition Evaluation) dataset\n3. Switchboard dataset\n4. Voxceleb dataset\n5. MUSAN (MUSAN corpus of US English speech) dataset (used for data augmentation)\n6. RIRS_NOISES dataset (used for data augmentation)", "predicted_evidence": ["For the sake of clarity, the datasets notations are defined as in table 1 and the training data for the six subsystems are list in table 2, 3, and 4.", "Multitask architecture is proposed in BIBREF6. It is a hybrid multi-task learning based on x-vector network and ASR network. It aims to introduce phonetic information by another neural acoustic model in ASR to help speaker recognition task. The architecture is shown in Fig. FIGREF13.", "Only the first TDNN layer of the x-vector network is shared with the ASR network. The phonetic classification is done at the frame level, while the speaker labels are classified at the segment level.", "To train the multitask network, we need training data with speaker and ASR transcribed. But only Phonetic dataset fits this condition and the data amount is too small to train a neural network. So, we need to train a GMM-HMM speech recognition system to do phonetic alignment for other datasets. The GMM-HMM is trained using Phonetic dataset with features of 20-dimensional MFCCs with delta and delta-delta, totally 60-dimensional. The total number of senones is 3800. After training, forced alignment is applied to the SRE, Switchboard, and Voxceleb datasets using a fMLLR-SAT system.", "For each neural network, its training data are augmented using the public accessible MUSAN and RIRS_NOISES as the noise source. Two-fold data augmentation is applied for etdnn/ams, ftdnn/as, resnet, multitask and cvector subsystems. For eftdnn/ams subsystem, five-fold data augmentation is applied."]}
{"question_id": "56b66d19dbc5e605788166e168f36d25f5beb774", "predicted_answer": "The question asks which subsystem outperformed the others, but the provided context does not explicitly state which subsystem performed the best. It describes the different subsystems, their architectures, and the experiments conducted, but it does not provide a direct comparison of their performance or declare a winner. Therefore, it is not possible to determine which subsystem outperformed the others based on the provided context.", "predicted_evidence": ["For each neural network, its training data are augmented using the public accessible MUSAN and RIRS_NOISES as the noise source. Two-fold data augmentation is applied for etdnn/ams, ftdnn/as, resnet, multitask and cvector subsystems. For eftdnn/ams subsystem, five-fold data augmentation is applied.", "Multitask architecture is proposed in BIBREF6. It is a hybrid multi-task learning based on x-vector network and ASR network. It aims to introduce phonetic information by another neural acoustic model in ASR to help speaker recognition task. The architecture is shown in Fig. FIGREF13.", "C-vector architecture is also one of our proposed systems in paper BIBREF7. As shown in Fig. FIGREF15, it is an extension of multitask architecture. It combines multitask architecture with an extra ASR Acoustic Model. The output of ASR Acoustic Model is concatenated with x-vector's frame-level output as the input of statistics pooling. Refer to BIBREF7 for more details.", "This paper describes the systems developed by the department of electronic engineering, institute of microelectronics of Tsinghua university and TsingMicro Co. Ltd. (THUEE) for the NIST 2019 speaker recognition evaluation (SRE) CTS challenge BIBREF0. Six subsystems, including etdnn/ams, ftdnn/as, eftdnn/ams, resnet, multitask and c-vector are developed in this evaluation. All the subsystems consists of a deep neural network followed by dimension deduction, score normalization and calibration. For each system, we begin with a summary of the data usage, followed by a description of the system setup along with their hyperparameters. Finally, we report experimental results obtained by each subsystem and fusion system on the SRE18 development and SRE18 evaluation datasets.", "For the sake of clarity, the datasets notations are defined as in table 1 and the training data for the six subsystems are list in table 2, 3, and 4."]}
{"question_id": "2d924e888a92dc0b14cdb5584e73e87254c3d1ee", "predicted_answer": "No, they do not reduce language variation of text by enhancing frequencies. Instead, they enhance frequencies to increase the contribution of related words or phrases to topics, which leads to more coherent and informative topics.", "predicted_evidence": ["In our approach we develop the idea of BIBREF16 that proposed to construct similarity sets between ngram phrases between each other and single words. Phrases and words are included in the same similarity set if they have the same component word, for example, weapon \u2013 nuclear weapon \u2013 weapon of mass destruction; discrimination \u2013 racial discrimination. It was supposed that if expressions from the same similarity set co-occur in the same document then their contribution into the document's topics is really more than it is presented with their frequencies, therefore their frequencies should be increased. In such an approach, the algorithm can \"see\" similarities between different multiword expressions with the same component word.", "Boyd-Graber et al. BIBREF4 indicate that to be understandable by humans, topics should be specific, coherent, and informative. Relationships between the topic components can be inferred. In BIBREF1 four topic visualization approaches are compared. The authors of the experiment concluded that manual topic labels include a considerable number of phrases; users prefer shorter labels with more general words and tend to incorporate phrases and more generic terminology when using more complex network graph. Blei and Lafferty BIBREF3 visualize topics with ngrams consisting of words mentioned in these topics. These works show that phrases and knowledge about hyponyms/hypernyms are important for topic representation.", "In this paper we describe an approach to integrate large manual lexical resources such as WordNet or EuroVoc into probabilistic topic models, as well as automatically extracted n-grams to improve coherence and informativeness of generated topics. The structure of the paper is as follows. In Section 2 we consider related works. Section 3 describes the proposed approach. Section 4 enumerates automatic quality measures used in experiments. Section 5 presents the results obtained on several text collections according to automatic measures. Section 6 describes the results of manual evaluation of combined topic models for Islam Internet-site thematic analysis.", "In this paper we presented the approach for introducing thesaurus information into topic models. The main idea of the approach is based on the assumption that if related words or phrases co-occur in the same text, their frequencies should be enhanced and this action leads to their mutual larger contribution into topics found in this text.", "It can be seen that if we add phrases without accounting component similarity (Runs 2, 3), the quality of topics decreases: the more phrases are added, the more the quality degrades. The human scores also confirm this fact. But if the similarity between phrase components is considered then the quality of topics significantly improves and becomes better than for unigram models (Runs 4, 5). All measures are better. Relational coherence between kernel elements also grows. The number of added phrases is not very essential."]}
{"question_id": "3ed8ac1ba4df6609fa7de5077d83e820641edc5e", "predicted_answer": "The domains they explore are related to Islam, specifically:\n\n1. Islam informational portal \"Golos Islama\" (Islam Voice) (in Russian), which contains news articles related to Islam and articles discussing Islam basics.\n\nIn other words, the domain is Islamic studies or Islamic information.", "predicted_evidence": ["For each combined model, we ran two experiments with 100 topics and with 200 topics. The generated topics were evaluated by two linguists, who had previously worked on the Islam thesaurus. The evaluation task was formulated as follows: the experts should read the top elements of the generated topics and try to formulate labels of these topics. The labels should be different for each topic in the set generated with a specific model. The experts should also assign scores to the topics' labels:", "We also considered thematic analysis of an Islam Internet site and evaluated the combined topic models manually. We found that the best, understandable topics are obtained by adding domain-specific thesaurus knowledge (domain terms, synonyms, and relations).", "In our approach, at first, we include related single words and phrases from a thesaurus such as WordNet or EuroVoc in these similarity sets. Then, we add preliminarily extracted ngrams into these sets and, this way, we use two different sources of external knowledge. We use the same LDA-SIM algorithm as described in BIBREF16 but study what types of semantic relations can be introduced into such similarity sets and be useful for improving topic models. The pseudocode of LDA-SIM algorithm is presented in Algorithm SECREF3 , where INLINEFORM0 is a similarity set, expressions in similarity sets can comprise single words, thesaurus phrases or generated noun compounds.", "We can compare this approach with the approaches applying the generalized Polya urn model BIBREF8 , BIBREF9 , BIBREF10 . To add prior knowledge, those approaches change topic distributions for related words globally in the collection. We modify topic probabilities for related words and phrases locally, in specific texts, only when related words (phrases) co-occur in these texts.", "To estimate the quality of topic models in a real task, we chose Islam informational portal \"Golos Islama\" (Islam Voice) (in Russian). This portal contains both news articles related to Islam and articles discussing Islam basics. We supposed that the thematic analysis of this specialized site can be significantly improved with domain-specific knowledge described in the thesaurus form. We extracted the site contents using Open Web Spider and obtained 26,839 pages."]}
{"question_id": "e1ab241059ef1700738f885f051d724a7fcf283a", "predicted_answer": "According to the text, the authors used the following thesauri:\n\n1. WordNet\n2. EuroVoc\n3. RuThes\n4. The Islam thesaurus (a domain-specific thesaurus containing more than 5 thousand Islam-related terms)", "predicted_evidence": ["In this paper we describe the approach to integrate whole manual thesauri into topic models together with multiword expressions.", "The results of the evaluation are shown in Table 5. The table contains the overall expert scores for a topic model (Score), kernel uniqueness as in the previous section (KernU), perplexity (Prpl). Also for each model kernels, we calculated the average number of known relations between topics\u2019s elements: thesaurus relations (synonyms and direct relations between concepts) and component-based relations between phrases (Relc).", "In our approach, at first, we include related single words and phrases from a thesaurus such as WordNet or EuroVoc in these similarity sets. Then, we add preliminarily extracted ngrams into these sets and, this way, we use two different sources of external knowledge. We use the same LDA-SIM algorithm as described in BIBREF16 but study what types of semantic relations can be introduced into such similarity sets and be useful for improving topic models. The pseudocode of LDA-SIM algorithm is presented in Algorithm SECREF3 , where INLINEFORM0 is a similarity set, expressions in similarity sets can comprise single words, thesaurus phrases or generated noun compounds.", "We also considered thematic analysis of an Islam Internet site and evaluated the combined topic models manually. We found that the best, understandable topics are obtained by adding domain-specific thesaurus knowledge (domain terms, synonyms, and relations).", "To combine knowledge with a topic model, we used RuThes thesaurus together with the additional block of the Islam thesaurus. The Islam thesaurus contains more than 5 thousand Islam-related terms including single words and expressions."]}
{"question_id": "a4b77a20e067789691e0ab246bc5b11913d77ae1", "predicted_answer": "According to the text, the definition of hate speech is speech that targets disadvantaged social groups in a manner that is potentially harmful to them. In many countries, hate speech is defined as speech that targets minority groups in a way that could promote violence or social disorder.", "predicted_evidence": ["What constitutes hate speech and when does it differ from offensive language? No formal definition exists but there is a consensus that it is speech that targets disadvantaged social groups in a manner that is potentially harmful to them BIBREF0 , BIBREF1 . In the United States, hate speech is protected under the free speech provisions of the First Amendment, but it has been extensively debated in the legal sphere and with regards to speech codes on college campuses. In many countries, including the United Kingdom, Canada, and France, there are laws prohibiting hate speech, which tends to be defined as speech that targets minority groups in a way that could promote violence or social disorder. People convicted of using hate speech can often face large fines and even imprisonment. These laws extend to the internet and social media, leading many sites to create their own provisions against hate speech. Both Facebook and Twitter have responded to criticism for not doing enough to prevent hate speech on their sites by instituting policies to prohibit the use of their platforms for attacks on people based on characteristics like race, ethnicity, gender, and sexual orientation, or threats of violence towards others.", "What constitutes hate speech and when does it differ from offensive language? No formal definition exists but there is a consensus that it is speech that targets disadvantaged social groups in a manner that is potentially harmful to them BIBREF0 , BIBREF1 . In the United States, hate speech is protected under the free speech provisions of the First Amendment, but it has been extensively debated in the legal sphere and with regards to speech codes on college campuses. In many countries, including the United Kingdom, Canada, and France, there are laws prohibiting hate speech, which tends to be defined as speech that targets minority groups in a way that could promote violence or social disorder. People convicted of using hate speech can often face large fines and even imprisonment. These laws extend to the internet and social media, leading many sites to create their own provisions against hate speech.", "If we conflate hate speech and offensive language then we erroneously consider many people to be hate speakers (errors in the lower triangle of Figure 1) and fail differentiate between commonplace offensive language and serious hate speech (errors in the upper triangle of Figure 1). Given the legal and moral implications of hate speech it is important that we are able to accurately distinguish between the two. Lexical methods are effective ways to identify potentially offensive terms but are inaccurate at identifying hate speech; only a small percentage of tweets flagged by the Hatebase lexicon were considered hate speech by human coders. While automated classification methods can achieve relatively high accuracy at differentiating between these different classes, close analysis of the results shows that the presence or absence of particular offensive or hateful terms can both help and hinder accurate classification.", "Our results also illustrate how hate speech can be used in different ways: it can be directly send to a person or group of people targeted, it can be espoused to nobody in particular, and it can be used in conversation between people. Future work should distinguish between these different uses and look more closely at the social contexts and conversations in which hate speech occurs. We must also study more closely the people who use hate speech, focusing both on their individual characteristics and motivations and on the social structures they are embedded in.", "Consistent with previous work, we find that certain terms are particularly useful for distinguishing between hate speech and offensive language. While f*g, b*tch, and n*gga are used in both hate speech and offensive language, the terms f*ggot and n*gger are generally associated with hate speech. Many of the tweets considered most hateful contain multiple racial and homophobic slurs. While this allows us to easily identify some of the more egregious instances of hate speech it means that we are more likely to misclassify hate speech if it doesn't contain any curse words or offensive terms. To more accurately classify such cases we should find sources of training data that are hateful without necessarily using particular keywords or offensive language."]}
{"question_id": "ba39317e918b4386765f88e8c8ae99f9a098c935", "predicted_answer": "They train a classification model, specifically a three-class classification model, to differentiate between hate speech, offensive language, and neither.", "predicted_evidence": ["Previous work on hate speech detection has identified this problem but many studies still tend to conflate hate speech and offensive language. In this paper we label tweets into three categories: hate speech, offensive language, or neither. We train a model to differentiate between these categories and then analyze the results in order to better understand how we can distinguish between them. Our results show that fine-grained labels can help in the task of hate speech detection and highlights some of the key challenges to accurate classification. We conclude that future work must better account for context and the heterogeneity in hate speech usage.", "We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no majority class.", "We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label.", "Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%.", "Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people."]}
{"question_id": "22c125c461f565f5437dac74bf19c2ef317bad86", "predicted_answer": "The dataset has 33,458 Twitter users.", "predicted_evidence": ["We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label.", "We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no majority class.", "Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people.", "Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%.", "We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used."]}
{"question_id": "4a91432abe3f54fcbdd00bb85dc0df95b16edf42", "predicted_answer": "The dataset consists of 25,000 tweets that were manually coded by CrowdFlower workers, which is a random sample from a larger corpus of 85.4 million tweets.", "predicted_evidence": ["We then test a variety of models that have been used in prior work: logistic regression, na\u00efve Bayes, decision trees, random forests, and linear SVMs. We tested each model using 5-fold cross validation, holding out 10% of the sample for evaluation to help prevent over-fitting. After using a grid-search to iterate over the models and parameters we find that the Logistic Regression and Linear SVM tended to perform significantly better than other models. We decided to use a logistic regression with L2 regularization for the final model as it more readily allows us to examine the predicted probabilities of class membership and has performed well in previous papers BIBREF5 , BIBREF8 . We trained the final model using the entire dataset and used it to predict the label for each tweet. We use a one-versus-rest framework where a separate classifier is trained for each class and the class label with the highest predicted probability across all classifiers is assigned to each tweet. All modeling was performing using scikit-learn BIBREF12 .", "We first use a logistic regression with L1 regularization to reduce the dimensionality of the data. We then test a variety of models that have been used in prior work: logistic regression, na\u00efve Bayes, decision trees, random forests, and linear SVMs. We tested each model using 5-fold cross validation, holding out 10% of the sample for evaluation to help prevent over-fitting. After using a grid-search to iterate over the models and parameters we find that the Logistic Regression and Linear SVM tended to perform significantly better than other models. We decided to use a logistic regression with L2 regularization for the final model as it more readily allows us to examine the predicted probabilities of class membership and has performed well in previous papers BIBREF5 , BIBREF8 . We trained the final model using the entire dataset and used it to predict the label for each tweet. We use a one-versus-rest framework where a separate classifier is trained for each class and the class label with the highest predicted probability across all classifiers is assigned to each tweet.", "We first use a logistic regression with L1 regularization to reduce the dimensionality of the data. We then test a variety of models that have been used in prior work: logistic regression, na\u00efve Bayes, decision trees, random forests, and linear SVMs. We tested each model using 5-fold cross validation, holding out 10% of the sample for evaluation to help prevent over-fitting. After using a grid-search to iterate over the models and parameters we find that the Logistic Regression and Linear SVM tended to perform significantly better than other models. We decided to use a logistic regression with L2 regularization for the final model as it more readily allows us to examine the predicted probabilities of class membership and has performed well in previous papers BIBREF5 , BIBREF8 . We trained the final model using the entire dataset and used it to predict the label for each tweet.", "Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%.", "Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people."]}
{"question_id": "7c398615141ca416a32c9f72dbb785d3a6986a0f", "predicted_answer": "Fine-tuning all layers hurts performance on the SST-2 sentiment classification dataset.", "predicted_evidence": ["The central objective of our paper is, then, to determine how many of the last layers actually need fine-tuning. Why is this an important subject of study? Pragmatically, a reasonable cutoff point saves computational memory across fine-tuning multiple tasks, which bolsters the effectiveness of existing parameter-saving methods BIBREF5. Pedagogically, understanding the relationship between the number of fine-tuned layers and the resulting model quality may guide future works in modeling.", "Our research contribution is a comprehensive evaluation, across multiple pretrained transformers and datasets, of the number of final layers needed for fine-tuning. We show that, on most tasks, we need to fine-tune only one fourth of the final layers to achieve within 10% parity with the full model. Surprisingly, on SST-2, a sentiment classification dataset, we find that not fine-tuning all of the layers leads to improved quality.", "When every component except the output layer and the task-specific layer is frozen, the fine-tuned model achieves only 64% of the original quality, on average. As more layers are fine-tuned, the model effectiveness often improves drastically\u2014see CoLA and STS-B, the first and fourth vertical pairs of subfigures from the left. This demonstrates that gains decompose nonadditively with respect to the number of frozen initial layers. Fine-tuning subsequent layers shows diminishing returns, with every model rapidly approaching the baseline quality at fine-tuning half of the network; hence, we believe that half is a reasonable cutoff point for characterizing the models.", "An emerging line of work questions the need for such a parameter-loaded model, especially on a single downstream task. BIBREF3, for example, note that only a few attention heads need to be retained in each layer for acceptable effectiveness. BIBREF4 find that, on many tasks, just the last few layers change the most after the fine-tuning process. We take these observations as evidence that only the last few layers necessarily need to be fine-tuned.", "In this paper, we present a comprehensive evaluation of the number of final layers that need to be fine-tuned for pretrained transformer-based language models. We find that only a fourth of the layers necessarily need to be fine-tuned to obtain 90% of the original quality. One line of future work is to conduct a similar, more fine-grained analysis on the contributions of the attention heads."]}
{"question_id": "441be93e2830cc0fc65afad6959db92754c9f5a8", "predicted_answer": "Yes, they do test against the large version of RoBERTa.", "predicted_evidence": ["We conduct our experiments on NVIDIA Tesla V100 GPUs with CUDA v10.1. We run the models from the Transformers library (v2.1.1; BIBREF15) using PyTorch v1.2.0.", "The central objective of our paper is, then, to determine how many of the last layers actually need fine-tuning. Why is this an important subject of study? Pragmatically, a reasonable cutoff point saves computational memory across fine-tuning multiple tasks, which bolsters the effectiveness of existing parameter-saving methods BIBREF5. Pedagogically, understanding the relationship between the number of fine-tuned layers and the resulting model quality may guide future works in modeling.", "Our fine-tuning procedure closely resembles those of BERT and RoBERTa. We choose the Adam optimizer BIBREF24 with a batch size of 16 and fine-tune BERT for 3 epochs and RoBERTa for 10, following the original papers. For hyperparameter tuning, the best learning rate is different for each task, and all of the original authors choose one between $1 \\times 10^{-5}$ and $5 \\times 10^{-5}$; thus, we perform line search over the interval with a step size of $1 \\times 10^{-5}$. We report the best results in Table TABREF5.", "Finally, for the large variants of BERT and RoBERTa on SST-2 (second subfigure from both the top and the left), we observe a surprisingly consistent increase in quality when freezing 12\u201316 layers. This finding suggests that these models may be overparameterized for SST-2.", "We choose BERT BIBREF0 and RoBERTa BIBREF2 as the subjects of our study, since they represent state of the art and the same architecture. XLNet BIBREF1 is another alternative; however, they use a slightly different attention structure, and our preliminary experiments encountered difficulties in reproducibility with the Transformers library. Each model has base and large variants that contain 12 and 24 layers, respectively. We denote them by appending the variant name as a subscript to the model name."]}
